{"Paper_Body":"Abstract The liver is the most common site of metastatic disease 1 . Although this metastatic tropism may reflect the mechanical trapping of circulating tumour cells, liver metastasis is also dependent, at least in part, on the formation of a \u2018pro-metastatic\u2019 niche that supports the spread of tumour cells to the liver 2 , 3 . The mechanisms that direct the formation of this niche are poorly understood. Here we show that hepatocytes coordinate myeloid cell accumulation and fibrosis within the liver and, in doing so, increase the susceptibility of the liver to metastatic seeding and outgrowth. During early pancreatic tumorigenesis in mice, hepatocytes show activation of signal transducer and activator of transcription 3 (STAT3) signalling and increased production of serum amyloid A1 and A2 (referred to collectively as SAA). Overexpression of SAA by hepatocytes also occurs in patients with pancreatic and colorectal cancers that have metastasized to the liver, and many patients with locally advanced and metastatic disease show increases in circulating SAA. Activation of STAT3 in hepatocytes and the subsequent production of SAA depend on the release of interleukin 6 (IL-6) into the circulation by non-malignant cells. Genetic ablation or blockade of components of IL-6\u2013STAT3\u2013SAA signalling prevents the establishment of a pro-metastatic niche and inhibits liver metastasis. Our data identify an intercellular network underpinned by hepatocytes that forms the basis of a pro-metastatic niche in the liver, and identify new therapeutic targets.     Main To understand the mechanisms that underlie the formation of a pro-metastatic niche in the liver, we used the LSL-Kras G12D \/+ ;LSL-Trp53 R127H \/+ ;Pdx1-cre (KPC) mouse model of pancreatic ductal adenocarcinoma (PDAC) 4 , 5 . We looked for features of a pro-metastatic niche in the livers of over-16-week-old tumour-bearing KPC mice and 8- to 10-week-old non-tumour-bearing (NTB) KPC control mice, which lack PDAC but harbour pancreatic intraepithelial neoplasia (PanIN) 6 . Compared to control mice, the livers of KPC mice contained increased numbers of myeloid cells, accompanied by an increase in the deposition and expression of fibronectin and type I collagen (COL1) (Fig. 1a , Extended Data Fig. 1a\u2013d ). Orthotopic implantation of KPC-derived PDAC cells into wild-type mice recapitulated these changes (Extended Data Fig. 1e\u2013i ). As shown previously 7 , 8 , matrix deposition did not require myeloid cells (Extended Data Fig. 1j\u2013l ). These results are consistent with evidence that myeloid cell accumulation and extracellular matrix deposition are key components of a pro-metastatic niche 7 , 8 , 9 , 10 . Fig. 1: Primary PDAC development induces a pro-metastatic niche in the liver. a , Images and quantification of myeloid cells, fibronectin (FN), and COL1 in the liver. Arrows indicate Ly6G + cells. Numbers in parentheses on plots indicate the number ( n ) of mice. Data pooled from two experiments. TB, tumour-bearing; NTB, non-tumour-bearing. b , Images of the liver and quantification of PDAC\u2013YFP cells. Control mice ( n = 14) and NTB KPC mice ( n = 10) were intrasplenically injected with PDAC\u2013YFP cells, and the liver was analysed after 10 days. Data representative of two independent experiments. c , Scatter plot of transcriptome data. FPKM, fragments per kilobase of exon per million mapped fragments ( n = 5 for both groups). Scale bars, 50 \u03bcm ( a ) and 1 cm ( b ). Statistical significance calculated using one-way analysis of variance (ANOVA) with Dunnett\u2019s test ( a ) and two-tailed Mann\u2013Whitney test ( b ). Data represented as mean \u00b1 s.d. Source data Full size image We next evaluated the susceptibility of the liver to metastatic colonization. Yellow fluorescent protein (YFP)-labelled KPC-derived PDAC cells (PDAC\u2013YFP) 6 were injected into control mice and KPC mice. The metastatic burden was threefold higher in KPC mice, and metastatic lesions were detected in the livers of KPC mice at increased frequency and size with enhanced proliferation (shown using Ki-67) (Fig. 1b , Extended Data Fig. 2a, b ). Similar findings were observed using a YFP-negative KPC-derived cell line (Extended Data Fig. 2c, d ). Orthotopic implantation of PDAC cells also increased the susceptibility of the liver to metastatic colonization, and this finding was independent of the presence of CD4 + and CD8 + T cells (Extended Data Fig. 2e\u2013s ). We next performed mRNA sequencing on RNA isolated from the livers of control and KPC mice. We identified 275 differentially expressed genes (Extended Data Fig. 3a, b , Supplementary Data 1 ) and found that genes upregulated in KPC mice were associated with immune-related processes (Extended Data Fig. 3c ). Notably, genes encoding myeloid chemoattractants, including SAA and members of the S100 family, were upregulated in KPC mice 11 , 12 , 13 (Fig. 1c , Extended Data Fig. 3d, e ). We also found enrichment of immune-related pathways, particularly the IL-6\u2013JAK\u2013STAT3 signalling pathway (Extended Data Fig. 3f , Supplementary Table 1 ). We validated our results by examining the livers of KPC mice for the presence of phosphorylated STAT3 (pSTAT3). STAT3 was activated in 80\u201390% of hepatocytes from KPC mice, compared to less than 2% of hepatocytes in control mice (Extended Data Fig. 3g, h ). By contrast, we did not detect activation of STAT1 signalling (Extended Data Fig. 3i ). Orthotopic implantation of PDAC cells also induced phosphorylation of STAT3 in hepatocytes (Extended Data Fig. 3j, k ). As IL-6 is fundamental to STAT3 signalling in hepatocytes 14 , we examined the livers of control mice ( Il6 +\/+ ) and IL-6 knockout mice ( Il6 \u2212\/\u2212 ) orthotopically injected with PBS or PDAC cells. Tumour-implanted Il6 \u2212\/\u2212 mice displayed a decrease in STAT3 activation, particularly in hepatocytes (Fig. 2a , Extended Data Fig. 4a ). This loss in STAT3 activation was accompanied by reductions in myeloid cell accumulation and extracellular matrix deposition without alterations in the morphology and density of liver sinusoids (Fig. 2a and Extended Data Fig. 4a-d ). We also observed reduced expression of SAA, other chemoattractants, and extracellular matrix proteins (Fig. 2b , Extended Data Fig. 4e ). Genetic ablation of Il6 , however, did not alter proliferation, vascular density, or primary tumour growth (Extended Data Fig. 4f, g ). Il6 \u2212\/\u2212 mice were also less susceptible than control mice to metastatic colonization, and blockade of the IL-6 receptor (IL-6R) similarly inhibited the formation of a pro-metastatic niche in the liver (Fig. 2c\u2013e , Extended Data Fig. 4h\u2013s ). Notably, genetic ablation of Il6 or blockade of IL-6R did not completely inhibit STAT3 signalling, suggesting that IL-6-independent mechanisms may contribute to STAT3 activation. Fig. 2: IL-6 is necessary for the establishment of a pro-metastatic niche in the liver. a , b , n = 5 and 6 for Il6 +\/+ mice and n = 4 and 5 for Il6 \u2212\/\u2212 mice orthotopically injected with PBS or PDAC cells, respectively. a , Quantification of pSTAT3 + cells, myeloid cells, and fibronectin. b , mRNA levels of Saa1 and Saa2 in the liver. c \u2013 e , n = 4 and 5 for Il6 +\/+ mice and n = 4 for Il6 \u2212\/\u2212 mice orthotopically injected with PBS or PDAC cells, respectively. All groups were intraportally injected with PDAC\u2013YFP cells on day 10. c , d , Images of liver and flow cytometric analysis. e , Quantification of PDAC\u2013YFP cells. Data representative of two independent experiments ( a \u2013 e ). Scale bars, 1 cm. Statistical significance calculated using one-way ANOVA with Dunnett\u2019s test. Data represented as mean \u00b1 s.d. Source data Full size image IL-6 promotes the development and progression of PDAC 15 , 16 , 17 , 18 . To identify the source of IL-6, we orthotopically injected PBS or PDAC cells into Il6 +\/+ and Il6 \u2212\/\u2212 mice and measured the concentration of IL-6 at distinct anatomic sites (Extended Data Fig. 5a ). We detected IL-6 only in tumour-implanted Il6 +\/+ mice, with the highest concentration of IL-6 found in the primary tumour (Extended Data Fig. 5b, c ). Although Il6 mRNA was undetectable in the liver, lung, and malignant cells, we observed Il6 mRNA in host cells adjacent to CK19-expressing PDAC cells (Extended Data Fig. 5d\u2013g ). Human primary tumours displayed a similar expression pattern (Extended Data Fig. 5h ). Moreover, Il6 mRNA was detected in \u03b1-SMA + stromal cells located adjacent to PanIN and PDAC cells in KPC mice (Extended Data Fig. 5i\u2013k ). We also found that primary pancreatic tumour supernatant activated STAT3 signalling in hepatocytes, and this was reduced in the presence of anti-IL-6R antibodies (Extended Data Fig. 6a, b ). These results show that IL-6 released by non-malignant cells within the primary tumour is a key mediator of STAT3 signalling in hepatocytes. To study a role for hepatocytes in directing liver metastasis, we generated mice that lacked Stat3 in hepatocytes ( Stat3 flox\/flox Alb-cre ). Compared to control mice ( Stat3 flox\/flox ), tumour-implanted Stat3 flox\/flox Alb-cre mice lacked features of a pro-metastatic niche (Fig. 3a\u2013c , Extended Data Fig. 6c ) and failed to produce SAA (Fig. 3d\u2013f ). However, deletion of Stat3 in hepatocytes did not affect liver sinusoid density or morphology and did not alter the size, proliferation, or vascular density of the primary tumour (Extended Data Fig. 6d\u2013f ). The livers of tumour-implanted Stat3 flox\/flox Alb-cre mice were also less susceptible to metastatic colonization (Extended Data Fig. 6g\u2013l ). In addition to its expression in hepatocytes (Extended Data Fig. 6m ), mRNA for SAA was detected in colonic cells 19 and in cells present in the periphery of the primary tumour (Extended Data Fig. 6n ). However, both cell types maintained comparable levels of SAA mRNA despite deletion of Stat3 in hepatocytes. Fig. 3: STAT3 signalling in hepatocytes orchestrates the formation of a pro-metastatic niche in the liver. a , Study design for b \u2013 f ( n = 4 for Stat3 flox\/flox mice injected with PBS or PDAC cells; n = 8 and 7 for Stat3 flox\/flox Alb-cre mice injected with PBS and PDAC cells, respectively). b , c , Quantification of pSTAT3 + cells, myeloid cells, and fibronectin. d , mRNA levels of Saa1 and Saa2 in the liver. e , Images of Saa1 and Saa2 mRNA in liver cells. Dashed lines and asterisks indicate sinusoids and hepatocytes, respectively. f , Concentration of circulating SAA. Data representative of two independent experiments ( a \u2013 f ). Scale bars, 50 \u03bcm. Statistical significance calculated using one-way ANOVA with Dunnett\u2019s test. Data represented as mean \u00b1 s.d. Source data Full size image SAA proteins are acute phase reactants 20 . Consistent with elevated levels of circulating SAA in tumour-implanted mice (Fig. 3f ), patients with PDAC displayed elevated levels of circulating SAA (Extended Data Fig. 7a ). Overexpression of SAA and pSTAT3 by hepatocytes was also observed in five of seven patients with liver metastases (Fig. 4a , Extended Data Fig. 7b ). Notably, high levels of circulating SAA correlated with worse outcomes (Extended Data Fig. 7c ). Elevated levels of circulating SAA were also observed in patients with non-small-cell lung carcinoma (NSCLC) with liver metastases, and overexpression of SAA by hepatocytes was detected in the livers of patients with colorectal carcinoma (CRC) (Extended Data Fig. 7d, e ). In addition, compared to tumour-implanted control mice ( Saa +\/+ ), double-knockout Saa1 \u2212\/\u2212 Saa2 \u2212\/\u2212 mice (hereafter referred to as Saa \u2212\/\u2212 mice) implanted with PDAC or MC-38 CRC cells failed to show features of a pro-metastatic niche in the liver, though genetic ablation of Saa1 and Saa2 had no effect on primary tumour growth (Fig. 4b\u2013e , Extended Data Fig. 7f\u2013s ). SAA was also necessary for IL-6-mediated formation of a pro-metastatic niche and for fibrosis and myeloid cell recruitment in the setting of liver injury (Extended Data Fig. 8 ). Fig. 4: SAA is a critical determinant of liver metastasis. a , Images of SAA in the livers of healthy donors (top) and patients with PDAC with liver metastases (bottom). Dashed lines and asterisks indicate sinusoids and hepatocytes, respectively. Data representative of one experiment. b , Quantification of pSTAT3 + cells, myeloid cells, and fibronectin ( n = 5 for all groups orthotopically injected with PBS or PDAC cells). For c \u2013 e , n = 4 and 5 for Saa +\/+ mice and n = 5 and 6 for Saa \u2212\/\u2212 mice orthotopically injected with PBS and PDAC cells, respectively. All groups were intraportally injected with PDAC\u2013YFP cells on day 10. c , d , Images of liver and flow cytometric analysis. e , Quantification of PDAC\u2013YFP cells. Data representative of two independent experiments ( b \u2013 e ). Scale bars, 50 \u03bcm ( a ) and 1 cm ( c ). Statistical significance calculated using one-way ANOVA with Dunnett\u2019s test. Data represented as mean \u00b1 s.d. Source data Full size image Tissue inhibitor of metalloproteinases 1 (TIMP1) 7 , 8 and macrophage migration inhibitory factor (MIF) 9 , 10 have been implicated in the promotion of metastasis. However, expression of these molecules was not affected by IL-6\u2013STAT3\u2013SAA signalling (Extended Data Fig. 9 ). We next determined whether formation of a pro-metastatic niche in the liver is dependent on the anatomical proximity of the pancreas to the liver. To this end, we looked for features of a pro-metastatic niche in the livers of CD45.1 and CD45.2 mice that were parabiotically joined (Extended Data Fig. 10a ). Although only CD45.2 mice were implanted with PDAC cells, both mice displayed myeloid cell accumulation and fibrosis in the liver (Extended Data Fig. 10b\u2013g ), suggesting that formation of this niche is not dependent on the anatomical distance between the tumour and the liver. We also investigated whether SAA has a role in establishing a pro-metastatic niche in the lung. Development of PDAC in KPC mice induced accumulation of Ly6G + myeloid cells and deposition of fibronectin within the lung, but IL-6\u2013STAT3\u2013SAA signalling was not required for the formation of a pro-metastatic niche in the lung (Extended Data Fig. 10h\u2013o ). Our data provide insight into the mechanisms that direct liver metastasis. Although recent studies have suggested a role for tumour-intrinsic factors in driving metastatic spread of cancer 7 , 8 , 9 , 10 , 21 , 22 , 23 , we provide evidence that inflammatory responses mounted by hepatocytes are critical to liver metastasis. Mechanistically, hepatocytes orchestrate this process through activation of IL-6\u2013STAT3 signalling and the subsequent production of SAA, which alters the immune and fibrotic microenvironment of the liver to establish a pro-metastatic niche (Extended Data Fig. 10p ). Our findings suggest that therapies that target hepatocytes might prevent liver metastasis in patients with cancer. Methods Mice CD45.2 (wild type, C57BL\/6J), CD45.1 (B6.SJL- Ptprc a Pepc b \/BoyJ), Il6 knockout ( Il6 \u2212\/\u2212 , B6.129S2- Il6 tm1Kopf \/J ) , Stat3 flox \/ flox (B6.129S1- Stat3 tm1Xyfu \/J), and Alb - cre +\/+ (B6.Cg-Tg(Alb-cre)21Mgn\/J) mice were obtained from the Jackson Laboratory. Stat3 flox\/flox mice were bred to Alb - cre +\/+ mice to generate Stat3 flox \/+ Alb-cre +\/\u2212 mice, which were backcrossed onto Stat3 flox\/flox mice to generate Stat3 flox\/flox Alb-cre +\/\u2212 mice. These mice were then bred to each other to create Stat3 flox\/flox Alb-cre +\/+ and Stat3 flox\/flox Albumin-cre +\/\u2212 mice ( Stat3 flox\/flox Alb-cre ), and Stat3 flox\/flox Albumin-cre \u2212\/\u2212 mice ( Stat3 flox\/flox ). Kras LSL-G12D \/+ Trp53 LSL-R172H \/+ Pdx1-cre (KPC) mice and Trp53 LSL-R172H \/+ Pdx1-cre (PC) mice were as previously described 4 , 5 . Saa1 and Saa2 double-knockout ( Saa \u2212\/\u2212 ) mice were as previously described 24 and provided by the University of Kentucky College of Medicine. Saa \u2212\/\u2212 mice used for experiments had been bred to obtain a 99.9% C57BL\/6 background using the Jackson Laboratory Speed Congenic Service 24 . All transgenic mice were bred and maintained in the animal facility of the University of Pennsylvania. Animal protocols were reviewed and approved by the Institute of Animal Care and Use Committee of the University of Pennsylvania. In general, mice were monitored three times per week for general health and euthanized early based on defined endpoint criteria including tumour diameter \u22651 cm, ascites, lethargy, loss of \u226510% body weight, or other signs of sickness or distress. Clinical samples All patient samples were obtained after written informed consent and were de-identified. Studies were conducted in accordance with the 1996 Declaration of Helsinki and approved by institutional review boards of the University of Pennsylvania and the Mayo Clinic. To obtain plasma from healthy donors, patients with PDAC patients, and patients with NSCLC, peripheral whole blood was drawn in EDTA tubes (Fisher Scientific). Within 3 h of collection, blood samples were centrifuged at 1,600 g at room temperature for 10 min with the brake off. Next, the plasma was transferred to a 15-ml conical tube without disturbing the cellular layer and centrifuged at 3,000 g at room temperature for 10 min with the brake off. This step was repeated with a fresh 15-ml conical tube. The plasma was then stored at \u201380 \u00b0C until analysis. Biopsy results, computed tomography, and\/or magnetic resonance imaging records were used to determine sites of metastasis in patients with PDAC or NSCLC whose plasma samples were used for assessment of SAA levels. Liver specimens from healthy donors were obtained by percutaneous liver biopsy, and acquisition of liver specimens from patients with liver metastases was as previously described 25 . Liver specimens from patients with CRC with liver metastases were obtained from the Cooperative Human Tissue Network (CHTN). Patient characteristics are shown in Supplementary Table 2 . Cell lines PDA.69 cell line (PDAC cells) was used for intrasplenic and orthotopic injection, and PDA.8572 cell line (PDAC\u2013YFP cells) was used for intrasplenic, intraportal, and retro-orbital injections. These cell lines were derived from PDAC tumours that arose spontaneously in KPC mice, as previously described 4 , 26 . The MC-38 cell line, which was used for orthotopic implantation, was purchased from Kerafast. Cell lines were cultured in DMEM (Corning) supplemented with 10% fetal bovine serum (FBS, VWR), 83 \u03bcg\/ml gentamicin (Thermo Fisher), and 1% GlutaMAX (Thermo Fisher) at 37 \u00b0C, 5% CO 2 . Only cell lines that had been passaged fewer than 10 times were used for experiments, and trypan blue staining was used to ensure that cells with >95% viability were used for studies. Cell lines were tested routinely for Mycoplasma contamination at the Cell Center Services Facility at the University of Pennsylvania. All cell lines used in our studies tested negative for Mycoplasma contamination. Animal experiments For all animal studies, mice of similar age and gender were block randomized in an unblinded fashion. Male and female mice aged between 8 to 12 weeks were used unless indicated otherwise. Mice were age- and gender-matched with appropriate control mice for analysis. Sample sizes were estimated based on pilot experiments and were selected to provide sufficient numbers of mice in each group for statistical analysis. For orthotopic and intrasplenic injections of pancreatic tumour cells, mice were anaesthetized using continuous isoflurane, and their abdomen was sterilized. After administering analgesic agents and assessing the depth of anaesthesia, we performed a laparotomy (5\u201310 mm) over the left upper quadrant of the abdomen to expose the peritoneal cavity. For orthotopic injection, the pancreas was exteriorized onto a sterile field, and sterile PBS or pancreatic tumour cells (5 \u00d7 10 5 cells suspended in 50 \u03bcl of sterile PBS) were injected into the tail of the pancreas via a 30-gauge needle (Covidien). Successful injection was confirmed by the formation of a liquid bleb at the site of injection with minimal fluid leakage. The pancreas was then gently placed back into the peritoneal cavity. For intrasplenic injection, 150 \u03bcl sterile PBS was drawn into a syringe and then sterile PBS or pancreatic tumour cells (5 \u00d7 10 5 cells suspended in 100 \u03bcl sterile PBS) was gently drawn into the same syringe in an upright position as previously described 27 . After the spleen was exteriorized onto a sterile field, pancreatic tumour cells were injected into the spleen via a 30-gauge needle. Successful injection was confirmed by whitening of the spleen and splenic blood vessels with minimal leakage of content into the peritoneum. Splenectomy was then performed by ligating splenic vessels with clips (Horizon) then cauterizing them to ensure that there was no haemorrhage. Afterwards, the remaining blood vessels were placed back into the peritoneal cavity. For both procedures, the peritoneum was closed with a 5-0 PDS II violet suture (Ethicon), and the skin was closed using the AutoClip system (Braintree Scientific). Following surgery, mice were given buprenorphine subcutaneously at a dose of 0.05-0.1 mg\/kg every 4\u20136 h for 12 h and then every 6\u20138 h for 3 additional days. Mice that were orthotopically injected with pancreatic tumour cells were analysed after 20 days, unless indicated otherwise in study designs. Mice that were intrasplenically injected with PDAC cells were analysed after 10 days. For intraportal injection of pancreatic tumour cells and hydrodynamic injection of expression vectors, mice were anaesthetized using continuous isoflurane, and their abdomen was sterilized. After administration of analgesic agents, median laparotomy (10 mm) was performed, and the incision site was held open using an Agricola retractor (Roboz). After exposure of the peritoneal cavity, the intestines were located and exteriorized onto a sterile field surrounding the incision site to visualize the portal vein. Throughout the procedure, the intestines were kept hydrated with sterile PBS that was pre-warmed to 37 \u00b0C. For intraportal injection, sterile PBS or pancreatic tumour cells (5 \u00d7 10 5 cells suspended in 100 \u03bcl sterile PBS) were injected into the portal vein via a 30-gauge needle. Successful injection was confirmed by partial blanching of the liver. For hydrodynamic injection, 1 \u03bcg of pLIVE expression vectors was suspended in sterile saline corresponding to 8% of mouse body weight as previously described 28 . Vectors were injected into the portal vein via a 27-gauge needle within 5\u20138 s. Successful injection was confirmed by complete blanching and swelling of the liver. For both procedures, a sterile gauge was then held over the injection site for 1 min to ensure that no injected contents would leak into the peritoneal cavity. Afterwards, the intestines were placed back into the peritoneal cavity, and the peritoneum and skin were closed with a suture and autoclips, respectively. Following surgery, mice were given buprenorphine subcutaneously as described above. Intraportal injection of pancreatic tumour cells was performed on day 10, and metastatic burden in the liver was evaluated on day 20, unless indicated otherwise in study designs. For orthotopic implantation of colorectal tumour cells, wild-type mice were first subcutaneously injected with MC-38 (1 \u00d7 10 6 cells suspended in 100 \u03bcl of sterile PBS) into the right flank. After 10 days, mice were euthanized, and subcutaneous tumours were collected. Tumours were then cut into small pieces, each 3 \u00d7 3 mm in size, and placed in sterile PBS on ice until implantation. Mice were anaesthetized using isoflurane, and their abdomen was sterilized. Following administration of analgesic agents, median laparotomy was performed as described above. Implantation of colorectal tumour tissues into the caecum was then performed as previously described 29 . After we placed the intestines back into the peritoneal cavity, the peritoneum and skin were closed with a suture, and mice were given buprenorphine as described above. Mice were analysed after 10 days. For parabiotic joining of mice, female CD45.2 mice were orthotopically injected with sterile PBS or pancreatic tumour cells as described above and co-housed with age-matched female B6 CD45.1 mice. Each parabiotic pair was housed in a separate cage to maximize bonding between partners. After one week, parabiotic partners were anaesthetized using continuous isoflurane, and their flanks were sterilized. After administration of analgesic agents, longitudinal skin flaps from the lower limb to the upper limb were created, and everted skin flaps were sewn using a suture. In addition, the knees and olecranons of parabiotic partners were joined together using a suture for additional stabilization. Following surgery, mice were given buprenorphine subcutaneously at a dose of 0.05-0.1 mg\/kg every 4-6 h for 5 days. Parabiotically joined mice were analysed after 20 days. For administration of antibodies, the abdomen of mice was sterilized, and anti-CD4 antibodies (GK1.5, 0.2 mg), anti-CD8 antibodies (2.43, 0.2 mg), anti-IL-6R antibodies (15A7, 0.2 mg), or rat isotype control antibodies (LTF-2, 0.2 mg) were suspended in 100 \u03bcl sterile PBS. Antibodies were subsequently injected into the peritoneum via a 30-gauge needle. All antibodies used in in vivo experiments were obtained from BioXCell. To deplete F4\/80 + myeloid cells, clodronate-encapsulated liposomes (Liposoma) were administered by intraperitoneal injection according to the manufacturer\u2019s protocol. For induction of liver injury, mice were intraperitoneally injected with CCl 4 (Sigma, 1 ml\/kg body weight) dissolved in sunflower seed oil as previously described 30 . Detailed information on antibodies and reagents used in experiments can be in found in Supplementary Table 3 . Microscopic analysis For preparation of formalin-fixed paraffin-embedded (FFPE) sections, dissected tissues were fixed in 10% formalin for 24 h at room temperature, washed twice with PBS, and then stored in 70% ethanol solution at 4 \u00b0C until they were embedded in paraffin and sectioned at 5 \u03bcm. For preparation of cryosections, dissected tissues were embedded in Tissue-tek O.C.T. (Electron Microscopy Sciences) and frozen on dry ice. Frozen tissues were stored at \u201380 \u00b0C until they were sectioned at 7 \u03bcm. Automated immunohistochemistry, immunofluorescence, and RNA in situ hybridization were performed on FFPE sections using a Ventana Discovery Ultra automated slide staining system (Roche). Reagents were obtained from Roche and ACDBio (Supplementary Table 3 ) and used according to manufacturer\u2019s protocol. Images were acquired using a BX43 upright microscope (Olympus), an Aperio CS2 scanner system (Leica), or an IX83 inverted multicolour fluorescent microscope (Olympus). Manual immunohistochemistry of mouse tissues for SAA was previously described 31 . For manual multicoloured immunofluorescence staining, O.C.T. liver cryosections were briefly air dried and fixed with 3% formaldehyde at room temperature for 15 min. For intracellular staining, sections were permeabilized with methanol at \u201320 \u00b0C for 10 min immediately after formaldehyde fixation. Sections were then blocked with 10% normal goat serum in PBS containing 0.1% TWEEN 20 for 30 min. For intracellular staining, 0.3% Triton X-100 was added to the blocking solution for permeabilization of cellular and nuclear membranes. Sections were incubated with primary antibodies (Supplementary Table 3 ) in the blocking solution for 1 h at room temperature or overnight at 4 \u00b0C, followed by washing with PBS containing 0.1% TWEEN 20. Sections were then incubated with secondary antibodies (Supplementary Table 3 ) in the blocking solution for 1 h at room temperature or overnight at 4 \u00b0C. After washing, sections were stained with DAPI to visualize nuclei and subsequently with Sudan Black B in 70% ethanol to reduce autofluorescence, as previously described 32 . Immunofluorescence imaging was performed on an IX83 inverted multicolour fluorescent microscope (Olympus). For quantification of cells and extracellular matrix proteins, five random fields were acquired from each biological sample. Flow cytometry Mice were euthanized, and the liver and lung were removed after the blood was drained by severing the portal vein and inferior vena cava. The liver and lung were rinsed thoroughly in PBS before mincing with micro-dissecting scissors into small pieces (<0.5 \u00d7 0.5 mm in size) at 4 \u00b0C in DMEM containing collagenase (1 mg\/ml, Sigma-Aldrich), DNase (150 U\/ml, Roche), and Dispase (1 U\/ml, Worthington). Tissues were then incubated at 37 \u00b0C for 30 min with intermittent agitation, filtered through a 70-\u03bcm nylon strainer (Corning), and washed three times with DMEM. Cells were resuspended in ACK lysing buffer (Life Technologies) at room temperature for 15 min to remove red blood cells. After washing three times with DMEM, cells were counted and stained using Aqua dead cell stain kit (Life Technologies) following the manufacturer\u2019s protocol. For characterization of immune cell subsets, cells were washed three times with PBS containing 0.2 mM EDTA with 2% FBS and stained with appropriate antibodies (Supplementary Table 3 ). For quantification of PDAC\u2013YFP cells, cells were not stained with any antibodies. Lastly, cells were washed three times with PBS containing 0.2 mM EDTA with 2% FBS and examined using a FACS Canto II (BD Biosciences). Collection and analysis of the peripheral blood was as previously described 26 . FlowJo (FlowJo, LLC, version 10.2) was used to analyse flow cytometric data and generate 2D t -SNE plots. Detection of IL-6, SAA, and TIMP1 Mice that were orthotopically implanted with PDAC cells were euthanized, and primary tumours were removed and weighed. In addition, blood samples were collected from the portal vein and left ventricle of the heart using a 27-gauge needle. Tumours were rinsed thoroughly in PBS and minced with micro-dissecting scissors into small pieces (<0.5 \u00d7 0.5 mm in size) at 4 \u00b0C in serum-free DMEM at 1 mg of tissue per 1 \u03bcl medium. Tumour suspensions were then centrifuged at 12,470 g at 4 \u00b0C for 15 min, and tumour supernatant was collected and stored at \u201380 \u00b0C until analysis. A similar procedure was performed to obtain pancreas supernatant from mice that were orthotopically injected with PBS. To collect the serum, blood samples were allowed to clot at room temperature for 30 min. Samples were then centrifuged at 12,470 g at 4 \u00b0C for 15 min, and the serum was collected and stored at \u201380 \u00b0C until analysis. IL-6 levels in tumour or pancreas supernatant and serum were assessed using a cytometric bead array (BD Biosciences) following the manufacturer\u2019s protocol. Samples were examined using a FACS Canto II (BD Biosciences), and data were analysed using FCAP Array (BD Biosciences, version 3.0). SAA and TIMP1 levels in mouse serum samples were measured using a commercially available enzyme-linked immunosorbent assay kit (Thermo Fisher) following the manufacturer\u2019s protocol. Similarly, SAA levels in plasma samples collected from healthy donors and patients with PDAC as described under \u2018Clinical samples\u2019 were measured using a commercially available human enzyme-linked immunosorbent assay kit (Thermo Fisher) following the manufacturer\u2019s protocol. RNA and quantitative PCR Mouse organs and cells were stored in TRIzol (Thermo Fisher) at \u201380 \u00b0C until analysis. Samples were thawed on ice and allowed to equilibrate to room temperature before RNA was isolated using a RNeasy Mini kit (Qiagen) following the manufacturer\u2019s protocol. cDNA synthesis was performed as previously described 33 . Primers for quantitative PCR were designed using the Primer3 online program 34 , and sequences were analysed using the Nucleotide BLAST (NCBI) to minimize non-specific binding of primers. Primers were synthesized by Integrated DNA Technologies, and their sequences can be found in Supplementary Table 4 . Quantitative PCR was performed as previously described 33 . Gene expression was calculated relative to Actb (\u03b2-actin) using the \u2206 C t formula, and fold change in gene expression was calculated relative to the average gene expression of control groups using the \u2206\u2206 C t formula. Genes with C t greater than or equal to 30 were considered not detected. QuantSeq 3\u2032 mRNA sequencing and data analysis RNA was isolated from the livers of control mice and NTB KPC mice as described above and submitted to the Genomics Facility at the Wistar Institute. After the quality of RNA was assessed using a 2100 Bioanalyzer (Agilent), samples were prepared using a QuantSeq 3\u2032 mRNA-Seq library prep kit FWD for Illumina (Lexogen) following the manufacturer\u2019s protocol and analysed on a NextSeq 500 sequencing system (Illumina). FASTQ files were uploaded to the BaseSpace Suite (Illumina) and aligned using its RNA-Seq Alignment application (version 1.0.0), in which STAR was selected to align sequences with maximum mismatches set to 14 as recommended by Lexogen. Output files were analysed using Cufflinks Assembly & DE application (version 2.1.0) in the BaseSpace Suite to determine differentially expressed genes, which were used to generate an expression heatmap and a FPKM scatter plot. In addition, these genes were analysed using ClueGO (version 2.3.3) 35 and CluePedia (version 1.3.3) 36 , which are applications of Cytoscape software (version 3.5.1) 37 . Functional grouping of biological processes was performed on the basis of kappa score. Gene Ontology data 38 , 39 downloaded on 23 January 2018 were used for analysis. Gene set enrichment analysis (version 3.0) 40 was used to determine biological processes that were differentially enriched in experimental groups. In vitro studies To isolate primary hepatocytes for in vitro studies, mice were anaesthetized using continuous isoflurane, and their abdomen was sterilized. After administering analgesic agents and assessing the depth of anaesthesia, we performed a laparotomy (10\u201315 mm) along the midline of the abdomen to expose the peritoneal cavity. The intestines were then located and exteriorized to visualize the inferior vena cava and portal vein. The inferior vena cava was cannulated via a 24 gauge Insyte Autoguard cathether (BD), and the liver was perfused using 50 ml liver perfusion medium (Thermo Fisher) at a flow rate of 8\u20139 ml\/min using a peristaltic pump. At the start of perfusion, the portal vein was severed to drain the blood from the liver. Successful perfusion was confirmed by blanching of the liver, which was subsequently perfused using 50 ml liver digest medium (Thermo Fisher) at the same flow rate. Both liver perfusion medium and liver digest medium were pre-warmed to 42 \u00b0C in a water bath. After perfusion, the liver was carefully transferred to a Petri dish containing William\u2019s E medium (Sigma) supplemented with 10% FBS, 83 \u03bcg\/ml gentamicin, and 1% GlutaMAX. To dissociate hepatocytes from the liver, cell scrapers were used to create small cuts (5 mm) on the surface of the liver, and the tissue was gently shaken. Dissociated cells were then filtered through a 100-\u03bcm nylon strainer (Corning) and centrifuged at 50 g at 4 \u00b0C for 5 min. After the supernatant was discarded, cells were resuspended in a solution consisting of isotonic Percoll (Sigma) and supplemented William\u2019s E medium (2:3 ratio). Cells were then centrifuged at 50 g at 4 \u00b0C for 10 min to obtain a pellet enriched in hepatocytes. The supernatant was discarded, and hepatocytes were resuspended in supplemented William\u2019s E medium. Cell viability and number were determined using trypan blue staining, and 5 \u00d7 10 4 hepatocytes were seeded in each well of a 48-well plate pre-coated with collagen. Hepatocytes were incubated in supplemented William\u2019s E medium for 4 h at 37 \u00b0C, 5% CO 2 to allow attachment to the plate. The medium was then switched to HepatoZYME-SFM (Thermo Fisher) supplemented with 83 \u03bcg\/ml gentamicin and 1% GlutaMAX. Medium was replenished every 24 h for the next 48\u201372 h. For hepatocyte activation assays, hepatocytes were incubated in supplemented HepatoZYME-SFM mixed with (i) serum-free DMEM, (ii) primary pancreatic tumour supernatant, or (iii) serum-free DMEM containing 250 ng\/ml IL-6 (Peprotech) for 30 min at 37 \u00b0C, 5% CO 2 . All mixtures were made in a 1:1 ratio, and each condition was run in triplicate. For the in vitro IL-6R blockade experiment, hepatocytes were pre-incubated with 5 \u03bcg\/ml anti-IL-6R antibodies for 2 h before being stimulated with tumour supernatant. After stimulation, medium was carefully removed, and formaldehyde and methanol were used to fix and permeabilize hepatocytes, respectively, as described above. Hepatocytes were then stained for pSTAT3 (Supplementary Table 3 ), and their nuclei stained with DAPI. Immunofluorescence imaging was performed on an IX83 inverted multicolour fluorescent microscope (Olympus). Statistical analysis Statistical significance was calculated using Prism (GraphPad Software, version 7) unless indicated otherwise. Multiple comparisons testing was performed using one-way ANOVA with Dunnett\u2019s test. Paired group comparisons test was carried out using two-tailed Wilcoxon matched-pairs signed rank test. Unpaired group comparisons test was performed using two-tailed unpaired Student\u2019s t test or two-tailed Mann\u2013Whitney test. Comparison of Kaplan\u2013Meier overall survival curves was performed using log-rank (Mantel-Cox) test. P values less than 0.05 were treated as significant. The experiments were not randomized and the investigators were not blinded to allocation during experiments and outcome assessment, unless stated otherwise. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this paper. Data availability QuantSeq 3\u2032 mRNA sequencing data have been deposited in the Gene Expression Omnibus (GEO) under accession number GSE109480 . Source Data are provided for all figures and extended data figures. All data are available from the corresponding author upon reasonable request. ","News_Body":"When cancer spreads to another organ, it most commonly moves to the liver, and now researchers at the Abramson Cancer Center of the University of Pennsylvania say they know why. A new study, published today in Nature, shows hepatocytes\u2014the chief functional cells of the liver\u2014are at the center of a chain reaction that makes it particularly susceptible to cancer cells. These hepatocytes respond to inflammation by activating a protein called STAT3, which in turn increases their production of other proteins called SAA, which then remodel the liver and create the \"soil\" needed for cancer cells to \"seed.\" The researchers show that stopping this process by using antibodies that block IL-6\u2014the inflammatory signal that drives this chain reaction\u2014can limit the potential of cancer to spread to the liver. \"The seed-and-soil hypothesis is well-recognized, but our research now shows that hepatocytes are the major orchestrators of this process,\" said senior author Gregory L. Beatty, MD, Ph.D., an assistant professor of Hematology-Oncology at Penn's Perelman School of Medicine. Jae W. Lee, an MD\/Ph.D. candidate in Beatty's laboratory, is the lead author. For this study, the team first used mouse models of pancreatic ductal adenocarcinoma (PDAC), the most common type of pancreatic cancer and currently the third leading cause of cancer death in the United States. They found that nearly all hepatocytes showed STAT3 activation in mice with cancer, compared to less than two percent of hepatocytes in mice without tumors. They then partnered with investigators at the Mayo Clinic Arizona and other Penn colleagues to show that this same biology could be seen in patients with pancreatic cancer as well colon and lung cancer. Genetically deleting STAT3 only in hepatocytes effectively blocked the increased susceptibility of the liver to cancer seeding in mice. The team collaborated further with investigators at the University of Kentucky to show that IL-6 controls STAT3 signaling in these cells and instructs hepatocytes to make SAA, which acts as an alarm to attract inflammatory cells and initiate a fibrotic reaction that together establish the \"soil.\" \"The liver is an important sensor in the body,\" Lee said. \"We show that hepatocytes sense inflammation and respond in a structured way that cancer uses to help it spread.\" The study also found that IL-6 drives changes in the liver whether there's a tumor present or not, implying that any condition associated with increased IL-6 levels\u2014such as obesity or cardiovascular disease, among others\u2014could affect the liver's receptiveness to cancer. Researchers say this provides evidence that therapies which target hepatocytes may be able to prevent cancer from spreading to the liver, a major cause of cancer mortality. ","News_Title":"Cancer most frequently spreads to the liver; here's why","Topic":"Medicine"}
{"Paper_Body":"Abstract Magnetic resonance imaging and spectroscopy are versatile methods for probing brain physiology, but their intrinsically low sensitivity limits the achievable spatial and temporal resolution. Here, we introduce a monolithically integrated NMR-on-a-chip needle that combines an ultra-sensitive 300 \u00b5m NMR coil with a complete NMR transceiver, enabling in vivo measurements of blood oxygenation and flow in nanoliter volumes at a sampling rate of 200 Hz.     Main Methods based on nuclear magnetic resonance (NMR) are powerful analytical techniques in the life sciences, using nuclear spins as specific nanoscopic probes. Despite substantial advances in magnetic resonance (MR) hardware and methodology, NMR is still limited by its poor sensitivity (compared, for example, with optical methods), hindering in particular its use in the study of brain physiology and pathology. Recently, integrated circuit (IC)-based NMR systems have been introduced 1 , 2 , 3 , 4 , 5 to simplify the hardware complexity of MR experiments and to boost sensitivity. Integration of the MR detection coil with the transceiver on a single IC 4 , 5 laid the foundation for millimeter-size, sensitive MR systems for in situ and in vivo applications such as palm-size NMR spectrometry 1 and NMR spectroscopy of single cells 5 . Here, we present a monolithic needle-shaped NMR-on-a-chip transceiver (Fig. 1a,b ) that makes the advantages of IC-based NMR available for various applications in neuroscience. With its miniaturized on-chip coil, low-noise performance and compact, 450 \u00b5m-wide needle design, our NMR-on-a-chip transceiver simultaneously improves sensitivity as well as spatial and temporal resolution. In contrast to conventional microcoils 6 , 7 , the micrometer-scale interconnecting wires between the on-chip coil and the electronics combined with the fully differential design reduce the pickup of parasitic MR signals and electromagnetic interference. This enables interference-free in vivo experiments in a defined region of interest. Compared to conventional functional MR imaging (fMRI), the on-chip microcoil removes the need for time-consuming spatial encoding and allows for a continuous recording of MR signals in a nanoliter volume with millisecond resolution. Fig. 1: Schematic overview of the target application of the needle-shaped NMR-on-a-chip transceiver, the ASIC design and the experimental setup. a , The NMR needle is inserted into the target brain area, for example the somatosensory cortex, to perform localized and fast functional MR experiments. b , Fully integrated NMR-on-a-chip spectrometer with an on-chip planar broadband detection coil. The transceiver electronics include a low-noise receiver with quadrature demodulation, an H-bridge-based PA and a frequency synthesizer (containing a phase-frequency detector (PFD), a charge pump (CP) and a quadrature signal generator (IQ)). c , Experimental setup around the NMR needle: the ASIC is glued and bonded on a small carrier PCB and connected via a ribbon cable to the signal conditioning PCB. This setup can be mounted either on a carrier with a sample container and a conventional 8 mm surface coil as reference for system characterization, such as linewidth, sensitivity and SNR, and MR imaging (in vitro setup) or on an animal bed for neuronal experiments to measure changes in blood oxygenation and flow in rats (in vivo setup). The bed or carrier is placed inside a 14.1 T small-animal scanner and the system is completed by a commercial data-acquisition card and a LabVIEW-based console located in the control room. Full size image To achieve the required detection sensitivity in a form factor that is suitable for localized in vivo experiments in brain tissue, we realized a complete NMR spectrometer as a complementary metal-oxide-semiconductor (CMOS) application-specific integrated circuit (ASIC) (Fig. 1b ). This low-power (20 mW) NMR-on-a-chip transceiver features an on-chip, 24-turn, 300 \u00b5m outer diameter, transmit\/receive (TX\/RX) NMR coil. The RX path contains a complete quadrature receiver with an overall noise figure of 0.7 dB including a phase-locked loop (PLL)-based frequency synthesizer and protection switches for the low-noise amplifier (LNA). The TX path features an H-bridge power amplifier (PA) operating from a 3.3 V supply and driven by the on-chip PLL that produces a maximum coil current of 15 mA at 600 MHz. Owing to its amplitude and phase modulation capabilities, the on-chip electronics allow for the use of standard imaging sequences and spectroscopy techniques. In mechanical postprocessing, we first ground the manufactured chips down to a thickness of 100 \u00b5m and then shaped them as a needle with a wafer dicer. We used two different setups for in vitro characterization and for in vivo neuronal rat experiments in a 14.1 T small-animal scanner (Fig. 1c ). After first-order manual shimming, the NMR needle achieves a spectral linewidth of 12 Hz in a water phantom (Supplementary Fig. 1 ) and 53 Hz for in vivo experiments (Supplementary Fig. 2 ). We determined the sensitivity of the NMR needle using a three-dimensional gradient echo (3DGRE) sequence, resulting in a sensitive volume of 9.8 nl (Fig. 2a and Supplementary Fig. 3 ) and a time-domain spin sensitivity of \\(2.0 \\times 10^{13}{\\,\\mathrm{spins}}\\,\\mathrm{per}\\,\\sqrt {{\\mathrm{Hz}}}\\) . Compared to a conventional 8 mm surface coil, the NMR needle\u2019s signal-to-noise ratio (SNR) per spin is 40 times higher ( Methods ). We obtained 3DGRE images of a polyimide phantom with an isotropic resolution of 13 \u00b5m in less than 15 min, demonstrating the excellent MR imaging capabilities of the NMR needle (Supplementary Fig. 4 ). Fig. 2: In vitro measurement of the sensitive volume and representative experimental results from in vivo rat forepaw stimulation experiments. a , Single-shot (that is, no averaging) 3DGRE image of the sensitive volume V sens of the NMR needle immersed in 10 mM Gd-doped water ( N = 1). b , Coronal anatomical MR image recorded with a conventional surface coil, showing the precise needle location (no averaging, N = 1). The inset shows an overlay from EPI fMRI with a contralateral activation from the stimulation of the left paw in the implantation region of the needle (average of N = 20 stimulation blocks). c , Axial anatomical MR image showing the precise needle location and implantation depth ( N = 1). The inset shows an overlay from EPI fMRI ( N = 20). The presented data for b and c are representative of 12 animals. d , Contralateral BOLD response showing activations in each of the 20 identical 30 s stimulation blocks of a T R = 5 ms acquisition sequence ( N = 1 block for each curve). The stimulation period t stim = 6 s in each block is indicated by the gray background. e , Mean \u03bc and standard deviation \u03c3 of contralateral BOLD responses (average of N = 20 blocks) from EPI fMRI and NMR needle FIDs for stimulations of the left paw with different temporal (t) resolutions (for tSNR calculation see Methods and Supplementary Table 1 ). f , Ipsilateral BOLD responses ( N = 20) from EPI fMRI and NMR needle FIDs for stimulations of the right paw. g , Fit of \u0394 M 0, i from the functional measurements in e indicating the inflow effect for short T R ( N = 20). h , Fit of \\({\\mathrm{\\Delta }}R_{2,{i}}^ \\ast\\) from the functional measurements in e for multiple T R ( N = 20). i , Combined plot of mean values \u03bc from g and h ( N = 20). The presented data for d to i are representative of seven animals. Full size image As an in vivo benchmark application of our NMR sensing platform against conventional MR systems for neuronal measurements, we selected the detection of changes in blood flow and oxygenation in rats upon electrical forepaw stimulation. For this purpose, we slowly inserted the NMR needle 1.5 mm deep into the rat\u2019s somatosensory cortex 8 ( Methods ). The in vivo setup (Fig. 1c ) allows for the recording of the typical NMR response after a pulse excitation, the so-called free induction decay (FID), with the NMR needle as well as conventional fMRI using echo planar imaging (EPI) with a surface coil. We also used the surface coil to determine the needle location via high-resolution anatomical MR imaging of the implantation region. The overlays of conventional EPI data on the anatomical MR images show the responses to the stimulation at the needle location (Fig. 2b,c ). The effect of hemodynamic changes on the time course of the FID acquired with the NMR needle is twofold. First, changes in cerebral blood flow (CBF) modulate the initial FID amplitude through a change of inflowing unsaturated blood into the sensitive volume of the coil. Furthermore, changes in local oxygenation of blood (BOLD effect) alter the decay rate \\(R_{2}^{\\ast}=1\/T_{2}^{\\ast}\\) of the FID. To capture both effects, we calculated the area under each magnitude FID, obtaining a time series with a temporal resolution of up to 200 Hz. We corrected these time series further for temporal stability and physiological noise (Supplementary Fig. 5 ). A stimulation experiment to measure CBF and BOLD changes consisted of 20 identical 30 s blocks with a stimulation for t stim = 6 s, followed by a resting period of 24 s ( Methods ). A corrected time course for an NMR repetition time of T R = 5 ms shows a contralateral response to each of the 20 identical stimulations of the left paw (Fig. 2d ). The signal detected with the NMR needle upon stimulation has an amplitude around 1% and displays a very small delay with respect to the onset and the end of the 6 s stimulation. Signals measured with the needle at T R = 1 s have a similar lineshape, relative signal change \u0394 S \/ S and temporal SNR (tSNR) as the reference EPI time course (Fig. 2e ), while being recorded in a substantially lower sensitive volume (9.8 nl compared to a region of interest (ROI) of 12 \u00b5l). Compared to a single EPI voxel, the volume-normalized tSNR of the NMR needle at T R = 1 s is 150-fold increased (Supplementary Table 1 ). Increasing the sampling rate of the needle FIDs to 20 Hz and 200 Hz results in a faster tracking of hemodynamic changes. The ipsilateral responses to a stimulation of the right paw (Fig. 2f ) show no measurable effect in any of the measurements, which confirms that the signals measured in the contralateral cortex (Fig. 2e ) represent hemodynamic responses. To separate the changes of local CBF and blood oxygenation in the contralateral responses, we fitted each individual i th FID time course to a physiological model and verified the results by numerical simulations ( Methods ). For a repetition time T R = 1 s, the blood within the sensitive region fully exchanges within one T R , therefore no inflow-related magnitude change \u0394 M 0, i was observed (Fig. 2g ). The \u0394 M 0, i for both short T R (5 ms and 50 ms) are around 0.5%, which corresponds to a change in CBF of about 15\u201330 ml per 100 g per min (Supplementary Fig. 6a ), or 13% to 25% assuming a baseline CBF of 120 ml per 100 g per min(ref. 9 ). This is in the lower range of reported values of 20% to 90% based on MR perfusion measurements 10 , 11 , most likely due to different anesthetics or a potential inclusion of larger vessels. Observed changes in local blood oxygenation \\({\\mathrm{\\Delta }}R_{2,{i}}^ \\ast\\) are between 1.5 Hz and 2 Hz across all chosen T R (Fig. 2h ), which relates to a local oxygenation change around 15% to 20% (Supplementary Fig. 6b ). The measured changes in \\({\\mathrm{\\Delta }}R_{2,{i}}^ \\ast\\) are comparable to quantitative \\({\\mathrm{\\Delta }}T_2^ \\ast\\) measurements in humans and rats ranging between 1 Hz and 6 Hz (refs. 12 , 13 ). Despite the unprecedented temporal resolution of 5 ms, our results indicate neither the presence of an initial dip (a short and small BOLD signal decrease attributed to oxygenation decrease prior to any subsequent blood flow and oxygenation increase 14 ) nor a mismatch between CBF and oxygenation changes (Fig. 2i ). Our NMR needle targets the deep cortical layers of rodents where no initial dip was detected in a previous study 15 . Conventional fMRI studies with large voxel sizes often report a substantial mismatch between CBF and oxygenation changes 16 . However, combined optical measurements of CBF and oxygenation show that this mismatch is only visible at the venous side, but not at the capillary level or at the artery side 17 . Our data thus support, in agreement with optical measurements 17 , that the temporal mismatch between oxygenation and CBF changes is strongly reduced in deep cortical regions. Although preliminary in nature, our results demonstrate the power of CMOS-based in vivo MR experiments and the NMR needle\u2019s potential for future applications in neuroscience. Applications may reveal currently unknown dynamics in the laminar-specific hemodynamic response and the underlying physiology of fMRI with layer-specific resolution, and even effects that are not related to hemodynamics. The NMR needle allows correlation of the continuously detectable and locally acquired MR signals with other recordings, such as local field potentials or optically detected changes in local calcium concentration at a comparable sampling bandwidth and spatial resolution. This provides the possibility of discovering novel effects or fingerprints of neuronal activation inside the continuously evolving MR magnetization. This might include the detection of local geometric changes, for example cell swelling, or the direct detection of bulk neuronal currents through their induction of a local magnetic field 18 , 19 . The sensitive volume of the NMR needle is comparable to the thickness of a single cortical layer, the extension of a cortical column or small subcortical nuclei. Moreover, the scalable CMOS design is well suited to form an array of small coils along the shaft of the needle to collect signals from different cortical layers simultaneously with individual coils and without the need to move the sensor. Additionally, the NMR needle can be extended by an array of electrophysiological electrodes individually connected to integrated preamplifiers for (multisite) in vivo electrical stimulation and recording 20 , 21 on a single sensor chip, thus opening up the path for future multi-modal brain sensing platforms. As the presented NMR needle achieves a similar spatiotemporal resolution as electrophysiology or optical brain recording while offering the specificity and versatility of NMR, IC-based in vivo NMR is a promising approach to close the gap between these complementary imaging modalities. Therefore, we believe that our approach can help to disentangle physiologic processes within the neural network and that this technology can potentially uncover MR effects beyond the conventional hemodynamic signal responses to provide even deeper insights. Methods CMOS chip design and operation The needle-shaped NMR-on-a-chip transceiver has been fabricated in a 130 nm CMOS technology from GlobalFoundries. The postprocessed chips are 3,000 \u00b5m long, 450 \u00b5m wide and 100 \u00b5m thick. The microcoil is located at the needle tip, which is formed in a mechanical postprocessing step, while the bondpads are placed on the opposite end to allow for an implantation depth up to 2 mm. The transceiver electronics substantially improve and extend a previously published version of an NMR-on-a-chip transceiver 4 for the target application of this paper. Here, the extreme form factor required special care in the design of all electrical interconnects to avoid an undesirable coupling into the RX path from the TX path or the frequency synthesizer. The on-chip RX path is fully differential to suppress Hall and magnetoresistive effects inside the strong B 0 field and incorporates a current-reuse LNA with a common mode feedback. The LNA provides an input referred voltage noise of \\(1.26\\,{\\mathrm{nV}}\\,\\mathrm{per}\\,\\sqrt {{\\mathrm{Hz}}}\\) over a bandwidth from 30 MHz to 700 MHz. At 600 MHz, the LNA degrades the intrinsic on-chip coil SNR by 9% corresponding to a noise figure of 0.7 dB. Importantly, in contrast to conventional MR coil arrays, the high-impedance on-chip LNA provides an efficient coil decoupling, allowing for an arbitrary placement of multiple coils along the needle for future microcoil arrays. Active quadrature Gilbert cell mixers follow the LNA and demodulate the detected NMR signal at f NMR to the desired low to intermediate frequency (low-IF) f IF in the range of 10 kHz to 100 kHz. The signals at the low-IF are further amplified and converted to single-ended signals v out,I and v out,Q in the buffer stage to minimize the number of required bondpads. The TX path operates at 3.3 V compared to the 1.5-V RX supply to increase the maximum coil current for pulsed excitation. The H-bridge PA is a nonresonant design to maximize the current in the NMR coil for a given supply voltage 22 and can be disabled during RX operation without requiring additional series switches, which would otherwise decrease the TX performance. The TX signal and the quadrature local oscillator signals f LO,sin and f LO,cos for the RX path are generated from a low-frequency reference using an integer- N PLL, enabling micrometer-length radio frequency interconnects and facilitating the electrical connection of the needle. A frequency shift keying (FSK) of the PLL reference f ref allows for an on-resonance excitation and a low-IF RX operation outside the 1\/ f noise region of the receiver. In vitro setup Each postprocessed NMR needle was glued to a carrier printed circuit board (PCB), with two-thirds of the chip extending beyond the PCB edge to enable implantation. The ASIC was wire bonded onto the carrier PCB, which was in turn connected to the 6 \u00d7 3 cm 2 large signal conditioning PCB containing amplifiers for the NMR signals, clock and signal buffers and the power supply for the ASIC. The same assembly was used in the in vitro and the in vivo setups (Fig. 2b ), both being designed for operation inside a 14.1 T, 26 cm horizontal bore magnet (Magnex Scientific). The in vitro setup features a sample basin with a diameter of 13 mm and a height of 7 mm, which was filled with 700 \u00b5l of deionized or 10 mM gadolinium (Gd)-doped water. Conventional planar coils with 8 mm and 10 mm diameters were placed below the basin and interfaced to the standard BioSpec spectrometer (Bruker BioSpin). Simulation and measurement of the sensitive volume Finite-element electromagnetic simulations of the on-chip coil\u2019s unitary B field, B u , were carried out to characterize its inhomogeneity (Supplementary Fig. 7 a\u2013c), leading to a nonuniform flip angle distribution in the sample during TX and a nonuniform sensitivity during RX 23 . The flip angle was selected by choosing the output current of the H-bridge amplifier via TX supply modulation and an appropriate pulse length. The resulting signal intensity versus pulse length for the maximum TX supply of 3.3 V (Supplementary Fig. 7 d) has its peak value at 13 \u00b5s. The sensitive volume of the planar microcoil with its inhomogeneous field distribution was defined consistently throughout the study using the following definition. An image slice parallel to the coil surface at a distance of 0.1 d coil , where d coil is the diameter of the coil, is selected in which the signal ROI is defined as a centered square with a side length of 0.5 d coil . The image signal \\(\\hat S\\) is determined from the mean \u03bc of the individual voxel intensities I S ,1 ,\u2026, I S , i within the signal ROI according to \\(\\hat S = \\mu \\left( {I_{S,1}, \\ldots ,I_{S,i}} \\right)\\) . The sensitive volume is then defined as the volume with a signal amplitude of at least 10% of the signal \\(\\hat S\\) , resulting in a simulated sensitive volume of the microcoil of 9.5 nl (Supplementary Fig. 3 a\u2013d). The simulation results were validated experimentally in 10 mM Gd-doped water. The nutation curve was measured in simple pulse-acquire experiments (Supplementary Fig. 7 d). The sensitive volume was assessed in a 3DGRE imaging experiment using T R = 30 ms, pulse time T P = 10 \u03bcs, echo time T E = 4.77 ms, acquisition time T acq = 5.1 ms, number of averages N avg = 1, matrix size 128 \u00d7 128 \u00d7 128, isotropic voxel size 13 \u00b5m, field of view (FOV) 1.7 \u00d7 1.7 \u00d7 1.7 mm 3 , scan time 8 min 12 s and manual shim. The measured sensitive volume was 9.8 nl and simulation and measurement were in good agreement. B 0 map and manual shim The B 0 map in Supplementary Fig. 8 was recorded with the in vitro setup using a 10 mm surface coil underneath the deionized water-filled basin using two 3DGRE sequences with different echo times T E1 and T E2 with T R = 47.86 ms, flip angle FA = 30\u00b0, T E1 = 2.56 ms, T E2 = 5.88 ms, T acq = 2.15 ms, N avg = 1, matrix size 256 \u00d7 175 \u00d7 256, isotropic voxel size 45 \u00b5m, FOV 11.5 \u00d7 7.9 \u00d7 11.5 mm 3 , scan time 35 min 44 s and global shim. The NMR needle causes local susceptibility variations in the order of \u00b1500 Hz. To improve the linewidth and thereby the needle\u2019s frequency domain SNR, a manual shim procedure was used, where the three first-order shim gradients were iteratively changed until a minimum linewidth was found. Modifying higher-order shim gradients did not result in a significant improvement. The resulting x gradient was between \u221211,000 Hz cm \u22121 to \u22128,000 Hz cm \u22121 , while the y and z gradients were between \u22122,000 Hz cm \u22121 and +2,000 Hz cm \u22121 . The B 0 map in Supplementary Fig. 8b has a field gradient in the x direction of \u221210,000 Hz cm \u22121 , which is in good agreement with the results found during the manual shim procedure. A spectral linewidth of 12 Hz (corresponding to 0.02 ppm) was achieved on a water phantom (Supplementary Fig. 1b ). Under in vivo conditions, the intrinsic relaxation time \\(T_2^ \\ast\\) of brain tissue is about 20 ms to 30 ms (corresponding to a linewidth of 16 Hz to 11 Hz) and can be even shorter in regions with high blood volume fraction. Thus, the needle linewidth of 12 Hz does not substantially degrade the intrinsic tissue linewidth for in vivo NMR experiments. Image SNR and system sensitivity The sensitivity of the described microcoil was compared with a conventional 8 mm-diameter surface coil. The time-domain spin sensitivity is defined as the minimum detectable number of spins with an SNR of three in 1 s of measurement time. This is a suitable figure of merit for comparing coil sensitivities because it directly relates to the minimum detectable voxel size. It can be computed from the image SNR per spin, which in turn can be determined from a single magnitude image according to the National Electrical Manufacturers Association Standards Publication MS 1-2008 (ref. 24 ) as detailed in Anders et al. 3 . For the 8- mm coil, 3DGRE imaging with T R = 30 ms, FA = 35\u00b0, T E = 4.77 ms, T acq = 1.28 ms, N avg = 1, matrix size 128 \u00d7 128 \u00d7 128, isotropic voxel size 100 \u00b5m, FOV 13 \u00d7 13 \u00d7 13 mm 3 and scan time 8 min 12 s was used (Supplementary Fig. 9b ). Using the definition of \\(\\hat S\\) , a noise ROI (10 \u00d7 10 voxel in each of the four corners) to determine the image noise \\(\\hat \\sigma _{\\mathrm{N}}\\) and following the procedure of Anders et al. 3 , the time-domain spin sensitivities for both coils were determined. With SNR needle = 68.9 and SNR coil = 383 from Supplementary Fig. 9 , voxel sizes \\(\\Delta_{{\\mathrm{needle}}}^3 = \\left( {13\\,{{\\upmu {\\mathrm{m}}}}} \\right)^3\\) and \\(\\Delta_{{\\mathrm{coil}}}^3 = \\left( {100\\,{{\\upmu {\\mathrm{m}}}}} \\right)^3\\) , T ACQ,needle = 5.1 ms, T ACQ,coil = 1.28 ms ( T R = 30 ms, T E = 4.77 ms, number of phase encoding steps N PE = 128 \u00d7 128, N avg = 1, spin density \\(N_{{\\mathrm{s}},{\\mathrm{H}}_2{\\mathrm{O}}} = 6.7 \\times 10^{28}\\,{\\mathrm{spins}}\\,{\\mathrm{m}}^{-3}\\) for both), the time-domain spin sensitivities of the two coils are \\(8.0 \\times 10^{14}\\,{\\mathrm{spins}}\\,{\\mathrm{per}}\\,\\sqrt {{\\mathrm{Hz}}}\\) for the 8 mm coil and \\(2.0 \\times 10^{13}{\\mathrm{spins}}\\,{\\mathrm{per}}\\,\\sqrt {{\\mathrm{Hz}}}\\) for the NMR needle. This corresponds to a 40-fold improvement in spin sensitivity and a 1,600-fold improvement in imaging time of the presented NMR needle compared to the surface coil. MR imaging The imaging capabilities of the NMR needle were demonstrated using a polyimide foil phantom with 50 \u00d7 50 \u03bcm 2 laser-cut square openings (Supplementary Fig. 4 ). This phantom was immersed in 10 mM Gd-doped water and the needle placed in close proximity to the foil. A 3DGRE image with an isotropic resolution of 13 \u00b5m was recorded using T R = 50 ms, T P = 8 \u03bcs, T E = 4.77 ms, T acq = 5.1 ms, N avg = 1, matrix size 128 \u00d7 128 \u00d7 128, isotropic voxel size 13 \u00b5m, FOV 1.7 \u00d7 1.7 \u00d7 1.7 mm 3 , scan time 13 min 39 s and manual shim. Animal preparation Fifteen healthy, anesthetized rats (Sprague-Dawley, male, 402 \u00b1 49 g, 11 \u00b1 2 weeks) were examined in the 14.1-T small-animal scanner. The study was approved by the local authorities (Regierungspr\u00e4sidium T\u00fcbingen, Germany) and was in full compliance with the guidelines of the European Community for the care and use of laboratory animals. The rats were anesthetized with urethane (1.2 g kg \u22121 body weight, more if necessary to maintain anesthesia). The body temperature was monitored via a rectal probe and kept constant at around 37 \u00b0C by a heating pad. Breathing rate, oxygen saturation and heart rate were monitored throughout surgery and experiment with a pulse oximeter (MouseOx, Starr Life Sciences). During surgery, the animal was supplied with a gas mixture of two-thirds nitrous oxide and one-third oxygen for additional analgesia. The head was shaved, disinfected, fixed in a stereotactic frame and treated with a local analgesic (Lidocaine). The skull was removed around the somatosensory cortex, 3.5 mm off the midline and 0.5 mm posterior to the bregma 8 . The NMR needle was attached to a holder, which was fixed to the skull with bone cement. The needle was then slowly inserted between 1.5 mm and 2 mm into the brain. One animal died during the surgery and for a second one a head trauma was observed during surgery; therefore, no experiments were conducted with those two animals. For the MR measurements, the animal was positioned in an MR-compatible bed with ventilation through a controlled pumped-air facemask. A conventional oval-shaped 20 \u00d7 30 mm 2 surface coil was attached horizontally around the implanted needle. Two pairs of needle electrodes were placed between the toes of both anterior paws for peripheral sensory stimulation. Protocol for in vivo MR experiments The position of the needle in the brain after insertion was confirmed via anatomical MR images, acquired with the surface coil (3DGRE matrix size 384 \u00d7 384, FOV 45 \u00d7 40 mm 2 , eight 1 mm slices, T E = 2.89 ms, T R = 200 ms, FA = 20\u00b0). In all functional experiments, a 6 s stimulus (9 Hz, 300 \u00b5s pulses, 2.5 mA), interleaved with 24 s rest periods was repeated 20 times. In combination with an initial 20 s rest period with dummy pulses to reach the equilibrium magnetization, this resulted in a total duration of 620 s for one measurement. Conventional functional experiments were performed by transmitting and receiving with the surface coil. A GRE EPI sequence was used (matrix size 64 \u00d7 48, FOV 43 \u00d7 38 mm 2 , eight 1 mm slices, T E = 9 ms, T R = 1,000 ms, bandwidth 300 kHz). At this stage, one animal had the needle implanted outside the target area, which was confirmed by anatomical images. A second animal did not show any response to the stimulation, neither in conventional EPI nor in the needle experiments, most likely due to an improper anesthesia, which is also a common reason for unsuccessful experiments in conventional fMRI 25 . In three experiments, no valid signal could be measured with the NMR needle, which was found to be caused by mechanical stress at the CMOS\u2013PCB interface during the implantation. This was eliminated in subsequent experiments by an improved probe head design and using a different epoxy glue (EPO-TEK 353ND-T, Epoxy Technology Inc.). In all remaining in vivo experiments, the NMR needle experiments were successful, and all functional experiments were carried out. Here, in seven animals, a change in CBF and blood oxygenation could be observed, while in one animal, no activation could be measured. In the unsuccessful experiment, although near the relevant region, it is most likely that the needle was not close enough to the active brain area. The functional NMR needle experiments were performed using pulse-acquire sequences with 10 \u00b5s pulse length without gradients. Different repetition times of T R = 1,000 ms, 50 ms and 5 ms were used corresponding to 600, 12,000 and 120,000 FIDs per experiment. The complex quadrature time-domain signals were sampled at 2 MS s \u22121 and 16 bit resolution, saved as raw data and evaluated offline after the experiment. Modeling of blood flow and oxygenation changes Neuronal activation triggers a cascade of hemodynamic changes such as increased local CBF and blood oxygenation. To quantify these changes, the relation between the individual FID i and the mean FID mean was assumed to be $${\\mathrm{FID}}_i\\left( t \\right) = \\frac{{M_{0,i}}}{{M_{0,{\\mathrm{mean}}}}} \\times {\\mathrm{FID}}_{{\\mathrm{mean}}}\\left( t \\right) \\times \\exp \\left( {\\frac{t}{{{\\mathrm{\\Delta }}T_{2,i}^ \\ast }}} \\right),$$ where i = 1,\u2026, n is the number of the FID, t is the time elapsed after the excitation pulse and \\({\\mathrm{\\Delta }}T_{2,i}^ \\ast\\) is the absolute change of \\(T_2^ \\ast\\) of the i th FID. The factor M 0, i \/ M 0,mean models the relative change of the initial amplitude of each FID i caused by the inflow effect, resulting in \\({\\mathrm{\\Delta }}M_{0,i} = \\left( {M_{0,i}\/M_{0,{\\mathrm{mean}}}} \\right) - 1 > 0\\) during activation. An increased oxygenation level prolongs the decay of the FID i resulting in $${\\mathrm{\\Delta }}T_{2,i}^ \\ast = 1\/{\\mathrm{\\Delta }}R_{2,i}^ \\ast = 1\/(R_{2,i}^ \\ast - R_{2,\\mathrm{mean}}^ \\ast ) > 0$$ To estimate \u0394 M 0, i and \\({\\mathrm{\\Delta }}R_{2,i}^ \\ast\\) , an exponential fit was applied to $$M_{0,i} \\exp \\left( {{\\mathrm{\\Delta }}R_{2,i}^ \\ast t} \\right) = M_{0,{\\mathrm{mean}}} \\frac{{{\\mathrm{FID}}_i\\left( t \\right)}}{{{\\mathrm{FID}}_{{\\mathrm{mean}}}\\left( t \\right)}}$$ for each individual FID. A two-compartment model consisting of extravascular and intravascular space and corresponding T 1 relaxation times of 2.46 s and 3.16 s (ref. 26 ) was used to simulate the inflow-related signal increase for different excitation flip angles (Supplementary Fig. 6a ). These results do not depend on the chosen fractional blood volume (FBV) and total volume of the two-compartment model and are independent of the repetition time T R up to a certain limit where the saturated blood pool can fully exchange with fresh blood within one T R . This limit is at about T R = 500 ms assuming a baseline CBF of 120 ml per 100 g per min and an FBV of 2% (ref. 9 ). Similar to earlier research 27 , 28 , 29 , randomly oriented cylinders with different radii have been used to model the change of the FID decay rate \\(\\left( {{\\mathrm{\\Delta }}R_2^ \\ast } \\right)\\) for different FBV and different changes in local oxygenation, \u0394LOX (Supplementary Fig. 6b ). Data analysis All data processing was performed with MATLAB 2017b (The MathWorks Inc.) unless noted otherwise. The volumes of all 3DGRE images were reconstructed by a three-dimensional fast Fourier transform (FFT) without filtering. The B 0 map from Supplementary Fig. 8 was calculated from two 3DGRE sequences with different echo times T E1 and T E2 voxel-by-voxel by extracting the phase difference between the two echoes, unwrapping the calculated volume, and these were converted to hertz by scaling with the difference in the echo times \u0394 T E . The functional data from the EPI acquisitions were analyzed using Analysis of Functional Neuroimages (v.17.2.02, National Institute of Mental Health) 30 including slice timing correction for the interleaved acquisition and anatomical co-registration. The activation maps were computed on a voxel-by-voxel basis using temporal autocorrelations to calculate the statistically significant maps with thresholds of P < 0.01 (false discovery rate-corrected). Only clusters comprising at least 10 voxels were considered significant. These maps generated the ROIs, which were then used to extract the averaged and concatenated time course in MATLAB. The complex FIDs of the NMR needle were filtered with a Gaussian bandpass filter around the low-IF frequency of 70 kHz with a bandwidth of 5 kHz to remove unwanted noise. The magnitude of the filtered FID was then integrated from 150 \u00b5s to 20 ms (4.5 ms for T R = 5 ms) resulting in a single value per FID. Those values were corrected for long-term drifts by applying a second-order polynomial fit over the entire dataset of a 620 s time series and for physiological noise originating from breathing and heart rate by applying narrow-band notch filters at the corresponding frequencies extracted from the measurements with the breathing pad and the pulse oximeter. Additionally, the functional signals were low pass filtered with a 3 Hz Gaussian filter to reduce the noise, since they did not contain any visible stimulation-related features beyond that frequency (Supplementary Fig. 5 ). Calculation of tSNR The tSNR describing the temporal stability of an fMRI signal is the most important figure of merit for the performance of fMRI systems. The measured tSNR values for all signals shown in Fig. 2e are given in Supplementary Table 1 for the raw signal and after each of the signal filtering steps. Although the sensitive volume of 9.8 nl of the needle is about 50 times smaller than a single voxel of the EPI fMRI (530 nl), the tSNR values of the NMR needle at T R = 1,000 ms are only 30% worse than the EPI fMRI results, which were measured over an ROI of 22 voxels with a total volume of 12 \u00b5l. The volume-normalized tSNR of the needle for T R = 1,000 ms was about 150 times better than for a single voxel of the reference EPI fMRI experiment. Those results are directly comparable, since both experiments are limited by the short \\(T_2^ \\ast\\) of the tissue. Decreasing T R leads to saturation effects, which therefore results in lower signal amplitudes (Supplementary Fig. 2 ), and, consequently, also in lower raw tSNR for T R = 50 ms and for T R = 5 ms. The second-order baseline correction and the physiological filter produce only a minor tSNR increase, indicating that long-term drifts and physiological noise (mostly heart beat and breathing) do not significantly degrade the NMR needle time course. Since hemodynamic changes are significantly slower than the sampling rates of 20 Hz and 200 Hz, the application of a 3 Hz Gaussian lowpass filter results in an effective tSNR increase of the oversampled physiological signal. The resulting volume-normalized tSNR for T R = 50 ms and T R = 5 ms are 227 and 337 times larger than for the EPI fMRI, respectively. The noise performance of the NMR needle under different conditions was measured to separate different noise sources. The time-domain noise for the needle increased by merely 20% if the needle was surrounded by a water phantom instead of air, confirming that the system noise is dominated by the ohmic resistance of the detection coil and that sample noise is negligible, which is typical for NMR microcoils. The time-domain SNR for the NMR needle immersed in deionized water with T R = 1,000 ms was 330. In the in vivo measurement with T R = 1,000 ms shown in Fig. 2e , the time-domain SNR is reduced to 240, mainly due to the 20% lower proton density of brain tissue compared to water 31 . The additional decrease of the tSNR to 131 compared to the time-domain SNR of 240 is caused by short-term temporal instabilities, which are not corrected by the second-order fit. If those drifts are also corrected, the tSNR increases to 220, which is very close to the time-domain SNR of 240. Statistics and reproducibility From the in vivo experiments, we show representative datasets in Fig. 2b\u2013h and in Supplementary Figs. 2 and 5 . We were able to reproduce similar results for neuronal activation experiments in 7 animals for the NMR needle and in 12 animals for the conventional EPI fMRI. As we observed slightly different response times and amplitudes for different animals, we did not calculate averages for the datasets. The slightly different responses could be caused by several different factors. The most important factor here is the precise location and implantation depth of the needle. Deeper brain areas are known to have a lower magnitude of oxygenation changes 32 . Two other important factors are the anesthesia and the distance between the needle microcoil and nearby vessels and their size. Also, in the EPI results, different degrees of neuronal responses were observed, which are likely caused by an imperfect anesthesia 25 . Reporting Summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability The data that support the findings of this study are available from the corresponding authors upon request. ","News_Body":"A team of neuroscientists and electrical engineers from Germany and Switzerland developed a highly sensitive implant that enables to probe brain physiology with unparalleled spatial and temporal resolution. They introduce an ultra-fine needle with an integrated chip that is capable of detecting and transmitting nuclear magnetic resonance (NMR) data from nanoliter volumes of brain oxygen metabolism. The breakthrough design will allow entirely new applications in the life sciences. The group of researchers led by Klaus Scheffler from the Max Planck Institute for Biological Cybernetics and the University of T\u00fcbingen as well as by Jens Anders from the University of Stuttgart identified a technical bypass that bridges the electrophysical limits of contemporary brain scan methods. Their development of a capillary monolithic nuclear magnetic resonance (NMR) needle combines the versatility of brain imaging with the accuracy of a very localized and fast technique to analyze the specific neuronal activity of the brain. \"The integrated design of a nuclear magnetic resonance detector on a single chip supremely reduces the typical electromagnetic interference of magnetic resonance signals. This enables neuroscientists to gather precise data from minuscule areas of the brain and to combine them with information from spatial and temporal data of the brain\u00b4s physiology,\" explains principal investigator Klaus Scheffler. \"With this method, we can now better understand specific activity and functionalities in the brain.\" According to Scheffler and his group, their invention may unveil the possibility of discovering novel effects or typical fingerprints of neuronal activation, up to specific neuronal events in brain tissue. \"Our design setup will allow scalable solutions, meaning the possibility of expanding the collection of data from more than from a single area\u2014but on the same device. The scalability of our approach will allow us to extend our platform by additional sensing modalities such as electrophysiological and optogenetic measurements,\" adds the second principal investigator Jens Anders. The teams of Scheffler and Anders are very confident that their technical approach may help demerge the complex physiologic processes within the neural networks of the brain and that it may uncover additional benefits that can provide even deeper insights into the functionality of the brain. With their primary goal to develop new techniques that are able to specifically probe the structural and biochemical composition of living brain tissue, their latest innovation paves the way for future highly specific and quantitative mapping techniques of neuronal activity and bioenergetic processes in the brain cells. ","News_Title":"Scientists develop first implantable magnet resonance detector","Topic":"Physics"}
{"Paper_Body":"Abstract Clonal haematopoiesis involves the expansion of certain blood cell lineages and has been associated with ageing and adverse health outcomes 1 , 2 , 3 , 4 , 5 . Here we use exome sequence data on 628,388 individuals to identify 40,208 carriers of clonal haematopoiesis of indeterminate potential (CHIP). Using genome-wide and exome-wide association analyses, we identify 24 loci (21 of which are novel) where germline genetic variation influences predisposition to CHIP, including missense variants in the lymphocytic antigen coding gene LY75 , which are associated with reduced incidence of CHIP. We also identify novel rare variant associations with clonal haematopoiesis and telomere length. Analysis of 5,041 health traits from the UK Biobank (UKB) found relationships between CHIP and severe COVID-19 outcomes, cardiovascular disease, haematologic traits, malignancy, smoking, obesity, infection and all-cause mortality. Longitudinal and Mendelian randomization analyses revealed that CHIP is associated with solid cancers, including non-melanoma skin cancer and lung cancer, and that CHIP linked to DNMT3A is associated with the subsequent development of myeloid but not lymphoid leukaemias. Additionally, contrary to previous findings from the initial 50,000 UKB exomes 6 , our results in the full sample do not support a role for IL-6 inhibition in reducing the risk of cardiovascular disease among CHIP carriers. Our findings demonstrate that CHIP represents a complex set of heterogeneous phenotypes with shared and unique germline genetic causes and varied clinical implications. Main As humans age, somatic alterations accrue in the DNA of haematopoietic stem cells (HSCs) due to mitotic errors and DNA damage. Alterations that confer a selective growth advantage can lead to the expansion of particular cell lineages, a phenomenon called clonal haematopoiesis. The presence of clonal haematopoiesis has been associated with an increased risk of haematological neoplasms, cytopaenias, cardiovascular disease (CVD), infection and all-cause mortality 1 , 2 , 3 , 4 , 5 . For this reason, identifying germline causes of clonal haematopoiesis has the potential to improve our understanding of initiating events in the development of these common diseases. Large-scale studies of the germline causes of clonal haematopoiesis have used samples from the UKB and other large cohorts, but those studies have been limited mostly to clonal haematopoiesis phenotypes that can be assessed using single nucleotide polymorphism (SNP) array genotype data, such as mosaic chromosomal alternations (mCA) and mosaic loss of sex chromosomes 4 , 7 , 8 (mLOX and mLOY). Identifying individuals with CHIP, which is defined by somatic protein-altering mutations in genes that are recurrently mutated in clonal haematopoiesis, requires sequencing of blood 1 , 2 . Once a clone has expanded sufficiently, the somatic variants from this clone can be captured along with germline variants by exome sequencing. Since exome sequencing captures protein-altering variants, its large-scale application enables the detection of readily interpretable rare variant association signals, and can elucidate critical genes and pathways and potential therapeutic targeting 9 , 10 . So far, the largest genetic association study of CHIP has included 3,831 CHIP mutation carriers in a sample of 65,405 individuals and has identified four common variant loci 11 . Here, we use exome sequencing data to characterize CHIP status in 454,803 UKB 10 and 173,585 Geisinger MyCode Community Health Initiative (GHS) participants. We then conduct a common variant genome-wide association study (GWAS) and rare variant and gene burden exome-wide association study (ExWAS) of CHIP by leveraging 27,331 CHIP mutation carriers from the UKB. We perform a replication analysis using 12,877 CHIP mutation carriers from the GHS cohort. To identify germline predictors of specific clonal haematopoiesis driver mutations, we also conduct GWAS and ExWAS in carriers of CHIP mutations from individual CHIP genes. We then compare genetic association findings for CHIP to those from analyses of other clonal haematopoiesis phenotypes determined from somatic alterations in the blood, including mCA, mLOX, mLOY and telomere length. Although GWAS of these non-CHIP clonal haematopoiesis phenotypes have been conducted 4 , 7 , 12 , none have evaluated the effect of rare variation. The ExWAS we perform here represents the first systematic large-scale exploration of the effect of rare variants on the genetic susceptibility of these phenotypes. Finally, we examine the clinical consequences of somatic CHIP mutations and germline predictors of CHIP in several ways. We first conduct a PheWAS 13 of germline predictors of CHIP to understand their biological functions, and test cross-sectional phenotype associations of CHIP carrier status across 5,194 traits in the UKB. We then test the risk of incident cancer, CVD and all-cause mortality among specific CHIP gene mutation carriers and use Mendelian randomization to test for evidence of causal associations between CHIP and phenotypes of interest. Calling CHIP We used exome sequencing data from 454,803 and 173,585 individuals from the UKB and GHS cohorts, respectively, to generate large callsets of CHIP carrier status ( Methods ). In brief, we called somatic mutations using Mutect2 in a pipeline that included custom QC filtering (Extended Data Fig. 1a ), and ultimately restricted our analysis to 23 well defined and recurrent CHIP-associated genes. This focused analysis identified 29,669 variants across 27,331 individuals in the UKB (6%), and 14,766 variants across 12,877 individuals in the GHS (7.4%). DNMT3A , TET2 , ASXL1 , PPM1D and TP53 were the most commonly mutated genes in both cohorts (Extended Data Fig. 2a ). Although the GHS cohort had a wider age range, and therefore a larger number of older individuals, the prevalence by age was similar across cohorts, and reached approximately 15% by 75 years of age (Extended Data Fig. 1b,c ). Prevalence of CHIP gene-specific mutations was consistent with recurrence patterns, with mutations in the most commonly mutated CHIP genes beginning to increase in prevalence at younger ages (Extended Data Fig. 1d,e and Supplementary Note 1 ). Somatic mutations within the IDH2 and SRSF2 genes co-occurred significantly more frequently than expected in both the UKB and GHS cohorts, whereas DNMT3A mutations co-occurred less frequently with other mutations than expected (Extended Data Fig. 2b,c and Supplementary Table 1 ). Among individuals with multiple CHIP mutations (Supplementary Note 2 and Supplementary Fig. 1 ), JAK2 mutations consistently had the highest variant allele fraction (VAF) (Supplementary Fig. 1b ). CHIP demographics Compared with controls, CHIP carriers in both the UKB and GHS cohorts were older and more likely to be heavy smokers, consistent with previous studies 11 (Table 1 ). Although our cohorts were predominantly comprised of European ancestry individuals, the prevalence of CHIP was similar across all ancestries (Supplementary Fig. 2 ). In multivariate logistic regression models, each additional year of age was strongly associated with an increased risk of CHIP in the UKB (odds ratio [range] = 1.08 [1.077\u20131.082], P < 10 \u2212300 ) and GHS (odds ratio = 1.06 [1.057\u20131.063], P < 10 \u2212300 ), and heavy smoking was strongly associated with CHIP carrier status in both UKB (odds ratio = 1.17 [1.14\u20131.21], P = 7.32 \u00d7 10 \u221224 ) and GHS (odds ratio = 1.24 [1.10\u20131.41], P = 6.3 \u00d7 10 \u22124 ). Overall, our results suggest that the prevalence of CHIP doubles every 9\u201312 years of life. These associations with age and smoking were stronger when restricting to high-VAF (\u22650.1) CHIP carriers. In our multivariate modelling, women were significantly more likely to be CHIP mutation carriers than men in the UKB (odds ratio = 1.08 [1.05\u20131.11], P = 6.01 \u00d7 10 \u22127 ), but not in the GHS (odds ratio = 1.01 [0.93\u20131.11, P = 0.77]). These associations were consistent when restricting to high-VAF CHIP carriers, although the risk of high-VAF CHIP was not significantly greater in women in the UKB (odds ratio = 1.035 [0.99\u20131.08], P = 0.126). Table 1 Descriptive statistics for CHIP mutation carriers Full size table Genetic association with CHIP carrier status We first conducted genetic association analyses in the UKB cohort to identify germline loci associated with the risk of developing CHIP. In the common variant (minor allele frequency (MAF) > 0.5%) GWAS, which included 25,657 cases and 342,869 controls with European ancestry, we identified 24 loci (21 novel loci) harbouring 57 independently associated variants (Fig. 1 and Supplementary Table 2 ). To confirm these signals, we conducted a replication analysis in 9,523 CHIP cases and 105,502 controls of European ancestry from the GHS cohort. We estimated that we had sufficient statistical power in the GHS to detect 19.99 true and directionally consistent associations across lead SNPs from the 24 loci we identified in the UKB and achieved nominally significant ( P < 0.05) replication for 15 SNPs (Supplementary Table 2 ). We used conditional analysis and statistical fine-mapping to further evaluate the independence of our genome-wide associations and found results to be consistent across methods (Extended Data Fig. 3 , Supplementary Note 3 , Supplementary Tables 3 \u2013 6 and Supplementary Fig. 3 ). Fig. 1: GWAS of CHIP. Manhattan plot showing results from a genome-wide association analysis of CHIP. Twenty-four loci reach genome-wide significance ( P \u2264 5 \u00d7 10 \u22128 , dashed line), and top-associated variants per locus are labelled with biologically relevant genes. Three of these loci have been previously identified (black), whereas 21 represent novel associations (red). Loci with suggestive signal ( P \u2264 5 \u00d7 10 \u22127 ) are labelled in grey. Association models were run with age, age 2 , sex and age \u00d7 sex, and 10 ancestry-informative principal components as covariates. P -values are uncorrected and are from two-sided tests performed using approximate Firth logistic regression. Full size image We next sought to identify rare germline variants associated with CHIP. Since the CHIP phenotype is based on the presence of rare somatic variants in recurrently mutated genes, rare germline variants potentially misclassified as somatic can lead to false association signals. To address potential misclassification, we evaluated median VAF and association with age for each rare germline variant or gene burden associated with CHIP. We also conditioned these rare variant analyses on independent common variant signals to address confounding due to linkage disequilibrium (LD) (Supplementary Note 4 ). Ultimately, we identified a single rare germline frameshift variant in the CHEK2 gene that was significantly associated with CHIP (odds ratio = 2.22 [1.89\u20132.61], P = 8.04 \u00d7 10 \u221222 ; Supplementary Table 7 ), remained so after conditioning on common variant signals (odds ratio = 2.90 [1.93\u20134.34], P = 2.40 \u00d7 10 \u22127 ), and replicated in the GHS (odds ratio = 1.56 [1.19\u20132.04], P = 1.22 \u00d7 10 \u22123 ). The two cancer-associated genes ATM and CHEK2 were associated with an increased risk of CHIP via rare variant gene burden testing (Supplementary Table 8 ), and we also found a significant gene burden association between rare loss of function (and missense) variants in the telomere maintenance and DNA replication associated gene CTC1 and an increased risk of CHIP (odds ratio = 1.55 [1.32\u20131.81], P = 5.24 \u00d7 10 \u22128 ). Of these three gene burden associations, the ATM and CHEK2 signals were replicated in the GHS ( P = 8.22 \u00d7 10 \u22125 and P = 0.03, respectively), and VAF and age-association calculations suggested that all three of these gene burden signals were driven by germline variation. We also performed genome-wide association analyses in individuals of non-European ancestral background (Supplementary Note 5 and Supplementary Table 9 ). For each germline variant associated with CHIP and prioritized by clumping and thresholding, conditional analysis or fine-mapping (see Methods ), we queried its associations across 937 binary and quantitative health traits from the UKB for which we have previously performed genetic association analysis 10 (Supplementary Table 10 ). Overall, the traits with significant associations consisted predominantly of blood measures (that is, cells counts and biomarker levels), anthropometric measures related to body size, autoimmune phenotypes and respiratory measures. SNPs with the largest number of significant phenotypic associations included those at the HLA , TP53 , ZFP36L2 and THADA, CD164 and MYB loci (Extended Data Fig. 4 ). Whereas associations with blood cell counts and biomarker levels are probably the direct result of expansion of individual cell lineages in blood, association with autoimmune phenotypes could reflect the consequences of disrupted immune system differentiation related to clonal haematopoiesis. Analyses of individual CHIP gene mutations To identify CHIP subtype-specific risk variants, we defined gene-specific CHIP phenotypes for each of the eight most commonly mutated CHIP genes. For each subtype, we selected individuals with mutations in one of the eight genes and no mutations in any of the other genes used to define CHIP. We then conducted genetic association analyses comparing these single CHIP gene carriers to CHIP-free controls, with replication in the GHS, and observed shared, unique, and opposing effects of associated loci on CHIP subtypes, including 8 genome-wide significant loci that were not significant in our overall analysis of CHIP (Fig. 2a , Extended Data Fig. 5 and Supplementary Tables 11 \u2013 19 ). Fig. 2: Germline effect size comparisons across CHIP and Forest plots of PARP1 and LY75 missense variants. a , Using results from CHIP gene-specific association analyses, effect sizes of index SNPs are compared across CHIP subtypes. SNPs were chosen as those that were independent on the basis of clumping and thresholding (with some refinement based on our conditionally independent variant list) and genome-wide significant in at least one association with CHIP or a CHIP subtype. Certain loci showed notably different effects across CHIP subtypes, as seen at the CD164 locus, which was associated with DNMT3A CHIP and ASXL1 CHIP but not TET2 CHIP, and the TCL1A locus, which was associated with increased risk of DNMT3A CHIP but reduced risk of other CHIP subtypes (blue rectangles). b , Forest plots are shown reflecting the protective associations of a PARP1 missense variant (rs1136410-G) and two LY75 missense variants (rs78446341-A, rs147820690-T) with our DNMT3A CHIP phenotype in the UKB and GHS cohorts. Centre points represent odds ratios as estimated by approximate Firth logistic regression, with errors bars representing 95% confidence intervals. P -values are uncorrected and reflect two-sided tests. Numbers below the cases and controls columns represent counts of individuals with homozygote reference, heterozygote and homozygous alternative genotypes, respectively. Full size image DNMT3A , which was the most commonly mutated gene in the overall CHIP phenotype, had the largest number of significantly associated loci ( n = 23), most of which overlapped with the overall CHIP association signals. Six loci achieved genome-wide significance in our DNMT3A CHIP analysis that were not significant in our overall analysis ( RABIF , TSC22D2 , ABCC5 , MYB , FLT3 and TCL1A ; Extended Data Fig. 5 ). Although most loci harboured variants that increased CHIP risk, two exceptions are noteworthy (Fig. 2b ). At the PARP1 locus on chromosome 1, a tightly linked block of around 30 variants (29 in the 95% credible set from fine-mapping; Supplementary Table 6 ) with an alternate allele frequency (AAF) of 0.15 was associated with reduced risk of DNMT3A CHIP (odds ratio = 0.87 [0.84\u20130.90], P = 2.70 \u00d7 10 \u221217 ). PARP1 has a role in DNA damage repair, and many variants in this block have been identified across multiple transcriptomic studies of blood as PARP1 expression quantitative trait loci (eQTLs) that associate with reduced PARP1 gene expression 14 , 15 , 16 , 17 . Furthermore, a missense variant (rs1136410-G, V762A) that is predicted as likely to be damaging (combined annotation dependent depletion (CADD) score = 27.9) is a part of this LD block, and has recently been reported to associate with improved prognosis and survival in myelodysplastic syndromes 18 (MDS). At a locus on chromosome 2, rs78446341 (P1247L in LY75 ) was associated with reduced risk of DNMT3A CHIP (odds ratio = 0.78 [0.72\u20130.84], P = 3.70 \u00d7 10 \u221210 ), and was prioritized by fine-mapping (Extended Data Fig. 3 ). LY75 features lymphocyte-specific expression (Supplementary Fig. 4a ), and is thought to be involved in antigen presentation and lymphocyte proliferation 19 . We also identified a second rare (AAF = 0.002) missense variant (rs147820690-T, G525E) that associated with reduced risk of DNMT3A CHIP at close to genome-wide significance (odds ratio = 0.48 [0.36\u20130.63], P = 1.15 \u00d7 10 \u22127 ). This variant was predicted as likely to be damaging (CADD = 23.6) and remains associated (odds ratio = 0.63 [0.51\u20130.77], P = 4.80 \u00d7 10 \u22126 ) when conditioning on common variant signal in this locus (that is, this rare variant signal is independent of the common variant signal in this locus). This variant was also prioritized by fine-mapping (Extended Data Fig. 3 and Methods for jointly fine-mapping common and rare variants). Finally, these signals in PARP1 and LY75 replicated in the GHS (Fig. 2b ). Among loci associated with multiple CHIP subtypes (Supplementary Note 6 ), we observed genome-wide significant association signals at the TCL1A locus that were not present in the overall CHIP analysis. This locus is notable because it exhibited genome-wide significant effects in opposing directions across CHIP subtypes (Extended Data Figs. 2a and 5 and Supplementary Table 20 ), with lead SNPs (for example, rs2887399-T, rs11846938-G and rs2296311-A) at the locus associated with an increased risk of DNMT3A CHIP (odds ratio = 1.14 [1.11\u20131.17], P = 2.13 \u00d7 10 \u221220 ) but a reduced risk of TET2 CHIP (odds ratio = 0.75 [0.71\u20130.80], P = 9.14 \u00d7 10 \u221222 ) and ASXL1 CHIP (odds ratio = 0.70 [0.65\u20130.76], P = 8.59 \u00d7 10 \u221218 ). Effect estimates from the other five CHIP gene-specific association analyses were also consistent with protective effects. This is consistent with findings from a recent genetic association study of CHIP in the TOPMed cohort 11 , which identified a genome-wide significant positive association of the TCL1A locus and DNMT3A CHIP as well as a nominally significant opposing signal for TET2 CHIP. Additionally, the DNMT3A CHIP-increasing allele has been found to reduce the risk of mLOY in a recent GWAS 7 . This observation suggests that DNMT3A CHIP is distinct among clonal haematopoietic subtypes with regard to the genetic influence of the TCL1A locus, which may relate to the fact that TCL1A has been reported to directly interact with and inactivate DNMT3A 20 . CHIP and mosaic chromosomal alterations To evaluate the relationship between CHIP and other forms of somatic alterations of the blood, we used phenotype information on other types of clonal haematopoiesis that are available for UKB participants 4 , 7 , 8 , 12 . We first evaluated the phenotypic overlap between CHIP and mLOY, mLOX and autosomal mosaic chromosomal alterations (mCAaut). CHIP is distinct from mCA phenotypes (mCAaut, mLOX and mLOY), with more than 80% of CHIP carriers having no identified mCAs (Supplementary Fig. 4b ). Furthermore, having an mCA is not significantly associated with being a CHIP carrier after adjusting for age, sex and smoking status (odds ratio = 1.02, P = 0.27). Carriers of only a single clonal haematopoiesis driver (that is, CHIP, mLOY, mLOX or mCAaut) were younger on average than those with multiple clonal haematopoiesis lesions, and mCAaut and CHIP carriers were youngest among single clonal haematopoiesis phenotype carriers (Supplementary Fig. 4c ). We then conducted GWAS and ExWAS analyses of these somatic alteration phenotypes and evaluated the germline genetic contributions shared between CHIP and these traits (Supplementary Fig. 5 and Supplementary Tables 21 \u2013 27 ). Genome-wide genetic correlation ( r g ) 21 , 22 was nominally significant between CHIP and mLOY ( r g = 0.27, P = 0.014 (uncorrected); Supplementary Table 21 ). Notably, variants at 4 loci (marked by the genes ATM, LY75, CD164 and GSDMC ) showed similar associations with both CHIP and mLOY, whereas variants at the SETBP1 locus were negatively associated with CHIP and positively associated with mLOY. These comparisons suggest that despite being distinct clonal haematopoietic phenotypes, CHIP and mLOY share multiple germline genetic risk factors. Although the common variant association analyses of these other somatic alteration phenotypes were undertaken for the purpose of comparing to CHIP, and our results are consistent with recent published associations for these non-CHIP UKB somatic alteration phenotypes 4 , 7 , 8 , we also identified novel rare variant and gene burden associations via ExWAS analyses (Supplementary Note 7 , Supplementary Tables 22 \u2013 27 and Supplementary Fig. 6 ). We also extended our ExWAS analysis to telomere length and identified multiple novel rare variant associations (Supplementary Note 8 and Supplementary Tables 28 \u2013 30 ). Phenotypic associations with CHIP Clonal haematopoiesis has been associated with an increased risk of haematologic malignancy and CVD, as well as other health outcomes including all-cause mortality and susceptibility to infection 3 , 4 , 23 , 24 . To test for expected as well as potentially novel associations, we performed cross-sectional association analyses across 5,041 traits (2,640 binary and 2,401 quantitative traits) from the UKB, curated as part of our efforts for the UKB Exome Sequencing Consortium. We performed Firth penalized logistic regression using CHIP gene mutation carrier status (that is, whether an individual had a mutation in our callset within a specific CHIP gene) as the binary outcome for 22 of the 23 CHIP genes in our callset (counts were too low for CSF3R ; Methods ), with age, sex and ten genetic principal components as covariates. Our results are consistent with previous findings, with the majority of associated phenotypes deriving from cardiovascular, haematologic, neoplastic, infectious, renal and\/or smoking-related causes (Fig. 3 , Supplementary Fig. 7 and Supplementary Table 31 ). Fig. 3: Phenome association profiles per CHIP subtype. Profiles are shown for each CHIP gene subtype reflecting phenome-wide association results. The y -axis (concentric circles) represents the proportion of phenotypes within a trait category that were nominally associated ( P \u2264 0.05) with carrier status of the CHIP gene. A CHIP gene had to have at least one disease category with the proportion of associated phenotypes \u2265 0.2 to be included in the figure. As expected, haematological traits show the largest proportion of phenotypic trait associations overall. The largest number of cancer associations are seen for DNMT3A CHIP, whereas JAK2 CHIP shows the highest proportion of cardiovascular associations. Respiratory associations are most pronounced for ASXL1 CHIP. SUZ12 CHIP shows a unique profile across CHIP subtypes, with a higher proportion of ophthalmological and endocrine associations. Association models were run with age, age 2 , sex and age \u00d7 sex, and ten ancestry-informative principal components as covariates. Full size image ASXL1 CHIP was associated with the largest number and widest range of traits, and many of these associations traced to correlates of smoking. SUZ12 CHIP showed a distinct association profile amongst CHIP genes, with a larger proportion of associations in endocrine and ophthalmologic traits than other CHIP genes. Many traits showed associations with DNMT3A CHIP and TET2 CHIP that were in opposing directions, including white blood cell count, platelet count and neutrophil count, which were all positively associated with DNMT3A CHIP and negatively associated with TET2 CHIP. These results are consistent with functional differences in the haematopoietic phenotypes of DNMT3A - and TET2 -knockout mice 25 . Notably, body mass index (BMI) and fat percentage were negatively associated with DNMT3A CHIP and other leukaemogenic CHIP mutations (for example, JAK2 , CALR and MPL ), but are positively associated with other CHIP subtypes (for example, TET2 and ASXL1 ). We also observed significant associations between JAK2 mutations and gout, which may reflect the increased uric acid production that can accompany haematopoiesis 26 and\/or renal disease 27 , or even uric acid-independent associations identified between anaemia and gout 28 . Given recent reports that clonal haematopoiesis is associated with an increased risk of COVID-19 and other infections 4 , 29 , we also tested for an association between CHIP and COVID-19 infection in the UKB cohort 30 . When restricting to CHIP carriers with VAF \u2265 10% (Supplementary Note 9 ), we found that CHIP carrier status was significantly associated with COVID-19 hospitalization (odds ratio = 1.26 [1.07\u20131.47], P = 4.5 \u00d7 10 \u22123 ) and severe COVID-19 infection (odds ratio = 1.55 [1.19\u20131.99], P = 8.5 \u00d7 10 \u22124 ) in logistic regression models that excluded individuals with any previous blood cancers and that adjusted for age, sex, smoking, BMI, type 2 diabetes, active malignancy, and five genetic principal components. Analyses at the CHIP subtype level suggested that PPM1D carriers may be at elevated risk of severe COVID-19 (odds ratio = 5.42 [1.89\u201312.2], P = 2.8 \u00d7 10 \u22124 ; Supplementary Note 9 ). Longitudinal disease risk among CHIP carriers Given the confounding that can bias cross-sectional association analyses, we performed survival analyses to evaluate whether individuals with CHIP at the time of enrolment and blood sampling in the UKB were at an increased risk of subsequent CVD, cancer and all-cause mortality. To do this, we generated aggregate longitudinal phenotypes of CVD, lymphoid cancer, myeloid cancer, lung cancer, breast cancer, prostate cancer, colon cancer and overall survival (that is, any death). Because prior longitudinal studies of CHIP and the risk of many of these outcomes have focused on high-VAF CHIP, we focused on CHIP carriers with VAF \u2265 0.10 for these analyses. To complement these longitudinal analyses, we used Mendelian randomization to evaluate the relationship between CHIP and subsequent disease (Extended Data Fig. 6a , Supplementary Note 10 and Supplementary Table 32 ). We observed a significantly increased risk of CVD in CHIP carriers (hazard ratio = 1.11 [1.03\u20131.19], P = 4.2 \u00d7 10 \u22123 ), which was driven by TET2 CHIP (hazard ratio = 1.31 [1.14\u20131.51], P = 1.3 \u00d7 10 \u22124 ; Supplementary Fig. 8a ). However, this risk estimate is lower than the hazard ratio of 1.59 recently reported by Bick et al. 6 in an analysis of CHIP from the first 50,000 UKB participants (hereafter referred to as the 50k UKB subset) with exome sequencing data available. Therefore, we restricted our analysis to the 50,000 individuals from the previous study and found that the estimated hazard ratio is indeed higher in this subset (hazard ratio = 1.30 [1.06\u20131.59], P = 0.013; Supplementary Fig. 8b ). Bick et al. also observed a cardio-protective effect of IL6R rs2228145-C (a genetic proxy for IL-6 receptor inhibition) among CHIP carriers in the 50k UKB subset, so we repeated that analysis in both the 50k UKB subset and the full UKB cohort ( n = 430,924 in these analyses). We observed the same CHIP-specific protective IL6R effect in the 50k UKB subset as previously reported (hazard ratio = 0.60 [0.40\u20130.89], P = 0.012), however we did not find any IL6R effect in the full cohort (hazard ratio = 0.99 [0.91\u20131.07], P = 0.784, n = 430,924; Extended Data Fig. 7a\u2013d ). These results were consistent when varying which CHIP mutations we used to define CHIP case status, as well as when using different VAF thresholds and a variety of CVD endpoint composites ( Methods ). We did not find any association between CHIP and CVD, nor a CHIP-specific protective IL6R effect, when repeating this analysis in the GHS cohort (Supplementary Figs. 8d and 9a, b ). Furthermore, we did not find evidence for a casual association between CHIP and CVD when using a two-sample Mendelian randomization approach (Supplementary Note 10 , Supplementary Fig. 10 and Supplementary Table 32 ). We next tested whether CHIP carriers are at an increased risk of haematologic and solid cancers, and whether risk differed by CHIP mutational subtype for the three most common CHIP genes (that is, DNMT3A , TET2 and ASXL1 ; Extended Data Figs. 7 \u2013 9 and Supplementary Figs. 11 \u2013 14 ). To control for the possibility that toxic chemotherapeutic treatment for previous cancers might drive the development of CHIP mutations 31 and\/or otherwise confound association analyses, we performed all analyses after excluding individuals with any diagnoses of cancer prior to DNA collection. As expected, we found CHIP carriers with VAF \u2265 0.10 to be at a significantly elevated risk of developing any blood cancer (hazard ratio = 3.88 [3.46\u20134.36], P = 9.10 \u00d7 10 \u2212117 ; Supplementary Fig. 11a ), and we identified similarly elevated risk when replicating these analyses in the GHS (Supplementary Fig. 11d ). We also estimated the risk of CHIP on neoplastic myeloid subtypes, including acute myeloid leukaemia (AML), MDS and myeloproliferative neoplasms (MPN), and found that high-VAF CHIP carriers have more than 23-fold increased risk of acquiring an MPN (hazard ratio = 23.11 [17.63\u201330.29], P = 1.60 \u00d7 10 \u2212114 ) (Extended Data Fig. 8 ). As expected, we identified a significant association between myeloid leukaemia and CHIP by Mendelian randomization (Supplementary Note 10 , Supplementary Fig. 12 and Supplementary Table 32 ). We then tested whether CHIP carriers had an increased risk of developing solid tumours, and found that high-VAF carriers are at significantly increased risk of developing lung cancer (hazard ratio = 1.64 [1.42\u20131.90], P = 1.10 \u00d7 10 \u221211 ), and more modest increased risk of developing prostate cancer (hazard ratio = 1.18 [1.05\u20131.32], P = 5.30 \u00d7 10 \u22123 ) and non-melanoma skin cancer (hazard ratio = 1.14 [1.04\u20131.24], P = 4.7 \u00d7 10 \u22123 ; Fig. 4 and Supplementary Fig. 13 ). We also observed a non-significant increased risk of developing breast cancer (hazard ratio = 1.14 [0.99\u20131.31], P = 0.062) and no increase in risk for the development of colon cancer (hazard ratio = 0.95 [0.78\u20131.15], P = 0.59; Supplementary Fig. 13 ). Models estimating event risk on the basis of CHIP mutational subtype (for example, DNMT3A CHIP) suggest that these associations with prostate and breast cancer are driven primarily by DNMT3A mutations. Only the association with lung cancer was replicated in the GHS (Fig. 13e ), although sample sizes were limited for the analyses in the GHS owing to how the biobank data were ascertained ( Methods ). Fig. 4: Increased risk of lung cancer among CHIP carriers. a , Forest plot and table featuring hazard ratio estimates from Cox proportional hazard models of the risk lung cancer among CHIP carriers. Error bars represent a 95% confidence interval. Associations are similar across common CHIP subtypes, as well as among CHIP carriers with lower VAF (\u22652%). Models are adjusted for sex, low density lipoprotein, high density lipoprotein, smoking status, pack years, BMI, essential primary hypertension, type 2 diabetes mellitus, and 10 genetic principal components specific to a European ancestral background. HR, hazard ratio. UKB 450K, the 450,00-participant full UKB dataset. DNMT3A+ represents subjects with DNMT3A CHIP and at least one other type of CHIP mutation. b , Estimated associations via four Mendelian randomization methods between CHIP and lung cancer. Each point represents one of 29 instrumental variables (that is, conditionally independent SNPs) that were identified in the UKB cohort as associated with CHIP. The x -axis shows the effect estimate (beta) of the SNP on CHIP in the UKB cohort, and the y -axis shows the effect estimate (beta) of the SNP on lung cancer in the GHS cohort. The slope of each regression line represents the effect size estimated by respective methods. IVW, inverse variance weighted. Full size image Given the strong associations between CHIP and both blood and lung cancers, and the associations between smoking and both CHIP and lung cancer, we performed additional analyses stratified by smoking status to test whether these associations were driven by smoking and merely marked by CHIP mutations. Although smoking status is difficult to ascertain, we used an inclusive \u2018ever smoker\u2019 definition to minimize the likelihood that individuals labelled as non-smokers had engaged in any smoking ( Methods ). High-VAF CHIP carriers had an increased risk of developing blood cancers in both smokers (hazard ratio = 3.95 [3.25\u20134.78], P = 2.80 \u00d7 10 \u221244 ) and non-smokers (hazard ratio = 3.97 [3.43\u20134.58], P = 1.10 \u00d7 10 \u221277 ; Supplementary Fig. 14a, b ). Notably, lung cancer risk for high-VAF CHIP carriers was significantly elevated among both smokers (hazard ratio = 1.67 [1.41\u20131.97], P = 1.5 \u00d7 10 \u22129 ) and non-smokers (hazard ratio = 2.02 [1.53\u20132.67], P = 8.30 \u00d7 10 \u22127 ; Extended Data Fig. 9a,b ). These associations were driven by DNMT3A and ASXL1 CHIP carriers, with both estimated to have elevated lung cancer risk in both smokers and non-smokers. We replicated the association between CHIP carrier status and lung cancer in both smokers and non-smokers in the GHS (Extended Data Fig. 9c,d ). Overall, these models suggest that CHIP mutation carriers are at an elevated risk of both blood cancer and lung cancer, independent of smoking status. We also found support for a causal association between CHIP and lung cancer (inverse variance weighted odds ratio (OR IVW ) = 1.55 [1.34\u20131.80], P = 8.90 \u00d7 10 \u22129 ; Fig. 4 and Extended Data Table 1 ), as well as more modest support for causal associations between CHIP and melanoma (OR IVW = 1.39 [1.13\u20131.1.71], P = 0.0021), CHIP and non-melanoma skin cancer (OR IVW = 1.26 [1.13\u20131.41], P = 5.30 \u00d7 10 \u22125 ), CHIP and prostate cancer (OR IVW = 1.20 [1.03\u20131.1.39], P = 0.017), and CHIP and breast cancer (1.17 [1.04\u20131.31], P = 0.01), when performing Mendelian randomization (Extended Data Fig. 6a , Supplementary Note 10 and Supplementary Table 32 ). Although there is a concern that variants predisposing to CHIP via cancer-associated pathways (for example, telomere biology, DNA damage repair and cell cycle regulation) may confound these associations via horizontal pleiotropy, Egger-based Mendelian randomization methods that account for this bias by fitting a non-zero intercept provided additional support for these associations. Finally, the risk of death from any cause was significantly elevated among high-VAF CHIP carriers (hazard ratio = 1.27 [1.18\u20131.36], P = 2.70 \u00d7 10 \u221211 ), and was similar across DNMT3A , TET2 and ASXL1 CHIP subtypes (Extended Data Fig. 6b ). In this study, we present the largest assessment to date of individuals with CHIP mutation carrier information, as well as the use of these calls to identify novel common and rare variant loci associated with CHIP and CHIP subtypes. These loci, which have shared, unique and opposing effects on the risk of developing different types of CHIP and other somatic alterations of the blood, highlight the fact that germline variants can predispose to clonal expansions, and that CHIP encapsulates a complex set of heterogeneous phenotypes. We further show that the genetic aetiology of CHIP is reflected in its clinical consequences, as the risk of various clinical conditions is differentially associated across CHIP gene mutations. The new loci identified in this study provide a foundation on which to investigate the biological mechanisms that lead to specific features of CHIP. For example, among CHIP-associated loci, variants in the TCL1A locus that are associated with an increase in the risk of DNMT3A CHIP have the opposite effect on the risk of all other CHIP and clonal haematopoiesis subtypes. Coupled with recent findings that link the role of TCL1A in mLOY to lymphocytes 7 (for example, B cells), our results further suggest TCL1A as a critical mediator of clonal haematopoiesis as well as clonal haematopoiesis subtype-specific differences. Several novel loci associated with DNMT3A CHIP harbour genes that are potential targets for the development of new treatments to prevent or slow the expansion of CHIP clones. Both PARP1 and LY75 contain missense variants associated with reduced risk of CHIP and of DNMT3A CHIP specifically. The variants in the PARP1 locus are significantly associated with reduced PARP1 gene expression in whole blood 32 ( P \u2264 1 \u00d7 10 \u221213 ), and the V762A missense variant (rs1136410-G) has been recently reported to associate with improved prognosis and survival in MDS 18 . Given the well-established role of PARP1 in DNA repair 33 , and that a recent CRISPR screen study in zebrafish identified PARP1 inhibition as a selective killer of TET2 mutant haematopoietic stem cells 34 , it seems plausible that a therapeutic strategy that inhibits PARP1 might be viable for the antagonization of CHIP clone expansion. Furthermore, PARP1 -inhibiting drugs are already approved for use in the treatment of BRCA-mutant cancers 35 . Conversely, PARP1 inhibition is known to cause haematologic toxicity and to increase the risk of treatment related haematologic malignancy 36 . Therefore, further research is needed to test whether PARP1 inhibition may be appropriate for use in antagonizing the expansion of CHIP clones, and whether any effect is clonal haematopoiesis subtype-specific. The more common LY75 missense variant (rs78446341-A, P1247L) is located in the extracellular domain of lymphocytic antigen 75 (also known as DEC-205 or CD205), and has a role in antigenic capture, processing and presentation 37 . The rarer LY75 missense variant (rs147820690-T, G525E) is located in a C-type lectin domain and reported to interact directly with this receptor\u2019s ligand. LY75 is expressed predominantly in haematopoietic-derived cells 37 , 38 (and particularly dendritic cells), and its ablation impairs T cell proliferation and response to antigen challenge 19 . The protective associations with this variant that we identified appear to be most pronounced for DNMT3A CHIP and mLOY, and highlight LY75 as a potential therapeutic target for the antagonization of clonal haematopoiesis in general. Although most of the phenotypic associations we observe in our cross-sectional analyses are expected associations with haematologic and oncologic traits, the associations we identify with obesity and body mass traits are of particular interest. This relationship between body mass and CHIP may relate to inflammatory or hormonal signalling, and directions of effect that we estimate are consistent with recent findings that DNMT3A CHIP reduces bone mineral density via increases in macrophage-mediated IL-20 signalling 39 . The fact that the association we report between obesity and body mass and CHIP are in opposing directions across CHIP subtypes (for example, negative in DNMT3A CHIP and positive in TET2 CHIP and ASXL1 CHIP) suggests that the relationship between CHIP and adiposity is complex and requires further investigation. Perhaps most unexpectedly, we found associations between CHIP and CVD to be more modest than previously reported 1 , 2 , 3 . DNMT3A mutations do not associate with CVD, which is consistent with the absence of any association between CHIP and CVD when applying Mendelian randomization. However, this pattern is not seen across CHIP associations with solid tumours, which we found to be driven by DNMT3A , and to be supported by Mendelian randomization. Overall, our results further clarify the role of CHIP mutational subtypes in the development of cancer and CVD and emphasize the importance of viewing (and potentially treating) different CHIP subtypes as distinct haematologic preconditions. Whereas Bick et al. 6 . found statistical support for reduced CVD incidence among CHIP carriers with an IL6R coding mutation (rs2228145-C) serving as a genetic proxy for IL-6 inhibition, we do not find any support for this association when extending their analysis from the first 50,000 exomes in the UKB to the full cohort of 450,000 exomes, nor when repeating this analysis in 175,000 exomes from the GHS cohort. The signal identified across the first 50,000 exomes may result from a chance ascertainment bias 40 . Alternatively, whereas the rs2228145-C variant is thought to mimic IL-6 inhibition, and therefore confer protection from heart disease 41 , neither our analysis nor Bick et al. found evidence that rs2228145 carriers are protected from CVD in subjects without CHIP. Therefore, it is possible that this mutation is a poor proxy for IL-6 inhibition, and that direct pharmacological inhibition of IL-6 may still antagonize the interplay between CHIP clone expansion and the onset of CVD. This study benefits from its biobank-scale size, which we leverage to further resolve clonal haematopoiesis subtypes and broadly assess clinical phenotypes associated with CHIP. However, limitations include the potential inclusion in our CHIP callset of a small number of germline variants, a lack of serial sampling, and a lack of experimental data to characterize the mechanisms underpinning the novel associations that we identify. Although we have taken many steps to ensure the quality of our callset and analysis (Supplementary Notes 11 and 12 and Supplementary Figs. 15 \u2013 18 ), the misclassification of somatic variants with high VAF as germline variants, and\/or the misclassification of true germline variants as somatic clonal haematopoiesis variants (for example, germline variants at genomic positions identified as clonal haematopoiesis hotspots) remain challenges inherent to calling and analysing CHIP and clonal haematopoiesis when using population scale genomic data. Serial sampling would enable the evaluation of changes to CHIP clones over time, and future studies that focus on such serial analysis at large scale will be able to better estimate CHIP subtype-specific clonal changes and clinical risk. Such increased data assets would also likely facilitate the identification of additional genes that show recurrent mutation during clonal haematopoiesis, as well as how such mutations relate to one another (that is, in dependency, mutual exclusivity and temporal order). Nonetheless, we identify many novel common and rare variant associations with CHIP and other clonal haematopoiesis phenotypes, which help to set the stage for future functional, mechanistic and therapeutic studies. On the whole, our analyses emphasize that CHIP is really a composite of somatic mutation-driven subtypes, with shared genetic aetiology and distinct risk profiles. Methods Study approval UKB study: ethical approval for the UKB study was previously obtained from the North West Centre for Research Ethics Committee (11\/NW\/0382). The work described herein was approved by UKB under application number 26041. GHS study: approval for DiscovEHR analyses was provided by the Geisinger Health System Institutional Review Board under project number 2006-0258. Exome sequencing and variant calling Sample preparation and sequencing were done at the Regeneron Genetics Center as previously described 10 , 40 . In brief, sequencing libraries were prepared using genomic DNA samples from the UKB, followed by multiplexed exome capture and sequencing. Sequencing was performed on the Illumina NovaSeq 6000 platform using S2 (first 50,000 samples) or S4 (all other samples) flow cells. Read mapping, variant calling and quality control were done according to the Seal Point Balinese (SPB) protocol 40 , which included the mapping of reads to the hg38 reference genome with BWA MEM, the identification of small variants with WeCall, and the use of GLnexus to aggregate these files into joint-genotyped, multi-sample VCF files. While certain UKB exome analysis efforts have used calls generated with the OQFE pipeline 42 , this pipeline has only been used to a limited degree for disease association analysis. Therefore, we chose to use calls from the SBP pipeline, which have been used very extensively for disease association analysis, including the largest set of association analyses done with UKB exome data 10 . Depth and allelic valance filters were then applied, and samples were filtered out if they showed disagreement between genetically determined and reported sex, high rates of heterozygosity or contamination (estimated with the VerifyBamId tool as a FREEMIX score > 5%), low sequence coverage, or genetically determined sample duplication. Calling CHIP To call CHIP carrier status, we first used the Mutect2 (GATK v4.1.4.0) somatic caller 43 to generate a raw callset of somatic mutations across all individuals. This software aims to use mapping quality measures as well as allele frequency information to identify somatic mutations against a background of germline mutations and sequencing errors. We used data generated from gnomAD v2 as the reference source for germline allele frequency 44 . We generated a cohort-specific panel of normals, which Mutect2 uses to estimate per-site beta distribution parameters for use in refining somatic likelihood assignment. Since CHIP is strongly associated with age, we chose 100 random UKB samples from 40 year olds and 622 samples from individuals less than 18 years of age in GHS to build these cohort-specific panels of normals. By evaluating the degree to which default Mutect2 filtering excluded known CHIP hotspot mutations, we noted that the default Mutect2 pass\/fail filters were too stringent. Therefore, we initially considered all Mutect2 variants (that is, even those that did not pass default Mutect2 filtering), and proceeded to perform our own QC and somatic mutation call refinement. As an initial refinement step, we selected variants occurring within genes that have been recurrently associated with CHIP according to recent reports from the Broad 2 , the TOPMed Consortium 11 , and the Integrative Cancer Genomics (IntOGen) project 45 . We then filtered putative somatic mutations using the outlined functional criteria 2 . Next, we performed additional QC steps, which consisted of (1) removing multi-allelic somatic calls, (2) applying sequencing depth filters (total depth (DP) \u2265 20; alternate allele depth (AD) \u2265 3, F1R2 and F2R1 read pair depth \u2265 1), (3) removing sites flagged as panel of normals by Mutect2 (unless previously reported), (4) removing indels flagged by the Mutect2 position filter, (5) removing sites within homopolymer runs (a sequence of \u22655 identical bases) if AD < 10 or VAF < 0.08, (6), removing missense mutations in CBL or TET2 inconsistent with somaticism (that is, P -value > 0.001 in a binomial test of VAF = 0.5), (7) removing novel (not previously reported) variants that exhibited characteristics consistent with germline variants or sequencing errors. That is, we excluded variants that had a median VAF \u2265 0.35, since approximately 97% of previously reported variants (that is, from a recent study of CHIP by the TOPMed consortium 11 ) had a median VAF < 0.35. Beyond this, we evaluated the frequency distributions of known variants (stratified by effect\u2014that is, missense or non-missense) to discern thresholds for newly identified variants (that is, AF (allele frequency) of novel variants \u2264 AF of previously reported variants). Additionally, novel G>T or C>A SNV calls were evaluated for oxidation artifacts 46 . Specifically, variants with a maximum alternate allelic depth < 6 (across all samples) and < 2 supportive reads from F1R2 (C>A) or F2R1 (G>T) mate pairs were removed, respectively. Given that > 90% of mutations belonged to 23 recurrent CHIP-associated genes, we restricted to variants occurring within these genes as a final step to maximize the specificity of our callset. These genes consisted of the 8 most frequent mutated CHIP genes ( DNMT3A , TET2 , ASXL1 , PPM1D , TP53 , JAK2 , SRSF2 and SF3B1 ), a collection of CHIP-associated genes containing SNV hotspots ( BRAF , CSF3R , ETNK1 , GNAS , KRAS , GNB1 , IDH2 , MPL , NRAS , PHF6 and PRPF8 ), and CHIP-associated genes of haematological interest ( CBL , CALR , RUNX1 and SUZ12 ). Our final CHIP set of CHIP mutation carriers consisted of 29,669 CHIP mutations across 27,331 unique individuals from UKB, and 14,766 CHIP mutations across 12,877 unique individuals from GHS. Variant allele fraction (VAF) was calculated using AD\/(reference allele depth (RD) + AD). Defining CHIP and mosaic phenotypes CHIP phenotypes were derived based on our mutation callset, whereas mosaic chromosomal alteration (mCA) phenotypes were derived based on previously published mCA calls from the UKB 4 , 7 , 8 . First, we used International Classification of Diseases (ICD) codes to exclude 3,596 samples from UKB and 1,222 samples from GHS that had a diagnosis of blood cancer prior to sample collection. We also excluded 13,004 individuals from GHS whose DNA samples were collected from saliva as opposed to blood. For all of the phenotypes we generated and analysed in this study, we used a combination of cancer registry data, hospital inpatient (HESIN) data, and data from general practitioner records to ascertain ICD10 codes. The majority of our cancer data came from the cancer registry, which we supplemented with the other sources. We then defined multiple CHIP and mosaic phenotypes based on whether carriers did (inclusive) or did not (exclusive) have other somatic phenotypes. For example, individuals with at least one CHIP mutation in our callset were defined as carriers for a CHIP_inclusive phenotype, whereas anyone with a CHIP mutation as well as an identified mCA was removed from this inclusive phenotype in order to define a CHIP_exclusive phenotype (20,606 cases and 342,869 controls). Our association analysis with CHIP used this CHIP_inclusive phenotype, which included 25,657 cases and 342,869 controls of European ancestry in UKB, and 11,821 cases and 135,106 controls of European ancestry in GHS. These counts reflect the samples with European ancestral origin that remain in each cohort after removing those with non-CHIP clonal haematopoiesis (60,991 in UKB and 0 in GHS, as we did not call mosaic chromosomal alterations in GHS), and those with missing meta data (348 in UKB and 4,893 in GHS). We defined mLOY carriers as male individuals with a Y chromosome mCA in the UKB mCA callset that had copy change status of loss or unknown, mLOX as individuals with an X chromosome mCA in the UKB mCA callset that had copy change status of loss or unknown, and mCAaut carriers as individuals with autosomal mCAs. We then refined these inclusive phenotypes to define exclusive versions, with mLOY_exclusive consisting of carriers with no X chromosome or autosomal mCAs (36,187 cases and 151,161 controls), mLOX_exclusive consisting of carriers with no Y chromosome or autosomal mCAs (10,743 cases and 364,072 controls), and mCAaut_exclusive consisting of carriers with no Y or X chromosomal alterations of any kind (11,154 cases and 364,072 controls). These exclusive phenotypes were used for all analyses comparing CHIP with mosaic phenotypes, as this approach facilitated the generation of four non-overlapping phenotypes (that is, CHIP, mLOY, mLOX, and mCAaut) that could be compared. We also defined CHIP gene-specific phenotypes by choosing carriers as those with mutations in our callset from a specific gene and no mutations in any other of the 23 CHIP genes defining our callset. For example, CHIP DNMT3A carriers were those with \u2265 1 somatic mutations in our callset within the DNMT3A gene, and no mutations in our callset in any of the other 23 CHIP genes we used for our final callset definition. The set of 364,072 controls used in UKB that had no evidence of any clonal haematopoiesis (that is, no CHIP or mCAs) was considered as our set of healthy controls, and was used across all association analyses in UKB. Genetic association analyses To perform genetic association analyses, we used the genome-wide regression approach implemented in REGENIE 47 , as described 10 . In brief, regressions were run separately for data derived from exome sequencing as well as data derived from genetic imputation using TOPMed 48 , and results were combined across these data sources for downstream analysis. Step 1 of REGENIE uses genetic data to predict individual values for the trait of interest (that is, a polygenic risk score), which is then used as a covariate in step 2 to adjust for population structure and other potential confounding. For step 1, we used variants from array data with a MAF > 1%, < 10% missingness, Hardy\u2013Weinberg equilibrium test P -value > 10 \u221215 and LD pruning (1,000 variant windows, 100 variant sliding windows and r 2 < 0.9), and excluded any variants with high inter-chromosomal LD, in the major histocompatibility region, or in regions of low complexity. For association analyses in step 2 of REGENIE, we used age, age 2 , sex and age \u00d7 sex, and 10 ancestry-informative principal components as covariates. For analyses involving exome data, we also included as covariates an indicator variable representing exome sequencing batch, and 20 principal components derived from the analysis of rare exomic variants (MAF between 2.6 \u00d7 10 \u22125 and 0.01). Significance cutoffs and rare variant burden testing were set according to the power calculations and logic outlined by Backman et al. 10 . In brief, we used P \u2264 5 \u00d7 10 \u22128 , P \u2264 7.14 \u00d7 10 \u221210 , P \u2264 3.6 \u00d7 10 \u22127 , for common, rare and burden associations, respectively. Results were visualized and processed using an in-house version of the FUMA software 49 . Association analyses were performed separately for different continental ancestries defined based on the array data, as described 10 . Replication of associations signals in the GHS cohort To calculate the power to achieve replication in the GHS cohort, we first adjusted for the effects of \u2018winner\u2019s curse\u2019, which are expected when choosing significant associations signals on the basis of a genome-wide threshold 50 . To do this, we used the conditional likelihood approach described by Ghosh et al. 51 as implemented in the winnerscurse R package (version 0.1.1), which adjusts the estimated betas from genome-wide significant associations signals. These adjusted effect estimates are provided in Supplementary Table 2 (column Effect_adj). We then used these adjusted effect estimates to calculate the expected power to detect each lead signal in the GHS replication phase using the GHS sample size, allele frequencies, CHIP prevalence, and an alpha level of 0.05. To summarize our expected power across the replication phase, we summed the power across all lead variants and reported the number of SNPs that replicated at P < 0.05 as a proportion of the cumulative power to detect those variants. Identifying independent signals from association results We used three different approaches to identify independent signals across loci that associated with CHIP. First, we used a clumping and thresholding approach (C&T) 52 in which index SNPs at each significantly associated locus were defined greedily as those with the lowest P -value. Clumping was then done by extending linkage blocks laterally to include all SNPs that have P < 1 \u00d7 10 \u22125 and r 2 > 0.1 with the index SNP. Any SNP within a clump was then removed from further analysis. This process was repeated as long as there was \u2265 1 additional SNP in the locus with P \u2264 5 \u00d7 10 \u22128 . After all clumps were made, we merged any clumps (that is, LD blocks) with overlapping genomic ranges. Since this approach did not feature any iterative conditioning nor model variant effects jointly, we also used conditional joint analysis as implemented in GCTA COJO 53 and statistical fine-mapping as implemented in FINEMAP 54 to identify independent\/causal signals. COJO was run with a subset of 10,000 unrelated European ancestry samples from UKB as an LD references, and with a COJO adjusted P -value threshold of 5 \u00d7 10 \u22126 , an info score threshold of 0.3, and a MAF cutoff of 0.01. FINEMAP was run with the shotgun stochastic search algorithm using a maximum of 30 causal variants. We included variants in the FINEMAP analysis that had P < 0.1 in inverse variance weighted meta-analysis, and MAF > 0.001. The LD matrices used for the FINEMAP analysis were constructed as weighted meta LD matrices derived from the LD matrices from UKB and GHS. The LD matrices from UKB and GHS were computed independently using the same sets of samples included in each GWAS. Fine-mapping variants at the LY75 locus To further evaluate whether the rare variant association at the LY75 locus (rs147820690-T) was independent of other common and rare variant signals, we performed joint fine-mapping (with FINEMAP) on common and rare variants at this locus while including rarer variants then used in our genome-wide fine-mapping. In contrast to the genome-wide fine-mapping described above, this fine-mapping sensitivity analysis was done only in the UKB, was focused on the LY75 locus, and included all variants in our dataset. That is, the fine-mapping analysis was run as described above, but with a MAF > 0.0000000001. While FINEMAP suggests 3 credible sets are most parsimonious at this locus (posterior probability = 0.8), which is consistent with the results we report when preforming genome-wide fine-mapping, the fourth credible set (posterior probability = 0.11) identifies rs147820690-T as the top signal (PIP = 0.133) among 9,417 variants in the 95% credible set. This fine-mapping approach also prioritizes rs78446341-A (CPIP = 0.92, CS = 2). Furthermore, the median pairwise LD between SNPs in this fourth credible set is very low (6.7 \u00d7 10 \u22124 , compared with 0.995, 0.962, and 0.831 for the first three credible sets, respectively). Therefore, these fine-mapping results provide additional support for both LY75 missense variants, as well as the fact that the rs147820690-T rare variant signal is not driven by the tagging of other rare variants. PheWAS across CHIP-associated variants Using 937 traits from the UKB, we queried association results for 171 SNPs from our GWAS of CHIP. These SNPs represent the union of those identified by clumping and thresholding, conditional analysis with GCTA COJO, and fine-mapping with FINEMAP (fine-mapped SNPs were chosen if they had one of the highest two posterior inclusion probabilities\u2014that is, PIPs\u2014in any credible set). While this group of SNPs does include signals with P < 5 \u00d7 10 \u22128 in our CHIP GWAS, these SNPs represent signals prioritized as conditionally independent and\/or likely to be causal, and we therefore deemed them worthy of exploration via PheWAS. Some of these subthreshold signals featured many significant PheWAS associations ( P < 5 \u00d7 10 \u22128 in the PheWAS), and likely merit further evaluation (for example, ZFP36L2 \/ THADA locus on chromosome 2, and THRB locus on chromosome 3). The traits used in this PheWAS represent the subset of the 5,041 traits used in our cross-sectional analyses of phenotypic association with CHIP mutations carrier status for which we have previously reported common variant associations 10 . In brief, for ICD10-based phenotypes, cases were required to have one or more records of diagnosis in the electronic health records, death registry data implicating the disease, or two or more diagnosis in outpatient data mapped to ICD10. For non-ICD10 phenotypes (quantitative measures, clinical outcomes, survey and touchscreen responses, and imaging derived phenotypes), data were derived from the UKB Showcase. Participants who did not meet the case definition for a given ICD10-based phenotype were removed from the analysis if they had one diagnosis code in the outpatient data, and included as controls if they had no diagnosis in the outpatient data. Supplementary Table 10 includes ICD10 codes as well as trait names and descriptions. Genetic comparisons between CHIP subtypes For pairwise comparisons between CHIP gene mutation subtypes, we used the union set of index SNPs (that is, independent signals in genome-wide significant loci) from all of our CHIP and CHIP gene subtype associations. This resulted in 93 variants, which we used to compare effect sizes estimates between CHIP subtype pairs. Genetic correlations were calculated using LDSC version 1.0.1 with annotation input version 2.2 22 . Defining smoking phenotypes We derived smoking phenotypes from the lifestyle and environment questionnaire in the UKB and from the electronic health records in the GHS. Since smoking is difficult to ascertain and control for, we used a variety of data to code multiple smoking phenotypes for various analyses. These smoking phenotypes consisted of (1) pack years, (2) number of cigarettes smoked per day, (3) age started\/stopped smoking (UKB only), (4) former\/current smoker, (5) ever smoker and (6) heavy smoker (smoked \u2265 10 cigarettes a day). The ever smoker phenotype was maximally inclusive, and coded as cases all individuals with any evidence of prior smoking across the aforementioned phenotypes. For our longitudinal analyses in UKB, we used the \u2018current smoker\u2019 and \u2018pack years\u2019 (which captures the cumulative effect of smoking over one\u2019s lifetime) as covariates in all models that did not stratify for smoking status. In the smoking stratified models, we stratified smokers based on the \u2018ever smoker\u2019 phenotype and further adjusted for pack years within the smokers subgroup. For our longitudinal analyses in GHS, we used the \u2018ever smoker\u2019 and \u2018pack years\u2019 phenotypes as covariates in all models that did not stratify for smoking status, and stratified smokers in the same manner as we did in the UKB analyses. For linear models that evaluated the overall relationship between age, sex, and smoking, we used the \u2018heavy smoker\u2019 coding. Otherwise, all other analyses used the aforementioned \u2018ever smoker\u2019 phenotype as a covariate. Phenotypic associations with CHIP To test for known as well as potentially novel associations, we used REGENIE 47 to perform Firth-corrected tests for association between our CHIP gene-specific phenotypes and 5,041 traits (2,640 binary traits and 2,401 quantitative traits) from the UKB (version 5). To do this, we coded each CHIP gene-specific phenotype as 1 if an individual had any somatic CHIP mutation in the gene and 0 otherwise and formatted these binary codings as pseudo-genotypes to analyse with REGENIE. Regression models were run as described previously, with age, sex, and genetic principal components as covariates 10 . After filtering out association tests where the total number of somatic carriers was <5, we were left with 83,779 total association tests (Supplementary Table 31 ). Only 22 out of 23 CHIP gene subtypes were tested for association across phenotypes as we did not have enough carriers of CSF3R mutations to meet our minimum threshold of 5 somatic carriers that were also disease cases. Quantitative traits were transformed using a reverse inverse normalized transformation (RINT); effect size estimates from these associations are in units of standard deviation. Traits used in this analysis did not exclude any samples on the basis of having a diagnosed haematological disease or malignancy prior to sequencing date. To visualize high-level phenotypic patterns across these CHIP gene-specific phenotypes (Fig. 3 ), we categorized phenotypes by disease group 10 , and calculated the proportion of phenotypes per disease group per gene that were associated at a P \u2264 0.05 alpha level (uncorrected). To visualize the most significant of these associations, we plotted effect sizes (Supplementary Fig. 7 ) by disease category for all associations with P \u2264 1 \u00d7 10 \u22125 . Risk modelling among CHIP carriers We performed longitudinal survival analyses using cox proportional hazard models (coxph function) as implemented in the survival R package. Given that CHIP is strongly correlated with age, models used age as the time scale with interval censoring with age at first assessment and age at event or censoring. This allows for an implicit adjustment for age within the proportional hazard models. In UKB, individuals with follow-up time in excess of 13.5 years (3% of the dataset) were censored due to departures from the proportional hazards model. Analyses were performed on individuals of European ancestral background. All models included 10 genetically determined European-specific principal components as covariates, and all analyses excluded individuals genetically determined to be third-degree relatives or closer. In GHS, we had limited sample size with which to perform these longitudinal analyses. This was because GHS samples were collected at later ages (due to the nature of the biobank and the timing of our partnership) and fewer patients had disease onset dates subsequent to sample collection (that is, the time period where the onset of CHIP can be evaluated). Furthermore, in GHS, we could not derive an all-cause mortality phenotype due to the nature of the EHR data available to us. This incomplete ascertainment may also explain why our odds ratio estimates for risk of haematologic malignancy among CHIP carriers are lower in the GHS cohort. We used a variety of CHIP codings as variables in our models to test for potential differences between high\/low VAF CHIP and\/or CHIP subtypes. First, we subset CHIP carrier status by gene ( DNMT3A , TET2 , ASXL1 , DNMT3A or TET2) and\/or VAF (\u22650.1) to test for potential differences between degree of clonal expansion (that is, high\/low VAF CHIP) and\/or CHIP subtypes. Additional analyses were run restricting CHIP mutation calls to previously reported variants (for example, Jaiswal et al. 2 ), as well as restricting to carriers of DNMT3A mutations with at least one mutation in another CHIP gene. Controls were defined with two approaches: (1) any individual without CHIP mutations (the coding used in the results we report) and (2) those without any genetic evidence of clonal haematopoiesis (that is, healthy controls, as defined above, which did not change our results). The CHIP gene-specific coding described above varies from the phenotypic coding definitions used in our GWAS\/ExWAS, which required carriers to have mutations only in the specified CHIP gene and no mutations in any other CHIP genes. Since mutational exclusivity becomes less common as VAF increases (that is, carrying a single mutation with VAF \u2265 0.1 and no other mutations), and substantially lowers sample size, we chose this adjusted definition for these longitudinal analyses of disease incidence. For the composite phenotypes described below, we relied heavily on ICD10 codes from cancer registry data, hospital records and general practitioner records, and supplemented these with self-reported data and procedure codes (OPCS4). We defined prevalent disease on the basis of event codes occurring before sample collection and used this definition to exclude samples from longitudinal analysis of incident disease. For these main analyses, we did not use any minimum number of days to diagnosis from sample collection as an additional filtering criterion (see Supplementary Note 12 for more details). In UKB, cardiovascular disease was defined with the following ICD10 codes obtained from primary care, HES (hospital episode statistics), or death registry data: I21, I22, I23, I252, I256, Z951, Z955, I248, I249, I241, I251, I255, I258, I259, I630, I631, I632, I633, I634, I635, I637, I638, I639, I651, ICD9 codes: 410, 412, and OPCS codes: K40, K41, K44, K45, K46, K49, K502, K75 and K471. ICD9\/ICD10\/OPCS diagnoses or procedures recorded prior to enrolment date and self-report codes 1075 (heart attack\/myocardial infarction), 1095 (cabg), 1523 (heart bypass), 1070 (coronary angioplasty or stent), 1583 (ischaemic stroke), 1083 (stroke) were used to identify prevalent CVD cases. These were chosen to best reflect the coding use by Bick et al. in their study of CHIP 6 . In GHS, we used ICD10 codes I20\u2013I25 and I60\u2013I69, CPT codes from 33510\u201333523 (CABG, not continuous), 33533\u201333536, 35500, 35572, 35600, and 92920\u201392975 (PCI, not continuous). We also adjusted the CVD coding in GHS to exclude cerebrovascular events (that is, excluded I60\u2013I69); association results were similar. The CVD coding we used for our Mendelian randomization analysis was comparable to these definitions but did not include ICD10 codes for cerebrovascular events. For the CVD models, we included sex, LDL, HDL, pack years, smoking status (current vs former, determined by self-reported data), BMI, essential primary hypertension, and type 2 diabetes mellitus as covariates. The results we reported used a composite of myocardial infarction (MI), coronary artery bypass graft (CABG), percutaneous coronary intervention (PCI), and coronary artery disease (CAD), based on the coding described above, and also included death from any of these events. Results were similar when our composite included ischaemic stroke (ISCH.TR), as well as when we repeated analyses with a subset of recurrent CHIP mutations derived from Jaiswal et al. 2 or restricting carrier calls to variants in DNMT3A or TET2 . We also excluded samples with any diagnosis of malignant blood cancer prior to sequencing ( n = 3,596). Missing LDL and HDL values were median imputed, and individuals on cholesterol medication had their raw LDL values increased by a factor of 1\/0.68, similar to Bick et al. 6 . IL6R missense variant (rs2228145-C) genotypes were modelled dominantly (coded as 1 for carriers of any allele and 0 otherwise), and we modelled the effect of this allele in CHIP -stratified proportional hazard models, and also tested for IL6R \u00d7 CHIP interaction in a full (non-stratified) model. Models considering only the initial 50k UKB individuals restricted to intersection between our unrelated UKB sample set and the samples reported by Bick et al. 6 . For visualization, Kaplan\u2013Meier estimates were generated with the survfit function in the aforementioned survival package (version 3.2.13) and plotted using the ggsurvplot function from the survminer package (version 0.4.9). For models of cancers and overall survival risk tested using all CHIP carriers, high-VAF (VAF \u2265 0.1) CHIP carriers, and carriers of specific CHIP gene mutations, we used unrelated European samples that did not have any cancer diagnoses prior to sample collection (N = 360, 051 after the removal of 33,816 samples with a prior diagnosis of cancer). Results were qualitatively the same when repeating these analyses without excluding samples that had a diagnosis of any malignant cancer prior to sample collection date. Cancer phenotype definitions were derived from medical records indicating the following ICD10 codes: C81\u2013C96, D46, D47.1, D47.3, D47.4 for blood cancers, C81\u2013C86, C91 for lymphoid cancers, C92, C94.4, C94.6, D45, D46, D47.1, D47.3, D47.4 for myeloid cancers, C50 for breast cancers, C34 for lung cancers, C61 for prostate cancers, C44 for non-melanoma skin cancers (NMSC), and C18 for colon cancers (five total solid cancers). Myeloid subtypes were defined as follow: AML (C92), MDS (D46), MPN (D47.1, D47.3, D47.4). Given the rareness and\/or non-specificity of myeloid codings C93\u201395, and that the majority of these codings overlapped with those that we used for the myeloid composite described above (that is, we already captured these samples using the previously described codings), we did not include these codings in our composite. However, we performed sensitivity analyses that used a myeloid definition that did include C93\u2013C95, with findings equivalent to those described in our main results (Supplementary Note 12 ). For our lymphoid composite, we decided to combine lymphoma with lymphoid leukaemia for multiple reasons. First, in some clinical diagnostic situations (for example, T cell lymphoblastic lymphoma and T cell lymphoblastic leukaemia; Burkitt lymphoma and mature B cell ALL), the distinction between \u2018leukaemia\u2019 and \u2018lymphoma\u2019 is made on the basis of blast percentage in bone marrow (that is, > 20% blasts diagnosed as leukaemia), and may not reflect meaningful biological differences. Consistently, 22% of C91 codings are already captured in our C81\u2013C86 codings. Moreover, the majority of cases across these codings correspond to tumours derived from mature B cells, namely chronic lymphocytic leukaemia (CLL) and mature non-Hodgkin lymphoma. Given data supporting that mature T cell lymphomas and also some mature non-Hodgkin B cell tumours may arise from hematopoietic stem and progenitor cells 55 , 56 , 57 , we considered the relationship between a composite of mature lymphoid tumours and CHIP. For blood cancers, we also included cases that self-reported leukaemia, lymphoma, or multiple myeloma. These models included the same covariates as described for CVD (with the exception that we did not adjust cholesterol level based on medication usage). Additionally, models estimating risk for sex-specific cancers (that is, prostate and breast) restricted to individuals of the relevant sex and did not adjust for sex as a covariate. For smoking stratified modelling of blood and lung cancer, we used our stricter definition of smoking (ever vs never) and included pack years as a covariate in models testing risk among smokers. To test a more conservative cutoff for excluding patients with a diagnosis of haematologic malignancy prior to sequencing (that is, exclude individuals with a diagnosis prior to 90 days after DNA collection date rather than prior to the DNA collection date itself), we conducted sensitivity analyses for the longitudinal modelling of the risk among CHIP carriers of acquiring blood cancers (for example, blood cancer, myeloid, lymphoid, AML, MDS and MPN). These results were the same as those reported in our main results (Supplementary Note 12 ). Polygenic risk scores Polygenic risk scores were calculated with Plink 58 as a weighted sum of the effects across all conditionally independent variants we identified with GCTA COJO (74 variants, P \u2264 5 \u00d7 10 \u22126 ) We performed association tests using logistic regression, with binary phenotypes of interest (that is, our CHIP subtype phenotypes\u2014for example, TET2 CHIP, and so on) as the dependent variable, this polygenic risk score as the independent variable of interest, and age, sex, smoking status (ever vs never), and 10 genetic principal components as covariates. Software The code is publicly available and can be found at  . The REGENIE software for whole-genome regression, which was used to perform all genetic association analysis, is available at  . GCTA v1.91.7 was used for approximate conditional analysis. SHAPEIT4.2.0 was used for phasing of SNP array data. Imputation was completed with IMPUTE5. Somatic calling was done with Mutect2 (GATK v4.1.4.0). We use Plink1.9\/2.0 for genotypic analysis as well as for constructing polygenic risk scores. FINEMAP was used for fine-mapping, and genetic correlations were calculated using LDSC version 1.0.1 with annotation input version 2.2. Beyond standard R packages, visualization tools, and data processing libraries (for example, dplyr, ggplot2 and data.table), we used the survival (version 3.2.13) and survminer (version 0.4.9) packages for survival analyses, the MendelianRandomization package for Mendelian randomization (version 0.6.0), and the winnerscurse package (version 0.1.1;  ) to adjust GWAS effect size estimates for the effects of Winner\u2019s Curse. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability Individual-level sequence data, CHIP calls and polygenic scores have been deposited with UK Biobank and are freely available to approved researchers, as done with other genetic datasets to date 10 . Individual-level phenotype data are already available to approved researchers for the surveys and health record datasets from which all our traits are derived. Instructions for access to UK Biobank data is available at  . Summary statistics from UKB trait are available in the GWAS catalogue (accession IDs are listed in Supplementary Table 33 ). As described 10 , the HapMap3 reference panel was downloaded from ftp:\/\/ftp.ncbi.nlm.nih.gov\/hapmap\/ , GnomAD v3.1 VCFs were obtained from  , and VCFs for TOPMED Freeze 8 were obtained from dbGaP as described in  . Data used for replication, such as DiscovEHR exome sequencing and genotyping data, and derived CHIP calls, can be made available to qualified, academic, non-commercial researchers upon request via a Data Transfer Agreement with Geisinger Health System (contact person: Lance Adams, ljadams@geisinger.com). Change history 17 February 2023 A Correction to this paper has been published:  ","News_Body":"A team of researchers at Regeneron Pharmaceuticals has identified new genomic variants associated with clonal hematopoiesis of indeterminate potential (CHIP). In their paper published in the journal Nature, the group describes how they used exome-wide and genome-wide association analyses to study differences in the blood of some people with somatic mutations. Nature has also published a Research Highlights piece in the same journal issue, discussing the work done by the New York team. Hematopoiesis is a process that results in the formation of cellular blood components. And clonal hematopoiesis is the part of the process that is involved in the development of cell lineages. The importance of the overall process is highlighted by the fact that every person produces approximately 300 billion new blood cells every single day of their life. Prior research has suggested that there are variants associated with clonal hematopoiesis of indeterminate potential in certain people\u2014each of which can have a unique impact. In this new effort, the team at Regeneron sought to find some of them by studying information held in very large datasets, such as the UK Biobank and the Geisinger MyCode Community Health Initiative. To find the variants they were after, the researchers focused their search efforts on 23 genes that have already been associated with CHIP. By searching through data on 628,388 individuals, they were able to identify 40,208 carriers of at least one variant associated with CHIP. They then conducted exome-wide and genome-wide studies of the carriers they had identified. In so doing, they were able to identify 24 loci\u201421 of which had not been seen before. The researchers also found that they were able to identify some variants that could be associated with clonal hematopoiesis and the length of telomeres in certain individuals. In another part of their study, the team analyzed health traits of people listed in the UK Biobank looking for associations between people who had CHIP variants and other issues. In so doing, they found associations between people who had clonal hematopoiesis variants and diseases such as COVID-19, heart problems, obesity and problems clearing infections of various types. They also found associations between individuals with CHIP and development of cancerous tumors and myeloid leukemias. ","News_Title":"New genomic variants associated with CHIP identified","Topic":"Medicine"}
{"Paper_Body":"Abstract The COVID-19 pandemic triggered a surge in demand for facemasks to protect against disease transmission. In response to shortages, many public health authorities have recommended homemade masks as acceptable alternatives to surgical masks and N95 respirators. Although mask wearing is intended, in part, to protect others from exhaled, virus-containing particles, few studies have examined particle emission by mask-wearers into the surrounding air. Here, we measured outward emissions of micron-scale aerosol particles by healthy humans performing various expiratory activities while wearing different types of medical-grade or homemade masks. Both surgical masks and unvented KN95 respirators, even without fit-testing, reduce the outward particle emission rates by 90% and 74% on average during speaking and coughing, respectively, compared to wearing no mask, corroborating their effectiveness at reducing outward emission. These masks similarly decreased the outward particle emission of a coughing superemitter, who for unclear reasons emitted up to two orders of magnitude more expiratory particles via coughing than average. In contrast, shedding of non-expiratory micron-scale particulates from friable cellulosic fibers in homemade cotton-fabric masks confounded explicit determination of their efficacy at reducing expiratory particle emission. Audio analysis of the speech and coughing intensity confirmed that people speak more loudly, but do not cough more loudly, when wearing a mask. Further work is needed to establish the efficacy of cloth masks at blocking expiratory particles for speech and coughing at varied intensity and to assess whether virus-contaminated fabrics can generate aerosolized fomites, but the results strongly corroborate the efficacy of medical-grade masks and highlight the importance of regular washing of homemade masks. Introduction Airborne transmission of infectious respiratory diseases involves the emission of microorganism-containing aerosols and droplets during various expiratory activities (e.g., breathing, talking, coughing, and sneezing). Transmission of viruses in emitted droplets and aerosols to susceptible individuals may occur via physical contact after deposition on surfaces, reaerosolization after deposition, direct deposition of emitted droplets on mucosal surfaces (e.g., mouth, eyes), or direct inhalation of virus-laden aerosols 1 , 2 . Uncertainty remains regarding the role and spatial scale of these different transmission modes (contact, droplet spray, or aerosol inhalation) for specific respiratory diseases, including for COVID-19 3 , 4 , 5 , 6 , 7 , in particular settings, but airborne transmission stems from the initial expiratory emission of aerosols or droplets. Consequently, the wearing of masks\u2014in addition to vigilant hand hygiene\u2014has been put forth as a means to mitigate disease transmission, especially in healthcare settings 8 , 9 , 10 , 11 . Much research has indicated that masks can provide significant protection to the wearer, although proper mask fitting is critical to realizing such benefits 12 , 13 , 14 , 15 . Alternatively, masks can potentially reduce outward transmission by infected individuals, providing protection to others 7 , 16 , 17 . There have been indications of asymptomatic carriers of COVID-19 infecting others 18 , 19 , 20 , leading to increasing, albeit inconsistent 21 , 22 , 23 , 24 , calls for more universal wearing of masks or face coverings by the general public to help control disease transmission during pandemics. It is therefore important to understand the efficacy of masks and face coverings of different types in reducing outward transmission of aerosols and droplets from expiratory activities. Results from epidemiological and clinical studies assessing the effectiveness of masks in reducing disease transmission suggest that mask wearing can provide some benefits 10 , 11 , especially with early interventions, but often the results lack statistical significance 25 , 26 , 27 , 28 , 29 , 30 , 31 . Laboratory studies provide another means to assess or infer mask effectiveness. Measurement of material filtration efficiencies can provide initial guidance on potential mask effectiveness for preventing outward transmission 15 , 32 , 33 , 34 , 35 , but do not directly address mask performance when worn. Early photographic evidence indicates masks can limit the spread of cough-generated particles 36 . Measurements using simulated breathing with an artificial test head showed the concentration of particles between 0.02 \u03bcm-1 \u03bcm decreases across masks of different types 37 . Also using simulated breathing, Green et al. 38 found surgical masks effectively reduced outward transmission of endospores and vegetative cells, with seemingly greater reduction of particles > 0.7 \u03bcm compared to smaller particles. Using volunteers, Davies et al. 32 found that surgical and home-made cotton masks substantially reduce emission of culturable microorganisms from coughing by healthy volunteers, with similar reduction observed over a range of particle sizes (from 0.65 \u03bcm to > 7 \u03bcm). Milton et al. 16 found that surgical masks substantially reduced viral copy numbers in exhaled \u201cfine\u201d aerosol (\u2264 5 \u03bcm) and \u201ccoarse\u201d droplets (> 5 \u03bcm) from volunteers having influenza, with greater reduction in the coarse fraction. This result differs somewhat from very recent measurements by Leung et al. 13 , who showed a statistically significant reduction in shedding of influenza from breathing in coarse but not fine particles with participants wearing surgical masks. They did, however, find that masks reduced shedding of seasonal coronavirus from breathing for both coarse and fine particles, although viral RNA was observed in less than half of the samples even with no mask, complicating the assessment. The above studies all indicate a strong potential for masks to help reduce transmission of respiratory illnesses. To date, however, none have investigated the effectiveness of masks across a range of expiratory activities, and limited consideration has been given to different mask types. Furthermore, no studies to date have considered the masks themselves as potential sources of aerosol particles. It is well established that fibrous cellulosic materials, like cotton and paper, can release large quantities of micron-scale particles (i.e., dust) into the air 39 , 40 , 41 , 42 . Traditionally, these particles have not been considered a potential concern for respiratory viral diseases like influenza or now COVID-19, since these diseases have been thought to be transmitted via expiratory particles emitted directly from the respiratory tract of infected individuals 43 . Early work in the 1940s indicated, however, that infectious influenza virus could be collected from the air after vigorously shaking a contaminated blanket 44 . Despite this finding, over the next 70 years little attention focused on the possibility of respiratory virus transmission via environmental dust; one exception was a study by Khare and Marr, who investigated a theoretical model for resuspension of contaminated dust from a floor by walking 45 . Most recently, work by Asadi et al. with influenza virus experimentally established that \u201caerosolized fomites,\u201d non-respiratory particles aerosolized from virus-contaminated surfaces such as animal fur or paper tissues, can also carry influenza virus and infect susceptible animals 46 . This observation raises the possibility that masks or other personal protective equipment (PPE), which have a higher likelihood of becoming contaminated with virus, might serve as sources of aerosolized fomites. Indeed, recent work by Liu et al. demonstrated that some of the highest counts of airborne SARS-CoV-2 (the virus responsible for COVID-19) occurred in hospital rooms where health care workers doffed their PPE, suggesting that virus was potentially being aerosolized from virus-contaminated clothing or PPE, or resuspended from virus-contaminated dust on the floor 47 . It remains unknown what role aerosolized fomites play in transmission of infectious respiratory disease between humans, and it is unclear whether certain types of masks are simultaneously effective at blocking emission of respiratory particles while minimizing emission of non-expiratory (cellulosic) particles. Here, we report on experiments assessing the efficacy of unvented KN95 respirators, vented N95 respirators, surgical masks, and homemade paper and cloth masks at reducing aerosol particle emission rates from breathing, speaking, and coughing by healthy individuals. Two key findings are that (i) the surgical masks, unvented KN95 respirators, and, likely, vented N95 respirators all substantially reduce the number of emitted particles, but that (ii) particle emission from homemade cloth masks\u2014likely from shed fiber fragments\u2014can substantially exceed emission when no mask is worn, a result that confounds assessment of their efficacy at blocking expiratory particle emission. Although no direct measurements of virus emission or infectivity were performed here, the results raise the possibility that shed fiber particulates from contaminated cotton masks might serve as sources of aerosolized fomites. Methods Human subjects We recruited 10 volunteers (6 male and 4 female), ranging in age from 18 to 45 years old. The University of California Davis Institutional Review Board approved this study (IRB# 844,369\u20134), and all research was performed in accordance with relevant guidelines and regulations of the Institutional Review Board. Written informed consent was obtained from all participants prior to the tests, and all participants were asked to provide their age, weight, height, general health status, and smoking history. Only participants who self-reported as healthy non-smokers were included in the study. Experimental setup The general experimental setup used was similar to that in previous work 48 , 49 . In brief, an aerodynamic particle sizer (APS, TSI model 3321) was used to count the number of particles between 0.3 to 20 \u03bcm in aerodynamic diameter; the APS counting efficiency falls off below ~ 0.5 \u00b5m, and thus the particles counted between 0.3 and 0.5 \u00b5m likely underestimate the true number. The APS was placed inside a HEPA-filtered laminar flow hood that minimizes background particle concentration (Fig. 1 a). Study participants were asked to sit so that their mouth was positioned in front of a funnel attached to the APS inlet via a conductive silicone tube. They then performed different expiratory activities while wearing no mask or one of the masks shown in Fig. 1 b and described in more detail below. A microphone was placed immediately on the side of the funnel to record the duration and intensity of talking and coughing activities (Fig. 1 c). The participants were positioned with their mouth approximately 1 cm away from the funnel entrance; the nose rest used in our previous setup 48 , 49 was removed to prevent additional particle generation via rubbing of the mask fabric on the nose rest surface. The air was pulled in by the APS at 5 L\/min, with 1 L\/min (20%) focused into the detector to count and size the cumulative number of particles at 1-s intervals (Fig. 1 d). Note that the funnel is a semi-confined environment, and not all expired particles were necessarily captured by the APS. The wearing of masks may redirect some of the expired airflow in non-outward directions (e.g., out the top or sides of the mask 50 ). Accordingly, we use the terminology \u201coutward emission\u201d when referring the to the particle emissions measured here. Therefore, the measurements reported here do not represent the absolute number of emitted particles and may underestimate contributions from particles that escape out the sides of the masks, but do allow relative comparisons between different conditions. The particle emission rates reported here from the APS are likely smaller than the total expiratory particle emission rates by, approximately, the ratio of the exhaled volumetric flowrate that enters the funnel to the APS sample rate. Figure 1 ( a ) Schematic of the experimental setup showing a participant wearing a mask in front of the funnel connected to the APS. ( b ) Photographs of the masks used for the experiments. ( c ) Microphone recording for a participant (F3) coughing into the funnel while wearing no mask. ( d ) The instantaneous particle emission rate of all detected particles between 0.3 and 20 \u00b5m in diameter. Surg.: surgical; KN95: unvented KN95 respirator; SL-P: single-layer paper towel; SL-T: single-layer cotton t-shirt; DL-T: double-layer cotton t-shirt; N95: vented N95 respirator. The subject gave her written informed consent for publication of the images in ( b ). Full size image All experiments were performed with ambient temperature between 22 to 24 \u00b0C. The relative humidity ranged from 30 to 35% for most experiments; a second round of testing, comparing washed vs. unwashed homemade masks, was performed at 53% relative humidity. Given the approximately 3-s delay between entering the funnel and reaching the detector within the APS, under all these conditions the aqueous components of micron-scale respiratory droplets had more than sufficient time (i.e., more than ~ 100 ms) to evaporate fully to their dried residual (so-called \u201cdroplet nuclei\u201d 51 ); see figure S3 of Asadi et al. 48 for direct experimental evidence of complete drying under these conditions. Although large droplets (> 20 \u00b5m) can require substantially more than 1 s to evaporate 52 , as shown here the vast majority of particles are less than 5 \u00b5m and thus unlikely to have originated at sizes larger than 20 \u00b5m. The size distributions presented here are based on the diameter as observed at the APS detector. Expiratory activities Participants were asked to complete four distinct activities for each mask or respirator type: (i) Breathing : gentle breathing in through the nose and out through the mouth, for 2 min at a pace comfortable for the participant. The particle emission rate was calculated as the total number of particles emitted over the entire 2-min period, divided by two minutes to obtain the average particles per second. (ii) Talking : reading aloud the Rainbow Passage (Fairbanks 53 and Supplementary Text S1), a standard 330-word long linguistic text with a wide range of phonemes. Participants read this passage aloud at an intermediate, comfortable voice loudness. Since participants naturally read at a slightly different volume and pace, the microphone recording was used to calculate the root mean square (RMS) amplitude (as a measure of loudness) and duration of vocalization (excluding the pauses between the words). The particle emission rate was calculated as the total number of particles emitted over the entire reading (approximately 100 to 150 s), divided by the cumulative duration of vocalization excluding pauses. Excluding the pauses accounts for person-to-person differences in the fraction of time spent actively vocalizing while speaking (approximately 82% \u00b1 5%) so that individuals who simply pause longer between words are not characterized with an artificially low emission rate due to vocalization. (iii) Coughing : Successive, forced coughing for 30 s at a comfortable rate and intensity for the participant. Similar to the talking experiment, the microphone data was used to determine the RMS amplitude of each cough, the number of coughs, and cumulative duration of coughing (excluding the pauses between the coughs). The particle emission rate was calculated as the total number of particles emitted during the 30 s of measurement, divided either by the number of coughs (to obtain particles\/cough) or by the cumulative duration of the coughs (to obtain particles\/s). (iv) Jaw movement : moving the jaw as if chewing gum, without opening the mouth, for 1 min, while nose breathing, to test whether facial motion in the absence of more extreme expiration caused significant particle emission. This activity technically counts as an expiratory activity since the participant was nose breathing, but the main intent was to assess whether facial motion appreciably alters particle emission, due either to gentle friction between the skin and the facemask yielding enhanced particle emission, or variable gap distances between the mask and skin allowing more or less particles to escape. The particle emission rate was calculated as the total number of particles emitted over the 1-min period, divided by 60 s to obtain the average particles per second. Mask types Participants completed each of the four expiratory activities when they wore no mask or one of the 6 different mask or respirator types: (i) A surgical mask (ValuMax 5130E-SB) denoted as \u201cSurg.\u201d, tested by 10 participants. (ii) An unvented KN95 respirator (GB2626-2006, manufacturer Nine Five Protection Technology, Dongguan, China), tested by 10 participants. (iii) A homemade single-layer paper towel mask (Kirkland, 2-PLY sheet, 27.9 cm \u00d7 17.7 cm) denoted as \u201cSL-P\u201d and tested by 10 participants. (iv) A homemade single-layer t-shirt mask, \u201cSL-T\u201d, made from a new cotton t-shirt (Calvin Klein Men\u2019s Liquid Cotton Polo, 100% cotton, item #1341469), tested by 10 participants. (v) A homemade double-layer t-shirt mask, \u201cDL-T\u201d, made from the same t-shirt material as the SL-T mask, and tested by 10 participants. (vi)A vented N95 respirator (NIOSH N95, Safety Plus, TC-84A-7448)) tested by 2 participants; shortages at the time of testing precluded a larger sample size. The primary difference between an N95 and KN95 respirator is where the mask is certified, in the US. (N95) or China (KN95). The homemade cloth masks (SL-T, and DL-T) were made according to the CDC do-it-yourself instructions for single- and double-layer t-shirt masks 54 . The homemade paper towel masks were made according to do-it-yourself instructions 55 . Photographs of all mask types are shown in Fig. 1 b. Prior to wearing each mask, participants were verbally given general guidance on how to put on each mask. In the case of surgical masks and KN95 respirators they were instructed to pinch the metal bar to conform the mask to the nose. No fit-testing of respirators, as mandated by OSHA standard (29 CFR Part 1910) 56 , was performed, with the intent of obtaining representative particle emission rates for untrained individuals without access to professional fitting assistance. Mask washing To test whether washing of the homemade cloth masks had any effect on the particle emission rate, a subset of 4 participants were asked to bring their double-layer t-shirt mask home and to hand-wash it with water and soap, rinse it thoroughly, and let it air-dry. These participants then returned and repeated the four activities with a brand-new DL-T mask and their washed DL-T mask to provide a direct comparison of washed versus unwashed fabric. Particle emission via hand-rubbing Besides the above experiments to measure the particle emission associated with different mask fabrics, we also performed a qualitative test of the friability of the masks by rubbing each mask by hand in front of the APS, using a procedure similar to that performed previously with paper tissues (cf. Figure 4 of Asadi et al. 46 ). Specifically, the mask was folded over on itself between thumb and index finger, and the mask material was rubbed against itself. A sample of each mask type was rubbed by hand by the same individual for 10 s in front of the APS, using to the best of their ability the same amount of force each time. The test was repeated 3 times for each mask type. The particle emission rate was calculated as the total number of particles emitted divided by the duration of rubbing (10 s). Note that this procedure does not preclude possible particle shedding from the skin of the experimentalist 57 ; the observed particle emission rates for different mask materials therefore represent only qualitative indications of the relative friability. Statistical analysis Box-and-whisker plots show the median (red line), interquartile range (blue box), and range (black whiskers). Stata\/IC 15 was used to perform Shapiro\u2013Wilk normality test on the particle emission rates for each activity. After log-transformation of the data, mixed-effects linear regression was performed to account for person-level correlations. Considering that we had only one primary random effect (person-to-person variability), all variances were set equal with zero covariances. Post hoc pairwise comparisons were performed and adjusted for multiple comparisons using Scheffe\u2019s method. Scheffe groups are indicated with green letters below each box plot; groups with no common letter are considered significantly different ( p < 0.05). Results Particle emission rates for the four expiratory activities are shown in Fig. 2 . Focusing first on breathing (Fig. 2 a), when participants wore no mask, the median particle emission rate was 0.31 particles\/s, with one participant (M6) as high as 0.57 particles\/s, and another participant (F3) as low as 0.05 particles\/s. This median rate and person-to-person variability are both broadly consistent with previous studies 48 , 51 . In contrast, wearing a surgical mask or a KN95 respirator significantly reduced the outward number of particles emitted per second of breathing. The median outward emission rates for these masks were 0.06 and 0.07 particles\/s, respectively, representing an approximately sixfold decrease compared to no mask. Wearing a homemade single layer paper towel (SL-P) mask yielded a similar decrease in outward emission rate, although not as statistically significant as the medical-grade masks. Figure 2 Particle emission rates associated with ( a ) breathing, ( b ) talking, ( c ) coughing, and ( d ) jaw movement when participants wore no mask or when they wore one of the six mask types considered. Scheffe groups are indicated with green letters; groups with no common letter are considered significantly different ( p < 0.05). Surg.: surgical; KN95: unvented KN95; SL-P: single-layer paper towel; U-SL-T: unwashed single-layer cotton t-shirt; U-DL-T: unwashed double-layer cotton t-shirt; N95: vented N95. Note that the scales are logarithmic and the orders of magnitude differ in each subplot. Full size image Surprisingly, wearing an unwashed single layer t-shirt (U-SL-T) mask while breathing yielded a significant increase in measured particle emission rates compared to no mask, increasing to a median of 0.61 particles\/s. The rates for some participants (F1 and F4) exceeded 1 particle\/s, representing a 384% increase from the median no-mask value. Wearing a double-layer cotton t-shirt (U-DL-T) mask had no statistically significant effect on the particle emission rate, with comparable median and range to that observed with no mask. Turning to speech (Fig. 2 b), the overarching trend observed is that vocalization at an intermediate, comfortable voice loudness (Figure S1 a and Table S1 ) yielded an order of magnitude more particles than breathing. When participants wore no mask and spoke, the median rate was 2.77 particles\/s (compared to 0.31 for breathing). The general trend of the mask type effect on the particle emission was qualitatively similar to that observed for breathing. Wearing surgical masks and KN95 respirators while talking significantly decreased the outward emission by an order of magnitude, to median rates of 0.18 and 0.36 particles\/s, respectively. Likewise, wearing the paper towel mask reduced the outward speech particle emission rate to 1.21 particles\/s, lower than no mask but representing a less pronounced decrease compared to surgical masks and KN95 respirators. In contrast, the homemade cloth masks again yielded either no change or a significant increase in emission rate during speech compared to no mask. The outward particle emissions when participants wore U-SL-T masks exceeded the no-mask condition by an order of magnitude with a median value of 16.37 particles\/s. Wearing the U-DL-T mask had no significant effect. The third expiratory activity \u2013 coughing \u2013 again yielded qualitatively similar trends with respect to mask type (Fig. 2 c). We emphasize that participants coughed at different paces, and therefore the number of coughs, cumulative cough duration, and acoustic power varied between participants (Figure S1 b, Figure S2, and Table S2). Nonetheless, we observe that coughing with no mask produced a median of 10.1 particles\/s, with most participants in the range of 3 to 42 particles\/s. For comparison, given a coughing rate of 6 times per minute, the median outward particle count due specifically to coughing over that minute is slightly smaller than that from breathing, and an order of magnitude smaller than talking over a minute (see Fig. S3 for equivalent numbers of particles per cough). Similar general trends as for breathing and speaking were observed for coughing when wearing the different mask types. The surgical mask decreased the median outward emission rate to 2.44 particles\/s (75% decrease), while the KN95 yielded an apparent but not statistically significant decrease to 6.15 particles\/s (39% decrease). The SL-P mask yielded no statistically significant difference compared to no mask. In contrast, the homemade U-SL-T and U-DL-T masks however yielded a significant increase in outward particle emission per second (or per cough) compared to no mask, with median emission rates of 49.2 and 36.1 particles\/s, respectively. Notably, one individual, M6, emitted up to two orders of magnitude more aerosol particles while coughing than the others, emitting 567 particles\/s with no mask. Even when M6 wore a surgical mask he emitted 19.5 particles\/s while coughing, substantially above the median value for no mask, although still a substantial decrease compared to no mask for this individual. Acoustic analysis of the coughing, both in terms of the root mean square amplitude (Figure S1 b) and the filtered power density, indicate that the coughs by M6 were not particularly louder nor more energetic than the others (see Figure S2 and Table S2). It is unclear what caused this individual to emit a factor of 100 more aerosol particles than average while coughing, although qualitatively, the coughs of M6 appeared to originate more from the chest, compared to other participants for whom coughs generally appeared to originate more from the throat; notably, this individual emitted a much closer to average amount of particles while speaking and breathing. Furthermore, the significantly higher aerosol particle emission compared to average during coughing for M6 persisted regardless of the mask type. Finally, Fig. 2 d shows the particle emission rate when participants moved their jaw, similar to chewing gum with their mouth closed, while only breathing through their nose. In general, jaw movement with nose breathing and no mask produced slightly fewer particles per second than the breathing activity (breathing in through nose and out through mouth), with a median rate of 0.12 particles\/s for no mask. As participants were still breathing with closed mouth during the jaw movement, the lower particle production likely results from participants exhaling through the nose rather than through the mouth 48 , 51 . Wearing a surgical mask or KN95 respirator had no statistically significant effect on particle emission from jaw movement compared to no mask. In contrast, wearing all other types of homemade masks (SL-P, U-SL-T, and U-DL-T) substantially increased the particle emission rate, with the single-layer mask yielding the most at 1.72 particles\/s. All of the above experiments were also repeated with vented N95 respirators, albeit with only 2 participants (due to shortages at the time of testing). The small sample size precludes significance testing, but in general the particle emission rates of the two tested were comparable to the surgical mask and unvented KN95 in terms of reduction in the overall emission rates. The emission rates presented in Fig. 2 represent the total for all particles in the size range 0.3 to 20 \u00b5m. We also measured the corresponding size distributions in terms of overall fraction for all trials (Fig. 3 ). In general, all size distributions observed here were lognormal, with a peak somewhere near 0.5 \u00b5m and decaying rapidly to negligible fractions above 5 \u00b5m. Breathing while wearing no mask emitted particles with a geometric mean diameter of 0.65 \u00b5m (Fig. 3 a), with 35% of the particles in the smallest size range of 0.3 to 0.5 \u00b5m. Regardless of the mask type, wearing masks while breathing significantly increased this fraction of particles in the smallest size range (e.g., to as high as 60% for KN95 respirator), shifting the geometric mean diameter toward smaller sizes. Talking with no mask yielded slightly larger particles compared to breathing, with mean diameter of 0.75 \u00b5m (Fig. 3 b). Wearing a mask while talking affected the size distribution in a qualitatively similar manner to that observed with breathing, in that a higher fraction of particles were in the smallest size range. Unlike breathing however, the U-SL-T and U-DL-T masks released the highest fractions of small particles (47% and 51%, respectively). Figure 3 Observed particle size distributions, normalized by particles\/s per bin, associated with ( a ) breathing, ( b ) talking, ( c ) coughing, and ( d ) jaw movement when participants wore no mask or one of the five mask types considered. Each curve is the average over all 10 participants. The solid lines represent the data using a 5 -point smoothing function. Data points with horizontal error bars show the small particles ranging from 0.3 to 0.5 \u03bcm in diameter detected by APS with no further information about their size distribution in this range. Surg.: surgical; KN95: unvented KN95; SL-P: single-layer paper towel; U-SL-T: unwashed single-layer cotton t-shirt; U-DL-T: unwashed double-layer cotton t-shirt; N95: vented N95. Full size image The effect of wearing a mask was more pronounced on the size distribution of the particles produced by coughing (Fig. 3 c). For no mask, the mean diameter of cough-generated particles was 0.6 \u00b5m. The majority of particles emitted were in the smallest size range (up to 57%) during coughing while wearing homemade masks (SL-P, U-SL-T, and U-DL-T). We also note that for coughing, which produced the highest rates of particle emission for of all expiratory activities tested, wearing homemade masks considerably reduced the fraction of large particles (> 0.8 \u00b5m). Finally, for jaw movement the overall size distributions for no-mask and with-mask cases were similar except that the fraction of smallest particles was lowest for no-mask and the surgical mask (Fig. 3 d). To provide a direct comparison of the efficacy of medical-grade and homemade masks in mitigating the emission of particles of different sizes, we divided the entire size range measured by APS (0.3 \u2013 20 \u00b5m) into three sub-ranges (smallest, 0.3 \u2013 0.5 \u00b5m; intermediate, 0.5 \u2013 1 \u00b5m; and largest, 1 \u2013 20 \u00b5m), and calculated the corresponding percent change in the median particle emission rate of each sub-range during breathing, talking, and coughing compared to no mask (Fig. 4 ). For the smallest particles, Fig. 4 a shows that up to a 92% reduction in 0.3 \u2013 0.5 \u00b5m particle emission rate occurred while wearing surgical and KN95 masks for breathing, talking, and coughing, with the KN95 yielding a smaller decrease of 20.5% in this size range. The SL-P mask caused a 60% reduction in 0.3 \u2013 0.5 \u00b5m particle emission for talking and breathing, but yielded a 77% increase for coughing. The least effective masks in terms of minimizing emissions of the smallest particles were the U-SL-T and U-DL-T masks, with U-SL-T substantially increasing the emission of 0.3 \u2013 0.5 \u00b5m particles by almost 600% for speech, and the U-DL-T mask yielding very slight changes for talking and breathing and an almost 300% increase for coughing. Qualitatively similar trends were observed for intermediate size particles in the range of 0.5 \u2013 1 \u00b5m (Fig. 4 b), with the medical-grade masks yielding significant reductions. The main difference for this size range is that the SL-P mask yielded a 15.7% decrease in particle emissions for coughing, and the U-DL-T mask provided up to 34.1% reduction in particle emissions for breathing and talking. Figure 4 Percent change in median particle emission rate (N) for 10 participants compared to no-mask median, while wearing different mask types and while breathing (blue points), talking (red points), or coughing (green points), for particles in the following size ranges: ( a ) smallest, 0.3\u20130.5 \u00b5m; ( b ) intermediate, 0.5\u20131 \u00b5m; ( c ) largest, 1\u201320 \u00b5m; and ( d ) all sizes, 0.3\u201320 \u00b5m. The dashed lines are to guide the eye. Surg.: surgical; KN95: unvented KN95; SL-P: single-layer paper towel; U-SL-T: unwashed single-layer cotton t-shirt; U-DL-T: unwashed double-layer cotton t-shirt. Full size image As for the largest particle sizes (1 \u2013 20 \u00b5m), the observed trends were again qualitatively similar to the intermediate particles (Fig. 4 c), with the medical-grade masks yielding large reductions. Notably, the U-DL-T mask emitted much fewer large particles for breathing and talking with approximately 60% reductions, but still a sizable 160% increase for coughing. The percent change in median particle emission over the entire size range of 0.3 \u2013 20 \u00b5m is presented in Fig. 4 d, which shows that the homemade masks in general yielded more particles in total for coughing, and had mixed efficacy in reducing particle emissions for breathing and talking. The key point is that the surgical and KN95 masks effectively decreased the particle emission for all expiratory activities tested here over the entire range of particle sizes measured by the APS. To help interpret our findings we also quantified the particles emitted from manual rubbing of mask fabrics. The results (Fig. 5 a) show that, in the absence of any expiratory activity, rubbing a surgical mask fabric generated on average 1.5 particles per second, while KN95 and N95 respirators produced fewer than 1 particle per second. In contrast, rubbing the homemade paper and cotton masks aerosolized significant number of particles, with the highest values for SL-P (8.0 particles\/s) and U-SL-T (7.2 particles\/s) masks. Intriguingly, we found that the size distribution of the particles aerosolized from homemade mask fabrics via manual rubbing (Fig. 5 b) was qualitatively different from when participants wore the same masks to perform expiratory activities. An extra peak appeared at approximately 6 \u00b5m and the fraction of small particles dropped to below 27%, suggesting that the frictional forces of fibers against fibers helped fragment and dislodge larger particulates into the air. Importantly, however, manual rubbing produced a sizeable number of particulates in the size range of 0.3 to 2 \u00b5m, commensurate with the range observed while the masks were worn during expiratory activities. Note that the coarse skin particulates (> 2 \u00b5m) released from hand during the mask fabric rubbing experiments could have contributed to the observed particle counts 57 . However, since this factor was the same in all the manual rubbing experiments, and only facemask fabrics differed, it is difficult to explain the observed trends solely in terms of friction between skin and mask fabrics. Moreover, although in these experiments the applied tribological force was not strictly controlled or quantified, the presented results strongly suggest that cotton fabric masks have much more friable material, consistent with our observation that more particles are emitted when participants perform expiratory activities in those cotton fabric masks. Figure 5 ( a ) Number of particles emitted per second of manual rubbing for all masks tested. Each data point is time-averaged particle emission rate over 10 s of rubbing. ( b ) Corresponding size distribution for homemade paper and cotton masks for a total of 30 s of manual rubbing in front of the APS. The solid lines represent the data using a 5 -point smoothing function. Data points with horizontal whiskers show the small particles ranging from 0.3 to 0.5 \u03bcm in diameter detected by APS. Surg.: surgical; KN95: unvented KN95; SL-P: single-layer paper towel; U-SL-T: unwashed single-layer cotton t-shirt; U-DL-T: unwashed double-layer cotton t-shirt; N95: vented N95. Full size image Since the cotton masks were all prepared from fabric that was brand new and unwashed, as a final test we hypothesized that perhaps washing the masks would remove surface-bound dust and otherwise friable material and decrease the emission rate. Our experiments do not corroborate this hypothesis. Handwashing the double-layer t-shirt mask with soap and water followed by air-drying yielded no significant change in the particle emission rate as compared to the original unwashed masks (Fig. 6 ). Moreover, manual rubbing of a washed double-layer cotton mask aerosolized slightly more particles than the unwashed mask. These results suggest that a single washing has little impact on the presence of aerosolizable particulate matter in standard cotton fabrics. Note also that the ranges observed here accord qualitatively with the prior measurements taken with the same 4 participants on a previous day (compare the results for each category in Fig. 6 versus the U-DL-T columns for the respective expiratory activities in Fig. 2 ). This observation suggests that day-to-day variability for a given individual is less than the person-to-person variability observed for all expiratory activities and mask types tested. Figure 6 Particle emission rate from breathing, talking, coughing and jaw movement for 4 participants wearing unwashed or washed double-layer t-shirt masks (U-DL-T vs. W-DL-T). Last column shows the particles emission rates for manual rubbing of washed and unwashed masks (three 10-s trials for each mask). Full size image Discussion Our results clearly indicate that wearing surgical masks or unvented KN95 respirators reduce the outward particle emission rates by 90% and 74% on average during speaking and coughing, respectively, compared to wearing no mask. However, for the homemade cotton masks, the measured particle emission rate either remained unchanged (DL-T) or increased by as much as 492% (SL-T) compared to no mask for all of the expiratory activities. For jaw movement, the particle emission rates for homemade paper and cloth masks were an order of magnitude larger than that of no mask (Fig. 2 d). These observations, along with our results from manual mask rubbing experiments (Fig. 5 ), provide strong evidence of substantial shedding of non-expiratory micron-scale particulates from friable cellulosic fibers of the paper and cloth masks owing to mechanical action 40 . The higher particle emission rate for jaw movement than for breathing is an indication of greater frictional shedding of the paper towel and cotton masks during jaw movement compared to breathing, at least as tested here. Likewise, the difference in the size distributions of mask rubbing and with-mask expiratory activities is likely due to the vigorous frictional force applied by hand on the masks. Regardless of the larger particles (> 5 \u00b5m), rubbing mask fabrics generates a considerable number of particles in the range of 0.3\u20135 \u00b5m similar to that observed for the expiratory activities. This finding corroborates the interpretation that some proportion of the particulates observed during expiration were particulates aerosolized from the masks themselves. Another factor to consider is that masks can reduce the intelligibility of the speech signal 58 , and can reduce the intensity of sounds passed through them by a significant amount (e.g., > 10 dB in Saedi et al. 59 ). Likely as a response to this, people will speak louder and otherwise adjust their speech when wearing masks. Mendel et al. 60 found that the measured intensity of speech was approximately the same for a group of speakers with and without surgical masks, suggesting that speakers increased the actual intensity of their speech when wearing masks. Fecher 61 found that speakers will actually produce louder output through some types of masks in cases where they overestimate the dampening effects of the mask. It is also possible that speakers may produce Lombard speech when wearing certain types of masks 62 . Lombard speech is louder, has a higher fundamental frequency, and tends to have longer vowel durations, all characteristics that may contribute to an increase in the emission of aerosols 48 , 49 . Our results showed that the root mean square amplitude of speech, as measured externally when participants wore any type of mask, equaled or exceeded that of the no-mask condition (Figure S1 a), suggesting that participants were indeed talking louder with the mask. Although an increase in the intensity of the speech signal when wearing masks would result in greater output of particles in these conditions 48 , the difference in the intensity of speech across the different conditions was not very large (Figure S1 a). As a result, this mechanism alone cannot explain the increased particle output in some of the masked conditions. Intriguingly, the root mean square amplitude of coughing decreased for most of the participants after they wore a type of mask (Figure S1 b), suggesting that they do not cough louder when they wear a mask, i.e., there is no Lombard effect for coughing. The substantial particle shedding by the cloth masks confounds determination of the cloth mask efficacy for reducing outward emission of particles produced from the expiratory activity. Measured material filtration efficiencies vary widely for different cloth materials 32 , 34 , 35 , 63 . The influence of particle shedding on such determinations has not been previously considered; our results raise the possibility that particle shedding has led to underestimated material filtration efficiencies for certain materials. While the material efficiency of the cotton masks was not determined here, we note that the use of the double-layer cotton masks reduced the emission of larger particles (both on a normalized and absolute basis), indicating some reasonable efficacy towards reduction of the expiratory particle emission. Further work differentiating between expiratory and shed particles, possibly based on composition, can help establish the specific efficacy of the cloth masks towards expiratory particles. That the masks shed fibers from mechanical stimulation indicates care must be taken when removing and cleaning (for reusable masks) potentially contaminated masks so as to not dislodge deposited micro-organisms. We also note that the emission reduction due to surgical masks was greater than the corresponding reduction due to KN95 respirators, although this difference was only significant for coughing (p < 0.05). That the surgical masks appear to provide slightly greater reduction than the KN95 respirators is perhaps surprising, as KN95s are commonly thought of as providing more protection than surgical masks for inhalation. Both surgical masks and KN95 respirators typically have high material filtration efficiencies (> 95%) 63 , although the quality of surgical masks can vary substantially 64 . The fit of surgical and KN95 respirators differs substantially. Here, no fit tests were performed to ensure good seals of the KN95 respirators. It may be that imperfect fitting of KN95 respirators allows for greater escape of particles from the mask-covered environment compared to the more flexible surgical masks. Regardless, all surgical masks, KN95 and N95 respirators tested here provided substantial reduction of particle emission compared to no mask. A particularly important observation was the existence of a coughing superemitter, who for unknown reasons emitted two orders of magnitude more particles during coughing than average (Fig. 2 c, red points for M6). This huge difference persisted regardless of mask type, with even the most effective mask, the surgical mask, only reducing the rate to a value twice the median value for no mask at all. Although the underlying mechanism leading to such enhanced particle emission is unclear, these observations nonetheless confirm that some people act as superemitters during coughing, similar to \u201cspeech superemitters\u201d 48 , and \u201cbreathing high producers\u201d 65 . This observation raises the possibility that coughing superemitters could serve as superspreaders who are disproportionately responsible for outbreaks of airborne infectious disease. Notably, the coughing superemitter was not a breathing superemitter or speaking superemitter, indicating that testing only one type of expiratory activity might not necessarily identify superemitters for other expiratory activities. As a final comment, we emphasize that here we only measured the physical dynamics of outward aerosol particle emission for different expiratory activities and mask types. Redirected expiratory airflow, involving exhaled air moving up past the nose or out the side of the mask, were not measured here but should be considered in future work. Likewise, more sophisticated biological techniques are necessary to gauge mask efficacy at blocking emission of viable pathogens. Our work does raise the possibility, however, that virus-contaminated masks could release aerosolized fomites into the air by shedding fiber particulates from the mask fabric. Since mask efficacy experiments are typically only conducted with fresh, not used, masks, future work assessing emission of viable pathogens should consider this possibility in more detail. Our work also raises questions about whether homemade masks using other fabrics, such as polyester, might be more efficient than cotton in terms of blocking expiratory particles while minimizing shedding of fabric particulates, and whether repeated washings might affect homemade masks. Future experiments using controlled bursts of clean air through the masks will help to resolve the source of these non-expiratory particles. Nonetheless, as a precaution, our results suggest that individuals using homemade fabric masks should take care to wash or otherwise sterilize them on a regular basis to minimize the possibility of emission of aerosolized fomites. Conclusions These observations directly demonstrate that wearing of surgical masks or KN95 respirators, even without fit-testing, substantially reduce the number of particles emitted from breathing, talking, and coughing. While the efficacy of cloth and paper masks is not as clear and confounded by shedding of mask fibers, the observations indicate it is likely that they provide some reductions in emitted expiratory particles, in particular the larger particles (> 0.5 \u03bcm). We have not directly measured virus emission; nonetheless, our results strongly imply that mask wearing will reduce emission of virus-laden aerosols and droplets associated with expiratory activities, unless appreciable shedding of viable viruses on mask fibers occurs. The majority of the particles emitted were in the aerosol range (< 5 \u03bcm). As inertial impaction should increase as particle size increases, it seems likely that the emission reductions observed here provide a lower bound for the reduction of particles in the droplet range (> 5 \u03bcm). Our observations are consistent with suggestions that mask wearing can help in mitigating pandemics associated with respiratory disease. Our results highlight the importance of regular changing of disposable masks and washing of homemade masks, and suggests that special care must be taken when removing and cleaning the masks. Data availability The datasets generated during and\/or analyzed during the current study are available in the Dryad Digital Repository,  . ","News_Body":"Laboratory tests of surgical and N95 masks by researchers at the University of California, Davis, show that they do cut down the amount of aerosolized particles emitted during breathing, talking and coughing. Tests of homemade cloth face coverings, however, show that the fabric itself releases a large amount of fibers into the air, underscoring the importance of washing them. The work is published today (Sept. 24) in Scientific Reports. As the COVID-19 pandemic continues, the use of masks and other face coverings has emerged as an important tool alongside contact tracing and isolation, hand-washing and social distancing to reduce the spread of coronavirus. The Centers for Disease Control and Prevention, or CDC, and the World Health Organization endorse the use of face coverings, and masks or face coverings are required by many state and local governments, including the state of California. The goal of wearing face coverings is to prevent people who are infected with COVID-19 but asymptomatic from transmitting the virus to others. But while evidence shows that face coverings generally reduce the spread of airborne particles, there is limited information on how well they compare with each other. Sima Asadi, a graduate student working with Professor William Ristenpart in the UC Davis Department of Chemical Engineering, and colleagues at UC Davis and Icahn School of Medicine at Mount Sinai, New York, set up experiments to measure the flow of particles from volunteers wearing masks while they performed \"expiratory activities\" including breathing, talking, coughing and moving their jaw as if chewing gum. Asadi and Ristenpart have previously studied how people emit small particles, or aerosols, during speech. These particles are small enough to float through the air over a considerable distance, but large enough to carry viruses such as influenza or coronavirus. They have found that a fraction of people are \"superemitters\" who give off many more particles than average. The 10 volunteers sat in front of a funnel in a laminar flow cabinet. The funnel drew air from in front of their faces into a device that measured the size and number of particles exhaled. They wore either no mask, a medical-grade surgical mask, two types of N95 mask (vented or not), a homemade paper mask or homemade one- or two-layer cloth mask made from a cotton T-shirt according to CDC directions. Credit: UC Davis Up to 90 percent of particles blocked The tests only measured outward transmission\u2014whether the masks could block an infected person from giving off particles that might carry viruses. Without a mask, talking (reading a passage of text) gave off about 10 times more particles than simple breathing. Forced coughing produced a variable amount of particles. One of the volunteers in the study was a superemitter who consistently produced nearly 100 times as many particles as the others when coughing. In all the test scenarios, surgical and N95 masks blocked as much as 90 percent of particles, compared to not wearing a mask. Face coverings also reduced airborne particles from the superemitter. Homemade cotton masks actually produced more particles than not wearing a mask. These appeared to be tiny fibers released from the fabric. Because the cotton masks produced particles themselves, it's difficult to tell if they also blocked exhaled particles. They did seem to at least reduce the number of larger particles. The results confirm that masks and face coverings are effective in reducing the spread of airborne particles, Ristenpart said, and also the importance of regularly washing cloth masks. ","News_Title":"Surgical, N95 masks block most particles, homemade cloth masks release their own","Topic":"Medicine"}
{"Paper_Body":"Abstract The inertial sliding of physisorbed submonolayer islands on crystal surfaces contains unexpected information on the exceptionally smooth sliding state associated with incommensurate superlubricity and on the mechanisms of its disappearance. Here, in a joint quartz crystal microbalance and molecular dynamics simulation case study of Xe on Cu(111), we show how superlubricity emerges in the large size limit of naturally incommensurate Xe islands. As coverage approaches a full monolayer, theory also predicts an abrupt adhesion-driven two-dimensional density compression on the order of several per cent, implying a hysteretic jump from superlubric free islands to a pressurized commensurate immobile monolayer. This scenario is fully supported by the quartz crystal microbalance data, which show remarkably large slip times with increasing submonolayer coverage, signalling superlubricity, followed by a dramatic drop to zero for the dense commensurate monolayer. Careful analysis of this variety of island sliding phenomena will be essential in future applications of friction at crystal\/adsorbate interfaces.     Main Systems achieving low values of dry sliding friction are of great physical and, potentially, technological interest 1 , 2 , 3 , 4 . Superlubricity\u2014the vanishing of static friction\u2014and the consequent ultra-low dynamic friction between crystal faces that are sufficiently hard and mutually incommensurate 5 , 6 , is experimentally rare and has been demonstrated or implied in only a relatively small number of cases, including telescopic sliding among carbon nanotubes 7 , 8 , sliding graphite flakes on a graphite substrate 9 , 10 , 11 , cluster nanomanipulation 12 , 13 and sliding colloidal layers 14 , 15 . It is essential that we increase the understanding of this phenomenon and, in view of potential nanotechnology applications, examine new and more generic systems beyond these. Submonolayer islands of rare gas atoms adsorbed on crystal surfaces offer an excellent platform to address friction at crystalline interfaces. Despite much experimental 16 , 17 , 18 , 19 , 20 , 21 , 22 and theoretical 23 , 24 , 25 , 26 , 27 work, superlubricity is a phenomenon that remains poorly explored in such systems. In the submonolayer range (0 < \u03b8 < 1, where \u03b8 is the coverage) and at low temperatures, adsorbate phase diagrams versus coverage \u03b8 are well known to display phase-separated two-dimensional (2D) solid islands, usually incommensurate with the surface lattice, coexisting with the 2D adatom vapour 28 , 29 . Using a quartz crystal microbalance (QCM), the inertial sliding friction of these islands is measured by the inverse of the slip time \u03c4 s = (1\/4\u03c0)[\u03b4( Q \u22121 )\/\u03b4 f ], the ratio of the adsorbate-induced change in inverse quality factor over the respective change in the substrate oscillation frequency 30 . The peak of the inertial force acting on an island deposited on the QCM is expressed as F in = \u03c1 isl SA (2\u03c0 f ) 2 (where \u03c1 isl is the 2D density at the centre of an adsorbed island of area S , and A and f are the oscillation amplitude and frequency, respectively), and equals the viscous frictional force F visc = Mv \/ \u03c4 s, (where M is the mass of the island and v is speed). This means that superlubricity should indirectly show up as an unusually large slip time. For over two decades, QCM work has shown that physisorbed atoms or molecules condense and generally slide above a submonolayer coverage \u03b8 sf , and \u03c4 s may typically reach values from hundreds of picoseconds to a nanosecond. These results 17 , 19 , 30 and the corresponding pioneering atomistic simulations 23 , 31 have provided much valuable initial information about the temperature and system dependence of inertial friction. So far, however, crucial aspects that specifically address the island structure of the adsorbate, the edge-originated pinning and in particular the change in commensurability and superlubricity with coverage (issues that, in our view, are important to nanofriction) have not yet come under scrutiny. Here, we present a joint experimental and theoretical study of the sliding of adsorbate islands on a crystalline substrate, and reveal surprising information about the exceptionally easy sliding suggestive of superlubricity, about its limiting factors (caused by edges and defects), and its eventual spontaneous demise at full coverage. Our chosen example is physisorbed Xe on Cu(111), a system for which the phase diagram is well studied (as is the case for other rare gas adsorbates on graphite and metal surfaces) 28 . Between \u223c 50 and 90 K, Xe monolayers condense on Cu(111) as a commensurate 2D solid. We conventionally designate this as unit coverage \u03b8 = 1, characterized by a density where is the commensurate adatom spacing. Low-energy electron diffraction at 50 K locates the Xe atoms on top of surface Cu atoms 32 , the planar distance ( a 0 ) of which is close to the Xe\u2013Xe spacing in bulk Xe ( a Xe = 0.439 nm). At lower temperatures, the full Xe monolayer is known from surface extended X-ray measurements to shrink into an \u2018overdense\u2019 ( \u03c1 > \u03c1 0 ) incommensurate structure, reaching commensurability only at 50 K following thermal expansion 33 . Conversely, the 2D atom density in Xe monatomic islands, which coexist with the adatom 2D vapour at submonolayer coverage, is not specifically known, but is often assumed to be equal to \u03c1 0 . Our results in fact show that the 2D crystalline Xe islands ( \u03b8 < 1) are slightly \u2018underdense\u2019 ( \u03c1 isl < \u03c1 0 ) and increasingly incommensurate with thermal expansion, reaching a 2D density 4% below \u03c1 0 near 50 K. In this incommensurate state, the 2D lattice inside the Xe islands should slide superlubrically over the Cu(111) substrate, as expected for a \u2018hard\u2019 slider. Indeed, even though the Xe\u2013Xe attraction V Xe\u2212Xe \u2248 20 meV is an order of magnitude smaller than the Xe\u2013Cu(111) adhesion energy E a \u2248 190 meV (ref. 28 ), it is an order of magnitude larger, and thus harder, than the weak Cu(111) surface corrugation, E c \u2248 1\u20132 meV (ref. 34 ). We also find that the large Xe\/Cu adhesion leads to another important consequence, the tribological impact of which has not generally been described. At monolayer completion (where the 2D adatom gas disappears), a positive 2D (spreading) pressure suddenly builds up as extra adatoms strive to enter the first layer and benefit from the substrate attraction, rather than forming a second layer where the attraction is much smaller. This process is clearly revealed in the QCM data for Xe\/Ag(111) obtained by Krim's group 17 , which indicate a Xe density increase of \u223c 5% following monolayer formation. Ideally, as the submonolayer coverage grows, this spontaneous density increase process should start at \u03b8 c \u223c \u03c1 isl \/ \u03c1 0 < 1 and continue until limited by either the build-up of 2D pressure, or by a strong accidental commensurability with the substrate, whichever comes first as \u03b8 grows beyond \u03b8 c . If corrugation, commensurability and entropic effects are ignored, and assuming for simplicity a first-neighbour Xe\u2013Xe attraction \u2013V , the potential energy density change following a monolayer density increase from \u03c1 to \u03c1 + \u03b4 \u03c1 is roughly estimated as which is minimal when where \u03bb and \u03bc are the Xe monolayer Lam\u00e9 coefficients, m is the mass of a Xe atom and v L is the monolayer longitudinal sound velocity. With parameters appropriate for the Xe monolayer and v L = 1.3 km s \u20131 ), this yields (\u03b4 \u03c1 \/ \u03c1 ) \u2248 6%, close to the experimental compression of Xe\/Ag(111). We note that a compression of this magnitude would amount to several kbar in bulk Xe. In the present case of Xe\/Cu(111), and unlike Xe\/Ag(111), the 2D density upward jump from \u03c1 isl is arrested to \u03c1 0 by commensurability, and we estimate its amount to \u223c 4%. The gist of these preliminary theoretical considerations is that, near 50 K, submonolayer Xe islands must be incommensurate and most probably superlubric, whereas the full monolayer is commensurate and probably pinned. QCM measurements The friction of the Xe monolayers was measured with a QCM. The microbalance consisted of an AT cut quartz disk whose principal faces were optically polished and covered by a gold keyhole electrode commercially evaporated on one face and a copper electrode on the other face ( Fig. 1 ). The QCM was driven at its fundamental mode with resonance frequency f res \u2248 5 MHz using a frequency modulation (FM) technique. An a.c. voltage V D was applied across the two electrodes at a frequency equal to that of its mechanical resonance and drove the two parallel faces of the quartz plate in an oscillating, transverse shear motion. Varying V D changed the power dissipated in the quartz and the amplitude A of the lateral oscillations of the electrodes. The latter quantity was calculated from the formula A = 1.4 Q S V D , where A is measured in picometres and V D is the peak driving voltage in volts (ref. 35 ). Figure 1a presents a family of resonance curves measured in vacuum and at T = 48 K for different values of A . On the horizontal axis the frequency of generator f is normalized to f res , and the vertical axis shows the corresponding amplified voltage V QCM normalized to the peak value V res . No variation in the resonance curve is detected within the amplitude range investigated. The continuous line is a nonlinear least-squares fit to the data, which yields a quality factor of the quartz of Q = 220,000 (ref. 20 ). The condensation of a film on the electrodes is signalled by a decrease in the resonance frequency f res . Any dissipation taking place at the solid\/film interface is detected by a decrease in the corresponding resonance amplitude V res (ref. 36 ). Figure 1: The quartz crystal microbalance and characterization of the Cu(111) electrode. a , Normalized resonance curves of the QCM measured in vacuum and at T = 48 K for different oscillating amplitudes A. The red line is a fit to the data for A = 7 nm, and f res \u2248 5 MHz represents the series resonance of the quartz crystal. Top left inset: sketch of the QCM with Cu and Au electrodes. Bottom left inset: the shear motion of the QCM at resonance. Arrows represent lateral displacements. Top right inset: Xe gas dosing on the QCM. b , Pole figure (stereographic plot) of the Cu [111] peak. The units of the contour plot labels are counts per second (c.p.s.). \u03c8 (sample tilt angle) is varied between 0 and 50\u00b0 and \u03d5 (sample rotation angle) between 0 and 180\u00b0. c , Continuous line: \u03d5 -averaged Cu [111] intensity as a function of \u03c8 . Dashed line: cos( \u03c8 ) law normalized to Cu [111] intensity at \u03c8 = 0\u00b0. d , STM derivative image of the Cu film. Image size: 150 \u00d7 150 nm. Full size image The Cu(111) electrode was prepared by depositing on the other bare quartz face a Cu (30 nm)\/Cr (10 nm) bilayer at room temperature under ultrahigh vacuum conditions using Knudsen effusion sources. The Cr buffer layer was used to promote adhesion between the quartz substrate and the Cu film. The deposition rates were 2.2 \u00c5 s \u20131 and 4.7 \u00c5 s \u20131 for Cr and Cu, respectively. Before deposition, the QCM was heated for 30 min to 250 \u00b0C to remove the condensed impurities. After deposition, the surface cleanliness of the Cu film was checked by X-ray photoelectron spectroscopy (XPS) ( Supplementary Section \u2018XPS characterization of Cu(111) surface\u2019 ). Figure 1b shows the XRD intensity of the Cu [111] reflection as a function of sample orientation (a pole figure). A well-defined peak close to a tilt angle of 0\u00b0 indicates that most of the Cu grains are oriented with the [111] crystal axis perpendicular to the sample surface. No other preferred orientations are detected. Geometrical effects due to the limited size of the sample with respect to the X-ray beam give rise to a broad peak at \u03c8 = 0\u00b0, with a cos( \u03c8 ) dependence. Figure 1c shows the [111] reflection intensity, averaged over the sample rotation angle \u03d5 , as a function of sample tilt angle \u03c8 . To discriminate between preferential orientation and finite sample size effects, a cos( \u03c8 ) law normalized to the maximum intensity of the peak is also shown in Fig. 1c . The measured intensity peak is clearly sharper than cos( \u03c8 ), indicating that the \u03c8 dependence of the [111] intensity is mostly due to the preferential orientation of the Cu grains. Figure 1d presents a scanning tunnelling microscope (STM) derivative image of the sample surface taken in situ immediately after deposition. Large flat grains 40\u201350 nm in lateral size are clearly visible. The typical area of the (111) terraces is A 0 \u2248 2.5 \u00d7 10 3 nm 2 . Xe was condensed directly onto the Cu(111) electrode of the QCM at temperatures between 47 and 49 K. Lower temperatures could not be reached due to the poor thermal coupling to the cold head of the cryocooler, and higher values were limited by evaporation of the Xe monolayer 22 . Within this very narrow temperature interval, no systematic and reproducible variations attributable to T were observed. Between consecutive deposition scans, the QCM was heated to \u223c 60 K to guarantee full evaporation of Xe and thermal annealing of the microbalance 37 . Figure 2 presents the measured slip time \u03c4 s of Xe at T = 47 K with a moderate oscillating amplitude of A = 7.4 nm for the Cu electrode. The coverage was deduced from the frequency shift, assuming for the monolayer an areal density of \u03c1 0 = 5.93 atoms nm \u20132 , corresponding to completion of the commensurate solid phase, equivalent to a frequency shift of 7.3 Hz. Besides some initial pinning ( \u03c4 s = 0) at the lowest coverages ( \u03b8 < 0.05) where Xe is known to condense at steps and defects 38 , the data show depinning with a rapid increase of \u03c4 s , reaching peak values of up to 4 ns, nearly an order of magnitude larger than the slip times measured with Xe on gold and on graphene at the same temperature 22 . This slipperiness of Xe islands on Cu(111) is all the more puzzling because it runs contrary to the pinning for commensurability that is, according to the literature 33 , supposed to be in place at 47 K. The large and rapidly rising submonolayer slip time is a dominant feature in Fig. 1 , which we will now qualify as evidence of incommensurability and superlubricity. The second and even more unusual feature is the sudden slip time collapse near \u03b8 \u2248 1, which also exhibits a mysterious variability between experiments. Both will be shown to signify an abrupt increase in density leading to a compressed monolayer. Figure 2: Slip time of Xe on Cu(111) as a function of film coverage. The scan was taken at T = 47 K with f res \u2248 5 MHz and at an oscillating amplitude of the Cu electrode of A = 7.4 nm. Inset: scans of Xe on Cu(111) taken for different Xe depositions on the same substrate at the same A and temperatures between 47 and 49 K. Note the sharp drop occurring at coverage near a full monolayer, albeit with large fluctuations. Full size image Theory and MD simulations of sliding islands The physics behind these experimental results can be directly addressed by frictional molecular dynamics (MD) simulations. The power P dissipated by a hard crystalline island of area S sliding on a crystal surface under a uniform force F is given by the sum P = P b + P e of an intrinsic \u2018bulk\u2019 term P b (the friction of an equal portion of infinite adsorbate with the same 2D density) and extrinsic or defect terms P e , representing corrections for the island finite size and substrate defects. The contribution to P e to take into account substrate imperfections and defects depends on the oscillation amplitude, and is reduced for smaller oscillation amplitudes, when fewer defects are overlapped by the moving island ( Supplementary Section \u2018Slip times measured at different oscillating amplitudes\u2019 ). The residual contribution to P e , present even on defect-free terraces and for all amplitudes, is the friction caused by the island finite size, and can be conventionally designated as an \u2018edge\u2019 contribution 27 . Quite generally, P e presents an area dependence different from that for P b , namely P b \u223c S versus P e \u223c S \u03b3 , where \u03b3 < \u00bd depending on the nature of the pinning centres 27 . The force dependencies of the two terms also differ. Extrinsic defects and\/or island edges imply a small but non-zero static friction force F se , so P e will vanish for either F < F se (due to pinning) or for F \u226b F se (where the sliding becomes asymptotically free). The bulk, intrinsic frictional power P b depends strongly on the commensurability of the island 2D lattice structure with the crystal surface lattice. Hard incommensurate islands have zero bulk static friction, and will therefore slide superlubrically, that is, with a relatively small kinetic friction, growing viscous-like with force F \u03bd , where \u03bd \u2248 1. This bulk friction of a superlubric slider, negligible at low-speed sliding such as with the \u03bcm s \u20131 typical speed of atomic force microscopy (AFM) 13 , becomes accessible in QCM, where peak speeds are many orders of magnitude higher (here v \u223c \u03c9A \u2248 0.23 m s \u20131 ). Commensurate systems are, on the other hand, pinned by static friction F sb , so P b is zero until the force reaches a large depinning force F \u223c F sb , at which point, as shown for example by colloidal simulations 15 , 39 , frictional dissipation has its peak. In our model, the Cu(111) substrate is treated as a fixed and rigid triangular lattice, exerting on the mobile Xe adatoms an average attractive potential of \u2013 E a = \u2212190 meV and with a corrugation of 1 meV between the Cu on-top site (energy minimum for a Xe adatom) and the Cu hollow site (energy maximum). Each Xe adatom is thus subject to the overall potential V Xe\u2212Xe + V Xe\u2212Cu . The Xe\u2013Xe interaction is modelled by a regular Lennard\u2013Jones 12-6 potential, with parameters \u03b5 = 20 meV and \u03c3 = 3.98 \u00c5. Smaller corrections due to three-body forces as well as substrate-induced modifications of this two-body force are ignored. The Xe\u2013Cu interaction is modelled by the modified Morse potential: where z 0 = 3.6 \u00c5 (refs 40 , 41 ). We define the modulating function, normalized to span the interval from 0 (top sites) to 1 (hollow sites), as where the constant is the nearest-neighbour distance of surface Cu atoms. The Morse potential energy parameter is given by \u03b1 ( x , y ) = \u2013 E a + M ( x , y ) E c . The inverse length \u03b2 in equation (3) is obtained by equating the second derivative of the potential to the experimental spring constant: where m is the atomic mass of xenon. With a perpendicular vibration energy of \u03c9 \u2243 2.8 meV (ref. 40 ), we obtain \u03b2 = 0.8 \u00c5 \u22121 . The equations of motion are integrated using a velocity\u2013Verlet algorithm, coupled to a Langevin thermostat with a damping coefficient \u03b3 = 0.1 ps \u22121 , a damping whose value is not critical and which is not applied to the translational degrees of freedom of the Xe island centre of mass. Islands are obtained by a circular cutting of a Xe monolayer, the radius of which determines the island size. The so-formed islands are deposited on the Cu substrate with a random orientation angle, as this is expected to occur experimentally and is not critical to the results. The simulation protocol for slip-time calculation starts with the system being heated at 48 K for 100 ps. The Xe centre-of-mass velocity along the x -axis is then set at v i = 100 m s \u20131 and the simulation evolves until motion stops. The slowdown is fit very well by an exponential, indicating a purely viscous friction. The slip time is extracted, after skipping the initial transient, by an exponential fit of the form , as shown in Fig. 3 for an island of diameter \u223c 60 nm. Figure 3: Spontaneous frictional slowdown of a 60 nm circular island. An island of density \u03c1 isl \/ \u03c1 0 = 0.96 was initially kicked at large speed and T = 48 K and then set free to move without thermostatting. The wide green line is obtained by superposition of five simulations. The excellent exponential fit (orange curve) confirms the viscous sliding of the island. Full size image The excellent exponential fit confirms that the island sliding is indeed viscous, with a slip time \u03c4 s as large as 5 ns, directly comparable with the experimental values of Fig. 2 . Moreover, even when no extrinsic defects were included, the slip time obtained still varied with the island area S , well fit by \u03c4 s \u22121 = ( a + bS \u03b3 \u22121 ), with \u03b3 \u2248 \u00bc (as shown in Fig. 4 ). This sublinear term exponent is the same as that found for sliding clusters 13 and is similar to that found in a recent study on adsorbate static friction, where it was due to the finite size of the island 27 . Under the reasonable assumption that increasing coverage corresponds to an increase in average island size, eventually reaching at full coverage ( \u03b8 \u2248 1) the size of the largest Cu(111) terraces ( \u223c 50 nm), the increase of the experimental slip time with coverage shown in Fig. 2 can be attributed to the progressively decreasing role of edges 27 . The intrinsic, defect-free slip time asymptotically reached in the large size limit is that dictated by the ideal Xe lattice, which is incommensurate, hard and superlubrically sliding with a friction growing linearly with speed. We conclude that the unusually large slip times at large submonolayer coverages signify precisely that Xe islands sliding on Cu(111) are asymptotically superlubric. Figure 4: Theoretical slip time from sliding simulations for incommensurate Xe islands of growing size, on a perfectly periodic potential representing Cu(111). The fit (dashed green line) shows that the size-controlled defect friction (here due to the island edge) gives way to very long slip times arising from bulk superlubricity in the large size limit ( \u03c4 s bulk \u2248 5.75 ns). The vertical line marks full monolayer coverage for an estimated experimental Cu(111) terrace size of 60 nm. When islands reach this size, we expect a spontaneous density increase with sudden slip time collapse due to commensurabiliy. Full size image The second striking experimental feature is the sudden drop of slip time near monolayer coverage. The physics behind this also emerges from simulation, where additional adatoms added near full coverage \u03b8 \u2248 1 are spontaneously incorporated into the first monolayer rather than forming a second layer. As trial simulations also confirm, the extra compressional energy cost implied by this incorporation is overcompensated by the adhesive energy gain, in agreement with equation (4). The resulting 4% growth in 2D density at monolayer completion, not far from the full theoretical 6%, is arrested by the intervening, and accidental, exact commensurability, well established experimentally 32 , 34 . As a result of this, the slip time of Xe\/Cu(111) falls (unlike that of, for example, Xe\/Ag(111), which remains incommensurate after densification 17 ), as seen in the experimental ( Fig. 2 ) and indicated in the theoretical ( Fig. 4 ) results. The 2D density jump, which destroys superlubricity with increasing adsorbate coverage near one monolayer, is a sudden, first-order event. As such, it is expected to occur with hysteresis, which implies a difference between atom addition and atom removal, as well as occasional differences between one compressional event and another. As shown in the inset of Fig. 2 , the Xe coverage at which the sudden slip time drop occurs is experimentally rather erratic, in agreement with this expectation. Experimental verification of this hysteresis is difficult because of the negligible pressure of the bulk vapour in equilibrium with the film, which makes it impossible to decrease the Xe coverage by pumping gas out at the temperature of the scan. Simulated insertion\/extraction of a Xe atom into\/out of a full underdense monolayer ( \u03c1 \/ \u03c1 0 = 0.96) yields very asymmetric energy evolutions, actually ending with highly defected, poorly reproducible states. That result supports hysteresis and suggests, moreover, the explanation for the randomness in the slip time drop observed in the real process of Fig. 2 being probably due to the relatively long, statistically distributed time needed for the intra-monolayer defects to heal away during the spontaneous compression process. Finally, the temperature behaviour is one important parameter that we cannot vary in our experiment, but that is worthy of comment. The compressed Xe\/Cu(111) monolayer has a much stronger thermal expansion than bulk Xe, which causes it to evolve from incommensurate and slightly overdense between 20 and 45 K (ref. 32 ) to commensurate at 50 K (refs 31 , 32 ) and 60 K (ref. 33 ), pushing it incommensurate again, and now slightly underdense, at higher temperatures (a fine feature hard to pick up by pioneering low energy electron diffraction (LEED) studies 42 ). Incommensurability and superlubricity at 77 K are strongly suggested by the exceedingly long slip times on the order of 20 ns observed by Coffey and Krim 18 . From superlubric islands to pinned monolayers The inertial sliding of submonolayer Xe islands on the Cu(111) surface just investigated offers an ideal playground to delve deeper into some important frictional phenomena. The long slip times and theoretically demonstrated incommensurability between adsorbed and substrate lattices characterize the sliding as asymptotically superlubric for large island areas, limited only by defect- and edge-related friction. A sudden spontaneous compression at monolayer completion caused by strong adhesion to the substrate is predicted and observed. Specific to Xe\/Cu(111) is the ensuing commensuration accidentally reached during compression, causing a peculiar transition from superlubric sliding with a large slip time to its sudden vanishing in the dense pinned state. Both the island superlubricity and the compressional transition, the latter generally leading from one to another incommensurate state, are general phenomena characterizing sliding friction for adsorbates on crystalline surfaces, and are not specific to the system under study. Technological nanodesign addressing the control of crystal friction properties at the most intimate level will have to take these elementary mechanisms into account. ","News_Body":"It's possible to vary (even dramatically) the sliding properties of atoms on a surface by changing the size and \"compression\" of their aggregates: an experimental and theoretical study conducted with the collaboration of SISSA, the Istituto Officina dei Materiali of the CNR (Iom-Cnr-Democritos), ICTP in Trieste, the University of Padua, the University of Modena e Reggio Emilia, and the Istituto Nanoscienze of the CNR (Nano-Cnr) in Modena, has just been published in Nature Nanotechnology. (Nano)islands that slide freely on a sea of copper, but when they become too large (and too dense) they end up getting stuck: that nicely sums up the system investigated in a study just published in Nature Nanotechnology. \"We can suddenly switch from a state of superlubricity to one of extremely high friction by varying some parameters of the system being investigated. In this study, we used atoms of the noble gas xenon bound to one another to form two-dimensional islands, deposited on a copper surface (Cu 111). At low temperatures these aggregates slide with virtually no friction,\" explains Giampaolo Mistura of the University of Padua. \"We increased the size of the islands by adding xenon atoms and until the whole available surface was covered the friction decreased gradually. Instead, when the available space ran out and the addition of atoms caused the islands to compress, then we saw an exceptional increase in friction.\" The study was divided into an experimental part (mainly carried out by the University of Padua and Nano-Cnr\/University of Modena and Reggio Emilia) and a theoretical part (based on computer models and simulations) conducted by SISSA\/Iom-Cnr-Democritos\/ICTP. \"To understand what happens when the islands are compressed, we need to appreciate the concept of 'interface commensurability',\" explains Roberto Guerra, researcher at the International School for Advanced Studies (SISSA) in Trieste and among the authors of the study. \"We can think of the system we studied as one made up of Lego bricks. The copper substrate is like a horizontal assembly of bricks and the xenon islands like single loose bricks,\" comments Guido Paolicelli of the CNR Nanoscience Institute. \"If the substrate and the islands consist of different bricks (in terms of width and distance between the studs), the islands will never get stuck on the substrate. This situation reproduces our system at temperatures slightly above absolute zero where we observe a state of superlubricity with virtually no friction. However, the increase in surface of the islands and the resulting compression of the material causes the islands to become commensurate to the substrate \u2013 like Lego bricks having the same pitch \u2013 and when that happens they suddenly get stuck.\" sample of crystalline copper used as a \u2018sliding\u2019 substrate. Credit: Nano-Cnr, Modena The study is the first to demonstrate that it is possible to dramatically vary the sliding properties of nano-objects. \"We can imagine a number of applications for this,\" concludes Guerra. \"For example, nanobearings could be developed that, under certain conditions, are capable of blocking their motion, in a completely reversible manner.\" ","News_Title":"Varying the sliding properties of atoms on a surface","Topic":"Nano"}
{"Paper_Body":"Abstract Objective To evaluate if induction of labour at 41 weeks improves perinatal and maternal outcomes in women with a low risk pregnancy compared with expectant management and induction of labour at 42 weeks. Design Multicentre, open label, randomised controlled superiority trial. Setting 14 hospitals in Sweden, 2016-18. Participants 2760 women with a low risk uncomplicated singleton pregnancy randomised (1:1) by the Swedish Pregnancy Register. 1381 women were assigned to the induction group and 1379 were assigned to the expectant management group. Interventions Induction of labour at 41 weeks and expectant management and induction of labour at 42 weeks. Main outcome measures The primary outcome was a composite perinatal outcome including one or more of stillbirth, neonatal mortality, Apgar score less than 7 at five minutes, pH less than 7.00 or metabolic acidosis (pH <7.05 and base deficit >12 mmol\/L) in the umbilical artery, hypoxic ischaemic encephalopathy, intracranial haemorrhage, convulsions, meconium aspiration syndrome, mechanical ventilation within 72 hours, or obstetric brachial plexus injury. Primary analysis was by intention to treat. Results The study was stopped early owing to a significantly higher rate of perinatal mortality in the expectant management group. The composite primary perinatal outcome did not differ between the groups: 2.4% (33\/1381) in the induction group and 2.2% (31\/1379) in the expectant management group (relative risk 1.06, 95% confidence interval 0.65 to 1.73; P=0.90). No perinatal deaths occurred in the induction group but six (five stillbirths and one early neonatal death) occurred in the expectant management group (P=0.03). The proportion of caesarean delivery, instrumental vaginal delivery, or any major maternal morbidity did not differ between the groups. Conclusions This study comparing induction of labour at 41 weeks with expectant management and induction at 42 weeks does not show any significant difference in the primary composite adverse perinatal outcome. However, a reduction of the secondary outcome perinatal mortality is observed without increasing adverse maternal outcomes. Although these results should be interpreted cautiously, induction of labour ought to be offered to women no later than at 41 weeks and could be one (of few) interventions that reduces the rate of stillbirths. Trial registration Current Controlled Trials ISRCTN26113652 . Introduction Adverse perinatal outcomes gradually increase after 40 gestational weeks and are substantially increased post-term (\u226542 weeks (\u2265294 days)). 1 2 The risk of stillbirth has been shown to increase after term, 1 2 3 4 5 and worldwide as much as 14% of stillbirths are associated with prolonged pregnancy. 2 Furthermore, maternal complications also increase with duration of pregnancy after 40 weeks. 1 To date, no agreement exists on how to manage late term (41 weeks+0 days to 42 weeks+0 days) pregnancies. The World Health Organization recommends induction of labour at 41 weeks, 6 and many countries offer induction of labour between 41 and 42 weeks to avoid prolonged pregnancy. 7 8 Randomised controlled trials have compared induction of labour with expectant management in prolonged pregnancies, most with inconclusive results for perinatal mortality and major morbidity. 9 The results from the latest Cochrane review (2018) showed lower rates of caesarean delivery and perinatal death but a higher rate of operative vaginal delivery in the induction group compared with the expectant management group. 9 After the latest Cochrane review and after the initiation of the present study, 10 two large randomised controlled trials examining low risk pregnancies have been published. A large trial from the United States, ARRIVE (A Randomized Trial of Induction Versus Expectant Management), compared induction of labour in nulliparous women at 39 weeks+0 days to 39 weeks+4 days with expectant management until 41 weeks+0 days. 11 No significant difference was found in perinatal outcome between groups, whereas the frequency of caesarean delivery was significantly lower in the early induction group. Another large recent trial from the Netherlands, INDEX (INDuction of labour at 41 weeks with a policy of EXpectant management until 42 weeks), compared induction of labour at 41 weeks+0 days to 41 weeks+1 day with expectant management until 42 weeks+0 days. 12 The results could not confirm non-inferiority for adverse perinatal outcome of expectant management, instead a significantly higher risk of adverse perinatal outcome was found in the expectant management group. No significant difference in the rate of caesarean delivery was found. The current practice in many centres in the United Kingdom and Scandinavia is to induce delivery no later than 42 weeks, but several studies suggest that the risk of perinatal mortality and morbidity has actually already increased significantly at 41 weeks. 3 4 5 The risk of stillbirth increases gradually from 39 weeks of gestation 13 and increases exponentially as the pregnancy approaches 42 weeks, 3-5 13 whereas the risk of neonatal mortality is not increased until 42 weeks according to most studies. 3-5 13 We therefore found it clinically justified to compare induction of labour at 41 weeks with expectant management and induction at 42 weeks for maternal and perinatal outcomes. At the start of the present trial, only two studies (one was an abstract) out of 30 included in the Cochrane review specifically compared induction of labour at 41 weeks with expectant management until 42 weeks. 14 15 We evaluated if induction of labour at 41 weeks+0-2 days compared with expectant management and induction of labour at 42 weeks+0-1 days was superior in terms of perinatal outcome in healthy women with a low risk pregnancy. Methods Study design SWEPIS (SWEdish Post-term Induction Study) was a multicentre, open label, randomised controlled superiority trial conducted in Sweden from May 2016 to October 2018. The trial was register based, with randomisation and most data collection done by using the Swedish Pregnancy Register. 16 Fourteen hospitals with antenatal clinics linked to the register were involved in the trial. Five of the hospitals were university clinics and nine were county hospitals comprising about 60 000 deliveries per year of the around 115 000 to 120 000 annual deliveries in Sweden. The trial was conducted according to the CONSORT guidelines. The protocol is available online (  ) and as a publication. 10 The trial was undertaken within the Swedish Network for National Clinical Studies within Obstetrics and Gynaecology (SNAKS). Participants Pregnant women were eligible for participation if they were aged 18 or more, understood oral and written information, and had a singleton pregnancy with a fetus in cephalic presentation at 40 weeks+6 days to 41 weeks+1 day according to ultrasound based dating in the first or early second trimester or for pregnancies after assisted reproduction according to the day of oocyte retrieval. Exclusion criteria were previous caesarean delivery or other uterine surgery, pregestational and insulin dependent gestational diabetes, hypertensive disorder of pregnancy, known oligohydramnios (amniotic fluid index <50 mm or deepest vertical pocket <20 mm) or small for gestational age fetus (estimated fetal weight \u22642 standard deviations according to the sex and gestational age specific Swedish reference), 17 diagnosed fetal malformation, contraindication to vaginal delivery, and any other maternal condition affecting the progress of the pregnancy to 42 weeks. Study logistics General information about the study was provided in the form of posters or videos in the waiting rooms at the antenatal clinics and by advertising in local newspapers. More detailed information was provided on the study website. When the pregnancies were at around 40 weeks, the midwives provided women with an oral account of the study in Swedish or written information in any of 17 other languages applicable to women who were non-Swedish. In the Stockholm region (five clinics), women were enrolled in association with a 41 week ultrasound scan, which is offered to all pregnant women in the region. This is a voluntary procedure, with almost 100% coverage, aiming to confirm a normal pregnancy (defined as mean fetal abdominal diameter >110 mm and normal amniotic fluid) before proceeding to 42 weeks. The midwife performing the ultrasonography answered questions about the study and handled the randomisation after written informed consent was obtained. In all other centres, women interested in taking part were invited to visit a research midwife who managed patient consent and randomisation. Outside the Stockholm region, 41 week scans were not routinely offered. Randomisation and masking Randomisation was done between 40 weeks+6 days and 41 weeks+1 day. Enrolled women were allocated to the induction group or expectant management group (controls). In the induction group, labour was induced within 24 hours of randomisation (ie, same or next day) but not earlier than 41 weeks+0 days. In the expectant management group, labour was induced at 42 weeks+0 days to 42 weeks+1 day. Allocation to a trial group, 1:1, was done with central online randomisation by dynamic allocation, a method that actively minimises the imbalance between the groups for each new patient that is randomised. Centre and parity (primiparity versus multiparity) were used as minimisation variables. The Swedish Pregnancy Register 16 set up the randomisation module, which was incorporated in the register but separate from the register data. Access to the randomisation module used a separate log-in system. The module also included an electronic case report form. After delivery and the neonatal period, we used the women\u2019s unique personal identification number to retrieve data on antenatal, delivery, and neonatal characteristics from the Swedish Pregnancy Register and Swedish Neonatal Quality Register. 18 Because most variables in the study were included in the quality registers, the study could be performed relatively fast and at low cost. Owing to the nature of the intervention it was not possible to blind participants or care givers. Strategies Induction of labour was carried out in the same way in both groups. At admission, the women were examined for blood pressure, proteinuria, fetal presentation by abdominal palpation, cervical status, and fetal wellbeing by cardiotocography. Amniotomy was performed if the fetal head was well engaged and the cervix was ripe (Bishop score \u22656 for primiparous women and \u22655 for multiparous women), followed by oxytocin infusion after 1-2 hours without spontaneous regular contractions. If the fetal head was not engaged or the cervix was less ripe, any of the following methods was used according to local routines: mechanical dilation with a Foley-like catheter, prostaglandin E1 (misoprostol, oral or vaginal), or prostaglandin E2 (dinoprostone, vaginal). After randomisation, no monitoring was offered within the framework of the trial. In Sweden, most antenatal clinics offer one follow-up visit after term, usually around 41 weeks, including measurement of blood pressure, fundal height, and fetal heart rate by doptone. Further examinations, induction of labour, or caesarean delivery are initiated for usual obstetric indications, such as decreased fetal movements, suspected fetal growth restriction, or pre-eclampsia. After 41 weeks, the threshold for interventions is low. Indication for a scheduled caesarean section included undiagnosed breech or transverse presentation with failed external version. Fetal scalp blood sampling (pH or lactate) was performed during labour when indicated. Outcomes The primary outcome was a composite perinatal outcome of mortality and morbidity. Perinatal mortality was defined as stillbirth and neonatal death (days 0-27). Neonatal morbidity was defined as one or more of several outcomes: Apgar score less than 7 at five minutes, pH less than 7.00 or metabolic acidosis (pH <7.05 and base deficit >12 mmol\/L) in the umbilical artery, hypoxic ischaemic encephalopathy grades 1-3, intracranial haemorrhage, convulsions, meconium aspiration syndrome, mechanical ventilation within 72 hours, or obstetric brachial plexus injury. Secondary neonatal outcomes were the individual components of the primary perinatal outcome, admission to a neonatal intensive care unit, Apgar score less than 4 at five minutes, birth weight, macrosomia (\u22654500 g), neonatal jaundice, therapeutic hypothermia, pneumonia, or sepsis. Secondary maternal outcomes were use of epidural anaesthesia, caesarean delivery, operative vaginal delivery, duration of labour (from onset of regular contractions to delivery), chorioamnionitis, shoulder dystocia, third or fourth degree perineal tear, postpartum haemorrhage (>1000 mL), wound infection, urinary tract infection, endometritis, sepsis, and breastfeeding at discharge from hospital and at four weeks post partum. Exploratory neonatal outcomes were neonatal hypoglycaemia, birth trauma (fracture of long bone, clavicle, or skull, other neurological injury, retinal haemorrhage, or facial nerve palsy), small for gestational age, 17 and large for gestational age. 17 Exploratory maternal outcomes were cervical tear, uterine rupture, hypertensive disorders of pregnancy (pre-eclampsia, gestational hypertension, eclampsia), venous thromboembolism, duration of stay in hospital, admission to intensive care unit, and mortality within 42 days. Data collection We retrieved data on maternal background, pregnancy and delivery characteristics, and perinatal outcomes from the Swedish Pregnancy Register and the Swedish Neonatal Quality Register. 16 18 Both are certified national quality registers initiated by Swedish healthcare professionals. Data prospectively entered in standardised electronic medical records by midwives and clinicians during pregnancy, delivery, and post partum is forwarded to the Swedish Pregnancy Register from all antenatal clinics and most delivery clinics. In the same way, the Swedish Neonatal Quality Register collects data on all newborns admitted to neonatal intensive care units at birth or within 28 days of life. We obtained vital statistics on maternal and neonatal mortality from Statistics Sweden. Study data were linked with data from the Swedish Pregnancy Register, Swedish Neonatal Quality Register, and Statistics Sweden using the unique personal identification number allocated to each person in Sweden at birth or after immigration. 19 For all newborns with a primary outcome we collected and scrutinised the medical records. The same process was undertaken in the women with a diagnosis of endometritis to rule out misclassification of sepsis. To estimate selection bias we compared the baseline characteristics and pregnancy outcomes of our study population with those of the Swedish background population. Monitoring Before the trial started, an independent Data and Safety Monitoring Board comprising a statistician, senior obstetrician, and senior midwife was formed to supervise the trial through regular reviews. The principle investigators reported serious adverse events immediately to the Data and Safety Monitoring Board, defined as any of perinatal or maternal death; need for neonatal intensive care because of meconium aspiration syndrome, asphyxia, intracranial haemorrhage, or other severe condition; severe maternal morbidity with admission to intensive care unit; and complication associated with induction of labour, such as placental abruption at insertion of Foley catheter, or uterine rupture. An interim analysis was planned when 50% of the women had been recruited and had delivered. Sample size and statistical analyses To reduce the primary outcome by one third, from 2.7% to 1.8% (superiority testing, level of significance 0.05, power 80%) by induction of labour at 41 weeks compared with expectant management until induction at 42 weeks, we needed a sample size of 10 038 women, 5019 in each randomisation group. This calculation assumed that for 10% of the women, management would not be consistent with the assigned strategy, thus also covering the same power for the per protocol analysis as for the intention to treat analysis. The composite primary outcome of 2.7% was based on data on perinatal outcomes included in our primary outcome in one Swedish region (Region Sk\u00e5ne) between 2000 and 2010. The statistical analyses were carried out according to a prespecified analysis plan. Main analyses were performed on the intention to treat population. The primary statistical analysis was the comparison between the induction group and the expectant management group for the primary perinatal composite outcome, with Fisher\u2019s exact test (lowest one sided P value multiplied by 2) at a significance level of 0.05. To compare secondary outcomes, we used Fisher\u2019s exact test for dichotomous variables, Fisher\u2019s non-parametric permutation test for continuous variables, Mantel Haenszel \u03c7 2 test for ordered categorical variables, and Pearsons\u2019s \u03c7 2 test for non-ordered categorical variables. For the primary efficacy variable (the perinatal composite outcome) and dichotomous secondary variables we calculated relative risks with corresponding 95% confidence intervals between the groups. For continuous secondary variables we calculated mean differences with 95% confidence intervals between the groups. Data are presented as means with standard deviations, medians with interquartile ranges, and numbers with percentages, as appropriate. The intention to treat population included all randomised women except those who withdrew consent or were lost to follow-up. In the intention to treat group we included women with spontaneous labour or prelabour rupture of membranes after randomisation but before induction, or with pregnancy complications necessitating interventions for medical reasons. A post hoc sensitivity analysis for the primary efficacy analysis was performed adjusted for the minimisation variables centre and primiparity or multiparity using multivariable logistic regression analysis with centre as fixed effect. Complementary analyses were performed for comparison of the primary perinatal composite outcome and secondary efficacy outcomes on the per protocol population. This population comprised all randomised women who completed the study without important deviations from the protocol. We defined the criteria for protocol deviation before data were analysed. For the induction group, protocol deviation was defined as induction at less than 41 weeks+0 days; labour induction, spontaneous labour, or caesarean delivery at more than 41 weeks+2 days because of scheduling error or delivery room unavailability; patient or provider preference; and non-medically indicated elective caesarean delivery. For the expectant management group, protocol deviation was defined as induction at more than 42 weeks+1 day, induction of labour at less than 42 weeks owing to scheduling error or patient or provider preference, and non-medically indicated elective caesarean delivery. Prespecified subgroup variables were maternal age (\u226535 years), nulliparity, and body mass index (\u226530). Logistic regression with treatment subgroup variable and the interaction term treatment\u00d7subgroup variable was used to test whether the effect of treatment differed between subgroups. All significance tests were two sided at the 0.05 level. Statistical analyses were performed with SAS System Version 9 for Windows (SAS, Cary, NC). Patient and public involvement Pregnant women were not involved in the design, outcome measures, or recruiting plans of the study, and they were not asked to give advice on interpretation of results. The results of the research will be disseminated to the participants and public through broadcasts, popular science articles, and newspapers. Results On 2 October 2018 the Data and Safety Monitoring Board strongly recommended the SWEPIS steering committee to stop the study owing to a statistically significant higher perinatal mortality in the expectant management group. Although perinatal mortality was a secondary outcome, it was not considered ethical to continue the study. No perinatal deaths occurred in the early induction group but six occurred in the expectant management group (five stillbirths and one early neonatal death; P=0.03). Recruitment took place from 20 May 2016 to 13 October 2018. Oral and written informed consent was obtained from 2762 women, who underwent randomisation. Overall, 1383 women were assigned to induction at 41 weeks and 1379 were assigned to expectant management until induction at 42 weeks ( fig 1 ). Supplementary table A shows recruitment according to trial centre. After randomisation but before intervention, two women in the induction group withdrew their consent to participate and for their data to be used, thus 1381 women in the induction group and 1379 women in the expectant management group were included in the intention to treat analysis. The two groups were similar at baseline ( table 1 ). Fig 1 Flowchart of eligibility, randomisation, delivery, and assessment Download figure Open in new tab Download powerpoint Table 1 Baseline characteristics of intention to treat population. Values are numbers (percentages) unless stated otherwise View this table: View popup View inline Compared with the Swedish background population, women in the study groups had a higher level of education and were more often born in Sweden (see supplementary table B). In the induction group, 14.1% (195\/1381) of the women had spontaneous onset of labour, 85.5% (1181\/1381) underwent induction, of whom 76.6% (905\/1181) had cervical ripening, and 0.4% (5\/1381) had a scheduled caesarean delivery ( table 2 ). Table 2 Delivery outcomes in intention to treat population. Values are numbers (percentages) unless stated otherwise View this table: View popup View inline In the expectant management group, 66.7% (920\/1379) of the women had spontaneous onset of labour and 33.1% (457\/1379) were induced, of whom 74.4% (340\/457) had cervical ripening and 0.1% (2\/1379) had a scheduled caesarean delivery. Management was not consistent with the assigned strategy in 3.5% (48\/1381) of women in the induction group and 2.0% (28\/1379) in the expectant management group ( fig 1 ). Median time from randomisation to delivery was 2 days (interquartile range 1-2 days) in the induction group and 4 (2-7) days in the expectant management group ( table 2 , fig 2 ). Median gestational age at delivery was 289 (288-289) days in the induction group and 292 (289-294) days in the expectant management group. Fig 2 Gestational age at delivery in intention to treat groups. The induction group included 1380 women because one woman was incorrectly randomised before 40 weeks+6 days and delivered before 40 weeks+6 days Download figure Open in new tab Download powerpoint Primary outcome The primary outcome occurred in 2.4% (33\/1381) of women in the induction group and 2.2% (31\/1379) of women in the expectant management group (relative risk 1.06, 95% confidence interval 0.65 to 1.73; P=0.90) ( table 3 ). Table 3 Perinatal outcome in intention to treat groups. Values are numbers (percentages) unless stated otherwise View this table: View popup View inline No stillbirths or neonatal deaths (0-27 days) occurred in the induction group (mortality rate 0.0%), whereas there were five stillbirths and one neonatal death (mortality rate 0.4%) in the expectant management group (P=0.03) between 41 weeks+2 days and 41 weeks+6 days. One stillbirth occurred on the labour ward soon after admittance. The postmortem examination showed a cardiovascular malformation, which according to specialists in paediatric cardiology could not be considered as lethal. In the other four stillbirths there were no explanations. One stillborn neonate was small for gestational age and the other stillborns had birth weights within normal range. The neonatal death was due to hypoxic ischaemic encephalopathy in a large for gestational age neonate. The number needed to treat with induction of labour at 41 weeks to prevent one perinatal death was 230. A low Apgar score (<7 at five minutes) was the main contributor to the primary outcome: 1.3% (18\/1381) in the induction group compared with 1.2% (16\/1374) in the expectant management group (relative risk 1.12, 95% confidence interval 0.57 to 2.19; P=0.88). The post hoc sensitivity analysis for the primary outcome with adjustment for the minimisation variables centre and parity showed similar results (1.05, 0.65 to 1.59; P=0.85). Secondary neonatal outcomes Table 3 shows the secondary neonatal outcomes. An Apgar score of less than 4 at five minutes occurred in 0.2% (3\/1381) in the induction group and 0.1% (1\/1374) in the expectant management group (relative risk 2.98, 0.31 to 28.66; P=0.63). Fewer newborns in the induction group were admitted to a neonatal intensive care unit: 4.0% (55\/1381) in the induction group versus 6.0% (82\/1374) in the expectant management group (0.67, 0.48 to 0.93; P=0.02). If neonates with a major birth defect (n=10) were excluded (antenatally detected major birth defect was an exclusion criterion at study entry) there was no significant difference in admittance to a neonatal intensive care unit. Fewer neonates in the induction group had jaundice treated with phototherapy or exchange transfusion: 1.2% (16\/1381) in the induction group versus 2.3% (32\/1374) in the expectant management group (relative risk 0.50, 95% confidence interval 0.27 to 0.90; P=0.03). Fewer neonates in the induction group had macrosomia: 4.9% (68\/1381) in the induction group versus 8.3% (114\/1379) in the expectant management group (0.60, 0.45 to 0.80; P<0.001). Other secondary outcomes did not differ. Maternal outcomes Tables 2 and 4 present the secondary maternal outcomes. Use of epidural anaesthesia was higher in the induction group: 52.8% (729\/1381) in the induction group versus 48.5% (669\/1379) in the expectant management group (relative risk 1.09, 95% confidence interval 1.01 to 1.17; P=0.03). The median duration of labour was shorter in the induction group (5.7 hours (interquartile range 2.9-10.3 hours) v 6.9 (3.8-11.5) hours in the expectant management group; P<0.001). Mode of delivery was similar in both groups: the rate of caesarean delivery was 10.4% (143\/1381) in the induction group and 10.7% (148\/1379) in the expectant management group (relative risk 0.96, 95% confidence interval 0.78 to 1.20; P=0.79). Indications for caesarean delivery did not differ between the groups. Table 4 Maternal adverse outcomes in intention to treat population. Values are numbers (percentages) unless stated otherwise View this table: View popup View inline Endometritis occurred in 1.3% (18\/1381) of women in the induction group and 0.4% (6\/1379) in the expectant management group (relative risk 3.00, 95% confidence interval 1.19 to 7.52; P=0.02). Other secondary adverse maternal outcomes, including postpartum haemorrhage and perineal tears grades 3 and 4, were similar between the groups ( table 4 ). Hypertensive disorders of pregnancy after randomisation (exploratory outcome) occurred in 1.4% (19\/1381) of women in the induction group compared with 3.0% (42\/1379) of women in the expectant management group (relative risk 0.45, 95% confidence interval 0.26 to 0.77; P=0.004). Per protocol analysis The prespecified analysis of the per protocol population included 1333 women in the induction group and 1351 women in the expectant management group. Figure 1 shows the reasons for violation of the protocol. Baseline characteristics were similar between the groups (supplementary table C). The primary perinatal adverse outcome occurred in 31 pregnancies in the induction group and 31 in the expectant management group (relative risk 1.01, 95% confidence interval 0.62 to 1.66; P=1.0) (supplementary table E). No stillbirths or neonatal deaths (0-27 days) occurred in the induction group (mortality rate 0.0%), whereas there were five stillbirths and one neonatal death (mortality rate 0.4%) in the expectant management group (P=0.03). Supplementary tables D to F show the secondary neonatal and maternal outcomes. Subgroup analyses Prespecified subgroup analyses on the primary outcome and selected secondary outcomes according to parity (parity 1 v parity >1), maternal age (<35 years v \u226535 years), and body mass index (BMI) (<30 v \u226530) were performed on the intention to treat population. In the intention to treat population, analyses of the primary outcome showed no significant difference in the treatment effect according to parity, age, or BMI (P=0.29, P=0.70, P=0.51, respectively, for the interaction). In total, five stillbirths and one early neonatal death occurred, all in the expectant management group; in 0.8% (6\/753) of the nulliparous women versus 0% (0\/626) in parous women, 1.1% (3\/279) in women aged 35 or older versus 0.3% (3\/1100) in women younger than 35, and 1.1% (2\/184) in women with a BMI of 30 or higher versus 0.4% (4\/1081) in women with a BMI less than 30. Because of the low mortality rate (n=6) no interaction analysis on mortality could be performed. Among nulliparous women, the rate of caesarean delivery was 16.7% (127\/762) in the induction group and 17.3% (130\/753) in the expectant management group (P=0.81). When testing if the effect of induction versus expectant management was similar across centres (Stockholm centres versus other centres\u2014that is, offering or not offering a routine ultrasound scan at 41 weeks) no significant interaction effect was found for the primary outcome (P=0.19) in the intention to treat population. Perinatal mortality in the expectant management group was 0.0% (0\/557) in Stockholm centres versus 0.7% (6\/822) in the other centres. Discussion In this large randomised trial, comparing induction of labour at 41 weeks with expectant management and induction at 42 weeks, we found no significant difference in the primary composite adverse perinatal outcome\u20142.4% in the induction group and 2.2% in the expectant management group (relative risk 1.06, 95% confidence interval 0.65 to 1.73, P=0.90). Perinatal mortality was, however, significantly lower in the induction group (no deaths) than expectant management group (five intrauterine deaths, one neonatal death; P=0.03). Furthermore, the induction group had lower admittance to a neonatal intensive care unit, fewer infants with neonatal jaundice requiring therapy, and fewer macrosomic infants. We found no significant difference in caesarean delivery rates between groups. Comparison with previous studies Post-term pregnancy (\u226542 weeks) is associated with an increased risk of adverse perinatal morbidity and mortality. 3 4 5 The risk appears to increase gradually after 40 weeks. 3 4 13 Results from most meta-analyses indicate that a policy of induction before 42 full weeks is associated with decreased perinatal mortality. 9 22 23 24 In our study all perinatal deaths occurred in nulliparous women. Nulliparity is not always recognised as a factor conferring increased risk of perinatal mortality, 2 25 26 but our results agree with a Swedish register study where stillbirths were significantly more common in nulliparous than multiparous women and the increase in neonatal mortality was seen at 41 full weeks in nulliparous women but not until 42 weeks in multiparous women. 3 If this finding can be replicated in future studies, it could mean that nulliparous women may require particular attention, and interventions such as labour induction might be even more important in this group. The benefit of early induction is supported by a recently published open label multicentre randomised trial (INDEX) from the Netherlands including 1801 women, in which induction at 41 weeks was associated with a lower composite adverse perinatal outcome (1.7%) compared with expectant management until 42 weeks (3.1%; P=0.045). 12 The perinatal mortality rate did not, however, differ significantly between the groups, with one death in the 41 weeks group and two in the 42 weeks group. It could be argued that the higher mortality in the expectant management group in our study is partly due to lack of routine fetal surveillance with cardiotocography or ultrasonography between 41 and 42 weeks unless there were clinical signs of complications. In general, however, the adverse perinatal outcomes were not higher in the expectant management group in our trial compared with the INDEX trial, and the median gestational age at delivery was higher in the expectant management group in our trial (292 days) than in the INDEX trial (289 days), which could augment mortality rates. No perinatal deaths occurred among women recruited in the Stockholm region, where all women are offered a routine ultrasound scan at 41 weeks (before randomisation), with the aim of identifying women with an increased risk for adverse outcomes. However, the rarity of perinatal death limits the power of a subanalysis by centre. Furthermore, two of the five cases of hypoxic ischaemic encephalopathy occurred in Stockholm and the composite neonatal morbidity was similar between Stockholm (24\/1122=2.1%) and the other centres (35\/1633=2.1%), which does not support that the 41 week ultrasound scan was critical. It is also uncertain to what extent ultrasonography or cardiotocography usually performed at two or three day intervals can prevent intrauterine or neonatal deaths, 7 26 27 and the evidence supporting that fetal monitoring prevents complications of post-maturity is considered weak. 7 The occurrence of endometritis was significantly higher in the induction group than expectant management group, which was unexpected but might well be a chance finding. Recent studies indicate that infectious morbidity is not higher for mechanical methods than for drugs for cervical dilation, 28 and the occurrence of endometritis is similar or lower in our trial than reported in most studies on labour induction. 28 29 30 Furthermore, the frequency of other maternal infections (chorioamnionitis, wound infections, urinary tract infections) and neonatal infections (sepsis, pneumonia) was not higher in the induction group. Strengths and weaknesses of this study We carried out a large national multicentre randomised controlled trial comparing induction at 41 weeks with expectant management and induction at 42 weeks, the latter being standard of care in Sweden at present. Regardless that only a minority of eligible women were informed or accepted participation ( fig 1 ), the study population was representative of a Swedish low risk population according to most baseline characteristics (supplementary table B). Another strength is that the participants were managed at the same level of care and methods of induction were applied irrespective of allocation arm, which was not always the case in previous randomised trials on post-term pregnancies. 12 31 Our trial does have some limitations. Although it could seem contradictory that a significant difference was found between groups in perinatal mortality, we found no difference in the composite adverse neonatal outcome. However, five of the six deaths were stillbirths in our trial, which have a quite different cause and array of risk factors 32 compared with neonatal mortality and morbidity. 33 Placental abnormality or dysfunction, umbilical cord complications, and growth restriction are considered causes of stillbirth 2 32 that could well be of increasing importance in late and post-term pregnancies. Another problem is that the composite primary outcome was defined somewhat broadly, predominated by an Apgar score of less than 7 at five minutes, which according to recent data might be a relatively weak predictor of more serious outcomes such as neurological morbidity and mortality, therefore an Apgar of less than 4 at five minutes is probably preferable. 34 The advantage of composite outcomes, however, is that the number of cases in each arm can be reduced, and carrying out the study becomes more realistic. Pregnant women were not involved in the design of our trial, which is a limitation 35 despite our impression that management of late term and post-term pregnancies is a prioritised area of research for many women. In a separate survey, to be published, we will be addressing pregnant women\u2019s experiences in the 41 and 42 week groups. The fact that half of the women (those recruited in the Stockholm region) underwent ultrasound measurement of amniotic fluid volume and abdominal diameter at 41 weeks, whereas such examinations were not performed systematically at the other centres might be regarded as both a limitation and a strength. It is difficult to determine whether outcomes were affected by this difference in policy, whereas such a management increases generalisability and reflects current obstetric practice in Sweden. 36 It is not clear whether the results are broadly generalisable. The study did include university, regional, and local hospitals, and women from 17 countries were eligible for inclusion. Different methods for labour induction, according to local practice, were allowed, and one large region used an extra ultrasound scan in gestational week 41 before inclusion. All these strategies increase the generalisability of the results. Although we performed several significance tests, also for secondary and exploratory outcomes, we have not corrected for multiple comparisons owing to the risk of not finding differences of high clinical importance for women. Conclusions and policy implications Our study found that induction of labour at 41 weeks compared with expectant management and induction at 42 weeks does not alter the composite perinatal outcome, the primary outcome of this study. However, a reduction of the secondary outcome perinatal mortality is observed without increasing adverse maternal outcome. The number needed to treat with induction of labour at 41 weeks to prevent one perinatal death was 230, which is lower than previous estimates. 9 22 23 Although these results should be interpreted cautiously, based on previous reports and the results of the present trial we suggest that labour induction should be offered to women at 41 weeks+0 days 12 or earlier 11 37 and could be one (of few) interventions that reduces the rate of stillbirths. What is already known on this topic Meta-analyses comparing induction of labour at or beyond term with expectant management have shown a generally improved perinatal outcome with induction It is not known if induction at 41 weeks results in a better outcome than expectancy and induction at 42 weeks What this study adds Induction of labour at 41 full weeks in low risk pregnancies is associated with a decreased risk of perinatal mortality compared with expectant management and induction of labour at 42 full weeks Other neonatal outcomes or caesarean delivery did not differ between groups Women with low risk pregnancies should be informed of the risk profile of induction of labour versus expectant management and offered induction of labour no later than 41 full weeks Acknowledgments Jonas Eriksson S\u00f6derling provided data from the Swedish Pregnancy Register and performed the statistical analysis for the Data Safety Monitoring Board reports, Stellan H\u00e5kansson provided data from the Swedish Neonatal Quality Register, Jesper Brodin provided data from Statistics Sweden, and Agneta Cedefors-Blom helped with secretarial assistance. Mattias Molin and Per Ekman, the Statistical Consulting Group, Gothenburg, performed the statistical analyses. Therese Svanberg at the Medical Library at Sahlgrenska University Hospital performed the literature search. Thanks to the members of the Data Safety Monitoring Board, Hans Wedel (chairman), Lars-\u00c5ke Mattson, and Elisabeth Jangsten for their assistance and to the women who participated in the trial. The SWEPIS study group: the midwives and doctors responsible at the local centres were: Uppsala University Hospital: Irina Sylwe; South \u00c4lvsborg Hospital: Lena Loubelo, Carolina Bergerum, and Serney B\u00f6\u00f6j; Department of Gynaecology N\u00e4rh\u00e4lsan, M\u00f6lndal: Maria Bullarbo; Sahlgrenska University Hospital, G\u00f6teborg: PhD candidates Anna Wessberg and Helena Nilver, and Pia Hempel, Martina S\u00f6derlund, Erica Ginstr\u00f6m Ernstad, and Monica Eriksson Orrskog; Stockholm: Karolinska University Hospital Huddinge and Solna, South Hospital, Danderyd Hospital, South BB, S\u00f6dert\u00e4lje Hospital: Helen Fagraeus, Annelie Sj\u00f6lund, and Eva Itzel Wiberg; Halland Hospital: Elisabeth Johansson, Sandra Holmstr\u00f6m, \u00c5sa Ponten, and Maud Ankardal; \u00d6rebro Hospital: Inger Nydahl, Sofia Saarv\u00e4li, and Camilla Hartin; Falun Hospital: Elisabeth Nordstr\u00f6m and Kerstin Fransson; Visby Hospital: Madelen Jacobsson; and North \u00c4lvsborg Hospital: Maria Olsson and Anna Hagman. Footnotes Contributors: UBW and SS are joint first authors and contributed equally to the study. UBW, HH, VS, and HE conceived and designed the study. UBW, HH, AW, SS, AKW, MJ, HF, and JW oversaw recruitment of study participants and collection of data at the local centres. UBW, HH, CB, HE, OS, and SS wrote the statistical analysis plan together with two statisticians (Mattias Molin and Nils-Gunnar Pehrsson, the Statistical Consulting Group, Gothenburg). UBW and MA did the data cleaning together with statistician Mattias Molin and Per Ekman. UBW, HH, CB, SS, MA, LL, VS, SBW, OS, GW, HE, and AW interpreted the data. UBW, MA, AW, SS, and HH wrote the first draft of the manuscript, which was then critically reviewed and revised by the other coauthors. HE, OS, and HH are joint senior authors. All authors approved the final version of the manuscript for submission. UBW, SS, HE, OS, and HH are guarantors. All authors had full access to all the data in the study and take responsibility for the integrity of the data and the accuracy of the data analysis. The corresponding author attests that all listed authors meet authorship and that no others meeting the criteria have been omitted. Funding: This study was supported by the Swedish state under the agreement between the Swedish government and the county councils, the ALF-agreement (ALFGBG-440301, ALFGBG-718721, ALFGBG-70940, ALFGBG-426401), the Health Technology Centre at Sahlgrenska University Hospital, the Foundation of the Health and Medical care committee of the Region of Vastra Gotaland, Sweden (VGFOUREG387351, VGFOUREG640891, VGFOUREG854081), Hjalmar Svensson Foundation, the foundation Mary von Sydow, born Wijk donation fund, Uppsala-\u00d6rebro regional research council (RFR-556711, RFR-736891), Region \u00d6rebro County research committee (OLL-715501), the ALF-agreement in Stockholm (ALF-561222, ALF-562222, ALF-563222), and Centre for Clinical Research Dalarna-Uppsala University, Sweden (CKFUU-417011). The funders had no role in study design, data collection, data analysis, data interpretation, or writing of the report. The researchers were independent of the funders. Competing interests: All authors have completed the ICMJE uniform disclosure form at  and declare: no support from any organisation for the submitted work; no financial relationship with any organisation that might have an interest in the submitted work in the previous three years; no other relationships or activities that could appear to have influenced the submitted work. AKW has received free reagents (PlGF) from Roche for a prediction study of pre-eclampsia. Ethical approval: This study was approved by the regional ethics board in Gothenburg in May 2014 (Dnr: 285-14) and later its complementary applications (T 905-15, T 291-16, T 1180-16, T 330-17, T 1066-17, T 087-18, T 347-18, T 961-18, T 1110-18). All participants gave informed written consent before taking part in the study. Data sharing: The full dataset is available from the corresponding author on reasonable request. The corresponding author (UBW) affirms that this manuscript is an honest, accurate, and transparent account of the study being reported; that no important aspects of the study have been omitted; and that any discrepancies from the study as planned have been explained. The corresponding author (UBW) had the final responsibility for the decision to submit for publication. This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 4.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See:  . ","News_Body":"Inducing labour at 41 weeks in low risk pregnancies is associated with a lower risk of newborn death compared with expectant management (a \"wait and see\" approach) until 42 weeks, suggests a trial published by The BMJ. Although the overall risk of death at 42 weeks is low, the researchers say induction of labour should be offered to women no later than 41 full weeks. It is generally accepted that there is an increased risk of problems (\"adverse perinatal outcomes\") for both mother and baby at or beyond 42 weeks of pregnancy. Some studies have suggested that inducing labour from 41 weeks onwards improves these outcomes, but there is no international consensus on how to manage healthy pregnancies lasting more than 41 weeks. Current practice in the UK and Scandinavia is to induce delivery for all women who have not gone into labour by 42 weeks. So researchers in Sweden set out to compare induction of labour at 41 weeks with expectant management until 42 weeks in low risk pregnancies. The trial involved 2,760 women (average age 31 years) with an uncomplicated, single pregnancy recruited from 14 Swedish hospitals between 2016 and 2018. Women were randomly assigned to induction of labour at 41 weeks (1,381) or expectant management (1,379) until induction at 42 weeks if necessary. The main outcome was a combined measure of babies' health, including stillbirth or death in the first few days of life (known as perinatal death), Apgar score less than 7 at five minutes, low oxygen levels, and breathing problems. Other outcomes included admission to an intensive care baby unit, Apgar score less than 4 at five minutes, birth weight, pneumonia, or sepsis. Type of delivery and mothers' health just after giving birth were also assessed. For the main outcome measure, the researchers found no difference between the groups (2.4% of women in the induction group had an adverse perinatal outcome compared with 2.2% in the expectant management group). Other outcomes, such as caesarean sections and mothers' health after giving birth, also did not differ between the groups. However, six babies in the expectant management group died compared with none in the induction group, and the trial was stopped early. The researchers estimate that, for every 230 women induced at 41 weeks, one perinatal death would be prevented. They point to some limitations, such as differences in hospital policies and practices, that could have affected the results. But they say women with low risk pregnancies \"should be informed of the risk profile of induction of labour versus expectant management and offered induction of labour no later than at 41 full weeks. This could be one (of few) interventions that reduces stillbirth,\" they conclude. This view is supported in a linked editorial by Professor Sara Kenyon and colleagues, who say induction at 41 weeks \"looks like the safer option for women and their babies.\" They stress that choice is important within maternity care, and say \"clear information about available options should be accessible to all pregnant women, enabling them to make fully informed and timely decisions.\" ","News_Title":"Trial suggests inducing labor over 'wait and see' approach for late term pregnancies","Topic":"Medicine"}
{"Paper_Body":"Abstract Despite considerable progress in schizophrenia genetics, most findings have been for large rare structural variants and common variants in well-imputed regions with few genes implicated from exome sequencing. Whole genome sequencing (WGS) can potentially provide a more complete enumeration of etiological genetic variation apart from the exome and regions of high linkage disequilibrium. We analyze high-coverage WGS data from 1162 Swedish schizophrenia cases and 936 ancestry-matched population controls. Our main objective is to evaluate the contribution to schizophrenia etiology from a variety of genetic variants accessible to WGS but not by previous technologies. Our results suggest that ultra-rare structural variants that affect the boundaries of topologically associated domains (TADs) increase risk for schizophrenia. Alterations in TAD boundaries may lead to dysregulation of gene expression. Future mechanistic studies will be needed to determine the precise functional effects of these variants on biology. Introduction Since the first major study over 70 years ago 1 , twin, family, and adoption studies have strongly and consistently supported the existence of a genetic basis for schizophrenia 2 , 3 , 4 . Its inheritance is complex with both genetic and non-genetic contributions indicated by estimates of pedigree-heritability (60\u201365%) 3 , 4 and twin-heritability (81%) 2 that are well under 100%. Although these genetic epidemiological results were fairly consistent, their validity was dependent on multiple assumptions and contained specific information about genetic architecture. In the past decade, genome-wide association (GWA) studies that genotyped hundreds of thousands of single-nucleotide polymorphisms (SNPs) in tens of thousands of cases and controls have directly evaluated the common-variant SNP-heritability of schizophrenia 5 , 6 , 7 . In the most recent study of 40,675 schizophrenia cases and 64,643 controls, the SNP-heritability of schizophrenia was 24.4% (SE 0.0091, liability scale), and 145 significant loci were identified 6 . SNP array data can also be used to assess rare copy number variants (CNVs). In the largest study to date of 21,094 cases and 20,227 controls 8 , eight CNVs reached genome-wide significance: CNV deletions at 1q21.1, 2p16.3 ( NRXN 1), 3q29, 15q13.3, and 16p11.2 (distal) and 22q11.2 plus CNV duplications at 7q11.23 and 16p11.2 (proximal). These events were uncommon and any one of these eight CNVs were present in 1.42% of cases and 0.15% of controls. There is evidence that rare coding single-nucleotide variants (SNVs) and insertion\u2013deletions (indels) contribute to risk in a low percentage of cases although few genes have been implicated from exome sequencing 9 , 10 , 11 . Thus, after a decade of increasingly larger studies, the discovered genetic variants that confer risk for schizophrenia are primarily common variants with subtle effects on risk 6 , 7 , 9 , 10 . The interpretation of common variant findings is markedly improved via the addition of functional genomic data from brain 7 , 12 , 13 ; nonetheless, there remains a gap between the pedigree- and twin-heritability estimates for schizophrenia and its SNP-heritability. Some argue that this gap is irrelevant as these different types of heritability are incompatible and as biological insights have always been the core goal of GWA for schizophrenia rather than accounting for twin\/pedigree heritability. It is also possible that the heritability gap is informative, that SNP array and WES are missing etiologically important genetic variation. GWA genotyping directly captures 500K-1M SNPs followed by imputation to indirectly assess 7\u201310 M variants. This process is imprecise as some regions of the genome are not well covered, and some non-SNP types of genetic variation are missed. WES provides data on the protein-coding fraction of the genome (~3%) and will miss many regulatory features. By evaluating high-coverage WGS data on 21,620 individuals in the TOPMed study, Wainschtein et al. 14 reported recovery of nearly all of the pedigree heritability for height and body mass index. The missing heritability was found to reside in rarer genetic variation (minor allele frequency (MAF) 0.0001\u20130.1) in regions of relatively low linkage disequilibrium (LD) and often outside of protein-coding portions of the genome. The fundamental reason for the missing heritability of height and body mass may merely be technical: the least expensive technologies only partly assess the genome with inexpensive SNP arrays capturing common variants in high LD regions and WES capturing much of the known protein-coding genome. The Wainschtein et al. finding is consistent with prior observations that rarer and evolutionarily younger SNPs have higher SNP-heritability for multiple complex traits 15 . To capture genetic variation as comprehensively as possible, WGS is required. WGS provides nucleotide-level resolution throughout the accessible genome along with detection of most structural variants (SVs). Many types of genetic variation are discoverable by WGS without regard to local LD, and these include SNVs and indels in low LD regions, uncommon or rare regulatory variants, rare SVs missed by SNP arrays and WES due to small size or complexity, and common SVs missed by SNP arrays. The NHLBI TOPMed Program recently published high-coverage (30\u00d7) WGS data of 53,831 diverse individuals that included ~381 M SNVs and ~29 M indels 16 . TOPMed WGS identified 16% more variants than low-coverage WGS (6\u00d7), with essentially all new variants being rare (MAF < 0.005); and 17% more coding variants than both low-coverage WGS and WES (30\u00d7). The distribution of variant sites in TOPMed WGS revealed that the vast majority of human genetic variation is rare and noncoding. There are a few published WGS studies of schizophrenia (Supplementary Table 1 ). Of these studies, many employed family-based designs and the largest case\u2013control WGS study had 321 schizophrenia cases and 148 controls. In this study, we analyze high-coverage WGS from 1162 schizophrenia cases and 936 ancestry-matched population controls. WGS data are generated using identical protocols at the same facility and all WGS data are jointly processed and analyzed. The schizophrenia cases also have SNP array 17 , 18 and exome sequencing data 9 , 10 which is compared to WGS to assess data quality. Our main objective is to evaluate the contribution to schizophrenia etiology from variants that are revealed by WGS but not by GWA and WES. To quantify phenotypic variance explained by rare variants, we estimate heritability using WGS. To identify the role of noncoding variants, we focus on empirically determined maps of sequence constraints 19 , 20 and functional genomic annotations generated in human brain 12 , 13 . We particularly focus on ultra-rare variants as this frequency class has a notable impact on schizophrenia risk in WES and CNV studies 8 , 10 . We replicate key prior reported excess in schizophrenia of loss-of-function (LOF) ultra-rare sequence variants in LOF-intolerant genes. We find an increased burden in schizophrenia of ultra-rare SVs that affect the boundaries of topologically associated domains. Results Overview Figure 1 gives an overview of the study. Our workflow was designed to evaluate the contribution of directly genotyped genetic variation across the allelic spectrum and evaluate genetic variation missed by prior approaches. Fig. 1: Overview of WGS analysis. WGS data were generated using identical protocols at the same facility and all WGS data were jointly processed and analyzed. The schizophrenia cases also had GWA SNP array and exome-sequencing data for comparison for the purpose of quality assessment. We started with 1165 schizophrenia cases and 942 ancestry-matched population controls. After QC, 1162 cases and 936 controls remained. Variant annotation focused on empirically determined annotation methods. Full size image Study samples and sequencing Following quality control, we analyzed WGS on 1162 schizophrenia cases and 936 ancestry-matched population controls from Sweden (total 2098 subjects). Cases were selected to have typical Swedish ancestry, unequivocal schizophrenia case status, and without a known pathogenic CNV (e.g., 22q11 deletion). Controls were group matched to cases by ancestry. The median WGS coverage per sample was 36.62 reads per base (Supplementary Fig. 1 ). For each group, we constructed a curve for mean fraction of bases covered deeper than a specified threshold as a function of depth of coverage. The shapes of the mean curves were similar between cases and controls (Supplementary Fig. 2 ). Principal components analysis confirmed the relative homogeneity of the sample (Supplementary Fig. 3 ). We took multiple steps to minimize chances of spurious associations with schizophrenia: (1) WGS for all subjects was performed at the same facility using identical procedures; (2) all WGS data were jointly processed; (3) variant calling was conducted jointly for all subjects; (4) all subjects were ethnic Swedes of similar empirical ancestry (Supplementary Fig. 3 ); and (5) in association analyses, we controlled for empirically determined potential confounders to mitigate impact on spurious association signals (Methods). As discussed more fully below, we did not find evidence of inflation (e.g., for common variant case-controls tests, \u03bb GC was 1.03 and LD score regression intercept was 0.997 (SE = 0.0065), which are inconsistent with systematic biases). WGS variant identification SNV\/indels: We detected 33,746,530 SNVs and 4,551,507 indels across the autosomes of the 2098 cases and controls. Individual subjects had a mean of 2,358,544 SNVs (range: 2,063,297\u20132,513,607), and 383,929 indels (range 329,678\u2013409,893; mean insertion size 3.09 bp and mean deletion size 3.59 bp). Of the full set of unique SNVs and indels detected in the WGS data, 45.43% of SNVs and 37.03% of indels were detected in only one individual as heterozygotes (singletons). These data included many variants not found in imputation reference panels. For example, when requiring exact match for chromosome, position, reference, and alternative allele, 15,688,760 SNVs are not in Haplotype Reference Consortium (HRC r1.1) reference panel 21 (stratified by MAF: 286,599 MAF > 0.05, 163,346 MAF 0.005\u20130.05, 15,238,815 MAF < 0.005); 21,574,998 SNVs are not in 1000 Genomes Project phase 3 (1000GP p3v5) reference panel 22 (61,993 MAF > 0.05, 309,989 MAF 0.005\u20130.05, 21,203,016 MAF < 0.005), and 12,341,197 SNVs are not in TOPMed 16 Freeze 3a (stratified by MAF: 28,179 MAF > 0.05, 29,233 MAF 0.005\u20130.05, 12,283,785 MAF < 0.005). We also called 57,785 SNVs and 8270 indels on chrX, and subjects had a mean of 6084 SNVs (range 5035\u20137183) and 1753 indels (range 1334\u20132091). To evaluate the capacity of WGS to detect SNVs or indels, we compared our WGS data to independent exome sequencing data on 1154 of the 1162 schizophrenia cases 10 . We estimated genotype accuracy by calculating the concordance rate between genotypes from WGS and WES 10 for all autosomes. For SNVs, genotype accuracy was 0.9999, 0.999, and 0.997 for homozygous reference, heterozygous, and homozygous non-reference genotypes (Supplementary Table 2a ). For indels, genotype accuracy was 0.998, 0.984, and 0.984 for homozygous reference, heterozygous, and homozygous non-reference genotypes (Table S2a ). When stratified by MAF, genotype accuracy estimates were consistent across common, low-frequency, rare, and ultra-rare variants, and similar to the overall genotype accuracy (Supplementary Table 2b\u2013e ). SVs: We detected 17,895 deletion (DEL) sites, 4129 tandem duplication (DUP) sites, 4458 inversion (INV) sites, and 27,808 mobile element insertions sites (MEI, including 23,432 ALU, 1429 SVA, and 2956 LINE1). The sizes of DEL, DUP, and INV ranged from 500 bp to 1 Mb, with median sizes of 2592 bp for DEL, 7179 bp for DUP, and 3265 bp for INV (Supplementary Fig. 4 ). The sizes of MEI ranged from 15\u20136019 bp, with a median size of 279 bp for ALU, 1162 bp for SVA, and 1780 bp for LINE1 (Supplementary Fig. 5 ). For any non-reference genotype, subjects carried a mean of 1241 DEL (range 657\u20131357), 183 DUP (range 157\u2013209), 373 INV (range 321\u2013878), 2663 ALU (range 2077\u20133439), 82 SVA (range 56\u2013107), and 249 LINE1 (range 196\u2013302). To evaluate the capacity of WGS to detect SVs, we compared WGS data to prior copy number variant data from GWA SNP array 18 and WES 23 on 1085 of the 1162 schizophrenia cases. First, INV, MEI, and common SVs are largely inaccessible to SNP arrays 18 and WES studies 23 . Second, prior GWA SNP array studies were limited to deletions and duplications >100 kb; however, >95% of DEL and >77% DUP detected from WGS were <20 kb. Consequently, SNP arrays found only 3.5% of DEL variants and 17.7% of DUP variants found by WGS (requiring 50% reciprocal overlap). Third, when restricted to exons, WES found only 13.7% of exonic DEL and 35.6% of exonic DUP variants found by WGS (based on 50% reciprocal overlap). Finally, for DEL and DUP variants that are comparable between technologies, we computed concordance rates between WGS and SNP array or WES (Supplementary Table 3 ). When compared to SNP arrays, we estimated that the concordance rate was 0.992 for DEL and 0.965 for DUP. When compared to WES, we estimated that the concordance rate was 0.987 for DEL and 0.967 for DUP. Repeat expansions: WGS can detect pathogenic disease-associated repeat expansions (e.g., the HTT CAG repeat that causes Huntington\u2019s disease), which are inaccessible to SNP arrays. We screened our samples for repeat expansions in 16 genes that are established causes of disease, and found that 16 cases and 7 controls had modest repeat expansions just within the predicted pathogenic range (Supplementary Table 4 ). Because no case or control had a register diagnosis consistent with these generally highly penetrant disorders, we assumed these were false positives or the modest repeat expansions were not long enough to cause disease. Burden analysis of ultra-rare SNV\/indels Consistent with recent studies 10 , we focused on ultra-rare sequence variants (URVs) including ultra-rare SNVs and indels. We defined URVs as found once in the WGS case\/control cohort and absent from independent population cohorts (i.e., gnomAD r2.0.2 allele count = 0 and non-psychiatric subset of ExAC r0.3 allele count = 0) 24 , 25 . From theory 26 and our calculations (Supplementary Fig. 6 ), power is low for single-variant analysis for MAF < 0.01. Collapsing methods are key approaches for rare variants and can enhance power by accumulating information across different rare variants that impact a gene\/locus or a set of genes\/loci 27 . We used burden testing as the primary analytical tool to contrast cases and controls for total event counts in genomic loci of interest. Burden testing is appropriate when most variants across a set of genetic loci impact phenotype in the same direction and with similar magnitude 27 . We estimated statistical power for burden tests and found that we had \u226580% power to detect association of URVs when the aggregated minor allele count (MAC) was 20 (i.e., aggregated MAF = 0.01) and the genotypic relative risk was \u22654.9 (assuming a type I error level of 1 \u00d7 10 \u22125 ). As a final step in quality control and following an approach previously established in the full Swedish sample 10 , we pruned samples that had an outlier total URV count mostly because of relatively higher ancestry heterogeneity 10 (Methods, Supplementary Figs. 7 and 8 ). We conducted burden analyses of URVs in 1104 cases and 921 controls (mean URV counts in cases vs controls: 4262 vs 4249, P = 0.4225, Supplementary Fig. 7 ). The total number of qualifying URVs in these samples was 8,073,782, of which 7,991,557 (98.9%) were noncoding. Full results are listed in Supplementary Table 5 and summarized below. For multiple-testing adjustment, we applied the Benjamini and Hochberg false discovery rate (BH-FDR) method to the family of hypotheses involving ultra-rare SNV\/indels which included a total of 74 tests (Supplementary Table 5 ). Confirmation of prior results: We first evaluated the prior WES finding that schizophrenia cases have an excess of damaging protein-coding URV (odds ratio [OR] = 1.07; 4877 cases and 6203 controls) 10 . As shown in Fig. 2 , we found an excess of LOF URVs in schizophrenia cases (OR = 1.082, P = 0.0002, BH-FDR multiple-testing adjusted P = 0.0049). This excess was notable (OR = 1.203, P = 0.0005, adjusted P = 0.0092) in genes that are intolerant to LOF variation (defined as pLI > 0.9 in the non-psychiatric subset of ExAC 24 , where pLI is the probability that a gene is intolerant to a LOF mutation). Increased burden was prominent in the subset of LOF-intolerant genes that are risk genes from WES for neurodevelopmental disorders 11 (OR = 2.983, P = 0.0011, adjusted P = 0.0163). A key advantage of WGS over WES for protein-coding regions is independence of design, coverage, and performance of exome capture baits 16 . The exome capture baits used in WES are imperfect, however, after multiple testing correction, we did not find any significantly increased burden of coding URVs outside of targeted exonic sequences of LOF-intolerant genes (Supplementary Fig. 9 , Supplementary Table 5 ). Fig. 2: Burden of coding ultra-rare SNVs and indels. X -axis: annotation class. Y -axis: odds ratio. Legend: exomes: coding variants in all genes; LOFtol: in genes tolerant to loss-of-function variation; LOFintol: in genes intolerant to loss-of-function variation. For each specific burden test, we used a vertical line to indicate the 95% confidence interval of odds ratio and a dot at the center of the vertical line to indicate the point estimate of odds ratio. Full size image Burden analysis of noncoding ultra-rare SNV\/indels in constrained regions: We defined variants as putatively noncoding if they did not alter sequence content of coding regions or splice dinucleotides of GENCODE protein-coding transcripts. These noncoding variants may confer risk via a variety of mechanisms (e.g., by altering an unannotated protein-coding transcript, untranslated regions, splicing, transcription factor binding, or an epigenetic site). We evaluated burden of noncoding variants that are more likely to be deleterious by focusing on ultra-rare noncoding variants that are likely to be subject to purifying selection in a manner similar to coding URVs. We compared case\/control burden of noncoding URVs across binned regions by sequence constraint for the human species 19 and by constraint across mammalian species 20 (Supplementary Fig. 10 ). The human constraint was built upon the context-dependent tolerance score (CDTS) which indicates the degree of depletion of genetic variation at the population level using 11,257 human genomes (the lower the percentile rank of CDTS the more constrained the region) 19 . The mammalian constraint was based on genomic evolutionary rate profiling (GERP) score which quantifies substitution deficits in multiple alignments (the higher the GERP score, the more constrained the region) 20 . We concentrated subsequent noncoding URV analyses on variants in regions that were highly constrained according to one of these two metrics (CDTS < 1% or GERP \u2265 4) due to prior observation that the overlap between CDTS (conservation in the current human population) and GERP (interspecies conservation) was limited and heavily enriched for protein-coding regions 19 . We did not observe a case excess of noncoding URVs that survived multiple test correction based on this criterion alone (OR = 1.009, P = 0.0342, adjusted P = 0.2819, Supplementary Fig. 10 ). Burden in annotations experimentally derived from human brain: Annotations from appropriate tissues help predict functional variants 7 , 28 . We compared case\/control burden of noncoding URVs in constrained regions (as defined above CDTS <1% or GERP \u2265 4) within functional annotations experimentally derived from human brain tissue known to effect gene expression. These annotations include open chromatin regions from ATAC-seq, frequently interacting regions (FIREs), topologically associating domains (TADs), and chromatin interactions from Hi\u2013C; epigenetic marks from ChIP-seq (CTCF, H3K27ac, and H3K4me3). We also included annotations of brain-expressed exons identified from long-read RNA-seq data 29 , as constrained noncoding URVs inside brain exons could impact functional noncoding elements within untranslated regions of annotated transcripts or protein-coding sequences from unannotated transcripts. We did not identify any single annotation with a significant case excess of URVs within constrained regions (Supplementary Fig. 11 , Supplementary Table 5 ). Burden in promoter regions: A recent study focused on de novo SNV\/indels found evidence for a contribution to autism spectrum disorder from variants in constrained nucleotides within promoter regions 30 . Defining promoter regions the same way as in An et al. 30 (2 kilobases (kb) upstream of an annotated transcription start site), we compared case\/control burden of noncoding URVs within constrained nucleotides (as defined above) in promoter regions of genes that are putatively LOF-intolerant (as defined above). No significant case excess was observed (OR = 0.966, P = 0.9812, adjusted P = 0.9907). A similar result was obtained when performing this test specifically on the subset of LOF-intolerant genes previously described as neurodevelopmental risk genes 11 (OR = 0.99, P = 0.5551, adjusted P = 0.6956). To take the three-dimensional genome into account, we used brain chromatin interaction data to identify any cis-regulatory elements (e.g. promoter and enhancers) connected with LOF-intolerant genes. No significant case excess was observed (OR = 1.006, P = 0.1904, adjusted P = 0.427). X chromosome: We tested male cases and controls to determine if the coding variant excess replicated in chrX genes. We did not detect a significant difference in synonymous variant burden or LOF variant burden but note that power was low. Burden analysis of ultra-rare SVs We performed analyses of ultra-rare SVs on the full sample (1162 cases and 936 controls) and on the subsample used for URV burden testing (1104 cases and 921 controls). Note that cases with known pathogenic CNVs or unusually high CNV burden were excluded 18 . Because all results were similar, we report the analysis results using the full sample. The total number of ultra-rare SVs in our sample was 6809 for DEL, 1917 for DUP, and 729 for INV. The sizes of these ultra-rare SVs were smaller than those from SNP arrays (DEL mean 15.2 kb for cases and 13.8 kb for controls; DUP mean 56.5 kb for cases and 52.6 kb for controls; and INV mean 100 kb for cases and 76 kb for controls). Confirmation of prior results: Higher genome-wide burden of rare SVs in schizophrenia cases has been repeatedly observed in studies using SNP arrays 8 , 18 (i.e., rare, large SVs with MAF < 0.01 and size > 100 kb). Burden was greater for SVs that were deletions, larger, or rarer. To calibrate our analyses, we verified this general pattern of findings using WGS SV calls (Supplementary Table 6 ). Genome-wide burden of ultra-rare SVs: Using the DEL, DUP, and INV genotypes described above, we evaluated the genome-wide burden of ultra-rare SVs (Supplementary Fig. 12 and Supplementary Table 7 ). We defined ultra-rare SVs as found once in the WGS case\/control cohort and absent from independent population cohorts 31 , 32 . Consistent with previous reports 8 , 18 , ultra-rare DEL were significantly enriched in cases (OR = 1.086, P = 0.0001, BH-FDR multiple-testing adjusted P = 0.0029). The burden of ultra-rare DUP and INV were similar between cases and controls (DUP: OR = 1.06, P = 0.0920, adjusted P = 0.2052; INV: OR = 1.015, P = 0.2903, adjusted P = 0.4009). Most of these ultra-rare SVs were noncoding (Supplementary Fig. 13 , 87.2% for DEL, 71.1% for DUP, and 89.4% for INV). When stratified by coding\/noncoding status, the results were similar (Supplementary Table 7 ). Burden in epigenomic annotations from human brain: We hypothesized that the elevated genome-wide burden of ultra-rare SVs may be partitioned across functional elements with evidence for gene regulation in the brain 13 . We focused on ultra-rare SVs that intersected \u226510% of the functional elements (Fig. 3 , Supplementary Table 8 ). Burden tests found a significant enrichment of ultra-rare SVs in schizophrenia cases that impacted TAD boundaries from adult (OR = 1.613, P = 0.0037, adjusted P = 0.0283) and fetal brain (OR = 1.581, P = 0.0039, adjusted P = 0.0283). No significant enrichment was found for any other class of functional elements. TAD boundaries have been shown to be under purifying selection. Multiple studies suggest that altering TAD boundaries results in the disarrangement of enhancer and promoter contacts, thus impacting local gene expression. Disruption of TAD boundaries by SVs have been associated with developmental disorders 33 , 34 . Fig. 3: Burden of ultra-rare SVs in brain epigenomic annotations and related analysis. For each specific burden test, we use a vertical line to indicate the 95% confidence interval of odds ratio and a dot at the center of the vertical line to indicate the point estimate of odds ratio. Labels on X -axis indicate the specific annotations that were considered. \u201cTADbou\u201d: TAD boundaries. Epigenomic annotations include TADbou.AdultBrain, TADbou.FetalBrain, ATACseq.AdultBrain, FIRE.AdultBrain, CTCF, H3K27ac, and H3K4me3. Regulatory elements connected with schizophrenia risk loci are labeled as \u201cgene.set.name_HiC.loops.int\u201d. For example, \u201cCELF4_ HiC.loops.int\u201d means regulatory elements of the CELF4 gene set identified via chromatin interaction (a.k.a HiC loops) data in human brain. Detailed information about gene sets considered can be found in Methods. Amongst the loci tested, only TAD boundaries derived from both fetal and adult brain tissue showed a significant degree of evidence for excess in cases relative to controls. Full size image Burden in regulatory elements connected with schizophrenia risk loci: We hypothesized that the elevated genome-wide burden of ultra-rare SVs may be partitioning to regulatory elements within schizophrenia risk loci. To take the three-dimensional genome into account, we used chromatin interaction data from adult brain to identify regulatory elements connected with schizophrenia risk loci, capturing any empirically defined cis-elements either nearby or distal 13 . As above, we performed a burden test using the 10% overlap criterion for any ultra-rare DEL, DUP, or INV in the regulatory elements of these schizophrenia risk loci. No significant enrichment in schizophrenia cases was found (Fig. 3 , Supplementary Table 8 ). Validation and analysis of ultra-rare TADs-affecting SVs To gain a deeper understanding, we followed up on the finding of significantly increased burden of ultra-rare SVs that affected TAD boundaries. We found that a higher rate of variants in cases versus controls was present when those variants were stratified by coding or non-coding status (Supplementary Fig. 14 , Supplementary Table 9 ) or by variant type (i.e., DEL, DUP, or INV; Supplementary Fig. 15 , Supplementary Table 10 ). Burden was greater for those variants that were DEL, or had larger overlap with TAD boundaries. Next, we attempted to verify the validity of those TADs-affecting ultra-rare DEL and DUP that were detected in schizophrenia cases. First, we looked up the GWA array data in the same samples (Supplementary Table 11 ). We found that 27.9% of these DEL and 52.6% of these DUP were concordant with GWA array data (50% reciprocal overlap) and were additionally confirmed by inspecting their WGS read alignments using IGV 35 (Supplementary Figs. 16 and 17 ). The remaining variants that were not found from GWA array data were notably smaller in size (median 7.6 kb) than those concordant (median 181 kb), suggesting that they may have been missed by GWA array technology. Second, for variants not verifiable using GWA arrays, we manually inspected their WGS read alignments using IGV 35 (Supplementary Figs. 18 and 19 ), and all were confirmed. Finally, we evaluated genomic features nearby those TADs-affecting ultra-rare SVs that were detected in schizophrenia cases (Supplementary Table 12 ). We found that these SVs span 4 \u2013 995 kb and 71% of them (67 out of 94) overlapped \u22651 genes. There was a notable difference between TADs-affecting ultra-rare DEL and DUP: 44.7% (17 out of 38) of DUP overlapped genes had high pLI scores or were genes implicated in schizophrenia or neurodevelopmental disorders, whereas 16.3% (7 out of 43) of DEL overlapped genes had high pLI scores or were implicated in neurodevelopmental disorders (H 0 : no difference between DEL and DUP, Fisher\u2019s exact test P = 0.0072). Furthermore, 36.8% (14 out of 38) of the DUP connected with 43 genes with high pLI scores or implicated in neurodevelopmental disorders via a high-confidence regulatory chromatin interaction (HCRCI); whereas 18.6% (8 out of 43) of the DEL connected with 18 genes with high pLI scores or implicated in neurodevelopmental disorders via a HCRCI (Fisher\u2019s exact test P = 0.0824). Our observations are consistent with a previous report that duplications display a more complex relationship with chromatin features than deletions 36 . INV was similar to DUP (Supplementary Table 12 ; H 0 : no difference between INV and DUP, Fisher\u2019s exact test P = 0.53). Common variants with large effects were not identified Because SNP arrays do not cover the entire genome even with imputation, we performed single-variant association analysis for all common variants obtained from WGS. Given the sample size of 2098 (1162 cases and 936 controls), we estimated that our sample had \u226580% power to detect risk variants with MAF = 0.25 and genetic relative risks \u22652.0, assuming a type I error level of 5 \u00d7 10 \u22128 (Supplementary Fig. 6 ). SNV\/indels: We analyzed 7,895,148 SNVs and 1,368,675 indels with MAF > 0.01 for association with schizophrenia (Supplementary Fig. 20 ). We obtained a \u03bb GC value of 1.03 and LD score regression intercept of 0.997 (SE = 0.0065), indicating no departure from null expectations or uncontrolled bias. Single-variant association analysis was done using logistic regression assuming an additive genetic model including PC2 as covariate for autosomes, and sex and PC2 as covariate for chrX. A number of variants exceeded genome-wide significance but, upon review, all 15 were false positives due to lack of read alignment support. These results are consistent with accumulated experience in schizophrenia genomics where larger sample sizes are required to detect common variant associations 7 . We believe that this null result is important: we have excluded the possibility of common variants (MAF > 0.01) with large effects in less accessible parts of the genome that have not been evaluated by GWA SNP arrays. SVs: Association analysis of common SVs has the potential to identify causative mutations leading to actionable findings, and much of this class of variants is inaccessible to SNP-based studies. Here we performed association analysis for SVs with MAF > 0.01, using logistic regression models and covariates as described above. The main analysis was for 2199 common DEL (Supplementary Fig. 21 ) but no association reached genome-wide significance. We then inquired into common DUP, INV, ALU, LINE1, and SVA, but also found no significant associations (Supplementary Figs. 22 \u2013 26 ). Heritability estimation using WGS Heritability is the proportion of phenotypic variance explained by genetic factors. Understanding the sources of missing heritability for schizophrenia \u2013 the discrepancy between pedigree-heritability of 60\u201365% 3 , 4 and common-variant SNP-heritability of 24% 6 \u2013 is important for experimental designs to identify additional trait loci and possibly for subsequent precision medicine initiatives. Using WGS data for height and body mass index, Wainschtein et al. recently found WGS-heritability very close to twin\/pedigree heritability 14 . WGS allowed them to include effects in genomic regions of low MAF and low LD, precisely the regions that are poorly captured by typical SNP arrays or imputation. Following Wainschtein et al.\u2019s approach 14 , we estimated schizophrenia heritability from our WGS data using 1151 cases and 911 controls (post-QC subjects and pairwise genetic relatedness < 0.05), and 17,364,971 sequence variants (post-QC autosomal SNV\/indels observed \u2265 3 times or MAF \u2265 0.0007). To evaluate the effect of progressive inclusion of more variants, we computed heritability in different ways by selecting WGS variants that corresponds to variant locations in HapMap3 37 , those imputable from 1000 G p3v5 22 and HRC r1.1 21 , and finally by including all WGS variants. First, we assessed common SNP-heritability in the WGS sample using the GREML single-component method implemented in GCTA 38 , 39 . Using 1,189,077 SNPs from WGS that corresponds to the SNP locations in HapMap3, the SNP-heritability was 0.45 (standard error [SE] 0.089, liability scale assuming lifetime risk of 1%). Using 7,141,717 SNV\/indels from WGS that corresponds to the variant locations imputable from 1000GP p3v5, the SNP-heritability was 0.48 (SE 0.091). These estimates are numerically greater than that estimated from SNP arrays in the full Swedish sample (5001 cases; GCTA SNP-heritability using HapMap3 data: 0.32, SE 0.03, and using 1000 Genomes data: 0.33, SE 0.03) 6 , presumably due to the fact that more stringent evidence of schizophrenia was used for samples selected for WGS than that in the full sample (Methods). Next, we evaluated SNP-heritability using 8,498,854 SNV\/indels from WGS that corresponds to the variant locations imputable from HRC r1.1. We used the recommended GREML-LDMS method in GCTA 39 , 40 because it is unbiased regardless the properties (e.g. MAF and LD) of the underlying causal variants (Supplementary Fig. 27 a). The estimated SNP-heritability was 0.52 (SE 0.22). Finally, we used all sequence variants (17,364,971 as above) from WGS and the GREML-LDMS method 39 , 40 to estimate WGS-heritability and partition additive genetic variance. We found the estimated WGS-heritability was 0.56 (SE 0.51). The point estimate of 0.56 is closer to pedigree-heritability (0.6\u20130.65, refs. 3 , 4 ), but the SE is large. For rare variants with MAF 0.0007\u20130.01, WGS variants in the low-LD group contributed to 0.40 of the phenotypic variance whereas variants in the high-LD group contributed to 0.01 of the variance (Supplementary Fig. 27 b). In contrast, for HRC-imputable variants, 0.06 and 0.03 of the phenotypic variance was contributed by variants in the low- and high-LD groups for MAF 0.0007\u20130.01 (Supplementary Fig. 27 a). The contribution to phenotypic variance from rare variants in low-LD with nearby variants was only revealed by WGS. These variants could only be directly assayed by WGS as they are not present in SNP arrays and their imputation is not accurate 14 . In sum, the point estimates for heritability were progressively larger as we included more variants and there was a sizable contribution from rare variants with low-LD metrics that are accessible only via WGS. However, our estimates of SNP- and WGS-heritability had large standard errors. This was due to limited sample size and case-control study design (i.e. not continuous trait as height or body mass). The WGS-heritability estimate had the largest SE which was additionally due to the large number of rare variants with low MAF and low LD. The sampling variance of SNP-based heritability estimate is approximately inversely proportional to sample size and is proportional to the effective number of independent variants 41 , 42 Furthermore, we likely underestimated WGS-heritability, especially the contribution from rare variants with MAF < 0.001: First, Wainschtein et al. 14 was able to include WGS variants with MAF as low as 0.0001 (corresponding to MAC \u2265 3 in TOPMed data with 21K subjects), whereas in this study we were limited to a minimum MAF of 0.0007 (corresponding to MAC \u2265 3 in 2K subjects). Second, based on a simulation using AbCD 43 assuming 2062 EUR individuals and 30x WGS, we have >99% power to detect variants at MAF > 0.001 but only >53% power to detect variants for MAF 0\u20130.001. The lowest MAF bin (MAF 0.0007\u20130.001) that we were able to consider in this study likely included only half of the variants that could have been observed in a sample with 10,000 subjects. We believe it notable that, although not conclusive, our results for schizophrenia are consistent with those of Wainschtein et al. for height and body mass 14 , 16 . These results imply that, with larger schizophrenia samples (e.g. a sample size of >33,000 is needed to obtain an SE of 0.02, refs. 14 , 41 , 42 ), WGS data may be able to fully recover the total additive genetic variance with desired precision and will allow further partitioning of the genome to finer MAF\/LD groups as well as a variety of functional annotations 14 , 42 . The still missing heritability of schizophrenia may be only misplaced, in precisely the blind spots of SNP arrays as has been anticipated for over a decade 44 . Discussion We have generated and analyzed a collection of WGS data for a set of patients ascertained for schizophrenia that to our knowledge is the largest described in a publication. The high depth and uniformity of coverage across the genome for these case data allowed us to detect the large majority of genetic variation that are present in the genome, including SNVs, indels, CNVs, mobile element insertions, and inversions. In addition, the availability of similar WGS data from Swedish controls allowed us to systematically measure the burden of these different classes of variation in a case\/control manner. Through the analysis of these data, we were able to replicate key prior reported excess in schizophrenia of LOF URVs in genes that are putatively LOF-intolerant as well as excess of rare deletions genome-wide. This means that we can be more confident that the load of such variants, while modest compared to the identified contribution of common variation to schizophrenia risk, are a subset of the total schizophrenia genetic risk architecture. Our finding that ultra-rare SVs in schizophrenia cases are enriched at TAD boundaries is not surprising. These variants seem to confer a level of relative risk comparable to protein-coding variants for which we have replicated an excess in schizophrenia. These regions have been reported as being depleted of deletions in human populations relative to the rest of the noncoding genome 36 , and clear phenotypic consequences associated with deletion of these elements have already been demonstrated in a number of other diseases 34 . TAD boundaries are critical to the formation and maintenance of chromatin structure 13 . The disruption of these boundaries has the potential to rearrange spatial orientation of regulatory elements that are needed for proper expression, as well as lead to the formation of entirely new TADs. Functional examples of such effects have already been described in mouse models for limb malformation 33 . Based on these prior observations it is unsurprising that of all noncoding loci, the burden of these SVs appears to be highest relative to controls in TAD boundaries. While our data support an excess of TAD-affecting ultra-rare SVs in schizophrenia cases relative to controls, the precise impact of these variants on gene expression and regulation has yet to be determined. Many of these SVs overlapped genes including some of the risk genes for neuropsychiatric disorders. Mechanistic studies are needed to clarify the precise genomic consequences of these TADs-affecting SVs in human brain. A possible future investigation would be to work with patient derived cells with these TADs-affecting SVs that we have identified and figure out what promoter-enhancer pairing looks like, and if there are any potential changes in gene expression. Our study has highlighted a specific hypothesis for future functional analyses. It will be critical to determine the precise functional effects of these variants on biology, which, in a manner similar to common variant risk, are likely to converge on higher order architectures of gene regulation 7 . We chose not to analyze rare mobile element insertions because variant calling for these variants appear to be noisy from our 30\u00d7 WGS and there was a lack of external dataset or analytic approach for the need of quality control. Increased somatic L1 insertions have been recently reported in neurons of schizophrenia patients using postmortem brain tissues 45 . The detection of somatic L1 insertions required very deep WGS (e.g. 200\u00d7) and tailored analytic methods (e.g. machine learning 45 ). For similar reasons, we also chose not to evaluate translocations and complex SVs in this study as we feel that these variants can be better detected from WGS using long-insert jumping libraries, deeper coverage, and targeted capture of breakpoints 46 . The analysis of noncoding variants from WGS data is challenging due to the sheer volume of the noncoding genome and limited methods to predict functional changes 28 , 30 , 46 . Recently the category-wise association study (CWAS) framework has been developed and applied to WGS studies of autism spectrum disorder using 7608 samples from 1902 families 28 , 30 , 46 . The CWAS approach applies multiple annotation methods to define tens of thousands of annotation categories each of which are tested for association and accounted for multiple testing. However, there is a trade-off between false positives and false negatives. In this study we adopted the spirit of the CWAS approach and focused on empirically determined annotation methods including (1) conservations of DNA sequence that were estimated from cataloging and comparing genetic variation across human and mammalian species 19 , 20 , (2) multiple epigenomic annotations that were experimentally generated from human brain 12 , 13 , (3) genes and regions that were empirically associated with psychiatric disorders. This approach combined with the relative homogeneity of the Swedish sample helped improve the power to identify functional variants while controlling for false discovery rate. We failed to detect an excess of risk variation beyond a couple of specific classes of variation, and we believe that this is largely due to a lack of power. Prior data has demonstrated that power to implicate common variation with schizophrenia risk is only sufficient with a much larger case\/control cohort, on the order of N case\/control >10,000 3 , 6 . This also applies to implication of genomic loci based on ultra-rare variation. Cohorts larger than ours have failed to implicate burden of ultra-rare coding variants in individual genes with schizophrenia risk 10 , and implication of SVs with schizophrenia at locus level resolution required cohorts far larger than ours 8 . Since we can assume that noncoding ultra-rare SNVs and indels will have a smaller relative risk conferred than damaging coding variants, it is clear that implication of this class of variation both across the genome and at locus level resolution will also require a far larger cohort size. Furthermore, larger samples will be necessary to ensure findings are replicable 30 . In sum, to effectively identify the subset of rare variation across the genome that confers schizophrenia risk in patients, we will need to follow the blueprint constructed for common variant GWAS. Substantial collaborative effort will be critical. WGS is expensive and generates a large quantity of sequence data that are difficult to efficiently store and analyze en masse. The financial and computational burden inherent to a case\/control WGS analysis with sufficient power for discovery is too much for individual groups or institutions, and will only be feasible through collaborative work in meta-analyzing case\/control WGS datasets. The WGS data we have generated are meant to be included in these future efforts. Methods Ethics We have complied with all relevant ethical regulations. The study protocol and all procedures on data from human research subjects were approved by the appropriate ethical committees in Sweden and the US (University of North Carolina [Institutional Review Boards], Karolinska Institutet [Regionala Etikpr\u00f6vningsn\u00e4mnden, Stockholm], University of Uppsala [Regionala Etikpr\u00f6vningsn\u00e4mnden, Uppsala]). All participants gave their written informed consent. All genomic coordinates are given in NCBI Build 37\/UCSC hg19. Subjects All schizophrenia cases included this study are from the Swedish Schizophrenia Study (S3). Detailed descriptions of S3 procedures are available elsewhere 17 and are briefly summarized here. S3 cases were identified via the Swedish Hospital Discharge Register that captures >99% of all inpatient hospitalizations in Sweden 47 .The register is complete from 1987 and augmented by psychiatric data from 1973 to 1986. The sampling frame is thus population-based and covers all hospital-treated patients. The Hospital Discharge Register contains dates and ICD discharge diagnoses for each hospitalization, and captures the clinical diagnosis made by attending physicians. Case inclusion criteria: \u22652 hospitalizations with a discharge diagnosis of schizophrenia or schizoaffective disorder, both parents born in Scandinavia, and age \u226518 years. Case exclusion criteria: hospital register diagnosis of any medical or psychiatric disorder mitigating a confident diagnosis of schizophrenia as determined by expert review, and included removal of 3.4% of eligible cases due to the primacy of another psychiatric disorder (0.9%) or a general medical condition (0.3%) or uncertainties in the Hospital Discharge Register (e.g., contiguous admissions with brief total duration, 2.2%). The validity of this case definition of schizophrenia is strongly supported as described in 17 . Ethical committees in Sweden and in the US approved all procedures and all subjects provided written informed consent (or legal guardian consent and subject assent). We also obtained permissions from the area health board to which potential subjects were registered. Potential cases were contacted directly via an introductory letter followed by a telephone call. If they agreed, a research nurse met them at a psychiatric treatment facility or in their home, obtained written informed consent, obtained a blood sample, and conducted a brief interview about other medical conditions in a lifetime. The S3 included more than 5000 schizophrenia cases, from which we selected 1165 cases for whole-genome sequencing (WGS) in the current study. Our main goal in selection was typical Swedish ancestry and clear schizophrenia caseness. Cases carrying known pathogenic copy number variants (CNVs) (e.g. 22q11del, 16p11dup) were not selected as a primary question of this study is to evaluate the contribution of novel loci on schizophrenia risk. DNA was extracted from peripheral blood samples. Specifically, our selection procedures required the following case inclusion criteria to be met: (1) have high-quality\/sufficient DNA that satisfied all criteria: concentration \u2265 80 \u00b5g\/ml, volume \u2265 150 \u00b5l, and purity ratio 1.7\u20132.2; (2) used in GWA study 17 ; (3) have typical Swedish ancestry defined by the first two PCs used in 17 ; (4) do not carry known large pathogenic CNVs and are not outliers for total number of CNVs as identified in Szatkiewicz et al. 18 ; (5) have stringent evidence of schizophrenia that satisfied all criteria: >8 inpatient or outpatient psychiatric treatment contacts for schizophrenia or schizoaffective disorder, \u226530 inpatient days for schizophrenia, \u22655 redeemed prescriptions for antipsychotics, and few or no treatment contacts for bipolar disorder. Institutional Review Boards at University of North Carolina and regional ethics committee at Karolinska Insitutet (Regionala Etikpr\u00f6vningsn\u00e4mnden, Stockholm) approved all study procedures and all subjects provided written informed consent. All control subjects included this study are from the SweGen project, a population-based high-quality genetic variant dataset for the Swedish population. One of the aims of SweGen is to enable WGS association studies for national patient cohorts studies in Sweden, by providing data on well-matched national controls selected on the basis of the genetic structure of the Swedish population. Detailed description of the SweGen subjects are available elsewhere 48 and are briefly summarized here. SweGen project included a total 1000 individuals, of which 942 individuals were selected from The Swedish Twin Registry (STR) 49 and 58 from The Northern Swedish Population Health Study (NSPHS) 50 . Both STR and NSPHS are population-based collections and were approved by local ethics committees. STR is a national registry of Swedish born twins established in the 1960s and, at present, holds information on 85,000 twin pairs. In total, 11,000 individuals from the STR (one per monozygous twin pairs) participated in TwinGene and had existing SNP array genotyping. The Twingene study is a nation-wide and population-based study of Swedish born twins agreeing to participate. The TwinGene sample collection represents the Swedish geographic population density distribution. Based on principal component analysis (PCA), 942 unrelated individuals were selected from TwinGene participants for whole-genome sequencing, mirroring the density distribution. All participants gave their written informed consent and the TwinGene study was approved by the regional ethics committee (Regionala Etikpr\u00f6vningsn\u00e4mnden, Stockholm, dnr 2007-644-31, dnr 2014\/521-32). NSPHS is a health survey in the northern Swedish country of Norrbotten. Based on PCA, 58 individuals were selected from NSPHS. The NSPHS study was approved by the local ethics committee at the University of Uppsala (Regionala Etikpr\u00f6vningsn\u00e4mnden, Uppsala, 2005:325 and 2016-03-09). All participants gave their written informed consent to the study including the examination of environmental and genetic causes of disease in compliance with the Declaration of Helsinki. Given the selected 1000 subjects that constitutes SweGen, a PCA using genotypes from high-density SNP arrays was performed and confirmed that the SweGen control cohort captured the diversity in the country. Furthermore, since STR and NSPHS are already established national sample collections that do not reflect recent migration patterns, the SweGen control cohort is likely to reflect the genetic structure of Swedish individuals that have been present in Sweden for at least one generation. From the SweGen subjects, we selected the 942 STR\/TwinGene individuals as controls in this study because of their matched ancestry with selected schizophrenia cases. Phenotype data was not allowed in the SweGen project in order to make a less restrictive access policy possible. Consequently, we were unable to screen for the presence of individuals with schizophrenia. However, we estimate that at most 1 control individual may carry a schizophrenia diagnosis (given the estimated schizophrenia prevalence of 0.0009 in the full STR\/TwinGene project of 11,000 individuals). Misclassification of a single control subject will not likely affect the results or the power of the study. DNA for the STR\/TwinGene individuals was extracted from blood. All S3 subjects, including those in this WGS study, had GWA SNP array genotyping 17 and exome sequencing 9 , 10 . DNA was extracted from peripheral venous blood for all subjects. GWAS array genotyping was done in six batches at the Broad Institute of MIT and Harvard using Affymetrix 5.0 (3.9%), Affymetrix 6.0 (38.6%), and Illumina OmniExpress (57.4%). Exome sequencing was done at the Broad Institute of MIT and Harvard in twelve separate waves. The first wave used Agilent SureSelect Human All Exon Kit and Illumina GAII. Other waves used a newer version Agilent SureSelect Human All Exon v.2 Kit and Illumina HiSeq 2000 and HiSeq 2500 instruments. Paired-end reads of 76 bp were used across all waves. Analyses of SNP array and exome sequencing data are previously published. Data on common SNPs is published in Ripke et al. 17 . Data on exonic SNVs and indels is published in Genovese et al. 10 . Data on large rare CNVs are published in Szatkiewicz et al. 18 . All data are in NCBI build 37\/UCSC hg19 coordinates. Whole-genome sequencing and data processing Library preparation and sequencing was performed by the National Genomics Infrastructure platform in Sweden. All cases and controls were processed using identical library preparation and sequencing protocols at two facilities. WGS libraries were prepared from ~1 \u03bcg DNA using Illumina TruSeq PCR-free DNA sample preparation kits targeting an insert size of 350 bp. Library preparation was performed according to the manufacturer\u2019s instructions. The protocols were automated using an Agilent NGS workstation and Beckman Coulter Biomek FXp. WGS clustering was done using cBot, and paired-end sequencing with 150 bp read length was performed on Illumina HiSeqX (HiSeq Control Software 3.3.39\/RTA 2.7.1) with v2.5 sequencing chemistry. Identical analysis pipelines (including software tool versions) were used for processing all case and control samples together For alignment, the workflow engine Piper 51 (v1.4.0) was used to perform pre-processing and variant discovery, coordinated using the National Genomics Infrastructure pipeline framework. Following the GATK guidelines, raw reads were aligned to the GRCh37 human reference genome (human_g1k_v37.fasta) using bwa mem 52 (v0.7.12). The resulting alignments (.BAM) were sorted and indexed using SAMtools 53 (v0.1.19). Alignment quality control statistics were gathered using qualimap 54 (v2.2). Alignments for the same sample from different flowcells and lanes were merged using Picard MergeSamFiles (v1.120,  ). For quality control of aligned sequence reads, we ran FastQC 55 on the BAM-files in order to understand sequencing quality and to identify outlier samples which might be subject to contamination. We analyzed a number of sequencing QC metrics (e.g., adapter content, per base N nucleotide content, per base sequence content, per base sequence quality, per sequence GC content, per sequence quality scores, sequence duplication level, and sequence length distribution). We analyzed a number of sequence coverage QC metrics produced by SAMtools flagstat (e.g., sequencing depth, percentage of mapped reads, percentage of properly paired reads, percentage of singletons, percentage of duplicates, and percentage of paired end reads with one mate mapped to a different chromosome). Finally, we checked uniformness of read coverage using BEDTools genomecov 56 , based on which we required that samples with good coverage have \u226580% of bases be covered at least 20\u00d7 for confident variant calling. These procedures identified one outlier sample (a schizophrenia case). We confirmed the identity of all subjects by comparing SNP genotypes from WGS to those from GWA SNP array genotyping 17 and exome sequencing 10 . Identity-by-decent was estimated using PLINK 57 (v1.9) for each sample between WGS-based genotypes and array- or WES-genotypes in overlapping SNPs. Based on this analysis, identity was confirmed for all samples (i.e. no sample swap was found). The identity of SweGen subjects have been confirmed previously in 48 . Variant discovery and genotyping - SNV and indels We processed all case and control BAM files together and performed joint genotyping of SNVs and indels across all samples using GATK (v3.3) 58 . The raw alignments were then processed following GATK best practices with GATK (v3.3). Alignments were realigned around indels using GATK RealignerTargetCreator and IndelRealigner, duplicate marked using Picard MarkDuplicates (v1.120), and base quality scores were recalibrated using GATK BaseRecalibrator. Finally, gVCF files were created for each sample using the GATK HaplotypeCaller (v3.3). Reference files from the GATK v2.8 resource bundle were used throughout. All these steps were coordinated using Piper (v1.4.0). Joint genotyping was conducted on all cases and controls as recommended by GATK 58 . Due to the large number of samples, 22 batches of 100 samples were merged into 22 separate gVCF files using GATK CombineGVCFs. The 22 individual gVCF files were split by chromosome and further combined with CombineGVCFs. As a result, a single gvcf file was obtained which was used as input for GATK GenotypeGVCF. Subsequently, SNVs and indels were extracted from the resulting gVCF files. To further select high-quality genetic variants, GATK VQSR filtering was executed on SNPs and indels separately using GATK VariantRecalibrator and ApplyRecalibration walkers. VQSR sensitivity thresholds were selected based on maximization of sensitivity of variant discovery in comparison with WES data previously performed on the same samples. GATK Variant Quality Score Recalibration (VQSR) was used to filter variants as recommended by GATK guidelines. The SNV VQSR model was trained using SNP sites from HapMap3.3 37 , 1000 Genomes Project (1000GP) sites found to be polymorphic on Illumina Omni 2.5 M SNP arrays 59 , 1000GP Phase 1 high-confidence SNPs 60 , and dbSNP 61 (v138). A 99.6% sensitivity threshold was applied to filter variants resulting in a Ti\/Tv ratio of 2.001. The indel VQSR model was trained using high-confidence indel sites from 62 , 1000GP and dbSNP (v138) and a 99.0% sensitivity threshold was used. The sensitivity thresholds were determined empirically by comparing to WES data in the same samples to optimize sensitivity and specificity of variant detection. We kept only the \u2018PASS\u2019 variants based on results of VQSR. Variant calling on sex chromosomes was performed separately from the autosomes. GATK Haplotype Caller walker was executed with ploidy = 1 flag on male samples except for PAR regions which were done with ploidy = 2. CombineGVCFs and GenotypeGVCFs were performed by analogy with the processing of the autosomes, see above. VQSR filtering was performed with the sensitivity thresholds inferred from the autosomes. To assess the robustness of the callset, we evaluated hard filters in comparison to VQSR filter. We constructed histograms of 16 variant quality metrics reported by GATK GenotypeGVCFs, manually selected reasonable thresholds for good quality variants, and performed hard filtering according to the selected thresholds. We found that these two filtering strategies, VQSR and hard filtering, gave nearly identical results confirming robustness of the final variant call set. Variant discovery and genotyping\u2014structural variants We applied three complimentary algorithms for the discovery and genotyping of structural variants (SVs). These algorithms were chosen for their established performance in the 1000GP 32 . We processed all case and control genomes together using protocols recommended by specific algorithms. We used ExpansionHunter 63 (v2.5.5) with default parameters to identify expansions of short tandem repeats. Using PCR-free WGS, ExpansionHunter can accurately genotype known pathogenic repeat expansions even when the expanded repeat is larger than the read length. With ExpansionHunter v2.5.5, the catalog of known pathogenic repeat expansions covers repeats in 16 genes: AR , ATN1 , ATXN1 , ATXN10 , ATXN2 , ATXN3 , ATXN7 , C9ORF72 , CACNA1A , CSTB , DMPK , FMR1 , FXN , HTT , JPH3 , and PPP2R2B . The sizes of the pathogenic repeat expansions are documented in the literature (Table S3 ). Using the disease thresholds, we identified pathogenic repeat expansions, and the number of cases and the number of controls carrying these pathogenic repeat expansions. We used Delly 64 (v0.7.7) with default parameters to detect and genotype three types of SV call sets: deletions, tandem duplications, and inversions that are between 500 bp and 500 Mb. We ran the default protocol for germline DNA and high-coverage sequencing. Specifically, for each type of SV, we (1) discover SV sites per sample using paired-end mapping signature and split-read refinement; (2) merge SV sites into a unified site list following strategies used by 1000GP 32 (i.e., for deletions and duplications: 70% reciprocal overlap and a max. breakpoint offset of 250 bp; for inversions: 90% reciprocal overlap and a max. breakpoint offset of 50 bp); (3) genotype the unified SV sites in all samples; (4) merge all genotyped samples to get a single VCF; and (5) apply the default germline SV filters to identify confident SVs (i.e., min. fractional ALT support = 0.2, min. SV size = 500 bp, max. SV size = 500 Mb, min. fraction of genotyped samples = 0.75, min. median GQ for carriers and non-carriers = 15, max. read-depth ratio of carrier vs. non-carrier for a deletion = 0.8, min. read-depth ratio of carrier vs. non-carrier for a duplication = 1.2, and \u201cPASS\u201d variants). Finally, we kept only high-confident genotypes that passed the per-sample genotype filter (i.e., FORMAT\/FT = PASS), and had additional support from read-depth-based copy number estimates (i.e., FORMAT\/CN < 2 for deletions, CN > 2 for duplications, and CN = 2 for inversion genotypes). We used the Mobile Element Locator Tool (MELT, v2) 65 to detect and genotype three types of mobile element insertions (MEI) including ALU, SVA, and LINE1. We used the MELT-SPLIT workflow with default parameters which consists four steps: (1) MEI discovery in individual samples; (2) group analysis whereby discovery information are merged across all samples to build models containing all available evidence for each candidate MEI site; (3) genotyping all WGS samples using the merged MEI discovery information; (4) final filtering and merging of individual samples into final VCF. We used the default filters (no-call filter, 5\u2032 and 3\u2032 evidence filter, discordant pair overlap filter, low complexity filter, and allele count 0 filter) and included in the final VCF only those variants that passed the default filtering of MELT. Evaluation of variant detection For SNV\/indels, we used variant calls from exome sequencing to evaluate genotype accuracy from WGS. We focused on the autosomes and estimated genotype accuracy by calculating the concordance rate between WGS-based genotypes and those obtained from exome sequencing across variants that overlapped between the two technologies. We calculated the overall concordance rate as well as concordance rates when WES-based genotypes are homozygous reference, heterozygous, and homozygous non-reference. In all calculations, only genotypes with sequencing depth \u2265 10 and GQ \u2265 20 were included in the comparison. Python code \u201cconcordance.py\u201d (  ) was used for this analysis. For deletions and duplications, we evaluated concordance using prior data from GWA SNP array or exome sequencing. Previously GWA genotyping arrays detected large and rare deletions and duplications genome-wide and WES detected rare exonic deletions and duplications in the same samples. We compared the concordance between WGS-based genotypes with those based on either GWAS array or exome sequencing across overlapping variants. Any overlapping variants must have \u226550% reciprocal overlap and occur in the same individual. We calculated the overall concordance rate as well as concordance rates when genotypes from the GWA array or exome sequencing are heterozygous and homozygous non-reference. Variant overlap was performed using BEDTools (v2.28.0). Quality control For subject quality control, we used PLINK (v1.9). In sum, subject QC excluded 9 subjects for failed sequencing quality metrics (1 case excluded), sex mismatch (1 control excluded), sex chromosomal abnormality (2 cases with XXY excluded), and one of any pair of subjects with high relatedness \\(\\hat \\pi \\, > \\, 0.2\\) (5 controls excluded). These procedures resulted in a final sample size of 2098 subjects (1162 schizophrenia cases and 936 controls), all of whom had SNV\/indel missing rate per sample < 0.01 and heterozygosity rate < 0.1. In selection of the schizophrenia cases, we excluded carriers of known large pathogenic CNVs and abnormally high total number of CNVs as identified by Szatkiewicz et al. 18 using SNP arrays. We confirmed this fact using SV calls from WGS. Sex check was performed using heterozygosity rate of sex chromosomes and by examining the coverage of sex chromosomes. This identified a sex mismatch when the reported sex does not match the biological sex and chromosomal abnormality when extra chromosomes were present. Relatedness testing and principal component analysis (PCA) were done following established pipelines using eligible bi-allelic autosomal SNPs using PLINK (v1.9). Of all bi-allelic autosomal SNPs, we removed variants that had minor allele frequency < 0.05, missing rate per variant >0.01, missing rate per variant in cases and controls >0.02 or P < 0.005, Hardy\u2013Weinberg equilibrium false discovery rate (FDR) < 1\u00d710 \u22126 (controls) or <1\u00d710 \u221210 (cases), or were in linkage disequilibrium ( r 2 > 0.05). Relatedness testing identified any pairs of subjects with \\(\\hat \\pi \\, > \\, 0.2\\) , based on which we removed one member of each relative pair. PCA estimated 20 PCs which were used in empirical evaluation of covariates to be included in association analyses. Furthermore, for quality control purpose, we performed PCA of our data together with 1000 Genomes Project data on HapMap individuals and SweGen data on NSPHS individuals. The same quality steps were followed for the identification of eligible SNPs in the combined data. For SNV\/indel quality control, we removed variants if missing rate per variant > 0.01 (before sample removal) and applied genotype QC by setting low quality genotypes with DP < 10 or GQ < 20 as missing. We then removed variants that were: monomorphic, missing rate per variant > 0.02 (after genotype QC and sample removal), missing rate per variant difference in cases and controls >0.02 or P < 0.005, Hardy\u2013Weinberg equilibrium FDR < 1\u00d710 \u22126 (controls) or <1\u00d710 \u221210 (cases). After QC, we extracted variants with minor allele frequency (MAF) \u2265 0.01 for common variant association analysis and the remaining for rare variant aggregated association analysis. These QC procedures were done using PLINK (v1.9). For SV quality control, we followed established pipelines 8 . SVs were removed if they overlapped by more than 66% with large genome gaps (e.g., centromeres), segmental duplications, or regions subject to somatic V(D)J recombination in white blood cells, with the logic that these variant calls are likely artifactual. Finally, we extracted variants with MAF \u2265 0.01 for common variant association analysis and the remaining for rare variant aggregated association analysis. These QC procedures were done using PLINK (v1.07). Annotation of variants We used VEP 66 (v91), vcfanno 67 (v0.2.9), and AnnotSV 68 (v1.1.1) for variant annotations. For population allele frequency annotations, we annotated SNV\/indels using population allele frequencies from gnomAD r2.0.2 genomes and ExAC r0.3 non-psych exomes 24 , 25 . For SVs, we annotated the variants using population allele frequencies from 1000GP and Database of Genomic Variants (DGV) 31 , 32 . We used the default settings in AnnotSV, i.e. a SV from 1000GP or DGV is reported if an overlap of >70% is found with a SV to annotate. For sequence constraints in humans, we annotated variants using the context-dependent tolerance score (CDTS) using the map of sequence constraint for the human species 19 . Files containing CDTS were downloaded from  . The downloaded CDTS scores were presented in 10 bp bins in hg38, which was liftover to GRh37\/hg19 for the analyses in this study. When a variant spans multiple CDTS bins, mean CDTS was computed and used to annotate the variant. For sequence constraints in mammals, we used the genomic evolutionary rate profiling (GERP) score 20 . For transcript-level annotations, we annotated variants with VEP (v91) using Ensembl transcripts from GENCODE 69 (v16). For SNVs\/indels, we further annotated the variants using annotation database dbNSFP 3.5_a. Exonic SNV\/indels are classified into groups following criteria based on those used in Genovese et al. 10 : synonymous, missense non-damaging, missense damaging (dbNSFP_MetaSVM_pred = \u201cD\u201d and dbNSFP_fathmm_MKL_coding_pred = \u201cD\u201d), and loss-of-function (stop-gain, frameshift, or splice donor\/acceptor). For brain exons annotations, we obtained a dataset of long-read RNAseq data from a published dataset of long-read RNA sequencing of human brain tissue 29 . The data came in the form of a BED file where each interval represents a uniquely observed exonic region in the data, along with the total number of reads aligning to the region. We took the subset of exons with at least 10 overlapping reads, sufficient support for the exon coming from an isoform that is unlikely to be mere transcriptional noise. We split exons into (1) those within coding loci, and (2) those outside coding loci by simply subsetting intervals on gene-based merged translation start\/stop intervals, representing a space where a novel coding exon could potentially be found. For brain epigenomics annotations, we relied on empirically generated annotations that have shown to be important to gene regulation in the brain. Epigenomic data are restricted to the autosomes. First, we used the open chromatin regions obtained from ATAC-seq on adult prefrontal cortex brain samples as reported in Bryois et al. 12 . ATAC-seq was performed on adult prefrontal cortex brain samples from 135 individuals with schizophrenia and 137 controls. A total of 118,152 high-confidence ATAC-seq peaks were identified. Second, we used the \u201ceasy-HiC\u201d readouts obtained from adult temporal cortex as described in Giusti-Rodr\u00edguez et al. 13 . \u201cEasy Hi\u2013C\u201d was applied to six postmortem samples ( N = 3 adult temporal cortex and N = 3 fetal cerebra) and 1.323 billion high-confidence cis-contacts were used for analyses. Three major read-outs were generated including frequently interacting regions (FIREs), chromatin interactions (a.k.a. Hi\u2013C loops), and topologically associating domains (TADs). FIREs were defined as 40-kb genomic bins with significantly more Hi\u2013C interactions (FIRE score P < 0.05). Chromatin interactions were defined as intra-chromosomal chromatin interactions between 10 kb bins that were >20 kb apart (i.e., not contiguous) and \u22642 Mb apart. FIREs are a small subset of all chromatin interactions, which have considerably more three-dimensional contacts. Chromatin interactions have a strong tendency to occur within TADs (discrete megabase-scale regions with less frequent interactions outside the regions). TAD boundaries are defined in 40 kb bins. Finally, we further included epigenetic marks (i.e. CTCF, H3K27ac, and H3K4me3) obtained from ChIP-seq using postmortem brain tissue from fetal and adult samples that were generated in 13 . Using gene model defined by GENCODE (v16), we assessed gene sets previously implicated in schizophrenia and neurodevelopmental disorders including: Loss-of-function (LOF) intolerant genes: we used genes from Lek et al. 24 . Calcium Channel gene set: we used the 26 genes from voltage-dependent calcium channel, available at  . CELF4 gene set: we used genes with \u201ciCLIP occupancy\u201d >0.2 from Supplementary Table 4 of Wagnon et al. 70 . CHD8 gene set: we used genes from Cotney et al. 71 . FMRP Darnell gene set: we used the 842 mouse genes from Supplementary Table 2 A of Darnell et al. 72 , including all genes with FDR < 0.01. NMDARC: we used a list of combined NMDAR and ARC complexes genes from Supplementary Table 9 of Kirov et al. 73 . PSD gene set: we used a gene list generated from human cortex biopsy data from Bayes et al. 74 . PSD-95 gene set: we used a gene list generated from human cortex biopsy data from Bayes et al. 74 . RBFOX gene sets: we selected RBFOX1\/2\/3 genes from Supplementary Table 1 of Weyn-Vanhentenryck et al. 75 . Genes.ID\/DD\/ASD: we selected 288 genes implicated in de novo variant studies from Supplementary Tables 15 \u2013 18 of Nguyen et al. 11 , based on q- value < 0.05 for developmental delay (DD), q -value < 0.1 for autism spectrum disorder (ASD), q -value < 0.1 for intellectual disability (ID), and q -value < 0.5 for epilepsy (EPI). SCZGWAS: genes implicated in schizophrenia common variant association studies, for which we used genes from the 145 regions known to be associated with schizophrenia from Pardinas et al. 6 . CMCqval05: The CommonMind Consortium (CMC) sequenced RNA from dorsolateral prefrontal cortex of schizophrenia cases ( N = 258) and control subjects ( N = 279), from which we selected genes implicated to have differential expression in human brain between cases and controls based on q -value < 0.05 76 . For certain tests of SNV\/indel burden we focused on burden within gene regions of a generalized coding transcript structure, broadly defined as 35 kb upstream of the most distal transcription start site to 10 kb downstream of the most distal transcription start site (transcript_35kb_10kb). Variant subsetting Protein coding sequences are defined using protein-coding transcripts from GENCODE (v16). We focused coding SNV\/indel analyses on a set of variants which to a high degree of confidence impact bases involved in the production of a functional protein. Coding variants have an at least one transcript-level IMPACT classification of LOW, MODERATE or HIGH according to VEP (v91). We defined noncoding SNV\/indels if they did not alter sequence content of coding regions or splice dinucleotides of GENCODE protein-coding transcripts. Noncoding variants only have IMPACT classifications of MODIFIER according to VEP (91). For SVs, we followed criteria used in Brandler et al. 77 to define coding versus noncoding variants. Protein coding sequences are defined using the consensus coding sequence from GENCODE (v16). Coding deletions, duplications, or mobile element insertions are defined as those affecting any protein-coding sequences. Coding inversions are either having one or both breakpoints inside a protein-coding exon of a gene, or having breakpoints in two different introns of a gene and overlapped with at least one coding exon, or having one breakpoint in an intron of a gene and the other breakpoint outside of that gene. Inversions that inverted an entire gene or genes but had intergenic breakpoints were considered noncoding. From the post-QC variant callsets, we defined ultra-rare SNV\/indels as being a singleton within our WGS cases\/control cohort (allele count = 1 in the 2098 post-QC subjects) and absent from independent population cohorts (gnomAD genomes allele count = 0 and non-psychiatric subset of ExAC allele count = 0) 24 , 25 . This is because the full ExAC and gnomAD exome cohort include exome sequence data derived from schizophrenia case samples included in this study, and applying any MAF constraints using the full cohorts could bias association analysis results against schizophrenia cases. Subsetting of noncoding ultra-rare SNV\/Indels on annotations was done using in-house python scripts, VCFscreen v0.1 (  ), based on interval overlap with annotations defined by genomic coordinates. From the post-QC SV callset, we defined ultra-rare SVs as being single occurrence in our case\/control cohort (allele count = 1 in the 2098 post-QC subjects), as well as being absent in independent population cohorts including 1000GP and DGV 31 , 32 . Based on the default setting of AnnotSV 68 , a SV was absent in population cohorts if it did not overlap or overlapped <30% of any variant in the population databases. Subsetting of ultra-rare SVs on annotations was done using PLINK (v1.07) based on interval overlap. Power calculation and correction for multiple comparisons We used the R\/gap package (v1.2.1,  ) to estimate statistical power for association analyses. We assumed an additive model, lifetime risk of schizophrenia of 1%, and two type I error levels: (1) 5 \u00d7 10 \u22128 as an established genome-wide significance threshold for single-variant association, (2) 1 \u00d7 10 \u22125 as in Werling et al. 46 . We computed the minimal detectable genotypic risk ratio to achieve 20%, 80% power over a range of frequency of risk alleles in the population. For single-variant association test, the X -axis of the power plot represents the frequency of a single variant. For burden test, the X -axis of the power plots represents the aggregated frequency of a set of variants aggregated for a target region of interest. To correct for multiple comparisons in the analysis of common variant association, we used the established genome-wide significance threshold of 5 \u00d7 10 \u22128 . To correct for multiple comparisons in burden analyses of ultra-rare variants, we applied the Benjamini and Hochberg false discovery rate (BH-FDR) method to the family of hypotheses involving ultra-rare SNV\/indels which included a total of 74 tests summarized in Supplementary Table 5 , and to those involving ultra-rare SVs which included a total of 29 tests summarized in Supplementary Tables 7 and 8 . We used the p.adjust function in R (v3.2.2.,  ) to implement the BH-FDR method. We used a threshold of 0.05 on the FDR adjusted P values (a.k.a. q values) to consider statistical significance. Burden of ultra-rare SNV\/Indels Given that the large majority of ultra-rare SNVs and indels (URVs) are not assumed to confer risk in schizophrenia cases, we first tested the null that the total rate of these variants is not a significant predictor of schizophrenia status. Before outlier pruning, with 1162 cases and 936 controls, we fitted a simple logistic regression model with case\/control status as the dependent variable and count per sample of URVs as the predictor variable. We found that cases had a higher mean URV count (4456 vs. 4289, P = 0.002, two-sided), and that this was primarily driven by the presence of a portion of samples with unusually high URV counts. The URV outlier samples may be biasing the analysis of URVs even though they were not a concern for the analysis of common variants and SV, and will need to be removed. Following an approach previously established in the full Swedish sample 10 , we pruned samples that had an outlier total URV count, here defined as >6000 (Supplementary Fig. 7 ). The outlier samples appeared to have relatively higher ancestry heterogeneity (Supplementary Fig. 8 ) similar to the previous finding from the full Swedish sample in Genovese et al. 10 . After outlier pruning, we had 1104 cases and 921 controls and there was no evidence for a difference in mean URV count between cases and controls after this pruning step was carried out (4262 vs 4249, P = 0.4225, one-sided assuming higher burden in cases). Burden testing was done using VCFscreen (v0.1) and R (v3.2.2). All tests of URV burden in cases relative to controls were carried out using a logistic regression model framework that has been utilized in prior studies 10 . Specifically, the dependent outcome variable in logistic regression is phenotype (schizophrenia = 1, control = 0). The primary predictor is the count per sample of URVs that are specific to target region annotation, whether coding or noncoding. And, based on empirical evaluation, we included three covariate variables in logistic regression: mean_coverage, PC2 (the only PC of the 20 PCs determined from common SNPs which predicted sample case\/control status at P < 0.01), and total URV count per sample. We carried out one-sided statistical tests assuming increased burden of URV in cases. Logistic regression models were implemented by the glm function in R (v3.2.2). Odds ratios were computed to measure the increase in the likelihood of having disease per unit increase in URV burden. Empirical P -values were derived by 10,000 permutations by swapping phenotype labels. Burden of ultra-rare SVs Analysis was done using PLINK (v1.07) and R (v3.2.2). All tests of ultra-rare variant burden in cases relative to controls were carried out using a logistic regression framework that has been established in prior studies 8 , 18 . Analysis was done for each type of variants separately. In order to ensure the robustness of the analysis, we first empirically evaluated variables that could potential confound association results. We fit a multiple linear regression model, where dependent\/outcome variable was the genome-wide total number of ultra-rare SVs, and the independent\/predictor variables were sex, mean sequence coverage, and the first three principal components derived from common SNP genotypes. Only the first principal component (PC1) showed significant association with genome-wide burden of ultra-rare SVs. To control its potential confounding effect, we included PC1 as covariate in all tests of burdens of ultra-rare SVs. For genome-wide burden tests, we fit the following logistic regression model: y ~ covariate + global, where y is the outcome phenotype variable (schizophrenia = 1, control = 0), covariate is the empirically determined covariate variable (i.e. PC1), and global is the genome-wide total number of ultra-rare SVs. For burden tests in target regions, we fit the following logistic regression model for each target region: y ~ covariate + global + target_region, where target_region is the count per sample of ultra-rare SVs that are specific to the target region annotation. Variables global and target_region are computed based on input variants (i.e. coding, noncoding, or combined coding and noncoding). We carried out one-sided statistical tests assuming increased burden of ultra-rare SVs in cases. Logistic regression models were implemented by the glm function in R (v3.2.2). Odds ratios were computed to measure the increase in the likelihood of having disease per unit increase in the burden of ultra-rare SVs. Empirical P values were derived by 10,000 permutations by swapping phenotype labels. Single-variant association analysis Analysis was done using PLINK (v1.9). Following the general guideline for logistic regression, we used a MAF cutoff of 0.01 to ensure that there were at least 10 events in the less frequent category. Post-QC variants that had MAF > 0.01 were subject for single-variant association analysis. Established variant filters were used to ensure all variants had missing rate per variant < 0.02, missing rate per variant difference in cases and controls < 0.02 ( P > 0.005), and Hardy\u2013Weinberg equilibrium FDR < 1 \u00d7 10 \u22126 (controls) or <1 \u00d7 10 \u221210 (cases). To empirically determine confounding factors, we fit logistic regression models where the dependent outcome variable is phenotype (case\/control status) and the independent predictor variables are sex and the first 20 PCs determined from common SNPs. Only PC2 showed significant association with phenotype. Therefore, we included PC2 as covariate for the analysis of autosomal variants, and included PC2 and sex as covariates for the analysis of chromosome X. A logistic regression model with additive genetic model (Plink \u2013logistic) with empirically determined covariates was used to estimate association between single variants and schizophrenia. Statistical tests were two-sided. The established threshold of 5 \u00d7 10 \u22128 was used to identify genome-wide significance. Following association, we used IGV to inspect read alignments underlying each putative variant that exceeded the genome-wide significance threshold. False positives that had no IGV support were excluded. Manhattan plots were constructed using R (v3.2.2). Analysis was done separately for SNV\/indels, deletions, duplications, inversions, ALU, SVA, and LINE1. For SNV\/indels, deletions, duplications, and inversions, we filtered variants as described above. For ALU, LINE1, and SVA, we additionally restricted our attention to the most reliable variants by selecting variants with a quality score of 5 (best). For ALU, MAF was set to be >0.05 and Hardy\u2013Weinberg equilibrium threshold to FDR < 0.05 for both cases and controls. Heritability estimation Following Wainschtein et al. 14 , we started with the initial set of QC-passing subjects and post-QC SNV\/indels and additionally required that each variant observed at least three times in our dataset (i.e. MAF starts at 0.0007). Next we further removed one of each pair of individuals with estimated genetic relatedness >0.05. These procedures resulted in 1151 cases and 911 controls and 17,364,971 sequence variants to be used for narrow-sense heritability estimation. HapMap3 SNPs were downloaded from the HapMap ftp site. To identify imputable variants from Haplotype Reference Consortium (HRC) 21 and 1000 Genomes Project Consortium (1000GP) 22 , we used previously imputed data obtained by using SNP genotypes of the schizophrenia subjects from Illumina OmniExpress array genotyping and imputing the array genotype data to the HRC.r1.1 or the 1000GP p3v5 reference panel using EAGLE2 78 (v2.0.5). On the HRC imputation variants, we excluded variants with Imputation Score Info <0.8, individual missing rate >0.05, genotype missing rate >0.05, MAF < 0.0001 and P -value <1e-06 of Hardy\u2013Weinberg equilibrium test. On the 1000GP imputation variants, we excluded variants with Imputation Score Info <0.8, and allele frequencies <0.005 or allele frequencies >0.995 based on previous results 17 . Heritability analysis was done using GCTA 39 (v1.26.0, v1.92.3beta). We assumed lifetime risk of schizophrenia of 1%. We calculated principal components from 1,189,077 HapMap3 37 SNPs selected from the WGS data and included the first 10 PCs (calculated from the same set of HapMap3 SNPs) for the analyses conducted using GCTA\u2019s GREML-LDMS 40 . To test the robustness of the estimates, we repeated the analysis while correcting for the first 4 PCs, and for the first 12 PCs and found the results were similar. With the GREML-LDMS approach, a total of 14 MAF and LD bins were considered and the same set of bins were used for both the imputed-SNPs and for the WGS sequence variants. Specifically, we split the variants into seven different bins based on MAF (0.0007\u20130.0001, 0.001\u20130.01, 0.01\u20130.1, 0.1\u20130.2, 0.2\u20130.3, 0.3\u20130.4, 0.4\u20130.5) and for each bin of variants, computed SNP-based LD scores with the following parameters: \u2013ld-score-region 200, -ld-wind 10000, ld-rsq-cutoff 0. For a given bin of variants defined by MAF, we defined low LD as < median LD score, and high LD as \u2265median LD score. For each bin subsetted on MAF (and further split by LD), we used GCTA to produce a genetic relationship matrix (GRM) from the set of genotypes. We then used the REML function (via the Fisher scoring algorithm, as implemented in GCTA via \u2013reml-alg 1) to conduct a GREML-LDMS analysis. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability Summary statistics from single-variant association analysis in this study can be downloaded from Psychiatric Genomics Consortium\u2019s website at  . All other summary statistics and supporting data are available in Supplementary Information . Due to recent changes in Swedish and European Union regulations regarding genetic data, we are unable to deposit individual-level data into controlled-access repositories like dbGaP. Collaborative analyses are possible and can be pursued by contacting the authors. Code availability Analysis software used in this study include the following: HiSeq Control Software 3.3.39\/RTA 2.7.1; Piper (v1.4.0,  ); bwa (v0.7.12,  ); SAMtools (v0.1.19,  ); Picard (v1.120,  ); qualimap (v2.2,  ); FastQC (v0.11.4,  ); BEDTools (v2.28.0,  ); GATK (v3.3,  ); PLINK (v1.9,  ); PLINK (v1.07,  ); ExpansionHunter (v2.5.5,  ); Delly (v0.7.7,  ); MELT (v2,  ); VEP (v91,  ); vcfanno (v0.2.9,  ); AnnotSV (v1.1.1,  ); VCFscreen (v0.1,  ); R (v3.2.2.,  ); R\/gap package (  ); GCTA (v1.26.0, v1.92.3beta,  ); JMP (v11,  ); AbCD Calculator (  ). Python code \u201cconcordance.py\u201d and other relevant codes are posted at  . Change history 05 January 2022 A Correction to this paper has been published:  ","News_Body":"Most research about the genetics of schizophrenia has sought to understand the role that genes play in the development and heritability of schizophrenia. Many discoveries have been made, but there have been many missing pieces. Now, UNC School of Medicine scientists have conducted the largest-ever whole genome sequencing study of schizophrenia to provide a more complete picture of the role the human genome plays in this disease. Published in Nature Communications, the study co-led by senior author Jin Szatkiewicz, Ph.D., associate professor in the UNC Department of Genetics, suggests that rare structural genetic variants could play a role in schizophrenia. \"Our results suggest that ultra-rare structural variants that affect the boundaries of a specific genome structure increase risk for schizophrenia,\" Szatkiewicz said. \"Alterations in these boundaries may lead to dysregulation of gene expression, and we think future mechanistic studies could determine the precise functional effects these variants have on biology.\" Previous studies on the genetics of schizophrenia have primarily involved using common genetic variations known as SNPs (alterations in common genetic sequences and each affecting a single nucleotide), rare variations in the part of DNA that provide instructions for making proteins, or very large structural variations (alterations affecting a few hundred thousands of nucleotides). These studies give snapshots of the genome, leaving a large portion of the genome a mystery, as it potentially relates to schizophrenia. In the Nature Communications study, Szatkiewicz and colleagues examined the entire genome, using a method called whole genome sequencing (WGS). The primary reason WGS hasn't been more widely used is that it is very expensive. For this study, an international collaboration pooled funding from National Institute of Mental Health grants and matching funds from Sweden's SciLife Labs to conduct deep whole genome sequencing on 1,165 people with schizophrenia and 1,000 controls\u2014the largest known WGS study of schizophrenia ever. As a result, new discoveries were made. Previously undetectable mutations in DNA were found that scientists had never seen before in schizophrenia. In particular, this study highlighted the role that a three-dimensional genome structure known as topologically associated domains (TADs) could play in the development of schizophrenia. TADs are distinct regions of the genome with strict boundaries between them that keep the domains from interacting with genetic material in neighboring TADs. Shifting or breaking these boundaries allows interactions between genes and regulatory elements that normally would not interact. When these interactions occur, gene expression may be changed in undesirable ways that could result in congenital defects, formation of cancers, and developmental disorders. This study found that extremely rare structural variants affecting TAD boundaries in the brain occur significantly more often in people with schizophrenia than in those without it. Structural variants are large mutations that may involve missing or duplicated genetic sequences, or sequences that are not in the typical genome. This finding suggests that misplaced or missing TAD boundaries may also contribute to the development of schizophrenia. This study was the first to discover the connection between anomalies in TADs and the development of schizophrenia. This work has highlighted TADs-affecting structural variants as prime candidates for future mechanistic studies of the biology of schizophrenia. \"A possible future investigation would be to work with patient-derived cells with these TADs-affecting mutations and figure out what exactly happened at the molecular level,\" said Szatkiewicz, an adjunct assistant professor of psychiatry at UNC. \"In the future, we could use this information about the TAD effects to help develop drugs or precision medicine treatments that could repair disrupted TADs or affected gene expressions which may improve patient outcomes.\" This study will be combined with other WGS studies in order to increase the sample size to further confirm these results. This research will also help the scientific community build on the unfolding genetic mysteries of schizophrenia. ","News_Title":"Whole genome sequencing reveals genetic structural secrets of schizophrenia","Topic":"Medicine"}
{"Paper_Body":"Abstract In presidential nomination campaigns, individual state primaries and a national competition take place simultaneously. The relationship between divisive state primaries and general election outcomes is substantially different in presidential campaigns than in single-state campaigns. To capture the full impact of divisiveness in presidential campaigns, one must estimate both the impact of national party division (NPD) and the impact of divisive primaries in individual states. To do so, we develop a comprehensive model of state outcomes in presidential campaigns that incorporates both state-level and national-level controls. We also examine and compare several measures of NPD and several measures of divisive state primaries found in previous research. We find that both NPD and divisive state primaries have independent and significant influence on state-level general election outcomes, with the former having a greater and more widespread impact on the national results. The findings are not artifacts of statistical techniques, timeframes or operational definitions. The results are consistent\u2014varying very little across a wide range of methods and specifications. Access provided by Universit\u00e4t des es,  -und  Working on a manuscript? Avoid the common mistakes Introduction The divisive primary hypothesis, first suggested by Key ( 1953 ), posits that when a party\u2019s primary is competitive or the eventual nominee does poorly in the primary, the party suffers in the general election. However, in presidential elections, measuring the impact of divisiveness is complicated by the fact that campaigns are waged both at the state and national level. As a consequence, the relationship between divisive state primaries and general election outcomes is substantially different in presidential campaigns than in subnational campaigns. Substantial research exists on the impact of divisive state primaries, however this research generally ignores the important distinction between national and subnational elections. Presidential elections, unlike state-level elections, directly involve the national parties. In any state, a divided national party could have a negative impact on the performance of its presidential candidate even though that state\u2019s presidential primary was not divisive. Thus we do not know if national-level or state-level divisiveness exerts greater influence on state-level outcomes in presidential elections because existing models do not account for national party division (NPD). In a single-state primary, the winner of the popular vote becomes the party nominee. Presidential primaries are part of a larger, more complex environment. In presidential campaigns, individual state primaries do not determine the identity of the nominee. Rather, they select (or apportion) delegates to the national convention who then select the party nominee. It is common in the literature to use the term \u201cdivisive presidential primary\u201d either to describe the divisiveness of an individual state primary or to describe the divisiveness of the national party during the nomination process. There has long been concern that the presidential nomination process undermines party cohesion and encourages intraparty factionalism. When one national party is divided and the other party united, the divided party usually loses the election. The relative divisiveness of the national parties is a critical component of the national campaign, yet it is included neither in models of state primary divisiveness nor in models of aggregate presidential election outcomes. Excluding NPD from models of presidential election outcomes has the potential to bias the estimate of the impact of divisive state primaries or other variables. To measure the full impact of the divisiveness in presidential campaigns, it is necessary to measure both NPD as well as divisive primaries in individual states. NPD is not simply an aggregation of divisive state primaries. NPDs are deeper and larger than state party divisions. A set of divisive state primaries does not necessarily indicate a divided national party in the general election or vice versa. Footnote 1 Absent the influence of NPD, measuring the impact of divisive state primaries might seem relatively straightforward. However, studies of subnational divisive primaries have reached a confusing variety of conclusions (Lengle and Owen 1996 ). Footnote 2 Measuring the impact of divisive state primaries in presidential campaigns is further complicated by the impact of NPD. In this research, we establish and measure the impact of NPD and that of divisive state primaries (DSP). To these ends, we first develop a comprehensive model of state outcomes in presidential campaigns that incorporates both state and national-level controls. As we will explain, there are several ways to define the appropriate timeframe and to specify the model. We test several possible measures of NPD, and several measures of divisive state primaries found in previous research. We show that our findings are not artifacts of statistical techniques, timeframes or operational definitions. The results are consistent; varying only slightly across a wide range of methods and specifications. Divisive State Primaries in Presidential and Subnational Campaigns The causes and consequences of divisive presidential primaries are somewhat different than those of divisive subnational primaries. Footnote 3 The literature on divisive congressional and gubernatorial primaries posits a link between a divisive state primary and the general election outcome in that state. Because presidential nomination campaigns are sequential and national in scope, some of what occurs in individual state primaries spills over to other state contests. Footnote 4 Previous studies (Hacker 1965 ; Kenney and Rice 1987 ; Atkeson 1998 ; Lazarus 2005 ; Southwell 1986 ) have suggested that a divisive subnational primary decreases that party\u2019s vote because (a) supporters of the losing candidate are alienated or discouraged, (b) the primary battle provides rhetorical \u201cammunition\u201d for the opposing party, or (c) the state party\u2019s resources are depleted. Each of these effects manifests differently in presidential campaigns than in subnational campaigns. In a congressional or gubernatorial campaign, a competitive primary may divide the state party and deplete its resources, hurting its ability to compete in the general election. In a presidential campaign, a few divisive state primaries would neither divide the national party nor deplete its resources. Presidential candidates allocate resources to states based on their strategic importance; if state party resources are depleted in a battleground state, the national campaign will pump money into that state. Footnote 5 In a presidential campaign, because of national media coverage, supporters of losing candidates may be alienated even if there was not a divisive primary in their state. Rhetorical attacks made by intraparty rivals in a few primaries may be co-opted and disseminated nationally, influencing subsequent national media coverage of that candidate. More generally, the negative image of an internally divided national party may be a potent cue to general election voters. In presidential campaigns, some of the influences on general election outcomes derive not from divisive primaries in specific states but from a divided national party. An incumbent president may be challenged within his\/her own party during the primaries, but the challenge is national, not restricted to a specific state. Both incumbent and challenger choose the state primaries in which they will compete vigorously. That decision is based on national-level factors as well as state factors. Similarly, both frontrunners and challengers in non-incumbent nomination campaigns run in particular states to bolster their chances of winning the national nomination. Differences in the context and causes of divisiveness in presidential and subnational primaries help explain why various studies have come to such different conclusions\u2014analyzing presidential and subnational primaries separately leads to more clear and meaningful results. In this research, we focus exclusively on presidential primaries and their impact on state-level presidential general election results. Developing a Model of State Vote Outcomes We develop a comprehensive model of a state vote outcomes to estimate the impact of NPD, and to test a variety of operationalizations of divisive state primaries to determine the extent to which they influence the estimated effects. Examining the literature on divisive presidential primaries, we find that virtually all models are under-specified. Typically, they include few state-level controls and few, if any, national-level controls. This allows the possibility of excluded variable bias\u2014that the estimated impact of divisive state primaries includes some of the impact of excluded variables, typically inflating and biasing that estimate. Our model of state general election outcomes takes into account a wide set of national-level and state-level variables that generally correspond to the factors that influence individual voting behavior. The use of state-level data, including partisanship and ideology, should provide strong controls to measure the impact of NPD and to re-examine the divisive primary hypothesis. The Key Variables The dependent variable used here is the proportion of the major party state vote won by the Democratic Party in the general election. Footnote 6 To measure NPD, we use the proportion of delegate votes received by the Democratic nominee (on the first ballot) at the convention minus the corresponding proportion for the Republican nominee as a measure of relative NPD. Below, we will show that this measure, though not ideal, leads to results that are substantively the same as results obtained using very different measures of NPD such as the difference in aggregate poplar vote. To measure divisive state primaries we use the proportion of the state primary vote received by the eventual Democratic nominee minus the corresponding proportion received by the eventual Republican nominee as the measure of divisive state primaries (Kenney and Rice 1987 ). Below, we test a variety of possible measures of divisive state primaries to determine the degree to which the operationalization of this key variable influences the results. State-Level Variables Some previous studies of divisive primaries have controlled for state-level effects by including one or more previous presidential election results (Mayer 1996 ; Atkeson 1998 ). In this study, state-level effects are accounted for by controlling for state partisanship, state ideology, and the home states of the presidential and vice presidential nominees. Rabinowitz et al. ( 1984 ) analyzed the vote outcomes of the states and found that presidential elections are structured by party and ideology (Jackson and Carsey 1999 ; Erikson et al. 1993 ). Research at the individual and state levels shows that partisanship exerts substantial influence in presidential elections. State partisanship is measured here as the average of the most recent statewide votes for Governor, Senator and U.S. House. Previous presidential vote is not included since that might reflect national factors involving previous presidential politics rather than underlying state-level partisanship. In this model, state partisanship is not fixed; rather, its values often change to some degree from election to election. One might argue that previous voting in congressional and gubernatorial elections is not a good measure of partisanship for the southern states until recent decades. In their analysis, Rabinowitz et al. ( 1984 ) found that conservative Democratic states such as Alabama and South Carolina tended to cluster in a different part of the factor space than either liberal Democratic states or conservative Republican states. Thus, we include two measures of state ideology, a general left\u2013right scale and a scale that involves civil rights and social issues. Footnote 7 These controls should capture changing state-level effects. Footnote 8 Although the general ideology measure fails to capture some salient issues, it does reflect many issue-oriented differences across state populations. Civil rights issues (integration, voting rights, affirmative action, etc.) and social issues (abortion, gay rights, gun control, etc.) have been powerful over many elections and, critical to this study, their impact has been regional, affecting states differently (McCarty et al. 2006 ; Zaller 1992 ). Both ideology variables are measured using mean DW-NOMINATE roll call data scores for the U.S. House delegation in each state in the term prior to the presidential election (Poole and Rosenthal 1997 ). The expectation is that the civil rights\/social issues variable will have a negative effect on voting for the Democratic Party in elections since the 1970s. Candidate evaluation is difficult to measure at the state level. Polls, which could provide such information, are rarely consistent in format and rarely available for every state. Thus we are limited to controls for the home state of the presidential and vice presidential candidates, and the presidential candidate\u2019s home region. Presidential candidates tend to do better in their home region than elsewhere and both presidential and vice presidential candidates tend to do well in their home states. Each of these variables can take on values of \u22121, 0 or +1 (e.g., the Republican candidate\u2019s home state has a value of \u22121), although 0 is by far the most common. Home region is measured as all states adjacent to a candidate\u2019s home state (Holbrook 1991 ). National-Level Variables In the literature on forecasting presidential elections (e.g., Campbell 2001 ; Bartels and Zaller 2001 ) there is general agreement that the national economy has a powerful impact on the national popular vote. In the current research, national economic conditions are operationalized as the annual change in RDI. Since the dependent variable is Democratic vote share, this variable is multiplied by \u22121 when the incumbent president is Republican. Thus, credit or blame for the economy is directed at the incumbent party. Footnote 9 Several studies suggest that the apparent effect of divisiveness may be spurious (Jacobson and Kernell 1981 ; Kenney 1988; Atkeson 1998 ). It is quite possible that an unpopular incumbent would attract more or stronger challengers; similarly, a popular incumbent might \u201cscare off\u201d strong challengers. Thus the apparent relationship between divisiveness and vote outcomes may be an artifact of spuriousness\u2014both divisiveness and vote outcomes are strongly influenced by the strength of the incumbent. This argument is supported, directly or indirectly, by numerous studies of subnational primaries (e.g., Hacker 1965 ; Partin 2002 ; Lazarus 2005 ). We address this concern in two ways. At the presidential level, an unpopular incumbent may encourage intraparty challenges, which may exacerbate existing regional or ideological divisions within the party. However, unlike most subnational election campaigns, the out-party typically begins with 5\u20139 legitimate candidates (e.g., senators and governors) for the nomination, whether the incumbent is popular or not. Historically the number and \u201cquality\u201d of nomination candidates in the out-party (or open seat presidential campaigns) appears unrelated to the quality of the opposing party\u2019s candidate. Footnote 10 Ford (in 1976) and Carter (in 1980) did attract strong intraparty opponents, but such cases are rare. Popular incumbents like Nixon and Reagan were challenged by relatively strong fields of opponents. We also address this concern statistically. If the strength of the incumbent president is causing a spurious relationship to appear causal, including such a variable in the model would cause the parameter estimates of divisiveness to diminish or lose statistical significance. It is difficult to measure candidate quality directly, but we can do so by controlling for national economic conditions (which, except for 2008, rarely change much during the election year) and presidential approval, measured as the Gallup approval rating in January of the election year, before the primaries begin (Atkeson 1998 ). Footnote 11 . These variables reflect the perceived quality of the incumbent party candidate. (Indeed, these are the two most common factors in the presidential election forecasts.) Each party is a coalition of diverse elements. The longer a party holds power the more likely party fissures will develop (Campbell 2000 ). Thus, like the forecast models, we include a variable (\u201cterms\u201d) that controls for the length of time a party has held the White House. If the incumbent party has been in office for only one term, this variable has a value of 0; if it has been in office for two or more consecutive terms, this variable has a value of 1. We include a separate dummy variable indicating whether or not the incumbent president is running. Typically, the major parties nominate relatively centrist candidates, but occasionally one party nominates a relative extremist. In general, candidates who are perceived as more ideologically extreme are disadvantaged in presidential elections. Bartels and Zaller ( 2001 ) combined expert ratings of candidates 1948\u20131980 (Rosenstone 1983 ) with NES data 1984\u20131996. We extend this measure through 2012. Higher absolute values indicate greater relative extremism. In addition, we include a control variable for the impact of war. Wars have the capacity to divide parties and affect the decisions of candidates and voters. This variable is measured as the number of combat fatalities as a proportion of the national population. Increased fatalities are expected to disadvantage the incumbent party. War is measured as a national, not a state, variable because voters in all states receive national news of foreign affairs and because variations across elections are greater than those across states (Table 1 ). Footnote 12 Table 1 Descriptive statistics 1948\u20132012 Full size table Data and Methods Presidential elections are not singular national elections, as they are often treated, but are 51 separate contests. The influence of state factors is not included in most national studies. The model developed here measures the effects of NPD and divisive state primaries on presidential general election outcomes across space (states) and over time (elections). A pooled time-series allows both state-level and national-level effects to be tested concurrently. The model is applied to the set of presidential elections from 1948 though 2012. This study melds several research streams including those of divisive primaries and general election forecasting. The model used in this research differs from the national forecasting models in several ways. First, the purpose is explanation not prediction; we do not use previous presidential vote outcomes or national trial heat polls since they do not seem to add to the explanatory power of the model. Second, variables representing \u201cspecial circumstances\u201d such as Watergate or a Catholic candidate are not included. Instead, the model includes only factors that occur regularly in presidential elections. Third, the unit of analysis is the state rather than the nation. This study is not the first to develop a model of state-level presidential voting; Gelman and King ( 1993 ), Campbell ( 1992 ), Holbrook ( 1991 ), and Rosenstone ( 1983 ) have provided useful guidance. A time-series cross-sectional design should have enough power to generalize about the relationship between nomination campaigns and general elections. Because multiple units in time are observed, we need to control for the election year context (Stimson 1985 ). The substantive relevance of the election year is highlighted by Atkeson ( 1998 ). It is preferable to explicitly model national effects with actual national-level variables rather than leaving those effects in the \u201cblack box\u201d of an election year dummy. Because election year dummies would be perfectly collinear with the national variables that change over time but do not vary across states, random effects for time are employed. However, fixed-effects are used to capture state-level heterogeneity. More formally, the standard two-way error component panel data model (Baltagi 2005 ) is given by $$\\it Y_{it} \\, = {\\text{a}} + \\, X_{it} \\, {\\beta} \\, + \\, u_{it}$$ where \\(u_{it} = \\, \\mu_{i} + \\lambda_{t} + \\, v_{it}\\) and i indexes units and t indexes time. Thus u it is a compound error term with a unit specific error term \u00b5 i , an election specific error term \u03bb t , and an observation specific error term v it . OLS estimates of this model fail to account for both unit-specific and time-specific unobserved effects, which leads to incorrect standard errors (and potentially biased estimates if either unit- or time-specific effects are correlated with the independent variables). Thus this model is estimated via maximum likelihood using fixed effects for states (to explicitly model unit-specific effects) and random effects for time (since fixed effects for time prohibit the estimation of coefficients of variables, like NPD, that vary over time but not across states). A Wald test shows that the state-specific effects are jointly significant ( p < .001), and a likelihood ratio test against a model without random effects for time shows that the unrestricted model fits the data better than the restricted model ( p < .001). Footnote 13 1948 was chosen as the starting date because, as the first post-WW II election, it in some ways represents the beginning of the modern era of electoral politics and because national economic data (in particular RDI) are not available prior to this date. Although some state primaries played a role in some earlier nomination campaigns (1912, 1928\u20131944), they played a substantial role in the 1948 Republican campaign and in subsequent campaigns. Footnote 14 The 1968 election has been excluded because of the anomalous character of both the primary campaign and the general election makes it inappropriate for this research. Footnote 15 With only 16 national elections, one of them might be an influential outlier that biases the coefficients. As a test, the model was estimated repeatedly, each time omitting one election year. The effects on the key variables were minimal. There are 781 valid cases in the dataset. Footnote 16 Both primaries and caucuses are included. Lengle et al. ( 1995 ) analyzed the impact of caucuses, divisive primaries and non-divisive primaries on presidential election outcomes. Their results show very similar patterns for caucuses and non-divisive primaries, in sharp contrast to divisive primaries. They concluded that a caucus is a \u201cnon-divisive mechanism\u201d that is virtually identical to a non-divisive primary in its impact on the general election. Accordingly, caucuses were assigned a value of 0 in terms of primary divisiveness (i.e., no advantage to either party). This substantially reduces the number of missing cases, facilitating more reliable parameter estimates. As a test, the model was estimated without any caucus states (see online Table A-1.) Measuring National Party Division and Divisive State Primaries NPD is generally driven by elites: activists, office holders, and opinion leaders (see Steger 2008 ). The actions of elites, especially candidates, can either exacerbate or mitigate NPDs. Unified, cohesive parties help the nominee, but parties that are internally divided and lacking cohesion hurt their candidates\u2019 chances by diverting resources and tarnishing the nominee\u2019s image (Campbell 2004 ). When party officials work together, with little ideological conflict, their electoral and policy goals are relatively clear (see Herrera 1993 ). The greater the fragmentation, the less likely that one candidate will be seen as the dominant frontrunner in the \u201cinvisible primary\u201d. Lack of consensus during the invisible primary does not necessarily indicate party division during the general election campaign. Contested nominations are the norm in presidential campaigns; competition for the nomination does not indicate a divided party. A strong diverse field of candidates can exacerbate existing divisions; however, the dynamics of the system are such that one candidate could quickly capture the nomination. Footnote 17 When a nomination campaign is divisive, the nominee and party elites attempt to reunite the party. They will not be able to erase years of ideological, regional or demographic differences, but they may be able to persuade disparate factions to work together temporarily to help the party win the presidency. The appearance of unity or division at the convention can influence undecided voters who are just beginning to focus on the campaign (see Holbrook 1996 ). Footnote 18 Measuring National Party Division Measuring NPD presents multiple difficulties. Because this variable is central to our research, several measures were tested. Through the 1970s delegate votes at the national conventions provided a rough measure of divisiveness. Thus NPD could be measured as the proportion of convention delegate votes received by the Democratic nominee (on the first ballot) minus the corresponding proportion for the Republican nominee. This is a reasonable, though not ideal, measure of NPD, at least through the 1970s. However, beginning in the 1980s party elites have \u201cstage-managed\u201d convention votes (CV), perhaps in part to present the national television audience with the appearance of party unity. A delegate-based measure mainly taps the behavior of party activists, chosen during the presidential campaign by fellow partisan voters. The national conventions are typically the most influential events of the entire campaign (Holbrook 1996 ). They occur when many voters, especially independents and weak partisans are beginning to focus on the two parties and their nominees. Most nominees receive overwhelming support on the convention ballot; the exceptions occur when the national party is severely divided. As an alternative measure one could compare the proportion of the national primary vote won by the two nominees (aggregate primary vote, or APV). This measure is more appropriate to the post-reform period than to earlier elections. Footnote 19 Now, voting in a primary is consequential; delegates to the convention are allocated based on votes in primaries. Before 1972, there were few primaries and voting in primaries bore little if any relationship to the choice of the parties\u2019 nominees. Neither CVs nor APV is ideal. Thus we considered, tested, but eventually rejected, several other possible measures, including the nominee\u2019s New Hampshire primary vote (Norpoth 2001 ), the proportion of early-deciding partisans, and several variations of CV and APV (See online appendix). In the analyses to follow we estimate the impact of NPD using both CV and APV to demonstrate that our substantive conclusions about the impact of NPD do not depend on how the variable is measured. Measuring Divisive State Primaries There is no consensus on how best to operationalize state-level primary divisiveness. Different ways of operationalizing the concept might account for the differing results seen in previous studies. In presidential campaigns, a divisive state primary can be thought of as one in which the state primary electorate generally prefers a candidate(s) other than the eventual nominee. This implies that there is a large pool of voters who may be inclined to abstain or defect. This concept of divisiveness is measured by the proportion of the vote for candidates other than the eventual nominee (Born 1981 ). Kenney and Rice ( 1987 ) argue that the proportion of the state primary vote received by the eventual Democratic nominee minus the corresponding proportion received by the eventual Republican nominee is the best measure of state primary divisiveness (also see Atkeson 1998 ). This approach seems advantageous since it accounts for the relative divisiveness of the two parties\u2019 state primaries. The major alternative approach focuses on the competitiveness of the primary. A close, hard-fought primary may lead some voters to harbor intense negative feelings about the eventual nominee. This concept of divisiveness is measured by the vote margin between the two leading candidates in the primary (Lengle et al. 1995 ), sometimes operationalized as a dummy variable (e.g., less than 20 % vote margin). Although both approaches measure aspects of divisiveness that could influence the general election outcome, they relate to substantively different phenomena. Consider a state primary in which the eventual nominee comes in second with only 30 % of the vote while the winner of that primary receives 35 % (the remaining votes distributed among other candidates). Such a primary would be considered highly divisive by the former measure (support for other candidates) but relatively non-divisive by the latter (margin of victory). Several measures used in previous research were tested. The analyses will indicate the kind of \u201cdivisiveness\u201d that leads to diminished performance in the general election. Footnote 20 Results The main results of the analyses are shown in Table 2 . They indicate that both NPD and divisive state primaries are statistically significant at the .01 level (whether NPD is measured using CV or APV) and exert a potentially meaningful (i.e., non-trivial) impact on election results. The parameter estimates of the control variables vary, largely because of the different time-periods involved. As expected, the parameter estimates of the national economy, state partisanship and state ideology are all statistically significant and in the expected direction. Table 2 Impact of national party division and divisive state primaries on state vote outcomes Full size table The results indicate that the impact of divisive state primaries is limited, while the impact of NPD can be substantial. For example (using CV as the measure of NPD), if in a certain state one party\u2019s primary is divisive, with its eventual nominee receiving only 50 % of the state primary vote, and the other party\u2019s primary is non-divisive with its nominee receiving 90 %, the former would lose only 1.12 % in that state in the general election. In comparison, smaller differences in NPD lead to greater differences in the national outcome. If the nominee of one party receives 70 % of the vote at his\/her national convention while the nominee of the other party receives 90 %, then the former would lose 2.43 % in the national popular vote. During the 1948\u20132012 period, NPD ranged from \u201346.2 to +35.3 (negative values indicate greater division in the Democratic Party). Considering this range, the coefficient of .121 indicates that the effect of relative NPD on the Democratic popular vote varied from \u22125.6 % to +4.3 %. The mean absolute value of DSP for all states is 19.18 %. The mean absolute shift in state vote outcome caused by DSP is .52 %, and the maximum is 2.7 %. The mean absolute shift in state vote outcomes caused by NPD is 2.9 %, and the maximum shift is 5.59 %. Assuming all the DSP scores in an election have the same sign (which is plausible though unlikely) and setting both DSP and NPD at their (absolute) means, the mean national shift caused by NPD is more than five times the mean national shift caused by DSP. Footnote 21 The Dependent Variable A critical concern is the operationalization of NPD. We argue above that relative CV (per cent of first ballot votes for the Democratic nominee minus the corresponding percentage for the Republican nominee) is a suitable measure of NPD. We note however that since the 1980s, conventions and convention voting have become increasingly stage-managed. As indicated above, APV, is well suited to the post-reform era (from 1972 on). Thus we employ the basic model, as described above, in two ways: one using CV for the entire post WW II period (1948\u20132012), and the other using APV for the post-reform period (1972\u20132012). Table 2 shows the results of the two models, identical except for the measure of NPD: CV in one, APV in the other. Because these variables are measured on different scales, we do not expect the coefficients of CV and APV to be similar. We do however expect, if both are reasonable measures of NPD, that the coefficient of divisive state primaries will not be affected much by which measure of NPD is used. Indeed, this is what we find: the coefficients of NPD differ according to their scale of measurement (.121 using CV 1948\u20132012, .237 using APV 1972\u20132012) but both are statistically significant at the .01 level; the coefficients of divisive state primaries are strikingly similar (.0279 using CV, .0258 using APV; both significant at the .01 level). Measured as the proportion of CVs received by the Democratic nominee minus the corresponding number for the Republican nominee (CV), NPD indicates that, for example, if one nominee receives 90 % of the delegates, while the other receives 65 %, the latter would lose 3.03 % in the general election. The impact of NPD was at least 3.29 % in 9 of the 16 elections. The coefficient for NPD measured as relative APV (taking into account the difference in the unit of measure) is larger than when measured as relative CV, suggesting that the impact of NPD may be greater than estimated using CV. The similarity between the results using CV and APV provides evidence that the results are not very sensitive to the way that NPD is operationalized. Except as noted, the analyses to follow will use CV as the measure of NPD. Tests for Robustness A number of sensitivity tests were performed. To test for possible realignment effects and\/or the McGovern-Frasier reforms, the model was estimated with dummies for 1972 or later and for 1980 or later, as well as interactions with these variables. Both January and July presidential approval ratings were tested. The war variable was tested both as a state-level and as a national-level effect. Third party votes were incorporated in several ways. Changes in coefficients (using either CVs or APV) are negligible when these variables are included. To make sure that no one election was driving or distorting the results, the model was estimated repeatedly, each time excluding one election. The results are very consistent, varying slightly across a wide range of statistical methods, model specifications, and measurements. Neither possible realignment effects, the timing of presidential approval ratings, the way war was incorporated into the model, nor the exclusion of any one election led to results that failed to confirm the hypotheses. As shown in Table A-1 (online appendix), the parameter estimate for divisive state primary is similar if caucuses are excluded, though the parameter estimate for NPD is about 20 % lower; the substantive conclusions are unchanged. The parameter estimates are very similar whether or not 1968 is included. Nonetheless, for the reasons discussed above, in footnote 15, and the online appendix, we decided that the 1968 data are not appropriate to this research. The model was also estimated with a two-way random effects model, a GLS estimator for panel data with AR(1) serial correlation (Baltagi and Wu 1999 ), Panel Corrected Standard Errors (both with and without a lagged dependent variable), and a mixed model that adds a random effect on a third party variable, effectively allowing it to vary across elections (while maintaining fixed-effects for states). The results are robust to the use of these alternative statistical techniques. The size of the coefficients varies to some extent, but the substantive message is the same: NPD (using either the CV measure or the APV measure) has large and statistically significant effects on election outcomes and divisive state primaries have small but significant effects (Table A-1, online). With few exceptions, the results are very similar across varying specifications. The coefficient of NPD was very stable, and significant at the .01 level. (More precisely, the coefficients using CV as the measure of NPD are very similar to one another; the same is true of the coefficients using APV as the measure of NPD). The coefficient of divisive state primaries, measured in terms of support for the eventual nominees, was extremely stable and significant at the .05 level, two-tailed. (As discussed below, when measured by margin of victory, the divisive state primaries variable in not statistically significant.) The results of these tests indicate that that our findings are robust: both NPD and state primary divisiveness significantly influence state-level presidential outcomes. Footnote 22 Sensitivity to the Operationalization of DSP As discussed above, several measures have been used in previous research to represent divisive state primaries. The two main approaches are to define a divisive state primary as (a) one in which the eventual nominee does poorly, and (b) one in which the victory margin is small. Each can be represented either as two variables (one for each party) or as one variable (the difference between the two parties in that state). Furthermore, margin of victory can be represented either as a continuous or as a dummy variable (e.g., using a 20 % cutoff). As shown in Table 3 , the results indicate that operationalizing divisive state primaries in terms of the eventual nominee\u2019s performance is more reliable than operationalizing it in terms of victory margin. Footnote 23 Indeed, both of the former are statistically significant while none of the latter are. This suggests that diminished performance in the general election occurs because there is a large pool of voters who did not support the eventual nominee in the state primary rather than because some voters evaluate the nominee negatively because the state primary was close and competitive (Note that the coefficient of NPD is very stable and highly significant regardless of the way that divisive state primaries is measured). Table 3 Results of tests of alternative measures of divisive state primaries (DSP) Full size table Although the impact of divisive state primary (measured in terms of the nominees\u2019 relative performance) is statistically significant, its national impact is relatively minor (State party divisions are nonetheless quite important to state party leaders, who generally have little or no effect on NPD). Even in the most extreme possible case, where one party\u2019s nominee received 100 % of the state primary vote while his opponent received 0 %, the impact on the general election vote in that state is only 2.79 % (using CV; 2.58 % using APV). State partisanship and state ideology have a greater impact than divisive state primaries (using standardized coefficients for comparative purposes). This confirms the expectation that divisive state primaries have a small and usually inconsequential effect on electoral outcomes. On the other hand, NPD is one of the more influential variables. These results support the hypothesis that NPD potentially has a substantial negative effect on electoral outcomes. Footnote 24 A divisive state primary leads to a maximum possible decrease in a state\u2019s general election vote of 2.8 %, while a divided national party more often than not leads to a decrease of more than 3.2 %. The impact of divisive state primaries is limited to a subset of states while the impact of NPD is not. Taken together, these results are consistent with the thesis that the overall negative impact of NPD is greater than that of divisive state primaries. Even in a close election, it is unlikely that divisive state primaries would make the difference in terms of who wins the Electoral College, though NPD may well have such an effect. Substantive Impact of Divisive State Primaries and NPD To provide an overview of the potential substantive effects of DSP and NPD, we calculated the estimated vote in each state in each election, absent the effects of divisive primaries, and absent the effects of NPD. Table 4 shows the number of states (and electoral votes) that likely would have been won by the other party; below we describe the potential effects on each national election. These estimates are intended to be illustrative. There is no way to know which states actually would have switched. Candidate, media and voter behavior would have been different in various ways. The number of states that would switch is partly a function of the closeness of the election. A close national election combined with state or national divisiveness can lead to a number of states \u201cswitching\u201d. The table reflects in substantive terms the results of the statistical analysis: the impact of divisive primaries is small and limited while the impact of NPD is greater and more widespread. Table 4 Substantive impact of divisive state primaries and national party division (states and electoral votes expected to switch absent divisiveness effects) Full size table Based on the results shown in Table 4 , among all 16 elections, only 16 states likely would have switched because of divisive state primaries alone; 95 states would have switched because of NPD alone. In none of the 16 elections do the results indicate that more than two states, or more than 53 electoral votes, would have switched because of divisive state primaries. In comparison, there were six elections in which NPD would likely have switched at least eight states with more than 112 electoral votes. (This illustration uses CVs to measure NPD; these estimates are more modest than those using the APV measure). In six of the 16 elections, the impact of divisiveness would have substantially changed the results. Without the effects of state primary divisiveness, we estimate that Kerry in 2004 would have won Ohio and thus the presidency and in 1948 neither candidate would have won a majority of electoral votes. Without the effects of NPD, we estimate that in 2000 Gore would have won Florida and thus the presidency; in 1980, had it not been for the national division between Carter and Kennedy, Carter would have won 17 additional states, putting him within striking distance of Ronald Reagan. Similarly, in 1976 (absent the effects of NPD) Ford would have won New York, Texas, Pennsylvania and five other states leading to a substantial victory over Carter; in 1960, Kennedy would have won 18 additional states leading to a landslide victory over Nixon; and in 1948, Dewey would have won an additional 10 states, verifying the Chicago Tribune headline \u201cDewy defeats Truman\u201d. Conclusion The relationship between divisive state primaries and general election outcomes is substantially different in presidential campaigns than in subnational campaigns. To appropriately estimate the impact of divisiveness in presidential campaigns, one must measure both the impact of NPD and the impact of divisive primaries in individual states. To this end, we developed a comprehensive model of state outcomes in presidential campaigns and tested several measures of NPD and several measures of divisive state primaries. We find that, in presidential campaigns, both NPD and divisive state primaries significantly influence state-level general election outcomes, with the former having a greater and more widespread impact. In addition, we have demonstrated that the impact of state primary divisiveness is sensitive to how the concept is operationalized. One can conceptualize a divisive state primary as one in which the state primary electorate only weakly supports the eventual nominee, implying that there are many partisans who may abstain or defect. This is measured by the proportion of the vote for candidates other than the eventual nominee. Alternatively one can conceptualize a divisive state primary as close and competitive, causing some partisans to harbor negative feelings about the eventual nominee. This is measured by the vote margin between the two leading candidates in the primary. These two approaches relate to substantively different phenomena. The analyses indicates that the former leads to diminished performance in the general election (the latter is not statistically significant). The results indicate that the impact of divisive state primaries is limited, while the impact of NPD can be substantial. A divisive state primary leads to no more than a 2.8 % decrease in the general election in that state. In comparison, NPD more often than not leads to decreases of at least 3.2 % across states. The impact of divisive state primaries is limited to a subset of states while the impact of NPD is not. Taken together, these results confirm the general thesis that the overall negative impact of NPD is greater than that of divisive state primaries. This research demonstrates that NPD is a critical component of divisiveness in presidential campaigns, but one that generally is not included in previous research. By incorporating a comprehensive set of appropriate controls, we have reliably estimated the impact of NPD and of divisive presidential primaries. We show that the national component is potentially powerful; the state-level component pales in comparison. Previous analyses of divisive state presidential primaries have measured a minor effect while ignoring the greater effect. Although this research has focused on the relative impact of NPD and divisive state primaries, the analysis also sheds light on the behavior of states in presidential elections. Among other findings, it indicates that election-specific national factors are critical to understanding general election outcomes and that long-term state-level factors such as partisanship and ideology play a major role in state-level electoral behavior. Footnote 25 It is hoped that this study contributes substantially to resolving the controversy over the impact of divisive presidential primaries. It has been shown that a divisive state presidential primary does have a negative effect on the vote outcomes in the general election, although the magnitude of the effect is relatively small, unlikely to change the winner in more than one or two states in a presidential election. Because of the control variables included in the model, it is unlikely that this relationship is an artifact of unpopular incumbents, weak economies, or the partisan or ideological predispositions of state electorates. Furthermore, the results are consistent\u2014varying little across a wide range of methods and model specifications. Having established this, it would be useful and interesting to differentiate between the relative impact of early and late primaries, those with high versus low turnout, and those with few or many active candidates. These and other state-specific factors could cause some divisive state primaries to have greater or lesser impact on general election results than others. That would represent a potentially important avenue for future research. Such research would present a number of measurement problems. Primary turnout is difficult to gauge because the denominator is generally unknown. The three factors are inter-related: greater turnout tends to occur when there are more candidates which tends to happen early in the primary season. Similarly, once the race has been called, both turnout and the number of candidates diminishes. One implication for the parties is clear: in terms of winning the presidency, a divided national party does more damage than a set of divisive state primaries. Competition among candidates does not necessarily hurt a party\u2019s general election chances, but schisms within the party\u2019s base can be truly harmful. The analysis indicates that a divisive state primary will decrease the party\u2019s general election vote in that state, but usually by less than 2 %. In comparison, a divided national party decreases the party\u2019s vote across states, usually by more than 3 %. Party leaders no doubt understand that NPD leads to negative consequences in the general election. However, in most cases, they need not be concerned about the effects of divisive primaries in individual states. Except in the most pivotal states, such as Florida and Ohio, in a close national election, a decrease in the range of 1\u20132 % in the popular vote will not influence which party wins the Electoral College. Much can be gained by investigating the causes and consequences of divided national parties. What causes the underlying long-term divisions within the parties? Under which circumstances do presidential nomination campaigns exacerbate such divisions? What can candidates do before and after the convention to unite their party? And what can the parties do between elections to diminish the chances that divisions will intensify during the next campaign cycle? Notes A national party can have a set of divisive state primaries yet remain united (e.g., 2000 Republicans). Alternatively, a party could be divided nationally yet see few divisive state primaries, especially if \u201cdivisive\u201d is operationalized as a small victory margin. One candidate could win handily in some regions while losing by large margins in others (e.g., 1976 Republicans). In some elections there are numerous divisive state primaries yet the national party is able to unite during the general election campaign (e.g., 1976 Democrats, 1980 Republicans). In some elections there are few divisive primaries, yet the national party is severely divided at the convention and beyond (e.g., 1964 Republicans). Some state primaries are not even contested since they occur after the nomination has effectively been decided. Studies of subnational divisive primaries have reached a confusing variety of conclusions (Lengle and Owen 1996 ). Several found that such primaries negatively affect general election outcomes (e.g., Bernstein 1977 ), others found mixed effects (Born 1981 ; Kenney and Rice 1984 ), others found little or no effect (Hacker 1965 ; Kenney 1988), and some found a positive effect in the out-party (Westlye 1991 ; Partin 2002 ). Jacobson\u2019s ( 1978 ) work on congressional elections helps to make sense of these results. A congressional or gubernatorial incumbent whose reelection chances are relatively low may be challenged within his or her own party, leading to a (potentially) divisive primary that hurts the incumbent in the general election. On the other hand, challengers typically are not well-known\u2014a primary battle in the out-party brings media attention to the candidates in that party and thus raises their name recognition, a valuable resource in a congressional or gubernatorial race (Westlye 1991 ; Lazarus 2005 ). The nature of single-state primaries in presidential campaigns is dramatically different. Unlike sub-national primaries, candidates may or may not choose to compete vigorously in certain states. Thus it is possible for a number of presidential state primaries to be non-divisive (if it is clear which candidate is likely to win that state) even though the national campaign may be highly competitive. For example, in 1980 few Democratic state primaries were competitive; most were assumed to be easy victories for one candidate or the other and thus not seriously contested. Conversely, it is possible for there to be a number of divisive state primaries even though the result of the national campaign is not really in doubt. In 1976, for example, most non-Southern primary were seriously contested, yet the national Democratic party did not suffer substantial internal divisions and quickly united behind Jimmy Carter once the primaries ended. The divisive primary hypothesis is rooted in cognitive psychology, but there are several behavioral explanations that could cause the phenomenon. Voters may rationally use divisiveness as a cue for low candidate quality. One could hypothesize a divisiveness effect without making strong assumptions about voter rationality. For example, voters in Iowa and New Hampshire usually can choose among 5\u20139 potentially viable candidates; after Iowa and New Hampshire the field typically narrows to 2 or 3 viable candidates because the unsuccessful candidates withdraw. Thus the choices of voters in subsequent states is restricted. State party resources are rarely used during primary battles, whether subnational or presidential, rather resources come from the individual candidate campaigns. Economic and other national contextual variables are adjusted to account for the party of the incumbent president. After the mid-1970s, the second dimension is best characterized as reflecting \u201csocial issues\u201d such as abortion, busing, and gun control. (Poole, Keith; 2015; interview with author) As a test for possible realignment effects, the model was re-estimated with a dummy for the post-1968 period; the dummy is not significant and its inclusion barely alter the coefficients. There is some collinearity between the national economy and national party division (r = .67). A weak economy is often associated with divisions within the incumbent party. If the economic variable was excluded from the model, it would bias the coefficient of national party division, probably by artificially inflating the estimated impact of that variable. For example, there were roughly as many out-party candidates in the primaries opposing popular incumbents such as Reagan and Bill Clinton as there were opposing unpopular incumbents such as Ford and Carter. Similarly, the two most popular incumbents running in the past 50 years were Nixon and Reagan; both faced several strong candidates in the other party (Muskie, Humphrey, Wallace and Scoop Jackson in 1972; Mondale and John Glenn in 1984). In-party challenges to an incumbent president are rare. During the 1948\u20132012 period, nine incumbents faced no serious challenge in the primaries; only two (Ford and Carter) were challenged (though some would not classify Ford as a true incumbent). The case of Johnson in 1968 is open to interpretation\u2014Johnson was challenged but withdrew early (1968 is not included in our dataset). Results are substantively similar if July approval ratings are used. We believe that war, as measured by casualties, is a national-level phenomenon. Certainly there are variations across states during wartime but we believe that the difference between wartime and peacetime has a greater effect on the electorate than do variations across states. We tested state-level war deaths and found it was not statistically significant. It should be noted however that Karol and Miguel ( 2007 ) found state-level war casualties to be significant in their analysis of the 2004 election. To account for the possibility of serial correlation, the model was also estimated with Baltagi and Wu\u2019s ( 1999 ) GLS estimator for AR(1) panel data and OLS with a lagged dependent variable and Panel Corrected Standard Errors (Beck and Katz 1996 ); both yield substantively similar results to those presented in the text. (See table A-1, online appendix) A possible problem arises in that the statistical model assumes a continuous and unbounded dependent variable. While the general election outcome is indeed continuous, a proportion is, by definition, bounded. Paolino ( 2001 ) shows that when there are many cases close to the bounds (in this case 0 and 1), there are substantial benefits to using a maximum likelihood model for beta-distributed dependent variables. However, in this dataset there are no cases within .19 of the bounds and only 9 cases (about 1.2 %) within .25 of the bounds. As such, the gains from a beta-distributed dependent variable model would be minimal. Indeed, Paolino\u2019s replication of Atkeson ( 1998 ) uses a similar dependent variable and shows no difference between a model assuming an unbounded dependent variable and the beta-distributed dependent variable model. Since the McGovern\u2013Fraser reforms dramatically changed the nature of nomination campaigns, the model was also applied only to the elections of 1972\u20132012. Both the divisive state primary measure and the national party division measure in 1968 are anomalous. The nomination phase is unique in that one of the two leading candidates, Robert Kennedy, was assassinated before the convention, thus likely altering the impact of divisive state primaries on general election results. Also, Hubert Humphrey entered no primaries, thus every primary shows up as extremely divisive. The general election results are also anomalous because of the strong performance of a non-centrist third party candidate (see online appendix). Although we decided to exclude 1968 from the analysis, we tested the model with 1968 included. The parameter estimates for national party division and divisive state primaries were essentially unchanged (see online Table A-1). Two cases were excluded because neither the Democratic candidate nor electors pledged to him appeared on the ballot (Mississippi in 1960, Alabama in 1964). One was excluded because it was an extreme outlier (Johnson received less than 13 % in Mississippi in 1964). These outlying cases could bias the parameter estimates (Achen 1982 ). The Democrats lacked an early dominant frontrunner in 1976 and 1992, yet the party was relatively united by convention time. The 1972 and 1984 Democratic campaigns both had dominant early frontrunners, yet the party was divided and lost the general election. Typically we observe five to nine candidates in a presidential nomination contest that does not include an in-party incumbent; some of these campaigns lead to a divided national party; others do not. This research focuses on the potential negative impact of short-term national party division on that year\u2019s general election. It is important to differentiate between preexisting national party division (before the primaries) and national party division when it is most likely to impact general election results (during the primaries, at the convention, and beyond). The existence of long-term underlying division is not sufficient to hurt a party\u2019s general election vote. Rather, the harm becomes manifest when there is intense competition for the party\u2019s nomination and the nominee is unable to unite the party. Throughout the 1960s and 1970s, there were severe long-term divisions in the Democratic party while the Republican party was much more united. Nonetheless, in 1964 and 1976, the Democrats were mostly united and the Republicans seriously divided. Aggregate primary vote gets around the problem of stage-managed conventions but it is not a good measure for the pre-reform period. Fifty years ago, less than a third of the states used primaries (rather than caucuses), and many of them were either \u201cdelegate primaries\u201d, \u201cfavorite son\u201d or \u201cbeauty contest\u201d primaries. Nowadays, more than two-thirds of states use primaries, delegates are generally bound or committed to vote for a particular candidate. Primaries vary in many ways (timing, winner-take-all vs. proportional representation, open vs. closed, number of candidates, turnout, etc.). Although each of these is potentially related to divisiveness and thus reflected in our parameter estimates, we recognize that timing and the number of candidates (and possibly turnout) could cause some divisive state primaries to have greater or lesser impact on general election results than others. We address these concerns in the online appendix. Comparing the consequences of a 1-unit change in DSP with a 1-unit change in NPD may not be a fair comparison. It may be that changing DSP is \u201ceasy\u201d while changing NPD is \u201chard\u201d. In nomination campaigns, party leaders try to unify the national party as soon as possible; at the state level, candidates try to stay active, run in primaries and defeat opponents. It's easy to get national party unity when there's a popular incumbent running; it's hard to do so when there are several strong candidates representing different factions. It's easier to get high divisiveness scores when one party has selected its nominee and the other has not; it's harder to do so after both parties have selected their nominees (see online appendix). In this research, we seek to show that the full impact of divisiveness in presidential elections involves both state primary divisiveness and national party division. A model of election outcomes that does not include the latter is misspecified; thus the estimate of state primary divisiveness is potentially biased (see Table A-1 online). The coefficients of divisive state primaries measured in terms of support for the eventual nominees and by victory margin are not comparable because they are measured on different scales. Nonetheless, the former are statistically significant while the latter are not. The potential for deleterious effects is greatest when nomination candidates differ ideologically and regionally, as was the case in the 1980 Democratic campaign. In 2008, the Obama\u2013Clinton struggle did not prevent the party from winning the general election. However, we specifically test the effects of state and national party division in 2008. First, two interactive variables were created to see if the impact of divisive state primaries or national party division was different in 2008 than in other elections. Neither was significant, indicating no discernible difference in the impact of divisiveness in 2008. Similarly, estimating the analysis without 2008 produced nearly identical parameter estimates indicating that the 2008 election results fit the general pattern seen in previous elections. Other researchers reached similar conclusions (Henderson et al 2010 ; Makse and Sokhey 2010 ; Southwell 2010 ; but see Wichowsky and Niebler 2010 ). The analysis shows that state general election outcomes are influenced by both long-term and short-term factors and by both national-level and state-level factors. Among the state-level factors, state partisanship and both state ideology variables are statistically significant. A state in which the average previous congressional and gubernatorial Democratic vote was 60 %, for example, would tend to have more than a 2 % higher presidential vote than a state with 50 % previous Democratic vote. The difference in the presidential vote between a very moderate state and a state with the most extreme general ideology score would be approximately 7\u20138 %. The corresponding civil rights\/social issues ideology difference would be 4 %. In addition, a presidential candidate tends to receive about 3 % more in his home state than would otherwise be expected. ","News_Body":"Divided political parties rarely win presidential elections, according to a study by political science researchers at the University of Georgia and their co-authors. If the same holds true this year, the Republican Party could be in trouble this presidential general election. The study, which examined national party division in past presidential elections, found that both national party division and divisive state primaries have significant influence on general election outcomes. In this election cycle, the nominee of a divided Republican Party could lose more than 3 percent of the general election vote, compared to what he would have gained if the party were more united. \"History shows that when one party is divided and the other party is united, the divided party almost always loses the presidential election,\" said Paul-Henri Gurian, an associate professor of political science at UGA's School of Public and International Affairs. \"Consider, for example, the elections from 1964 through 1984; in each case the divided party lost.\" The study measures party division during the primaries and indicates how much the more divided party loses in the general election. The study found that divisive state primaries can lead to a 1 to 2 percent decrease in general elections votes in that state. For example, Hillary Clinton received 71 percent of the Democratic vote in the Georgia primary, while Donald Trump received 39 percent of the Republican vote. According to the historical model, a Republican-nominated Trump would lose almost 1 percent of the Georgia vote in the general election because of the divided state primary. National party division has an even greater and more widespread impact on the national results, often leading to decreases of more than 3 percent nationwide. Looking again at the current presidential election cycle, Trump had received 39.5 percent of the total national Republican primary vote as of March 16, while Clinton had received 58.6 percent of the Democratic vote. If these proportions hold for the remainder of the nomination campaign (and if these two candidates win the nominations), then Trump would lose 4.5 percent of the vote in the general election, compared to what he would have received if the national Republican Party was not divided. \"In close elections, such as 2000, 2004 and 2012, 4-5 percent could change the outcome in terms of which party wins the presidency,\" Gurian said. The results of this study provide political analysts with a way to anticipate the impact of each primary and, more importantly, the impact of the total national primary vote on the general election results. Subtracting the percent of the Republican nominee's total popular vote from that of the Democratic nominee and multiplying that by 0.237 indicates how much the Republican nominee is likely to lose in the November election, compared to what would otherwise be expected. The 4.5 percent figure calculated through March 16 can be updated as additional states hold their primaries. (The same can be done for each individual state primary by multiplying by 0.026.) ","News_Title":"Divided parties rarely win presidential elections, study shows","Topic":"Other"}
{"Paper_Body":"Abstract The abyssal ocean is broadly characterized by northward flow of the densest waters and southward flow of less-dense waters above them. Understanding what controls the strength and structure of these interhemispheric flows\u2014referred to as the abyssal overturning circulation\u2014is key to quantifying the ocean\u2019s ability to store carbon and heat on timescales exceeding a century. Here we show that, north of 32\u00b0 S, the depth distribution of the seafloor compels dense southern-origin waters to flow northward below a depth of about 4 kilometres and to return southward predominantly at depths greater than 2.5 kilometres. Unless ventilated from the north, the overlying mid-depths (1 to 2.5 kilometres deep) host comparatively weak mean meridional flow. Backed by analysis of historical radiocarbon measurements, the findings imply that the geometry of the Pacific, Indian and Atlantic basins places a major external constraint on the overturning structure.     Main Dense waters originating from the surface at high latitudes make up the overwhelming majority of the ocean volume. Once formed through heat loss and salt gain, they sink to depth and spread across the globe, carrying information about atmosphere\u2013ocean\u2013ice interactions into the slow-paced abyss and contributing to the ocean\u2019s long \u2018memory\u2019 of atmospheric conditions 1 . But the memory timescale and climate buffering effect of the deep ocean ultimately depend upon the rate at which these dense waters are removed from deep seas and returned to the surface. Physical controls on the volume and return pathways of dense waters are therefore key to the ocean\u2019s carbon and heat storage capacity and its role in centennial to multi-millennial climate variability 2 , 3 . The cycle of production, modification and consumption of dense water masses is often conceptualized as a meridional overturning circulation composed of two dynamically distinct limbs 4 , 5 ( Fig. 1a ): an abyssal, northward limb that carries the densest Antarctic-sourced waters (Antarctic Bottom Water, AABW) until they upwell into lighter waters of the Indian, Pacific and Atlantic basins; and a shallower, southward limb that carries these lighter deep waters to the Southern Ocean. Because it involves a gradual decrease in the density of AABW, the abyssal branch is considered to be essentially a diabatic circulation. In contrast, the southward flow of overlying deep waters is thought to be predominantly adiabatic, that is, density-preserving 6 , 7 . This dynamical divide is consistent with the two regimes apparent in the deep-ocean density distribution ( Fig. 1a ): north of the Antarctic Circumpolar Current and away from North Atlantic sinking, level density surfaces above depths of about 2.5 km appear to be compatible with an adiabatic arrangement of water masses, whereas the northward descent of abyssal density surfaces signals transformation of AABW as it travels north. The transition between diabatic and adiabatic regimes and the transition from northward to southward mass transport have been linked to the depth profile of basin-averaged mixing rates, and to surface wind forcing over the Southern Ocean 3 , 4 , 5 , 6 , 7 , 8 . Here we show that these two transitions are tied to the depth distribution of the seafloor and are separate from each other. Figure 1: Density surfaces, seafloor areas and the ocean\u2019s overturning. Climatologies 41 , 49 of neutral density ( a ) and zonally summed incrop areas (in units of square metres per degree of latitude and per (kilograms per cubic metre)) ( b ) as a function of latitude and pseudo-depth. The pseudo-depth of density surfaces is found by filling each latitude band from the bottom up with ocean grid cells ordered from dense to light. Density is contoured in black every 0.1 kg m \u22123 for \u03b3 \u2265 27.5 kg m \u22123 . Grey arrows in a give a simplified view of overturning flows. Flows oriented along (or across) density surfaces correspond to adiabatic (or diabatic) transports. This study focuses on the latitude range 32\u00b0 S\u201348\u00b0 N enclosed in white lines. PowerPoint slide Full size image The deep ocean communicates with the surface in two high-latitude regions ( Fig. 1a ): the North Atlantic, where deep waters are formed and exported southward to ventilate the 27.7\u201328.14 kg m \u22123 density range 9 , 10 ; and the Southern Ocean, where rising density surfaces allow deep waters to upwell primarily adiabatically 6 , 7 , 11 , 12 until they are converted into denser AABW or lighter intermediate and mode waters 5 . Note that we use neutral density 13 , denoted \u03b3 , as a globally consistent density variable and subtract 1,000 kg m \u22123 from all density values. Away from these two high-latitude regions, dense waters are isolated from surface exchanges: their density transformation and upwelling rely on deep diabatic processes. We henceforth focus on such processes and restrict the analysis to ocean waters deeper than 1 km between 32\u00b0 S and 48\u00b0 N. Geometry At depths of 1\u20132.5 km, ocean topography is dominated by relatively steep continental slopes and accounts for less than 8% of the total seabed area ( Fig. 2a and b ). Deeper, the emergence of flatter ridges decaying onto abyssal plains markedly increases the seafloor area per unit depth, which quadruples between depths of 2.5 km and 4.3 km. Depth layers therefore have unequal access to the seafloor: the quarter of the water volume which resides below 3.5 km occupies three-quarters of the seabed. This inequality is reinforced when considering the seafloor coverage of density layers\u2014that is, layers defined by a fixed density interval\u2014because the thickness of such layers generally increases with depth in the deep ocean ( Figs 1 and 2c and d ). By analogy with surface outcrop areas, the seafloor area that is intersected (covered) by a given density layer is termed the \u2018incrop\u2019 area. The relatively narrow 28\u201328.25 kg m \u22123 density range takes up over 80% of the ocean floor between 32\u00b0 S and 48\u00b0 N, with the lion\u2019s share going to waters of about 28.11 kg m \u22123 ( Fig. 2c and d ; see also Extended Data Fig. 1 ). Figure 2: Depth and density distributions of seafloor area over 32\u00b0 S\u201348\u00b0 N. a , Seafloor area per unit depth. c , Seafloor area per unit density, termed incrop area. The mean density of geopotential surfaces and the mean depth of density surfaces are indicated on the left y axes of a and c , respectively. b , d , Bottom-up cumulative seafloor area as a function of depth ( b ) or density ( d ). The lower and upper white lines depict respectively the northward\u2013southward and diabatic\u2013adiabatic transition levels tied to the seafloor distribution, as proposed in this work. Spreading ridges and abyssal plains dominate topography deeper than 2.5 km; steep continental slopes dominate at smaller depths. Northward-flowing AABW dominates waters deeper than 4.3 km (denser than 28.11 kg m \u22123 ); its southward return as relatively dense Pacific Deep Water (PDW), Indian Deep Water (IDW) or North Atlantic Deep Water (NADW) occurs predominantly at depths greater than 2.5 km (densities greater than 28 kg m \u22123 ). PowerPoint slide Full size image These simple geometric considerations have important implications for the consumption rate and upwelling pathways of dense waters. Deep-ocean sources of density transformation have long been recognized to be concentrated near the seafloor 14 , 15 , 16 , 17 , 18 , 19 , 20 , where boundary-catalysed turbulence and geothermal heating combine to erode the near-bottom stratification and progressively lighten bottom seawaters. The resulting near-bottom confinement of density loss suggests that deep water masses benefiting from a large seafloor coverage are more likely to be efficiently consumed than those isolated from the bottom. Consistent with the preferential lightening of bottom boundary waters, incrop areas tend to increase along the northward path of AABW and to slowly migrate towards smaller densities ( Fig. 1b ), indicative of a successive removal of incropping density layers and resultant homogenization of AABW 21 . The conjunction between the regime of sloping density surfaces and the presence of large incrop areas ( Figs 1b and 3b ; Extended Data Figs 2 , 3 , 4 ) is also suggestive of the dominant role of boundary transformation. Hence, the clustering of seafloor area around the 4\u20135.5 km and 28.11 kg m \u22123 levels may strongly influence the structure of cross-density transports and the associated meridional flows. Figure 3: Pacific seafloor and radiocarbon distributions. a , Zonally summed seafloor areas as a function of latitude and depth. b , Zonally summed incrop areas as a function of latitude and pseudo-depth. c , Along-density zonal mean radiocarbon content (\u0394 14 C) as a function of latitude and pseudo-depth. d , Schematic regime transitions, as in Fig. 2b and d . The pseudo-depth of density surfaces is defined as in Fig. 1 . In b and c , density is contoured in black every 0.1 kg m \u22123 for \u03b3 \u2265 27.5 kg m \u22123 . The lower and upper white curves depict, respectively, the local northward\u2013southward and diabatic\u2013adiabatic transition levels inferred from the incrop area distribution. Specifically, at each latitude y s , we calculate the \u03b3 -profile of summed incrop areas north of y s . The northward\u2013southward transition then corresponds to the density of the profile peak, while the diabatic\u2013adiabatic transition is defined as the smallest density at which the incrop profile decreases to 10% of its peak. All panels of this figure include the light-blue region shown in the inset map in a , which hosts the main Pacific abyssal overturning (Methods). The whole Pacific and southeastern Pacific are shown in Extended Data Fig. 2 . PowerPoint slide Full size image Water mass transformation To formally relate overturning flows to incrop areas, we first set out the link between cross-density transport and the vertical profile of diffusive density fluxes. Consider the volume V ( \u03b3 ) of waters denser than \u03b3 , bounded by the seafloor, the density surface A ( \u03b3 ) and latitudes y s < y n ( Fig. 4 ). We define the total geothermal and mixing-driven density fluxes entering V from below and from above as G ( \u03b3 ) and F ( \u03b3 ), respectively. For the volume V to remain unchanged, advection across A must balance local geothermal and mixing-driven density tendencies, such that 21 , 22 , 23 : where T ( \u03b3 ) denotes the mass transport through A ( \u03b3 ) and is termed the dianeutral transport. Equation (1) states that the transport across a given density layer is proportional to the net geothermal and mixing-induced density change within that layer. Geothermal heating causes only lightening, balanced by dianeutral upwelling ( T > 0, towards lower density). In contrast, mixing may be a density source or sink, requiring downwelling or upwelling, respectively. Figure 4: Sketch of a volume V of waters denser than \u03b3 , bounded by the density surface A ( \u03b3 ) and latitudes y s and y n . Density fluxes F and G entering V and the dianeutral mass transport T leaving V are also shown. The streamfunctions \u03c8 s ( \u03b3 ) and \u03c8 n ( \u03b3 ) are defined as the net southward mass transport below A ( \u03b3 ) at y s and y n , respectively. As an illustrative example, we show (dotted line) the surface of peak dianeutral upwelling (grey arrow). Mass conservation requires that this density surface corresponds to meridional flow reversal at y s (see velocity arrows on the left) if: (1) the along-density transport at y n is zero, as in the case of the Pacific and Indian basins given y n at their northern end; or (2) the along-density transport at y n is both southward and weak below the peak upwelling level, as we infer to be the case in the western Atlantic given y n = 48\u00b0 N (Methods). PowerPoint slide Full size image Dianeutral transports are thus controlled by the \u03b3 -profile of the total density flux entering successively denser water volumes, ( F + G )( \u03b3 ). In turn, we can relate basin-scale dianeutral transports to meridional, along-density flows by realizing that T must equal the zonally integrated meridional mass flux into V . Denoting by the streamfunction \u03c8 s (or \u03c8 n ) the net southward mass transport through the y s (or y n ) bounding latitude section of V ( Fig. 4 ), we have, by continuity: In the deep Indian and Pacific oceans, choosing y n at the closed northern end of each basin yields simply T = \u2212 \u03c8 s . This entails in particular that the density surface of peak dianeutral upwelling T north of y s defines the boundary between northward and southward flow at y s ( Figs 3d and 4 ). In the abyssal Atlantic, dianeutral upwelling may balance inflow from the south and north, so that the meridional flow reversal may lie at a denser level than the peak upwelling rate. Nonetheless, we find the two levels to match in this basin as well when choosing y n = 48\u00b0 N ( Fig. 4 , Extended Data Fig. 4 and Methods). Mixing scenarios Figure 5 shows the profile of the mixing-driven density flux F as well as the associated 32\u00b0 S\u201348\u00b0 N dianeutral transports under two idealized scenarios (Methods): scenario S1 has uniform local density fluxes throughout the ocean interior; scenario S2 has bottom-enhanced local density fluxes, with uniform bottom magnitude. The bottom-intensification of scenario S2 density fluxes is specified as an exponential decay from the seafloor with a 500 m e-folding scale, a structure representative of turbulence observations in the abyssal Brazil Basin 24 . Flux magnitudes are chosen so that both peak upwelling rates equal 25 \u00d7 10 6 m 3 s \u22121 , a mid-range estimate of maximum abyssal upwelling 5 , 9 , 10 constrained by velocity measurements at circulation nodes 25 , 26 , 27 , 28 , 29 . Shadings show the added contribution of a uniform mixing rate of 10 \u22125 m 2 s \u22121 , a typical level of mixing away from the direct influence of boundaries 14 , 30 , 31 , 32 , 33 . Figure 5: Density fluxes and dianeutral transports within 32\u00b0 S\u201348\u00b0 N. Density profiles of the total density flux F ( a ), the density flux averaged over density surfaces ( b ) and total dianeutral transports T ( c ) under scenarios S1 (orange) and S2 (blue). Shading denotes the added contribution to fluxes and transports of a uniform mixing rate of 10 \u22125 m 2 s \u22121 . PowerPoint slide Full size image Scenario S1 corresponds to a diabatic bottom boundary overlain by an adiabatic ocean interior: convergence of local density fluxes occurs only in the unstratified bottom boundary layer, where the density flux weakens to satisfy a no-flux boundary condition\u2014that is, the constraint that turbulent mixing cannot flux density across the seafloor 19 , 21 , 34 , 35 . The implied density transformation and dianeutral circulation are thus qualitatively identical to those that would be generated by a uniform geothermal density sink along the ocean bottom. The scenario results in diabatic upwelling peaking at \u03b3 = 28.11 kg m \u22123 and mostly confined to below the 28 kg m \u22123 density surface, matching the incrop area distribution ( Figs 2c , 5c and 6 ). Indeed, since a uniform density flux homogeneously lightens waters covering the ocean floor, scenario S1 implies that density layers upwell in proportion to their access to the seafloor ( Extended Data Fig. 5 ). Consequently, diabatic upwelling is restricted to the depth and density range of sizeable incrop areas, and the boundary between northward and southward meridional transport is the density surface of maximum incrop area ( Fig. 6 ). In particular, most of the southern-origin dense waters must then flow back to the Antarctic Circumpolar Current at depths greater than 2.5 km. Figure 6: Schematic abyssal overturning circulation north of 32\u00b0 S. The average depth of density surfaces is shown as a function of bottom neutral density and cumulative seafloor area. The surface of maximum incrop area ( \u03b3 = 28.11 kg m \u22123 ), corresponding to meridional flow reversal, and the surface marking the approximate transition between diabatic and adiabatic flow regimes ( \u03b3 = 28 kg m \u22123 ), are contoured in white. Straight and wiggly red arrows depict mixing-driven and geothermal buoyancy (\u2212 \u03b3 ) fluxes, respectively. To simplify the illustration, mixing-driven fluxes are taken to be uniform in the vertical direction, as in scenario S1. Density loss and diabatic upwelling are confined to near-bottom waters, which climb across density surfaces and along topography at a rate commensurate with the incrop area. Through mass conservation, this cross-density, along-bottom circulation maintains an along-density, interior circulation which supplies (or returns) dense waters from (or to) the Antarctic Circumpolar Current. Note that in the Atlantic these along-density flows may have an additional supply component from the subpolar North Atlantic. In the Indo-Pacific, a weakly ventilated shadow zone lies above the abyssal overturning circulation in the approximate 1\u20132.5 km depth range. PowerPoint slide Full size image Because geothermal heat fluxes exhibit relatively weak spatial variations away from ridge crests and contribute only bottom density losses, their impact on circulation is well described by the uniform-flux idealization of scenario S1 21 , 23 . In contrast, deep ocean mixing is observed to be dominated by patchy, topographically enhanced turbulence 15 , 16 , 17 , 19 , 32 , 33 , 36 , 37 , 38 . Such turbulence is generally associated with a bottom-enhanced local density flux, whereby lightening of densest waters occurs at the expense of densification immediately above. Scenario S2 explores the impact of an idealized, geographically homogeneous bottom-intensification of density fluxes. Under this scenario, density loss (or gain) generally dominates for density layers that have a larger (or smaller) incrop area than their underlying neighbour: the change in incrop area with height determines the dianeutral transport ( Extended Data Fig. 5 ). Upwelling is consequently found within waters denser than 28.11 kg m \u22123 , peaking just under this level, whereas density gain and downwelling characterize lighter waters ( Fig. 5c ). Hence, despite their structural differences ( Fig. 5a and b ), the simple scenarios S1 and S2 share two essential features: dianeutral upwelling peaks near the density level of maximum seafloor coverage, and decreases rapidly at lower densities. The complex spatial patterns of deep ocean turbulence could override these features. However, multiple lines of evidence indicate otherwise. First, examination of a range of bottom-intensified mixing scenarios analogous to scenario S2, where the magnitude of local density-flux profiles is not uniform but instead depends on bottom roughness, slope, stratification or internal wave generation rates (Methods), consistently shows upwelling peaking at or below the peak incrop surface and dwindling rapidly above ( Extended Data Fig. 6 ). Second, consideration of local density fluxes that decay above the bottom according to non-exponential and region-dependent profiles alters the distribution of density gain in the interior but preserves the near-bottom location of density loss, leading to a coupling between incrop and upwelling profiles similar to that inherent in the scenarios S1 and S2. Third, turbulence remote from boundaries, fed by interactions among internal waves and associated with weakly varying mixing rates 14 , 30 , 31 , 32 , 33 of the order of 10 \u22125 m 2 s \u22121 , drives only weak upwelling deeper than 2.5 km ( Fig. 5c , shaded areas). From a depth of 2.5 km to a depth of 1 km, the upwelling induced by a uniform diffusivity of the order of 10 \u22125 m 2 s \u22121 remains modest and fairly constant, demanding little net meridional flow. Radiocarbon evidence A separate line of evidence corroborates the leading-order control of dianeutral upwelling by incrop areas: analysis of historical radiocarbon measurements 39 , 40 confirms the tight connection between the density distribution of the seafloor and the overturning structure ( Fig. 3c ; Extended Data Figs 2 , 3 , 4 ). By mapping radiocarbon content (\u0394 14 C) along density surfaces (Methods), we find that: (i) the maximum incrop area accurately predicts the transition surface between northward and southward flow identified in each basin\u2019s \u0394 14 C distribution; (ii) the height at which the incrop area falls to 10% of its peak approximates the lower boundary of the relatively thick \u0394 14 C minimum (age maximum) observed at mid-depth in the Pacific and Indian oceans. Because of its size and connectedness, the Pacific basin shows the clearest signature of seafloor areas on the radiocarbon distribution ( Fig. 3 ). The strongest vertical \u0394 14 C gradient at 32\u00b0 S occurs at the density of the basin\u2019s peak incrop area, \u03b3 = 28.11 kg m \u22123 , where the inflow of relatively young waters underlies their southward return after a centuries-long journey in the abyssal Pacific. Water mass transformation estimates ( Fig. 5 and Extended Data Fig. 6 ) further indicate that most of this southward return flow takes place below the 28 kg m \u22123 density surface, the transition level above which seafloor availability becomes scarce ( Figs 2 and 3a and b ). The minimum \u0394 14 C centred around 2.3 km ( \u03b3 = 27.95 kg m \u22123 ) must then reflect weak upwelling of bottom waters to that depth. This inference is corroborated by the correspondence between the structure of the \u0394 14 C minimum and the depth or density distribution of seafloor areas ( Fig. 3 and Extended Data Figs 2 and 3 ): the oldest density layers appear to be those largely isolated from the ocean bottom and, thereby, from renewal via abyssal upwelling. The Atlantic and Indian oceans host more complex AABW pathways owing to their compartmentalization into many sub-basins. There, the leading role of inter-basin passages in transforming the northward-flowing AABW is clearly demonstrated by the bottom density field ( Extended Data Fig. 1 ). Indeed, substantial density drops from sub-basin to sub-basin largely reflect concentrated mixing within connecting AABW throughflows 16 , 29 , 36 . Such concentrated density transformation suggests that access to constrictive passages could be as strong a determinant of diabatic upwelling rates as is access to large seafloor areas. However, radiocarbon distributions show that abyssal circulation chokepoints do not host the peak dianeutral transports that define the meridional flow reversal (Methods and Extended Data Figs 3 and 4 ). Instead, deep straits and sills appear to reinforce the influence of incrop areas on the overall overturning structure: by contributing prominently to the homogenization of AABW, they favour the concentration of incrop areas to a narrow density range and the pivotal upwelling of end-basin waters 36 (Methods). Hence, water mass transformation scenarios and modern radiocarbon distributions together show that diabatic upwelling peaks near the density layer that has the largest seafloor coverage and decreases rapidly at lower densities. The robustness of this structure is due to two principal facts: (i) boundary mixing and geothermal heating restrict density loss to the bottom boundary; and (ii) density layers have strongly unequal access to the seafloor. Fact (ii) is first a consequence of the relative abundance of seafloor at depths greater than 2.5 km ( Figs 2 and 3a and b ; Extended Data Figs 2 , 3 , 4 ). It is further reinforced by fact (i), which underpins the progressive focusing of AABW into its lighter classes (which monopolize the floor of northern basins 21 , 36 ), and which favours the northward spreading of abyssal density surfaces 8 ( Fig. 1b ). Boundary-dominated transformation and the depth distribution of seafloor thus collude to shape an incrop area profile which peaks deeper than 4 km and decays to small values near 2.5 km depth. This collusion accentuates the segregation of water masses situated below and above the 2.5 km geopotential, and suggests that the main patterns of incrop area and upwelling diagnosed from the modern hydrography must hold across a broad range of ocean states. The implications of these patterns for the functioning of the meridional overturning north of 32\u00b0 S can be summarized as follows ( Figs 2 and 6 ): (i) strong upwelling (or downwelling) rates define a diabatic deep ocean regime at depths greater than 2.5 km, where most of the seabed lies; (ii) an overlying adiabatic regime, within 1\u20132.5 km depth, hosts non-negligible mixing but comparatively limited dianeutral transports; (iii) northward-flowing dense waters reside below the density layer with greatest access to the seabed, thus largely below 4 km depth, where seafloor availability is maximal; (iv) the majority of dense southern-origin waters returns southward within the diabatic regime, below 2.5 km depth. The first two conclusions rely essentially on knowledge of ocean bathymetry and could potentially be recast for a different depth distribution of seafloor. The latter two further assume a southern origin of the densest global-scale water mass. Conclusion (iv) implies that the circulation of dense Antarctic-origin waters in the Pacific and Indian basins is more compressed in the vertical than has been inferred in most inverse box models 5 , 9 , 10 , despite limited consistency among these inverse solutions (see discussions of Pacific and Indian abyssal pathways in the Methods). The depth profile of the southward return flow at 32\u00b0 S implied by idealized water mass transformation scenarios is also variable ( Fig. 5c and Extended Data Fig. 6 ), reflecting the lack of constraints on the global mixing distribution. These uncertainties and discrepancies call for further work to refine and reconcile different estimates of the abyssal overturning structure. Examining the silicic acid (Si(OH) 4 ) distributions 41 of the Pacific and Indian oceans, we find additional support for a relatively deep southward return flow ( Extended Data Fig. 7 ). In the northern part of the two basins, where deep Si(OH) 4 production is thought to be more intense and largely placed at the sediment\u2013water interface 42 , 43 , 44 , 45 , vertical maxima of Si(OH) 4 lie immediately above the depth range of large seafloor areas 46 . This concurs with relatively strong circulation and short residence times limiting Si(OH) 4 accumulation within this depth range. Further south, the maxima shift towards larger densities falling within the diabatic regime, consistent with southward mean flow promoting the export of Si(OH) 4 there. This latter feature is not visible in the eastern Indian Ocean, but the radiocarbon structure of this sub-basin clearly substantiates a geometric confinement of its abyssal overturning to depths greater than 3.7 km ( Extended Data Fig. 3 and Methods). The seafloor area distribution, generally absent from conceptual models or quantitative theories of the overturning, thus exerts major constraints on the volumes, pathways and interplay of dense water masses. In particular, within basins receiving only a southern influx of ventilated dense waters, the concentration of seafloor at abyssal depths (greater than 2.5 km) implies a partial disconnect between a relatively well ventilated abyss and more stagnant mid-depth waters. Accordingly, we propose that present-day Pacific and Indian waters straddling the mid-depth radiocarbon minimum do not embody returning AABW but rather lie in a shadow zone of the overturning ( Fig. 3d ), characterized by its isolation from surface and bottom boundary influences, and traversed by relatively weak mean meridional flow. The larger volume and longer residence time of shadow zone waters relative to the underlying diabatic abyss make them more likely to hold large carbon and nutrient reservoirs hidden from the atmosphere. However, an active northern surface source of deep water, as occurs in today\u2019s Atlantic and could have occurred in the Pacific 47 , 48 , may disrupt the mid-depth stores, and reduce the volume and influence of the dense southern-origin waters. Methods Dianeutral transports The water mass transformation estimates presented in Fig. 5 and Extended Data Fig. 6 use the global neutral density field of the WOCE hydrographic climatology 41 . In scenarios S1 and S2 and the scenarios of Extended Data Fig. 6 , the specified three-dimensional map of neutral density fluxes allows us to calculate the total density flux F through each density surface A ( \u03b3 ). The density derivative of F then yields the dianeutral transport T according to equation (1). The contribution of a fixed diffusivity illustrated by the shaded areas in Fig. 5 is obtained through the same procedure. Uncertainty in the obtained transport profiles reflects mostly the incomplete spatial coverage of the hydrographic observations that underlie the WOCE climatology and the limited horizontal and vertical resolution of the climatology. In spite of these limitations, substantial errors in the basin-scale structure of the density fluxes and dianeutral transports discussed here are not expected 21 . Sources of near-bottom turbulence, such as the breaking of internal waves 50 or the generation of submesoscale instabilities 51 , 52 , depend on local flow, topography and stratification conditions. In particular, topographic roughness, topography scales and bottom stratification enter scalings for the rates of bottom internal wave generation 53 , 54 . In addition, the presence of steep slopes or small-scale topographic features may catalyse near-bottom turbulence 51 , 52 , 55 , 56 , 57 . To explore the influence of these parameters, in Extended Data Fig. 6 we examine variations of the bottom-intensified mixing scenario S2 by setting the magnitude of local density-flux profiles proportional to the large-scale topographic slope squared; the large-scale topographic roughness; the small-scale topographic roughness 58 ; the horizontal wavenumber of small-scale topography 58 ; the bottom buoyancy frequency; the squared bottom buoyancy frequency; the internal tide generation rate 59 , 60 ; and the lee wave generation rate 61 . Roughness is defined as the variance of bathymetric height. Large-scale slopes and roughness are obtained by fitting planes over half-degree grid squares to the 1\/30\u00b0-resolution ETOPO2v2 bathymetry product 49 . Small-scale abyssal hills are not resolved by this product. To account for these we use the small-scale roughness and wavenumber parameters estimated by ref. 58 . In all eight cases, the average magnitude of the fluxes is adjusted to obtain a maximum upwelling rate of 25 \u00d7 10 6 m 3 s \u22121 . Only the structure of transports thus warrants interpretation. Extended Data Fig. 6 shows that, irrespective of the scenario, dianeutral upwelling is maximum at or below \u03b3 = 28.11 kg m \u22123 , is weak or negative at \u03b3 = 28 kg m \u22123 , and remains modest across the overlying regime of small incrop areas. Radiocarbon maps Radiocarbon content (\u0394 14 C, expressed in per mil) corresponds to the deviation of the measured 14 C\/ 12 C ratio relative to an atmospheric reference ratio, correcting for isotopic fractionation 62 . At leading order, the evolution of \u0394 14 C in the deep ocean is governed by advective\u2013diffusive processes and radioactive decay of about \u221210\u2030 every 83 years 63 , 64 . Mixing affects the deep \u0394 14 C distribution through both its impact on circulation and the direct diffusive redistribution of radiocarbon 64 , 65 . The latter effect dominates in particular when substantial divergence of diffusive 14 C fluxes coexists with weak divergence of diffusive density fluxes: mixing along density surfaces (an important process controlling the \u0394 14 C distribution 66 ) or depth-independent density fluxes are cases in point. We consider all \u0394 14 C values assembled in the GLODAPv2 data product 39 , 40 and pair these with \u03b3 values derived from corresponding hydrographic casts. The 2% of \u0394 14 C measurements (891 out of 36,541 measurements) for which concurrent hydrographic parameters are not available are assigned the \u03b3 value of the corresponding position in the WOCE climatology. Next, each \u0394 14 C cast is vertically interpolated onto a fixed series of 140 \u03b3 surfaces using a piecewise cubic Hermite interpolating polynomial. We then map \u0394 14 C along each \u03b3 surface independently. Grid point values are obtained as a weighted average of neighbouring measurements, the selection and weighting of which rely on the distance look-up table described at  . Specifically, weights are defined as 67 , 68 (1 \u2212 ( r \/1,200 km) 3 ) 3 , with r the shortest path from the mapped grid point to surrounding data points at the grid point depth, bypassing topographic obstacles. Only data points whose distance r to the mapped grid point is less than 1,200 km are retained in each weighted average. The resulting global three-dimensional (longitude, latitude and \u03b3 ) radiocarbon field is then plotted as a zonal average in Fig. 3 and Extended Data Figs 2 , 3 , 4 using the pseudo-depth reprojection described in the caption of Fig. 1 . In Extended Data Fig. 8 , we show an example map of \u0394 14 C at \u03b3 = 28.045 kg m \u22123 , together with the underlying observations. Uncertainty in the constructed maps originates from the individual \u0394 14 C measurement error, estimated as \u00b14\u2030 (ref. 69 ); errors in concurrent neutral density values; the limited spatio-temporal coverage of \u0394 14 C measurements; and the limitations of the mapping procedure. Given the sparse observational coverage, uncertainties relate primarily to the sampling density, which is lowest in the southeastern Pacific, the western Indian and the eastern Atlantic oceans ( Extended Data Fig. 8b ). The search radius of 1,200 km allows the vast majority of the ocean to be mapped, but smoothes out smaller-scale structures that may be present in the data. Note also that the presented radiocarbon maps do not correct for bomb-produced 14 C, nor for any source of temporal variability of \u0394 14 C. Because we focus on the northern abyssal ocean whose ventilation timescales typically exceed centuries, the influence of bomb 14 C should not noticeably affect the qualitative structure discussed here 63 . In particular, the surfaces of meridional flow reversal identified in radiocarbon distributions are corroborated by other hydrographic fields such as stratification, oxygen or silicic acid (see Extended Data Fig. 7 ). In contrast, these features are blurred in the bomb-corrected GLODAP climatology product 70 , whose accuracy may be reduced in the abyss 66 . The neutrally averaged radiocarbon climatology constructed for the present study is available for download at  . Pacific ocean abyssal pathways The density profile of summed incrop areas over the Pacific exhibits two different peaks ( Extended Data Fig. 9a and d ): a dominant peak at \u03b3 = 28.11 kg m \u22123 and a secondary peak at \u03b3 = 28.03 kg m \u22123 . The latter peak originates from the sub-basins situated east of the East Pacific Rise ( Extended Data Fig. 1 ). These sub-basins receive an inflow of Circumpolar Deep Water 71 , 72 , which is older than the AABW entering the southwestern Pacific but younger than southward-flowing deep waters ( Extended Data Fig. 2c and f ). Given that abyssal upwelling in the Pacific is inferred to be mostly confined to below the crest of the East Pacific Rise, we posit that all or part of the Circumpolar Deep Water inflow feeds a secondary overturning cell restricted to the southeastern Pacific and the 28\u201328.06 kg m \u22123 density range ( Extended Data Fig. 2 ). North of 32\u00b0 S this overturning cell is presumed to be separate from the main Pacific abyssal cell fed by AABW. Our analysis suggests that the bulk of the Pacific AABW waters returns to 32\u00b0 S at depths of more than 2.5 km. This result accords with early analyses of transport across zonal hydrographic sections of the North Pacific 73 , 74 , 75 and the South Pacific 71 , 76 but contrasts with more recent inverse estimates of overturning transports 10 , 72 , 77 , 78 , 79 , 80 , 81 at 32\u00b0 S. Although the latter estimates all imply a shallower return flow at this latitude, they exhibit substantial spread, most place the meridional flow reversal well above the 28.11 kg m \u22123 density surface, and many conflict with a high-resolution inverse study of the eastern South Pacific circulation 82 . The uncertainties carried by the inverse solutions at this location thus appear too large to permit validation or invalidation of the present results. Further work is required to reconcile the regime transitions proposed here with large-scale tracer budgets and to narrow down ranges for the strength and structure of the deep southward return flow. Indian Ocean abyssal pathways Extended Data Fig. 3 documents the relationships between the distributions of seafloor areas, incrop areas and radiocarbon content in the Indian basin. The latter hosts multiple sub-basins with different ventilation histories. For improved interpretation, sub-basins with overlapping latitude ranges are therefore shown in separate panels. Two separate AABW routes ventilate the abyssal Indian ocean 83 ( Extended Data Fig. 1 ): an east route through the Perth, Wharton and Cocos basins, with some connection to the Central Indian basin ( Extended Data Fig. 3a\u2013f ); and a west route through the Madagascar, Mascarene and Somali basins, and into the Arabian basin ( Extended Data Fig. 3g\u2013l ). Inflow of young, dense AABW in the eastern Indian Ocean is clearly seen in its radiocarbon distribution. Yet an unambiguous 14 C signature of the level of meridional flow reversal at 32\u00b0 S is not distinguishable, in part owing to the small ventilated volume of the sub-basin. Indeed, a distinct transition towards much older waters near 3.7 km depth, approximately coinciding with the weak incrop level (the inferred diabatic\u2013adiabatic transition), suggests that the seafloor distribution constrains the upwelling and southward return of young Antarctic-sourced waters to depths greater than 3.7 km. Such a compressed AABW overturning in the eastern Indian ocean is consistent with: (i) weak AABW throughflow to the Central Indian basin, whose abyssal radiocarbon activities are much lower than those of eastern Indian bottom waters; (ii) the steep topographic barriers bounding the sub-basin, which limit dianeutral upwelling; (iii) the location of the silicic acid maximum within the northern half of the sub-basin, above that overturning ( Extended Data Fig. 7f ). Additional deep-water overturning in the 28.1\u201328.17 kg m \u22123 density range driven by water mass transformation in the Central Indian basin is expected to be weak owing to the low mixing rates 33 , 84 and low radiocarbon concentrations observed north of 30\u00b0 S in this density range ( Extended Data Fig. 3i and l ). We thus interpret the apparent Si(OH) 4 tongue near 28.12 kg m \u22123 north of 15\u00b0 S ( Extended Data Fig. 7f ) as due to local production, diffusion and\/or horizontal recirculation, rather than net meridional flow 85 . The area of the western Indian ocean decreases more gradually, owing to the presence of weakly sloping ridges. As a result, large seafloor and incrop areas extend higher up in the water column, to about 2.5 km depth. The predicted northward\u2013southward and diabatic\u2013adiabatic transitions lie near 4 km depth (28.13 kg m \u22123 ) and 2.7 km depth (28.04 kg m \u22123 ), respectively. These compare well with the observed \u0394 14 C and Si(OH) 4 distributions ( Extended Data Figs 3c and 7c ). Nonetheless, sampling limitations ( Extended Data Fig. 8b ), lateral redistribution by mixing along density surfaces, and the inflow of relatively young waters of North Atlantic origin into deep layers of the basin 5 hinder clearer identification of the overturning structure in the radiocarbon data. The Arabian basin has no water denser than 28.13 kg m \u22123 , so that its contribution to the overturning appears to be restricted to the transformation of lighter deep waters. The inferred structure of the deep Indian ocean overturning contrasts with the results of steady geostrophic box inversions 10 , 77 , 79 , 81 , 86 , 87 , 88 , 89 , which suggest a shallower return of dense southern waters. However, published inverse estimates of the Indian overturning differ widely in structure and strength 84 . The complexity of the flow and topography of the basin probably plays a part in this scatter 80 , 87 , 89 . In particular, the limited number of abyssal density layers considered in the inverse box models prevents resolution of the compressed AABW overturning identified in the eastern Indian ocean: most inversions carry one layer denser than 28.15 kg m \u22123 in this sub-basin, and predict either net northward 81 or southward 89 flow in the layer at 32\u00b0 S. Further, the presence of multiple peaks in the total incrop area profile of the Indian ocean ( Extended Data Fig. 9c and f ), related to its topographic partitioning, suggests that the basin\u2019s overturning streamfunction may exhibit several abyssal peaks. The coexistence of several abyssal overturning cells traversing relatively small contrasts in depth, density and other properties could possibly explain the limited consistency of hydrographic inversions and their mismatch with water mass transformation estimates 84 , 90 . Atlantic ocean abyssal pathways AABW enters the Atlantic Ocean west of the Mid-Atlantic Ridge 83 ( Extended Data Figs 1 and 4a\u2013c ). As opposed to the situation in the Pacific Ocean, it carries a low radiocarbon signature relative to the overlying 14 C-rich NADW. At 32\u00b0 S, the boundary between northward-flowing AABW and southward-flowing NADW coincides with the strongest vertical \u0394 14 C gradient, observed at \u03b3 = 28.14 kg m \u22123 . This density surface is also the peak incrop surface across the western Atlantic, consistent with control by incrop areas of the level of meridional flow reversal. Transport analyses 9 , 10 and the climatological density field further indicate weak southward influx of waters denser than 28.14 kg m \u22123 at 48\u00b0 N, substantiating a match between the levels of meridional flow reversal, peak incrop area and maximum dianeutral upwelling. We infer that the pivotal upwelling across the 28.14 kg m \u22123 surface occurs mainly between 30\u00b0 N and 45\u00b0 N near 4.5 km depth, where the weak abyssal stratification and large seafloor availability combine to maximize incrops ( Extended Data Fig. 4b ). Additionally, weak seafloor and incrop areas shallower than 3 km imply that the dianeutral upwelling of bottom waters and their southward return are concentrated at depths greater than 3 km. The abyssal eastern Atlantic ( Extended Data Fig. 4d\u2013f ) is primarily fed from its western counterpart through the Chain (1\u00b0 S), Romanche (1\u00b0 N) and Vema (11\u00b0 N) fracture zones, with additional NADW inflow from the northern end of the basin 83 , 91 ( Extended Data Fig. 1 ). The \u0394 14 C levels at the outflow of the three fracture zones approach \u2212115\u2030, which is closer to western Atlantic NADW levels (about \u2212100\u2030) than to AABW levels (about \u2212150\u2030). This suggests that the outflows are dominated by NADW, not AABW, in accord with local observational surveys 28 , 92 , 93 , 94 , 95 and silicic acid distributions ( Extended Data Fig. 7g, h ). Consequently, the eastern Atlantic contributes primarily to the transformation and upwelling of NADW. Because the estimated AABW throughflow to the eastern basins 94 , 95 is only a fraction of the 32\u00b0 S Atlantic AABW input 27 , 96 , we conclude that dianeutral upwelling within the western Atlantic controls the present-day boundary between northward-flowing AABW and southward-flowing NADW. The seafloor and tracer distributions of the eastern Atlantic nonetheless indicate that, there as in the western Atlantic, dianeutral upwelling and AABW influence are most important below the 3 km geopotential. Radiocarbon evidence from the Atlantic and Indian basins thus bears out the relationship between the abyssal overturning structure and the depth and density distributions of the ocean floor. Major topographic obstacles and constrictions at sub-basin boundaries either (i) catalyse the transformation of AABW or (ii) restrict its access to certain sub-basins, but do not override this relationship. In situation (i), flow constrictions contribute to the creation of a more homogeneous bottom water mass, focusing incrop areas into a narrow density range and favouring rapid upwelling at the peak incrop layer downstream. Such inter-basin passages lie below the boundary between northward AABW transport and southward deep water transport. In situation (ii), topographic barriers limit the role of more isolated sub-basins in the transformation and upwelling of lighter deep waters. The additional deep boundary transformation occurring en route to and within these sub-basins (namely the eastern Atlantic, the Arabian basin and the Central Indian basin) concerns mostly waters lighter than AABW and denser than 28.05 kg m \u22123 ( Extended Data Fig. 9 ). Code availability Code for the generation and usage of the distance look-up table is available at  . Analysis scripts are available from the corresponding author on request. Data availability The global bathymetry product can be downloaded at  . The WOCE hydrographic climatology is available at  . GLODAPv2 radiocarbon data can be retrieved from  . The constructed radiocarbon climatology is made available by the authors at  . ","News_Body":"New research from an international team has revealed why the oldest water in the ocean in the North Pacific has remained trapped in a shadow zone around 2km below the sea surface for over 1000 years. To put it in context, the last time this water encountered the atmosphere the Goths had just invaded the Western Roman Empire. The research suggests the time the ancient water spent below the surface is a consequence of the shape of the ocean floor and its impact on vertical circulation. \"Carbon-14 dating had already told us the most ancient water lied in the deep North Pacific. But until now we had struggled to understand why the very oldest waters huddle around the depth of 2km,\" said lead author from the University of New South Wales, Dr Casimir de Lavergne.\"What we have found is that at around 2km below the surface of the Indian and Pacific Oceans there is a 'shadow zone' with barely any vertical movement that suspends ocean water in an area for centuries. The shadow zone is an area of almost stagnant water sitting between the rising currents caused by the rough topography and geothermal heat sources below 2.5km and the shallower wind driven currents closer to the surface. Before this research, models of deep ocean circulation did not accurately account for the constraint of the ocean floor on bottom waters. Once the researchers precisely factored it in they found the bottom water can not rise above 2.5km below the surface, leaving the region directly above isolated. While the researchers have unlocked one part of the puzzle their results also have the potential to tell us much more. \"When this isolated shadow zone traps millennia old ocean water it also traps nutrients and carbon which have a direct impact on the capacity of the ocean to modify climate over centennial time scales,\" said fellow author from Stockholm University, Dr Fabien Roquet. The article Abyssal ocean overturning shaped by seafloor distribution is published in the scientific journal Nature. ","News_Title":"How a 'shadow zone' traps the world's oldest ocean water","Topic":"Earth"}
{"Paper_Body":"Abstract Precipitation extremes will increase in a warming climate, but the response of flood magnitudes to heavier precipitation events is less clear. Historically, there is little evidence for systematic increases in flood magnitude despite observed increases in precipitation extremes. Here we investigate how flood magnitudes change in response to warming, using a large initial-condition ensemble of simulations with a single climate model, coupled to a hydrological model. The model chain was applied to historical (1961\u20132000) and warmer future (2060\u20132099) climate conditions for 78 watersheds in hydrological Bavaria, a region comprising the headwater catchments of the Inn, Danube and Main River, thus representing an area of expressed hydrological heterogeneity. For the majority of the catchments, we identify a \u2018return interval threshold\u2019 in the relationship between precipitation and flood increases: at return intervals above this threshold, further increases in extreme precipitation frequency and magnitude clearly yield increased flood magnitudes; below the threshold, flood magnitude is modulated by land surface processes. We suggest that this threshold behaviour can reconcile climatological and hydrological perspectives on changing flood risk in a warming climate. Introduction There is clear theoretical, model-based, and empirical evidence that global precipitation extremes, i.e. precipitation exceeding a high threshold, will increase in a warming climate 1 , 2 , 3 , 4 . However, there is greatly more uncertainty regarding the hydrologic response regarding flooding and there is not yet clear evidence for widespread increases in flood occurrence either in observations 5 , 6 , 7 , 8 , 9 , 10 or in model simulations 11 , 12 , 13 . While there is still a theoretical expectation that flood events will increase in a warming climate 14 , 15 , 16 , 17 , and while such flood increases have been documented regionally 18 , 19 , the absence of broader observational trends supporting this hypothesis is conspicuous. In the literature on hydrological processes, the lack of such trends is often attributed to changes in non-precipitation-flood drivers, such as temperature-driven decreases in snow accumulation and increases in evaporation that yield decreases in soil moisture 9 , 20 , 21 , 22 , 23 . Because of the compounding nature of different flood drivers, establishing a direct link between increases in extreme precipitation and increases in flooding is challenging 24 , 25 , 26 . Indeed, previous studies suggest that the strength of the relationship between precipitation and discharge may depend on a range of factors including catchment size, event magnitude 25 , 27 , and season 28 though the details of these complex relationships remain largely unknown and are hard to generalize. Further complicating such investigations is the rarity of extreme events with long return intervals and their sparseness in observed precipitation and streamflow records. Several approaches have been proposed to address this data scarcity problem, including: pooling observations across different catchments 29 or seasonal predictive ensemble members 30 , 31 ; tree-ring and historic reconstructions 32 , 33 ; stochastic streamflow generation 34 , 35 ; and ensemble modeling using Single Model Initial-condition Large Ensembles SMILEs 36 . To date, however, few studies have combined atmospheric SMILEs with hydrological models to obtain a SMILE of streamflow time series, i.e. a \u2018hydro-SMILE\u2019 37 , 38 , 39 . The availability of such a hydro-SMILE is crucial in assessing the relationship between future changes in extreme precipitation and flooding \u2013 particularly high-end extreme events (i.e., those occurring twice or fewer times per century), which are rare to nonexistent in observed time series. Here, we seek to reconcile the extreme precipitation-flood paradox in a warming climate: is there a precipitation threshold beyond which increasing precipitation extremes directly translate into increasing flood risk? We hypothesize that such a threshold should exist because moderately extreme events may be buffered by decreased soil moisture (due to warming) while very extreme events may quickly lead to soil saturation and subsequently to direct translation of precipitation to runoff. Using a hydro-SMILE approach, we consider precipitation and flood characteristics from historical (1961\u20132000) and warmer future (2060\u20132099) climates for 78 catchments in major Bavarian river basins (Main, Danube, and the Inn river with their major tributaries; henceforth Hydrological Bavaria) characterized by a wide variety of hydroclimates, soil types, land uses, and streamflow regimes 39 , 40 . We find that there does indeed exist a catchment-specific extremeness threshold (i.e. return interval threshold) above which precipitation increases clearly yield increased flood magnitudes, and below which flood magnitude is strongly modulated by land surface processes such as soil moisture availability. Ultimately, this finding may help reconcile seemingly conflicting climatological and hydrological perspectives on changing flood risk in a warming climate. Addressing the precipitation-flood paradox is simply not possible using observations alone, as the high-end extreme events of interest are rare to nonexistent in temporally limited observational records. This real-world data limitation effectively precludes statistical analyses of extreme events with return periods exceeding ~50 years. To overcome this problem, we use a hydro-SMILE to obtain a large number of extreme precipitation\u2013streamflow pairs. The hydro-SMILE consists of hydrological simulations obtained by driving a hydrological model with climate simulations from a single model initial-condition large ensemble (SMILE) climate model. The underlying model simulations were originally generated by Willkofer et al. 40 as part of the ClimEx project 41 . The hydro-SMILE simulations consist of daily streamflow (mm d \u22121 ), snow-water-equivalents (SWE, mm), and soil moisture (%) \u2013 all of which were obtained by driving the hydrological model WaSiM-ETH 42 with a 50-member ensemble of high-resolution climate input (spatial: 500 \u00d7 500 m 2 , temporal: 3 h) (for further information on the hydro-SMILE see Section \u201cHydro-SMILE\u201d). While such a large ensemble approach resolves the small or zero size problem for very extreme events, new sources of uncertainty do also arise. We acknowledge that the hydro-SMILE modeling chain is affected by uncertainties introduced through both the underlying climate and hydrological models. Climate model uncertainties include those relating to precipitation process-representation, downscaling, and bias-correction procedures, hydrological model uncertainties comprise model and parameter uncertainties. These latter uncertainties may be particularly relevant for the very extreme events under consideration in the present study because model calibration and evaluation rely upon observed events \u2013 and (as previously noted) modern observational records simply don\u2019t exist for events of the extreme magnitudes considered here. However, we point out that this particular element of the overall uncertainty is essentially irreducible, and will likely remain so until the length of the observed record increases substantially some decades in the future. As such, the use of a hydro-SMILE is an appropriate method \u2013 and arguably the singular method available, at present - to comprehensively and quantitatively address the extreme precipitation-flood paradox. Results Threshold behavior in flow response to extreme precipitation We first seek to assess whether there exists a return interval threshold beyond which precipitation ( P ) increases consistently translate into streamflow ( Q ) increases, and thereby to increases in flood magnitude. To do so, we use a hydro-SMILE consisting of a 50-member ensemble of 3-hourly precipitation and streamflow time series for Hydrological Bavaria (see Methods section \u201cStudy region\u201d and Supplementary Figure 1 ), which we aggregated to daily resolution. The hydro-SMILE was derived for the period 1961\u20132099 by combining the Canadian Regional Climate Model large ensemble CRCM5-LE 41 with the hydrological model WaSiM-ETH 40 , 42 (see Methods sections \u201cHydro-SMILE\u201d and \u201cHydrological model evaluation\u201d). From this ensemble, we extract precipitation\u2013discharge ( P \u2212 Q ) pairs for a historical (1961\u20132000) and future time period (2060\u20132099) by first applying a peak-over-threshold approach on precipitation and then identifying corresponding peak discharges (see Methods section \u201cEvent identification\u201d). We then empirically compute P and Q magnitudes for different levels of extremeness, i.e. mean events and progressively more extreme events with 10, 20, 50, 100, and 200 year return intervals, by pooling events extracted from the 50 ensemble members. Finally, we derive future relative changes in extreme event magnitudes by comparing magnitudes for a future period (2060\u20132099) with magnitudes of a historic period (1961\u20132000) (see Methods section \u201cChanges in event magnitudes and P \u2212 Q relationship\u201d). We find that median future changes in daily precipitation and corresponding discharge extremes overall catchments depend on their respective level of extremeness (here defined as their return interval, RI; Fig. 1 ). Precipitation frequency and magnitude are found to increase for all levels of extremeness, with the largest median increases corresponding to the most extreme events which is consistent with prior findings 43 , 44 , 45 . 50-year RI precipitation events (i.e. events of a magnitude occurring approximately twice per century), occur twice as often (a 100% increase) in the future period vs. the historical period, while the frequency of 200 years RI events increases by up to 200%. Median increases in precipitation magnitudes corresponding to these frequency increases range from an increase <10% for 50 year RI events to an up to 15% increase for 200 year events. Fig. 1: Future changes in precipitation ( P ) and streamflow ( Q ) magnitudes for different levels of extremeness overall 78 catchments. Relative changes [\u2212] in ( a ) event frequency and ( b ) peak magnitude for mean and progressively more extreme events (those with 10, 20, 50, 100, and 200 year empirical return intervals, respectively). Relative changes are computed by comparing event characteristics of a future period (2060\u20132099) to characteristics of a historical period (1961\u20132000). The gray bar in ( b ) shows the relative change in event timing (day of the year, negative values indicate earlier extreme event occurrence overall events). Meaning of boxplot elements: central line: median, box limits: upper and lower quartiles, upper whisker: min(max(x), Q 3 + 1.5 \u00d7 IQ R ), lower whisker: max(min(x), Q 1 \u2212 1.5 \u00d7 IQ R ), no outliers displayed. Full size image In notable contrast to precipitation changes, changes in flood frequency and magnitude exhibit a more complex response as a function of flood event extremeness. We find that there exists a return interval threshold below which flood frequency and magnitude decrease, and above which they increase. The mean location of this threshold across all catchments lies between event RIs of 20\u201350 years for both frequency and magnitude (Fig. 1 ). However, the exact location of this threshold is catchment-dependent (Fig. 2 ). Some catchments already show increases in magnitude\/frequency at very low thresholds (<10 years, lightly colored catchments), while in other catchments a threshold only emerges at very long return intervals (100 or 200 years, darkly colored catchments). A few catchments (20%) don\u2019t show any threshold behavior at all as they either exhibit uniformly increasing or decreasing discharges independent of the return interval. However, even in catchments without a distinct threshold, the discharge response becomes increasingly positive for increasing event magnitudes. Fig. 2: Catchment-specific return interval thresholds above which precipitation increases result in discharge increases. Relative changes (1 corresponds to 100% increase) in ( a ) event frequency and ( b ) peak magnitude for mean and progressively more extreme events (those with 10, 20, 50, 100, and 200 year empirical return intervals, respectively) for each of the 78 catchments (1 line = 1 catchment). Dashed lines denote catchments without a distinct threshold. Relative changes are computed by comparing event characteristics of a future period (2060\u20132099) to characteristics of a historical period (1961\u20132000). The approximate location of the return interval threshold is indicated using different line colors with darker colors representing higher return interval thresholds. Full size image This finding of a catchment-specific return interval threshold in a great majority of instances suggests that the extreme streamflow response in a warming climate changes sign, from negative to positive, when comparing more \u2018common\u2019 flood events (i.e. those occurring 5 or more times per century) to more \u2018rare\u2019 flood events (i.e. those occurring two or fewer times per century). This finding has major implications for the interpretation of time series of observed streamflow, as the historical record is often too short to robustly characterize changes in high-magnitude events occurring only several times per century, and any such threshold behavior might go undetected as a result. Still, the results corroborate findings by earlier studies suggesting that historical changes in flooding do, to some degree, depend on event extremeness 25 , 27 . Next, we assess which meteorological factors and catchment characteristics influence the location of the overall flood response threshold along the extremeness spectrum when considering median changes in extremes overall catchments. For this assessment, we compare historical and future precipitation and discharge extremes for (a) small (<1000 km 2 ) and large (>1000 km 2 ) catchments, (b) low- (<1000 m.a.s.l.) and high-elevation catchments (>1000 m.a.s.l.), (c) winter (Oct\u2013Mar) and summer (April\u2013Sept) events, (d) snow-influenced (>10 mm stored SWE) and rainfall-driven events (<10 mm stored SWE), and (e) events extracted using different precipitation temporal aggregation levels (1-day, 3-day, and 5-day accumulated precipitation) (see Methods section \u201cChanges in event magnitudes and P \u2212 Q relationship\u201d). Our results show that the threshold above which precipitation increases translate into increases in flood frequency and magnitude is strongly modulated by elevation, season, and event type (Figs. 3 , 4 ), but does not meaningfully depend upon the precipitation temporal aggregation level (Supplementary Figure 2 ) or upon catchment size (Supplementary Figure 3 ). This result may change if studying a dataset with a wider range of catchment sizes. However, when studying larger catchments, interactions of flood waves from different tributaries will have to be considered. The return interval threshold does not exist at all or occurs at a much lower extremeness level in high-elevation catchments (<10 years RI) versus low-elevation catchments (~50 years RI). In other words, precipitation frequency and magnitude increases in high-elevation catchments are more directly translated into flood frequency and magnitude increases than in low-elevation catchments for any given event extremeness level (Figs. 3c , 4c ). In addition to elevation, this threshold also depends on the season. In high-elevation catchments, discharge frequency and magnitude increases are stronger in winter than in summer. In contrast, flood frequency and magnitude mostly decrease in low-elevation catchments in winter while they increase in summer for high-magnitude events (Figs. 3b , 4b ). A substantial portion of this elevational separation in flood response may be explained by differences in extreme precipitation event type, i.e. whether an event is snow-influenced or rainfall-driven (Figs. 3d\u2013f , 4d\u2013f ). In low-elevation catchments, flood frequency and magnitude decrease for snow-influenced events caused by a decrease in extreme precipitation during such events while they increase for very extreme rainfall-driven events (return intervals >50 years) (Figs. 3e , 4e ). In contrast, high-elevation catchments show flood frequency and magnitude increases for both snow-influenced and moderately extreme rainfall-driven events (Figs. 3f , 4f ). This behavior would be consistent with a simultaneous decrease in mean snowpack accumulation and the number of rain-on-snow events 39 , 46 , 47 , 48 , 49 , 50 , which in some cases have lower peaks than solely rainfall-driven events 23 . Fig. 3: Factors influencing future frequency changes in extreme precipitation and discharge magnitudes for different levels of extremeness. Median relative change [\u2212] in P and Q frequency per season ( a \u2013 c ) and event type ( d \u2013 f ) across all, low-elevation, and high-elevation catchments for mean and progressively more extreme events (those with 10, 20, 50, 100, and 200 year return intervals, respectively). Relative changes are computed by comparing event characteristics of a future period (2060\u20132099) to characteristics of a historical period (1961\u20132000). Full size image Fig. 4: Factors influencing future magnitude changes in extreme precipitation and discharge for different levels of extremeness. Median relative change [\u2212] in P and Q magnitude per season ( a \u2013 c ) and event type ( d \u2013 f ) across all, low-elevation, and high-elevation catchments for mean and progressively more extreme events (those with 10, 20, 50, 100, and 200 year return intervals, respectively). Same as Fig. 3 , but here for future magnitude changes. Relative changes are computed by comparing event characteristics of a future period (2060\u20132099) to characteristics of a historical period (1961\u20132000). Full size image Flood-precipitation dependence strengthens In addition to assessing changes in precipitation and flood magnitude, we consider the (non-)stationarity of the relationship between the two variables over time in a warming climate. We compare different measures of dependence including correlation and extremal (i.e. tail) dependence 51 for progressively more extreme events for the historical and future period (see Methods section \u201cChanges in event magnitudes and P \u2212 Q relationship\u201d). Similar to changes in flood frequency and magnitude, we find that changes in the strength of the P \u2212 Q relationship overall catchments are generally positive above a certain return interval threshold and depend on event magnitude, season, and in particular elevation (Fig. 5 ). The median P \u2212 Q relationship changes overall 78 catchments are generally stronger in high- versus low-elevation catchments, and are also stronger in winter than in summer. In low-elevation catchments, the relationship weakens for moderate extreme events and intensifies only for very extreme events, particularly in summer. In high-elevation catchments, the relationship intensifies for both moderate and severe extremes. In these catchments, however, the strengthening of the relationship in winter decreases as events become more extreme, while it intensifies more strongly for the more extreme events in summer. These findings suggest that influences on the threshold above which the P \u2212 Q relationship strengthens are complex, and likely vary widely across hydroclimates as suggested by variations by season and event type. They are also suggestive of a potentially important role for antecedent land surface conditions in modulating the underlying relationship \u2013 a topic we explore further in the next section. Fig. 5: Factors influencing future changes in the P \u2212 Q relationship for different levels of extremeness. Median relative change [\u2212] in P \u2212 Q dependence (areal precipitation sum and peak discharge) per season across ( a ) all, ( b ) low-elevation, and ( c ) high-elevation catchments for correlation and tail dependence for progressively more extreme events (those with 10, 20, 50, 100, and 200 year return intervals, respectively). Relative changes are computed by comparing event characteristics of a future period (2060\u20132099) to characteristics of a historical period (1961\u20132000). Full size image Role of antecedent conditions in flood response We also assess the extent to which land surface and hydro-meteorological drivers beyond precipitation govern flood magnitudes at different levels of extremeness. For this assessment, we construct a multiple linear regression model that predicts flood magnitude (mean and 100 year RI) using a set of predictors: mean event precipitation, mean event temperature, mean event SWE, and mean event soil moisture anomalies, which are only weakly collinear according to the variable inflation factor (VIF does not exceed 10 for any pair and only exceeds 4 for very few pairs; see Methods section \u201cImportance of hydro-meteorological drivers\u201d). We consider the sign and magnitude of the associated regression coefficients, and their change between the two time periods of interest (historical: 1961\u20132000, future: 2060\u20132099). The regression analysis shows that flood magnitude is driven by different meteorological conditions and land surface processes whose importance varies widely by the level of extremeness, elevation, and season (Fig. 6 upper panel). For moderate and severe extremes at both low and high elevations, precipitation is positively related to discharge magnitude (i.e. for sufficiently extreme events, precipitation increases almost always lead to discharge increases). In contrast, the role of all the other drivers particularly that of temperature strongly depends on the level of extremeness, elevation, and season and is not statistically significant in all cases. Fig. 6: Importance of flood drivers in the past and its future changes. Importance of precipitation ( P ), temperature ( T ), snow-water-equivalent (SWE), and soil moisture (SM) as drivers of historical ( a ) moderate floods (median overall events identified in a catchment) and ( b ) extreme floods (100-yearly flood). Future changes in driver importance for ( c ) moderate floods and ( d ) extreme floods. All panels are divided into low- vs. high-elevation catchments and distinguish between summer and winter events. Turquoise and pink colors ( a , b ) indicate positive and negative correlation coefficients, respectively (coefficients not statistically significant at p < 0.05 level are hatched), and green and red colors ( c , d ) indicate increases and decreases in driver importance, respectively. Full size image In low-elevation catchments, temperature increases are associated with discharge decreases, particularly for moderate extremes (negative regression coefficients) (Fig. 6a ). In summer, higher temperatures mean higher evapotranspiration and therefore lower soil moisture, which means higher soil water storage capacity and therefore less direct runoff resulting from a given amount of precipitation. In winter, higher temperatures are associated with less snow accumulation and therefore less rain-on-snow events 46 , 47 , 49 , 50 , which can lead to smaller flood peaks because solely rainfall-driven events may not be as severe as rain-on-snow events 23 . While these temperature effects are strong for moderate floods, temperature loses importance moving toward more extreme events. This effect is particularly pronounced in summer, where the negative effect of temperature weakens while the positive relation between event magnitude and precipitation intensifies. In winter, temperature effects are still important, however, also to a smaller degree (Fig. 6b ). In low-elevation catchments during winter, soil moisture and snow accumulation are indeed important drivers of flood magnitude. Increases in soil moisture lead to increases in flood magnitudes, as precipitation can more directly be converted into a runoff. In contrast, more snow accumulation is related to smaller floods because water is temporarily stored in the snowpack, and does not form runoff until melting at some later point. While the soil-drying effects of increasing temperatures may lead to flood decreases in low-elevation catchments, they can also lead to flood increases in high-elevation catchments (particularly in winter). This effect arises largely from the phase change of precipitation, which falls increasingly as rain rather than snow in a warming climate 47 , and which has been directly linked with an increase in flood magnitude in such regions 23 . Interestingly, the positive association between temperature and flood magnitude at high elevations exists not only for moderate events, but also for very extreme events. Our analysis of future changes in flood driver importance further shows that the future relevance of precipitation as a flood driver increases for severe events while the importance of temperature increases for moderate but decreases for severe extremes (Fig. 6c\u2013d ). This may potentially be understood in the context of soil saturation as a modulating factor: for typical and even moderate events, antecedent soil-drying and snowpack losses resulting from warming temperatures oppose the effect of increasingly extreme precipitation volume; but for sufficiently severe precipitation events, the extremely large volume of water entering the system may be able to quickly saturate the soil column and overcome even a substantial degree of antecedent soil-drying. In addition, increasingly extreme precipitation may lead to infiltration excess even in the case when soils are not yet saturated. Collectively, these findings support the following generalization: the more extreme a flood event, the more important precipitation becomes as a singular driver \u2013 particularly in a warmer future climate. Discussion In this work, we demonstrate for hydrological Bavaria that there is an extremeness or return interval threshold, which varies by catchment, season, and event type, above which extreme precipitation increases outweigh the soil-drying effects of warming temperatures. This result suggests that in other regions around the globe with similar hydro-climates, i.e. temperate climates with pluvial or nival flow regimes, flood risk in a warming climate may also exhibit divergent changes above and below some locally-defined extremeness or return interval threshold. We further find that the hydrologic response to extreme precipitation varies predictably as a function of event magnitude in a warming climate, with streamflow responses becoming increasingly positive even in the few study catchments which do not exhibit distinct threshold behavior. This, when viewed in the context of prior research, may offer evidence for the broader geographic generalizability of our findings. We find that increases in precipitation yield larger and more consistent increases in flood magnitude for more extreme versus more moderate events which is supported by previous observational studies showing only weak dependence between extreme precipitation and moderate flood occurrence in the United States 10 , stronger increasing flood trends for extreme than moderate floods in Central Europe 27 , and trends in extreme discharge that only align with trends in floods for the rarest events in Australian catchments 25 . Thus, there does appear to be a growing body of real-world evidence suggestive of the existence of a precipitation-flood response threshold across a wider range of hydroclimatic and hydrologic regimes than explicitly considered in the present study. The complex influences of elevation, season, and event type upon the return interval threshold suggest that the location of this critical cross-over point may vary somewhat widely across regions of the world with varying topography and background climate. Substantial modulation of this threshold would likely occur depending on climatic factors such as aridity and the local relevance of snowmelt, catchment size, and land use and management. Consider, for example, a semi-arid or subtropical regime (as opposed to the moist mid-latitude regime that characterizes the catchments in the present study). In such a location, the return interval threshold might be higher due to drier antecedent soil conditions a temperature-related phenomenon we also see when comparing seasonally-varying summer with winter thresholds (Figs. 3 , 4 ). The existence of a high return interval threshold in drier Mediterranean regions is supported by observation-based studies that have demonstrated a stronger relationship between precipitation and discharge for larger versus smaller flood events in Spain 52 , and have shown decreases in the occurrence of moderate floods in southern Europe 9 , 27 . In contrast, if we consider cold high-latitude regions and\/or high altitude regions with a snow-dominant precipitation regime, the return interval threshold might be expected to be much lower. Indeed, this relationship is apparent from our threshold analysis for snow-influenced events in high-elevation regions (Figs. 3 , 4 ). Additionally, and as suggested by our results (Supplementary Figure 3 ) the return interval threshold may also be modulated by catchment area (generally increasing with catchment size). For larger river basins than the ones included in our Bavarian selection, this finding would imply higher return level thresholds than 20\u201350 years. Furthermore, direct human influence on streamflow such as dynamic reservoir operations and\/or flood management interventions might lead to higher return interval thresholds because smaller floods can be buffered by temporary water storage 53 . In contrast, urbanized catchments (characterized by a high fraction of water-impervious surfaces) might have lower return interval thresholds than catchments with unsealed surfaces because of a more direct relationship between extreme precipitation and flood response 54 , 55 . Such a return interval threshold might even vary from year to year in a single location-occurring at a higher level of event extremeness during drought versus pluvial periods. How exactly such a return interval threshold varies for different hydro-climates remains to be investigated using a global hydro-SMILE. Creating such a global hydro-SMILE for flood analyses requires the combination of a globally downscaled and bias-corrected atmospheric SMILE with a global hydrological model specifically calibrated for flood peaks. Satisfactory calibration for far-from-mean state conditions is challenging using calibration metrics commonly used for large-scale model calibration 56 and data storage and computational costs are high at a global scale when a large spatial domain is combined with a large ensemble size. In addition, global-scale models may not as accurately represent complex land surface processes as smaller-scale models and appropriate reference datasets for meteorology, soils, and hydrogeology are harder to obtain. Creating such a global hydro-SMILE therefore remains a considerable research effort, but one of substantial importance in a warming climate. There are two important implications arising from the existence of a return interval threshold above which increases in precipitation directly translate to increases in flood occurrence. First, this threshold existence suggests that previous studies that focused on less extreme floods, which have shown little change or even decreases in annual streamflow maxima or events with return intervals of less than ~20 years 57 , 58 , will likely be unrepresentative of changes in higher-magnitude events. A robust statistical signal is unlikely to arise in most historical datasets shorter than 100 years because the strongest link between increasing extreme precipitation and flood magnitude occurs for rare, high-magnitude events with return intervals exceeding 20\u201350 years. This result points to an important limitation of observation-only studies, as well as to the critical importance of large modeling ensembles that can yield larger sample sizes for rare, high-magnitude events. Second, our analysis suggests that despite historical uncertainties, large increases in flood magnitude are likely in a warming climate for the very largest events\u2013potentially including those unprecedented in the modern historical record (i.e., events with 200-year RI, Fig. 1 ). The fact that climate warming may act to decrease the magnitude of more moderate flood events while simultaneously increasing the magnitude of the most extreme events, however, highlights the considerable risk of developing a \u201cfalse sense of security\u201d based on recent historical experience. These findings therefore have major implications for climate adaptation and flood risk mitigation activities, as well as infrastructure design, in a warming climate. Ultimately, we suggest that this analysis may help reconcile seemingly conflicting perspectives in the climatological and hydrological literature on flood risk in a warming climate. The apparent \u201cprecipitation-flood\u201d paradox \u2013 whereby precipitation extremes have increased, but floods have not 5 , 24 \u2013 may in fact be fully resolved by separating flood events by their extremeness. In this sense, both perspectives may ultimately be correct: hydrologic evidence suggesting no consistent increase in recent flood magnitude because of land surface drying and the changing role of snow using observational records of limited length 9 , 59 , 60 is physically consistent with climatological arguments pointing to a large increase in the magnitude and frequency of historically rare or unprecedented precipitation events and subsequent flood risk 61 , 62 . Future research aimed at expanding the coverage of the regional hydro-SMILE approach to a wider range of hydrologic and climatological regimes will be critical in confirming the broader generalizability of our findings in the present study, but emerging observational evidence does suggest that threshold behavior in precipitation-flood response is plausible across a wide range of regimes in a warming climate 9 , 10 , 25 , 52 . In this work, we confirm that antecedent land surface conditions are indeed critical in modulating more common or moderate flood events, but that precipitation becomes the dominant driver for very extreme events and ultimately overwhelms the effects of soil moisture or snowpack. Finally, we emphasize that the inherent limitations of the historical observational record can be obviated through the use of a climate model large ensemble approach in combination with an advanced hydrological model\u2013a framework that might be useful for more broadly assessing complex and possibly non-linear changes in extreme events in the warming earth system. Methods Study region We study the relationship between extreme precipitation and flood events and its influencing factors in a warming climate for a set of 78 catchments with nearly natural flow conditions in Hydrological Bavaria (Supplementary Figure 1 ). This region comprises the Main, Danube, and Inn rivers with their major tributaries. This study region is particularly well suited to analyze variations in the precipitation\u2013discharge ( P \u2212 Q ) relationship because the constituent catchments are characterized by diverse topographic and climatic conditions, ranging from a wet alpine region in the south (1700 mm y \u22121 ) to a relatively flat and dry foreland to the north (700 mm y \u22121 ), and diverse soil types and land uses. The variations in these conditions lead to a wide range of hydrologic regimes, including snow-influenced regimes with flood peaks in spring and summer to primarily rainfall-influenced regimes with the main flood season in winter. While these regime types can be considered representative of the temperate climate zone with similar runoff regimes (pluvial to nival), our catchment selection does not cover other climate zones such as cold-climates, semi-arid to arid regions, and the tropics. Hydro-SMILE For this analysis, we use a hydro-SMILE, i.e. hydrological simulations obtained by driving a hydrological model with a Single Model Initial-Condition Large Ensemble (SMILE) climate model. The underlying simulations were originally generated by Willkofer et al. 40 as part of the ClimEx project 41 . The simulations consist of daily streamflow (mm d \u22121 ), snow-water-equivalents (SWE, mm), and soil moisture (%) \u2013 all of which were obtained by driving the hydrological model WaSiM-ETH 42 with a 50-member ensemble of high-resolution climate input (spatial: 500 x 500 m 2 , temporal: 3 h). The climate input consists of an ensemble provided through the Canadian Regional Climate Model version 5 nested with the Canadian Earth System Model 63 under RCP 8.5 64 \u2013 a \u2019high-warming\u2019 climate scenario. WaSiM-ETH is a distributed, mainly physically-based hydrological model comprising modules for evapotranspiration, interception, snow accumulation, and melt, glaciers, runoff generation, soil water storage, and discharge routing 42 . The model was set up for 98 catchments in Hydrological Bavaria by Willkofer et al. 40 using spatial information on elevation, slope, and exposition derived from a digital elevation model for Europe (EU-DEM 65 ), land-use derived from the CORINE land cover dataset 66 , soil characteristics derived from the European soil database (ESDB v2.0 67 ), and hydro-geology (hydraulic conductivity) derived from the Bavarian hydrogeology map 68 and the international hydrogeological map of Europe (IHME1500 v1.1. 69 ) to define global model parameters (i.e. parameters applied to the 98 catchments) describing evapotranspiration rates, infiltration rates, groundwater fluxes, snowmelt, and glacier dynamics and by calibrating four parameters, i.e. those related to recession and direct flow. These local parameters were calibrated for the period 2004\u20132010 using the dynamically dimensioned search algorithm 70 on the observed 3 h discharge of the 98 catchments provided by the Bavarian Environment Agency (Bayerisches Landesamt f\u00fcr Umwelt - LfU 71 ) and sub-daily observed interpolated meteorological input (i.e. precipitation, temperature, relative humidity, incoming shortwave radiation, and wind speed). The meteorological Sub-Daily Climatological REFerence dataset (SDCLIREF) created in the ClimEx-project is based on a combination of hourly and disaggregated daily station data. To obtain the disaggregated daily station data, the method of fragments 72 was used to extend the sub-daily record to 1981\u20132010 and to densify the station network. The station data were then interpolated to a 500 \u00d7 500 m 2 grid using a combination of multiple linear regression, considering elevation, exposition, latitude, and longitude, and inverse distance weighting similar to Rauthe et al. 73 . The dynamically dimensioned search algorithm used a multi-objective function targeted at optimizing flood characteristics composed of the Nash\u2013Sutcliffe efficiency ( E NS 74 ) and the Kling\u2013Gupta efficiency ( E KG 75 ), which both focus on high flows 76 , the log( E NS ), which emphasizes low flows, and the root-mean-squared error to standard deviation ratio ( R SR ), which quantifies volume errors. The overall objective function assigns a lot of weight to the metrics E NS and E KG because our study focuses on flood events: $$M=0.5\\times (1-{E}_{{{{{\\rm{NS}}}}}})+0.25\\times (1-{E}_{{{{{\\rm{KG}}}}}})+0.15\\times (1-{{\\mbox{log}}}({E}_{{{{{\\rm{NS}}}}}}))+0.1\\times {R}_{{{{{\\rm{SR}}}}}}.$$ (1) The calibrated model was first run for a reference period 1981\u20132010 with the sub-daily (3 h) observed interpolated meteorological input also used for model calibration. After running the model for the reference period, it was run for a simulation period 1961\u20132099 with meteorological data derived from the fifth-generation Canadian Regional Climate Model large ensemble (CRCM5-LE) 50 members 41 consisting of a dynamically downscaled version (0.11 \u2218 ; 12 km) of the second generation Canadian Earth System Model large ensemble (CanESM2-LE) 77 . The CRCM5-LE data were further bias-corrected using a quantile mapping approach 78 , 79 adjusted to sub-daily time steps and the SDCLIREF as the reference climatology (1981\u20132010). Correction factors were determined for each quantile bin for each month and sub-daily time step. To preserve the ensemble spread, all members were pooled to obtain the correction factors and these factors were subsequently applied to each ensemble member separately. The bias-corrected data were then further downscaled to 500 \u00d7 500 m 2 spatial resolution. The center point of each 0.11 \u2218 CRCM5-LE grid cell was treated as a virtual meteorological station and for each time step the anomaly from the mean state was interpolated to the 500 \u00d7 500 m 2 grid using inverse distance weighting. The interpolated anomalies were then multiplied\/added to the climatological reference fields from the SDCLIREF. Afterwards, the downscaled data were corrected in order to ensure the conservation of mass for each downscaled 0.11 \u2218 grid cell. Previous studies have demonstrated that CRCM5-LE (1) shows realistic patterns of daily and sub-daily extreme precipitation 80 and of the timing of annual maximum precipitation over Central Europe 81 ; (2) that its high-resolution allows for a realistic representation of local precipitation extremes, especially over coastal and mountainous regions 41 ; (3) that it is consistent with the EURO-Cordex ensemble 82 , and (4) that it compares well to other large ensembles with respect to regional precipitation pattern changes 81 . For the subsequent analyses of extreme precipitation and flood events, the 3 h meteorological and streamflow time series were aggregated to a daily scale and averaged over each catchment. Hydrological model evaluation We here evaluate the hydrological model for the 78 catchments used in this study for the reference period 1981\u20132010 using observed daily streamflow from the hydrological services of Bavaria and Baden-W\u00fcrttemberg (both in Germany), Austria, and Switzerland with respect to a set of measures including visual inspection, general efficiency metrics, and flood characteristics of events determined using a peak-over-threshold approach with the 98th flow percentile as a threshold and a minimum time lag of 10 days between successive events to ensure independence. The general efficiency metrics considered are the Kling\u2013Gupta efficiency 75 , Nash\u2013Sutcliffe efficiency 74 , volumetric efficiency, and mean absolute error, four metrics often used in flood simulation studies. The flood characteristics considered are the number of events, mean timing (day of the year), mean peak magnitude (mm d \u22121 ), mean volume (mm event \u22121 ), mean duration (days), and P \u2212 Q dependence. The start and end of events are determined as the time when discharge rises and falls below the threshold, respectively, event duration is defined as the time elapsing between the start and end of an event, and the volume as the cumulative flow exceeding the threshold over the whole event duration. The model shows a satisfactory performance qualitatively and quantitatively using general and flood-specific evaluation metrics (Supplementary Figure 4 ). Kling\u2013Gupta efficiencies ranged from the first quartile of 0.67 to the third quartile of 0.85, Nash\u2013Sutcliffe efficiencies from the first quartile of 0.56 to the third quartile of 0.8, and volumetric efficiencies from the first quartile of 0.68 to the third quartile of 0.8. The mean absolute error lay at 0.35 mm d \u22121 (Supplementary Figure 4a ). The flood-specific performance evaluation showed a slight underestimation of the number of events (relative error: 1st quartile: \u22120.14, median: \u22120.06, 3rd quartile: 0.07), a slight delay of the timing of flood occurrence (relative error: 1st quartile: \u22120.01, median: 0.05, 3rd quartile: 0.11), a slight overestimation of flood peaks (relative error: 1st quartile: \u22120.02, median: 0.07, 3rd quartile: 0.22), an overestimation of both flood volume (relative error: 1st quartile: 0.08, median: 0.32, 3rd quartile: 0.54) and duration (relative error: 1st quartile: \u22120.02, median: \u22120.14, 3rd quartile: 0.39), and an underestimation of P \u2212 Q dependence (relative error: 1st quartile: \u22120.35, median: \u22120.24, 3rd quartile: \u22120.04) (Supplementary Figure 4b ). Overall, the model performance with respect to high flows and flooding is satisfactory. In addition, the results of our change impact assessment are less affected by inconsistencies between observed and simulated flow because we assess relative rather than absolute changes in precipitation and flood magnitudes. Event identification Using the daily streamflow simulations from the 50 members of the hydro-SMILE, we identify pairs of extreme precipitation (i.e. areal sum over catchment) and corresponding streamflow for two non-overlapping periods of 40 years, a historical (1961\u20132000) and a warmer future period (2060\u20132099). Periods of 40 years were chosen to maximize the sample size while ensuring that the two periods are as distinct as possible. To identify these P \u2212 Q pairs, we first define daily extreme precipitation events (mm d \u22121 ) using the 99th percentile (determined on all days (including 0 precipitation days) using the full-time series 1961\u20132099) as a threshold and by prescribing a minimum time lag of 10 days between events in order to ensure independence (i.e. to enable declustering). This event extraction procedure results in roughly 2\u20132.5 events chosen per year on average depending on the catchment. Over the 2000 model years of data per time period (40 years across 50 ensemble members), we select approximately 5000 extreme events per catchment. The start of each precipitation event is defined as the day when precipitation exceeds 1 mm prior to the first threshold exceedance and the end of each precipitation event is defined as the time when precipitation falls below 1 mm after the final threshold exceedance (for an illustration of the event identification procedure see Supplementary Figure 5 ). Next, for each precipitation event, we identify the corresponding streamflow peak (mm d \u22121 ) within a time window from the start of the precipitation event to 5 days after the end of the precipitation event. Finally, for each event, we determine temperature ( \u2218 C) on the day of peak precipitation and snow-water-equivalent (mm) and soil moisture anomalies (deviation from the mean, percentage) on the day prior to the occurrence of the precipitation extreme. We repeat this event extraction procedure for two additional temporal aggregation levels (3-day and 5-day mean precipitation accumulations) in order to assess the effect of precipitation aggregation on future precipitation and discharge changes because event identification using different aggregation levels results in the extraction of different event sets. Changes in event magnitudes and P \u2212 Q relationship In the first part of our analysis, we use the P \u2212 Q event pairs identified to analyze how precipitation and corresponding flood magnitudes as well as the relationship between the two variables may change in the future. To do so, we compare the statistical characteristics of these variables for the future period (2060\u20132099) to the characteristics of the historical period (1961\u20132000). P and Q magnitudes are determined empirically by pooling events extracted from the 50 ensemble members for different levels of extremeness, i.e. \u2019mean\u2019 events (those which occur, on average, once or twice per year) and progressively more extreme events with 10, 20, 50, 100, and 200 year return intervals, respectively. Sample quantiles are computed for probabilities corresponding to different return periods T using: $$p=1-(\\mu \/T),$$ (2) where \u03bc is the mean inter-arrival time between events. The P \u2212 Q relationship is characterized for different dependence measures including Pearson\u2019s correlation coefficient and the tail dependence coefficient \\(\\overline{\\chi }\\) 51 , which provides a simple measure of extremal dependence, at different levels of extremeness (i.e. probabilities corresponding to return intervals of 10, 20, 50, 100, and 200 years). Future changes are expressed as relative changes with respect to the characteristics of the historical period. We identify factors potentially influencing the nature of change in P \u2212 Q magnitudes and relationship by looking at different levels of extremeness, i.e. return intervals, small and large catchments, high-elevation, and low-elevation catchments, winter and summer events, and snow-influenced and rainfall-driven events. The levels of extremeness considered for both P and Q are the mean and quantiles corresponding to return intervals of 10, 20, 50, 100, and 200 years. Within the 2000 model years available for analysis, roughly 10 events have a return interval of 200 years while roughly 200 events have a return interval of 10 years in each catchment. Small to medium-size catchments are distinguished from large catchments by setting an area threshold of 1000 km 2 83 , which results in 21 small and 57 large catchments. Similarly, low-elevation catchments are separated from high-elevation catchments using an elevation threshold of 1000 m above sea level 84 , which results in 55 low-elevation catchments and 23 high-elevation catchments. Winter events are defined as those events happening between October and March and summer events as those events occurring between April and September. Our results are not sensitive to the use of an alternative seasonal definition aligning with the start of the hydrological year (Nov\u2013April, May\u2013Oct). Throughout the analysis, snow-influenced events are defined as those events during which there was at least 10 mm of SWE while rainfall-driven events are those with less than 10 mm of SWE 47 . Importance of hydro-meteorological drivers In the second part of the analysis, we identify potential hydro-meteorological drivers influencing extreme precipitation and flood magnitudes and their statistical relationships. A comparison of driver importance for the two periods (historical and future) allows us to identify drivers losing or gaining importance in the future. For both periods, we fit multiple linear models to flood magnitudes (mean or quantiles for the 78 catchments) using four explanatory variables, all of which exhibit only weak collinearity according to the variable inflation factor, which lies around 1\u20132 for most variables and does not exceed 4 in most cases. The explanatory variables include mean event precipitation for each catchment (i.e. mean precipitation for the extreme events identified), mean event temperature, mean event SWE, and mean event soil moisture anomaly. Both flood magnitudes and the explanatory variables are standardized prior to model fitting by subtracting the mean and dividing by the standard deviation (z-scores) in order to make the resulting regression coefficients inter-comparable and easily interpretable. Comparing regression coefficients of the future model to the coefficients of the historical model (absolute changes) enables quantification of changes in future driver importance. Similar to the change analysis, we also distinguish between different levels of extremeness to determine how driver importance varies for events with different return intervals (mean and 100 year event), between low- and high-elevation catchments to define to which degree driver importance depends on catchment elevation, and between winter and summer events to shed light on how driver dependence varies by season. Data availability The raw data of the CRCM5-LE is publicly available to the scientific community (  ). The extreme precipitation-discharge pairs generated with the hydro-SMILE and analyzed in this study are available through HydroShare:  . Code availability The code used to process the data and to produce the figures can be requested from the first author. ","News_Body":"Climate change will lead to more and stronger floods, mainly due to the increase of more intense heavy rainfall. In order to assess how exactly flood risks and the severity of floods will change over time, it is particularly helpful to consider two different types of such extreme precipitation events: weaker and stronger ones. An international group of scientists led by Dr. Manuela Brunner from the Institute of Earth and Environmental Sciences at the University of Freiburg and Prof. Dr. Ralf Ludwig from the Ludwig-Maximilians-Universit\u00e4t M\u00fcnchen (LMU) have now shed light on this aspect, which has been little researched to date. They found that the weaker and at the same time more frequent extreme precipitation events (on average every 2 to 10 years) are increasing in frequency and quantity, but do not necessarily lead to flooding. In some places, climate change may even reduce the risk of flooding due to drier soils. Similarly, more severe and at the same time less frequent extreme precipitation events (on average less frequent than 50 years and as occurred in the Eifel in July 2021) are increasing in frequency and quantity, but they also generally lead to more frequent flooding. The team published the results of their study in the journal Communications Earth & Environment. In some places, climate change leads to lower flood risk \"During stronger and at the same time rarer extreme precipitation events, such large amounts of rainfall hit the ground that its current condition has little influence on whether flooding will occur,\" explains Manuela Brunner. \"Its capacity to absorb water is exhausted relatively quickly, and from then on the rain runs off over the surface, thus flooding the landscape. It's a different story for the weaker and more frequent extreme precipitation events,\" says Brunner. \"Here, the current soil conditions are crucial. If the soil is dry, it can absorb a lot of water and the risk of flooding is low. However, if there is already high soil moisture, flooding can occur here as well.\" So, as climate change causes many soils to become drier, the flood risk there may decrease for the weaker, more frequent extreme precipitation events\u2014but not for the rare, even more severe ones. Heavy rainfall will generally increase in Bavaria In the specific example of Bavaria, the scientists also predict how the different extreme precipitation events there will become more numerous. Weaker precipitation events, which occurred on average every 50 years from 1961 to 2000, will occur twice as often in the period from 2060 to 2099. Stronger ones, which occurred on average about every 200 years from 1961 to 2000, will occur up to four times more frequently in the future. \"Previous studies have proven that precipitation will increase due to climate change, but the correlation between flood intensities and heavier precipitation events has not yet been sufficiently investigated. That's where we started,\" explains Manuela Brunner. Ralf Ludwig adds, \"With the help of our unique dataset, this study provides an important building block for an urgently needed, better understanding of the very complex relationship between heavy precipitation and runoff extremes.\" This could also help to improve flood forecasts. 78 areas investigated In its analysis, the team identified so-called frequency thresholds in the relationship between future precipitation increase and flood rise for the majority of the 78 headwater catchments studied in the region around the Inn, Danube and Main rivers. These site-specific values describe which extreme precipitation events, classified by their occurring frequency, are also likely to lead to devastating floods, such as the one in July in the Eifel region. For its study, the research team generated a large ensemble of data by coupling hydrological simulations for Bavaria with a large ensemble of simulations with a climate model for the first time. The model chain was applied to historical (1961-2000) and warmer future (2060-2099) climate conditions for 78 river basins. \"The region around the headwater catchments of the Inn, Danube, and Main rivers is an area with pronounced hydrological heterogeneity. As a result, we consider a wide variety of hydroclimates, soil types, land uses and runoff pathways in our study,\" says Brunner. ","News_Title":"New study findings could help improve flood projections","Topic":"Earth"}
{"Paper_Body":"Abstract Physical exercise stimulates adult neurogenesis, yet the underlying mechanisms remain poorly understood. A fundamental component of the innate neuroregenerative capacity of zebrafish is the proliferative and neurogenic ability of the neural stem\/progenitor cells. Here, we show that in the intact spinal cord, this plasticity response can be activated by physical exercise by demonstrating that the cholinergic neurotransmission from spinal locomotor neurons activates spinal neural stem\/progenitor cells, leading to neurogenesis in the adult zebrafish. We also show that GABA acts in a non-synaptic fashion to maintain neural stem\/progenitor cell quiescence in the spinal cord and that training-induced activation of neurogenesis requires a reduction of GABA A receptors. Furthermore, both pharmacological stimulation of cholinergic receptors, as well as interference with GABAergic signaling, promote functional recovery after spinal cord injury. Our findings provide a model for locomotor networks\u2019 activity-dependent neurogenesis during homeostasis and regeneration in the adult zebrafish spinal cord. Introduction Neurotransmitter signaling is traditionally associated with communication between neurons. However, several reports suggest that neurotransmitters also influence critical aspects of neurogenesis, including proliferation, migration, and differentiation, under both physiological and pathological conditions 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 . The association between neurotransmitter signaling and neurogenesis appears to be primarily dependent on transmitter receptors that are not confined to neurons. Such receptors are now known to be expressed on diverse cell types in the central nervous system, including stem and progenitor cells 4 , 13 . Therefore, neuronal network activity can directly affect neurogenesis 8 , 14 . Previous studies highlighted a link between neurogenesis and neurotransmission, showing the direct effects of the cholinergic and GABAergic signaling in the modulation of the stem\/progenitor cells in the mammalian hippocampus and spinal cord 3 , 8 , 13 , 15 , 16 , 17 , yet it remains unclear how neuronal activity is linked to neurogenic activity in the adult spinal cord. Hence, we hypothesized that prolonged spinal network activity, after training, could stimulate the animal growth rate by engaging the spinal proliferative and neurogenic programs. In the early development of the vertebrate spinal cord, all neurons follow a specific genetic program that defines their identities and assigns them a specific neurotransmitter phenotype 18 . Spinal neurons are organized into distinct networks that integrate and process sensory and motor-related information important for various movements 19 , 20 , 21 . Among the spinal networks, the central pattern generators (CPGs) function as local \u201ccontrol and command\u201d centers that are essential for generating the rhythmicity and coordination required for muscle activity during locomotion 19 , 20 , 21 . At the level of spinal locomotor circuits, several classes of premotor interneurons use specific neurotransmitters, including glutamate, \u03b3-aminobutyric acid (GABA), glycine, and acetylcholine (ACh), to mediate their functions 22 . However, it is unknown whether these neurotransmitters released during locomotion can directly affect the neural stem\/progenitor cells (NSPCs) within the spinal cord. If so, by identifying neurotransmitters with neurogenic potential could expose the neurons that control these processes. Therefore, neurotransmitter signaling may play an essential activity-dependent role in regulating and fine-tuning the adult spinal cord neurogenesis. To determine whether physical activity can induce spinal cord neurogenesis, we applied an array of anatomical, pharmacological, electrophysiological, and behavioral approaches in adult zebrafish. Our data demonstrate that cholinergic (synaptic) and GABAergic (non-synaptic) neurotransmission regulates the activity of the NSPCs in opposite manners. We show that among spinal interneurons, it is the locomotor V2a interneurons that mediate the essential cholinergic input to NSPCs. The results demonstrate that spinal network activity plays a crucial role in modulating non-motor and non-neuronal functions in the nervous system besides generating motor behaviors. Results Physical activity induces animal growth and proliferation in the spinal cord Several studies have documented the impact of physical activity on neurogenesis in the mammalian hippocampus 11 , 23 , 24 , 25 , 26 . Unlike mammals, zebrafish retain a remarkable adult neurogenic capacity in many central nervous system areas, including the spinal cord 27 , 28 , 29 , 30 . We first tested whether physical activity leads to proliferative and neurogenic events and assayed global consequences on animal growth by using our recently developed forced swim protocol 31 . We observed that prolonged physical activity (>2 weeks) significantly increased animal growth (Supplementary Fig. 1 ). Combining our exercise protocol with the thymidine analog 5-bromo-2\u02b9-deoxyuridine (BrdU), a marker of DNA synthesis, we observed a 3-fold increase in the number of BrdU + cells in the spinal cord after 2 weeks of training (short-term survival; Fig. 1a\u2013c ). After a BrdU pulse, we could also trace the migrated cells out of the proliferative central canal niche (Fig. 1a, b ). After 2 weeks rest from the exercise, the proliferation rate dropped to the level of untrained control animals (Fig. 1a, c ), demonstrating the dynamic and reversible nature of exercise-induced proliferation. Fig. 1: Exercise-induced transient activation of the NSPCs and neurogenesis in the adult spinal cord. a Inverted confocal images from whole-mount adult zebrafish spinal cord hemisegments showing cycling (BrdU + ) cells in control animals (untrained), following 2 weeks of training and 2 weeks rest after training. b Similar distribution pattern of BrdU + cells in the spinal cord comparing untrained, trained, and resting zebrafish. c Quantification of BrdU + cells per hemisegment in different conditions show that the enhanced proliferation after training is reversible ( P = 4.418E-10). d Expression pattern of her4.1 : GFP (NSPCs; green) in close apposition of the adult zebrafish spinal cord\u2019s central canal. e The vast majority (~97.5%) of the her4.1 + cells (green) express the stem cell marker Sox2 (magenta). Arrowheads indicate double-labeled cells. f Cycling her4.1 + radial glia cells (BrdU + , magenta; GFP, green). Training increased the number of BrdU + \/ her4.1 + cells per hemisegment. g Quantification of the average BrdU + cells per spinal cord section co-expressing neuronal markers (mef-2, HuC\/D, or NeuN) in untrained (control) and trained animals. h Proportions of BrdU + cells expressing neuronal or glial markers are similar comparing untrained and trained animals. Quantification is based on the early neuronal marker mef-2. BrdU, 5-bromo-2\u02b9-deoxyuridine; CC, central canal; GFP, green fluorescent protein; her4.1, hairy-related 4, tandem duplicate 1; HuC\/D, elav3 + 4; mef-2, myocyte enhancer factor-2; NeuN, neuronal nuclei; NSPC, neural stem\/progenitor cell; Sox2, sex-determining region Y-box 2. Data are presented as mean \u00b1 s.e.m. or as box plots showing the median with 25\/75 percentile (box and line) and minimum\u2013maximum (whiskers). ** P < 0.01; *** P < 0.001; **** P < 0.0001; ns, not significant. For detailed statistics, see Supplementary Table 1 . Full size image NSPCs in the adult spinal cord respond by increased proliferative activity to physical training Specialized glial cells are lining the spinal cord\u2019s central canal, a proliferative niche harboring NSPCs in both fish and mammals 32 , 33 , 34 , 35 , 36 . In zebrafish, the calcium-binding protein calbindin (CB) selectively marks cells surrounding the spinal cord\u2019s central canal (Supplementary Fig. 2a ) 37 . Moreover, CB is extensively colocalized with the stem cell marker Sox2 (Supplementary Fig. 2b ) but is not expressed in the GABAergic cerebrospinal fluid contacting neurons (CSF-cNs; Supplementary Fig. 2c ) adjacent to the central canal 38 . To corroborate that CB selectively marks spinal NSPCs, we used the her4 . 1:GFP transgenic reporter line that marks NSPCs in the zebrafish CNS (Fig. 1d and Supplementary Fig. 2d ) 39 , 40 , 41 . We found that none of the radial glia-like GFP + cells expressed the neuronal marker HuC\/D (Supplementary Fig. 2e ; HuC\/D \u2212 ), all GFP + were CB + (Supplementary Fig. 2f ), and that the vast majority of GFP + cells were also expressing Sox2 (Fig. 1e ). Double labeling after BrdU pulse during physical training (Fig. 1f ) showed an increased number of her4.1:GFP + BrdU + cells after training (Fig. 1f ), indicating increased NSPCs\u2019 proliferation in response to physical activity. Most newborn cells differentiate into neurons after physical activity Next, we sought to examine the fate of new cells in the adult spinal cord 2 weeks after BrdU treatment (Supplementary Fig. 3 ). A majority (~68%) of the BrdU + cells expressed either the early differentiation neuronal marker mef-2 or the post-mitotic pan-neuronal markers HuC\/D and NeuN (Fig. 1g, h ). In contrast, a small fraction (~32%) of BrdU + cells expressed the glial marker GFAP (Supplementary Fig. 3a, b ). In both control and trained animals, the proportion of newborn cells expressing glial versus neuronal markers remained unaltered, suggesting that physical activity did not affect newborn cells\u2019 differentiation fate (Fig. 1h and Supplementary Fig. 3b ). NSPCs receive neuronal input during locomotion Next, we examined whether the spinal locomotor network is directly implicated in the activation of the NSPCs. We performed whole-cell patch-clamp recordings in single NSPCs while recording motor nerve activity of the ipsilateral CPG in ex vivo adult her4.1:GFP zebrafish preparation 42 , 43 (Fig. 2a ). We verified that the GFP + cells had glial physiological properties, such as hyperpolarized resting membrane potential (\u221268.27 \u00b1 0.7 mV), linear voltage\u2013current relations, and no generation of action potentials (Fig. 2a and Supplementary Fig. 4 ). Moreover, in the absence of the electrically induced fictive swimming, the NSPCs did not receive any synaptic input (Fig. 2a ). However, after initiating fictive locomotion by electric stimulation (10 pulses, 1 Hz) of the descending axons from the brainstem, we detected a strong periodic synaptic input in NSPCs at frequencies above 4 Hz (Fig. 2a, c ). Moreover, this input was always in phase to the CPG activity (Fig. 2b ). To further confirm that the NSPCs\u2019 inputs were causally associated with the CPG activity, we also recorded from contralaterally located NSPCs and found that they displayed out-of-phase relations (Fig. 2b ). This differential phase-locked association between the CPG activity and the NSPC input suggested that this was a locomotor network\u2019s activity outcome. Swimming burst frequency and strength showed no correlation to the NSPCs\u2019 response (Fig. 2d ). Nevertheless, the locomotor episodes\u2019 duration correlated with the periodic NSPCs\u2019 response (Fig. 2e ). Together, these data link locomotor network and NSPCs\u2019 activity but reveal neither the nature of this input nor the spinal locomotor interneurons involved. Fig. 2: NSPCs receive periodic input from the locomotor network. a Images of a NSPC ( her4.1:GFP + , white arrowhead) close to the spinal cord\u2019s central canal. Current steps do not produce action potentials in NSPCs. Ex vivo setup of the brain-spinal cord preparation allows simultaneous recordings of a spinal cord NSPC and ipsilateral motor nerves ( Nv ). Electrical stimulation (10 pulses at 1 Hz) of the descending inputs elicits a swimming episode. In the absence of swimming, NSPCs do not respond (top traces). During a fictive locomotor episode, NSPCs periodically receive strong inputs (bottom traces). b NSPC responses during swim phase and the locomotor cycle during simultaneous ipsilateral and contralateral recordings. c Graph showing the activity of different NSPCs as a function of instantaneous swimming burst frequency. Individual data points represent instantaneous swimming frequencies of all swimming cycles where the respective NSPCs responded. d No apparent correlation between the amplitude of the periodic NSPC responses and the swimming frequency. e Correlation ( R 2 = 0.9098) between the swim duration and the detected number of inputs to NSPCs. The dashed gray line represents the baseline. NSPC, neural stem\/progenitor cell. For detailed statistics, see Supplementary Table 1 . Full size image NSPCs receive synaptic cholinergic input and non-synaptic GABAergic input We performed whole-cell electrophysiological recordings from the NSPCs ( her4 . 1:GFP + ) upon stimulation with neurotransmitters using an intact ex vivo adult zebrafish spinal cord preparation. Among the tested neurotransmitters (glutamate, glycine, serotonin, ACh, and GABA; Supplementary Fig. 5a ), only ACh and GABA induced noticeable changes in the NSPCs (Fig. 3a, b and Supplementary Fig. 5b\u2013h ). Bath application of ACh (100 \u03bcM or 5 m\u039c) induced numerous excitatory postsynaptic currents (EPSCs) with variable amplitudes in a dose-dependent manner (Supplementary Fig. 5b, c ). The action potential blocker tetrodotoxin (TTX) affected neither the frequency nor the amplitude of the EPSCs, confirming the presence of ACh receptors on the NSPCs\u2019 membrane (Supplementary Fig. 5b, c ). Next, we aimed to distinguish between muscarinic and nicotinic-ACh receptors (Fig. 3a ) in this membrane-associated activity. Activation of muscarinic (muscarine, 500 \u03bcM) and nicotinic (nicotine, 100 \u03bcM) ACh receptors generated EPSCs with different frequencies and amplitudes (Fig. 3a ). Stimulation of nicotinic receptors better recapitulated the results induced by ACh compared to stimulation of muscarinic receptors, indicating that the cholinergic input on the NSPCs is predominantly mediated by nicotinic receptors (Fig. 3a ). Treatment with the selective \u03b17 nicotinic-ACh receptor antagonist methyllycaconitine (MLA, 10 \u03bcM) significantly reduced the recorded EPSCs (Fig. 3a ), further supporting the central role of nicotinic receptors. Next, we assessed the GABAergic responses to NSPCs (Fig. 3b and Supplementary Fig. 5d, f, h ). NSPCs responded to bath application of the neurotransmitter GABA by inducing a prominent inward tonic activation (depolarization; Fig. 3b and Supplementary Fig. 5h ) insensitive to TTX (Supplementary Fig. 5d ). These GABA-mediated tonic responses were blocked entirely by the GABA A receptor antagonist gabazine (10 \u03bcM; Fig. 3b ). Applying the selective GABA A receptor agonist muscimol (15 mM; Fig. 3b ) could also accurately generate tonic activation. Fig. 3: NSPCs respond to synaptic cholinergic input from the locomotor network and to non-synaptic GABAergic signaling. a Bath application of ACh induced inward currents in all recorded NSPCs (20 out of 20). Sample traces of the muscarine- and nicotine-induced inward currents in recorded NSPCs. Significant reduction of the induced ACh currents in the presence of the \u03b17 nicotinic receptor antagonist MLA (10 \u03bcM). Quantification of the frequency (Hz; P = 3.237E-5) and amplitude (pA; P < 0.0001) of the recorded cholinergic currents. b Exogenous application of GABA induced tonic activation of NSPCs (22 out of 22). GABAergic tonic responses were completely abolished in the presence of the GABA A receptor antagonist, gabazine (10 \u03bc\u039c). Exogenous application of muscimol (GABA A receptor agonist) induced tonic activation of NSPCs. Quantification of the amplitude (pA; P = 7.904E-8) and duration (s; P = 1.141E-7) of the GABA-related responses. c Schematic protocol for NSPC recordings during local electrical stimulation. Ten pulses (20 Hz) were applied to increase the probability of presynaptic release. Superimposed representative sample trace (in red) out of >40 sweeps (in black) from NSPC responses under control conditions, following application of the polysynaptic blocker mephenesin, and application of the selective nicotinic receptor antagonist MLA suggesting synaptic cholinergic, but not GABAergic activation of the NSPCs. Quantification of the average number of detected EPSCs per sweep and the average amplitude of the responses in control and after the application of polysynaptic blocker (mephenesin). d Application of MLA during locomotion abolishes the regular and strong input to NSPCs, implying a predominant role of nicotinic receptors. ACh, acetylcholine; GABA, \u03b3-aminobutyric acid; MLA, methyllycaconitine; NSPC, neural stem\/progenitor cell; Nv , motor nerve recording. The dashed gray line represents the baseline. Data are presented as mean \u00b1 s.e.m. and as violin plots. *** P < 0.001; **** P < 0.0001; ns, not significant. For detailed statistics, see Supplementary Table 1 . Full size image Next, we examined the nature of the cholinergic and GABAergic transmission to NSPCs. We applied electrical stimulation of the spinal cord to depolarize all neurons, thereby increasing neurotransmitter release (Fig. 3c ). Following the electrical stimulation, we observed that the NSPCs received solely monosynaptic cholinergic inputs, as they were unaffected (number and amplitude of recorded EPSCs) by the presence of the polysynaptic blocker mephenesin (Fig. 3c ), while activity was entirely blocked by the selective \u03b17 nicotinic-ACh receptor antagonist MLA (Fig. 3c ). These data suggest that the GABAergic signaling to NSPCs is non-synaptic, as previously found in the mammalian brain 3 , 8 , 44 . Furthermore, we observed that MLA abolished the regular strong input to the NSPCs during fictive locomotion, indicating that locomotion-induced signal is solely cholinergic and mediated through the \u03b17 nicotinic receptors (Fig. 3d ). Premotor V2a interneurons mediate the essential cholinergic input to NSPCs during locomotion To gain insight into the ACh\u2019s potential sources to NSPCs, we focused on spinal cholinergic interneurons 45 . Cholinergic V2a interneurons (INs) 22 (Fig. 4a ) are the principal components of the locomotor CPG 19 , 20 , 21 , 43 , 46 , 47 . Anatomical analysis revealed close appositions of V2a-IN axonal collaterals ( Chx10:GFP + ) to the central canal area (Fig. 4b , c) while we identified that ~40% of the NSPCs (CB + ) had close proximities, likely synaptic contacts, with the V2a-INs (GFP + ; Fig. 4b ). To functionally test whether V2a-INs could provide cholinergic input to NSPCs, we performed pair recordings from the same segment (Fig. 4f ). We observed that a train of action potentials elicited in V2a-INs failed to induce any postsynaptic responses to NSPCs (intra-segmental: 0 out of 15 pairs; Fig. 4f ). However, V2a-INs are long ipsilateral descending neurons, and we observed that all (25 out of the 25; n = 8 zebrafish) long descending (>10 segments) spinal cholinergic neurons in zebrafish were indeed V2a-INs (GFP + ; Fig. 4d ). Moreover, we identified that the descending cholinergic V2a-INs (~3\/hemisegment) have medium-to-large body size and specific dorsomedial location in the spinal cord (Fig. 4e ). Pair recordings obtained from distal segments (~5\u20137 segments apart; inter-segmental) revealed that action potentials in V2a-INs induced vigorous small-amplitude EPSCs in NSPCs in 22% of the cases (inter-segmental: 11 out of 50 pairs; Fig. 4f, g ). The recorded EPSCs were resistant to mephenesin (1 mM), a pharmacological agent shown to act as a potential polysynaptic transmission blocker in the mammalian spinal cord 48 (Fig. 4f and Supplementary Fig. 6 ). Yet, the observed changes in the duration of the recorded EPSCs after the application of mephenesin suggested that the interaction between V2a-INs and NSPCs comprises monosynaptic and polysynaptic inputs (Supplementary Fig. 6 ). To further determine whether these responses are cholinergic, we applied the nicotinic receptor antagonist MLA and observed that it completely abolished the monosynaptic EPSCs in NSPCs (Fig. 4f ). We noticed that the transmission between V2a-INs and NSPCs exhibited partial and complete failures in ~20% of the cases during train stimulation (Supplementary Fig. 7 ), suggesting that nicotinic-ACh receptors might undergo desensitization, which is a potential mechanism to control synaptic efficacy 49 , 50 . Next, we asked whether V2a-INs release cholinergic input to NSPCs during locomotion. Simultaneous recordings from connected pairs of V2a-INs and NSPCs revealed that while V2a-INs discharged rhythmically during swimming, NSPCs occasionally receive this cholinergic input (Fig. 4h ) as seen before (Fig. 2a ), implying that the nicotinic-ACh receptor desensitization most likely is responsible for the non-regular release of the ACh during locomotion. Fig. 4: Spinal locomotor V2a-INs contribute cholinergic inputs to NSPCs. a Large and dorsally located spinal cord V2a-INs ( Chx10:GFP + , green) are cholinergic (ChAT + , magenta). Arrowheads indicate double-labeled neurons ( Chx10:GFP + ChAT + ). b A sample stack from the central canal area showing the presence of V2a-IN ( Chx10:GFP + ) axonal collaterals (green) close to CB + NSPCs (magenta) with analysis of the proportion of the CB + NSPCs that are in close proximity with the V2a-IN ( GFP + ) processes. c Quantification of the probability of the V2a-IN axonal collaterals in the central canal region ( n = 15 zebrafish). d Representative whole-mount confocal image showing that all (25 out of 25; 100% from 8 zebrafish) long descending (dextran tracer, blue; >10 segments) cholinergic (ChAT + , red) neurons are V2a-INs (GFP + , green). Arrowheads indicate triple-labeled neurons. e Quantification and analysis of the number, size and location of long descending cholinergic V2a-INs in the adult zebrafish spinal hemisegments. f Sample average (~25 sweeps) traces from dual electrophysiological recordings between a premotor V2a-IN and NSPCs, located in the same segment (intra-segmental, 1) or 5\u20136 segments rostrally (inter-segmental, 2). Cholinergic connections were observed in the inter-segmental pairs (22%, 11 out of 50 pairs) but not in the intra-segmental pairs (0%, 0 out of 15). g Postsynaptic responses in the recorder NSPCs generated from suprathreshold (black, with action potential) and not from subthreshold (gray, without an action potential) short pulse depolarization of the V2a-IN. h Ex vivo setup of the brain-spinal cord preparation allows simultaneous recordings of a NSPC and ipsilateral descending V2a-INs during fictive locomotion. Sample trace of a connected pair that, while the V2a-IN discharges during fictive swimming, the NSPC receives occasional input. i Illustrat i on of recordings acquired during electrical stimulations. Ten pulses (20 Hz) of a rostral spinal cord segment were applied to depolarize V2a-INs connected to NSCs. Representative sample trace (in red) of superimposed sweeps (~20, in black) from not responding and responding NSPCs. Bath application of the selective nicotinic antagonist MLA (10 \u03bcM) abolished the recorded currents suggesting that they are cholinergic. Changes in the proportion of the recorded NSPCs that respond to electrical stimulation observed after training ( n : number of recorded NSPCs). The average number of detected events per stimulation sweep from both sites was significantly higher in trained animals, suggesting adaptive changes in the innervation and the cholinergic release to the NSCs ( P < 0.0001). The dashed gray line represents the baseline. CB, calbindin D-28K; CC, central canal; ChAT, choline acetyltransferase; EPSC, excitatory postsynaptic current; INs, interneurons; MLA, methyllycaconitine; NSPC, neural stem\/progenitor cell. Data are presented as mean \u00b1 s.e.m., as violin plots and as box plots showing the median with 25\/75 percentile (box and line) and minimum\u2013maximum (whiskers). **** P < 0.0001. For detailed statistics, see Supplementary Table 1 . Full size image Finally, we tested whether NSPCs adapt their response to signals from rostral segments (segments 9\u201311) during training (Fig. 4i ). We noticed that training caused an increase in recruiting NSPCs that receive a nicotine-mediated synaptic input from descending V2a-INs by 18% (Fig. 4i ). Moreover, we found that the number of detected EPSC events per stimulation sweep increased by ~150% (Fig. 4i ). These data suggest that physical activity increased the cholinergic release to NSPCs and expanded V2a-IN cholinergic synapses to NSPCs. However, their extensive and complex branched morphology precluded a strict quantification of this structural plasticity. Cholinergic and GABAergic receptors control the NSPCs\u2019 proliferation in an opposing manner To test whether manipulation of nicotinic-ACh and GABA A receptors impinges NSPCs\u2019 proliferation, her4.1 + BrdU + cells lining the central canal were quantified after a single administration of nicotine, GABA, or gabazine followed by a pulse with BrdU for 1 h (Fig. 5a ). We observed a significant increase in the number of her4.1 + BrdU + cells after exposure to nicotine and gabazine (Fig. 5a ). In contrast, GABA A receptor activation by ambient GABA significantly reduced the number of BrdU her4.1 + BrdU + cells (Fig. 5a ). Fig. 5: Cholinergic and GABAergic receptors control the NSPCs\u2019 proliferation in an opposing manner. a Microphotographs and analysis show that nicotine and gabazine increased the her4.1 + (green)\/BrdU + (magenta) cells, whereas GABA reduced the number of her4.1 + \/BrdU + cells in the examined spinal hemisegment ( P < 0.0001). b In vivo administration of ACh, muscarine, nicotine, and gabazine increased the number of BrdU + cells per hemisegment ( P < 0.0001). Administration of GABA reduced the number of BrdU + cells in the adult zebrafish spinal cord hemisegment ( P < 0.0001). c Co-administration of nicotine and gabazine generated the same number of BrdU + cells as the individual administration of nicotine or gabazine. Co-activation of the nicotinic-ACh receptors and the GABA A receptors produced the same number of BrdU + cells as in control (saline; P < 0.0001). d Application of ACh induced currents of the same frequency and amplitude in NSPCs before and after training, suggesting no changes in the cholinergic receptors following training. e Bath application of GABA before and after training revealed a significant reduction of the tonic activation amplitude without affecting its duration in the NSPCs. ACh, acetylcholine; BrdU, 5-bromo-2\u02b9-deoxyuridine; GABA, \u03b3-aminobutyric acid; GFP, green fluorescent protein; her4.1, hairy-related 4, tandem duplicate 1; The dashed gray line represents the baseline. Data are presented as mean \u00b1 s.e.m., as violin plots and as box plots showing the median with 25\/75 percentile (box and line) and minimum\u2013maximum (whiskers). * P < 0.05; ** P < 0.01; *** P < 0.001; **** P < 0.0001; ns, not significant. For detailed statistics, see Supplementary Table 1 . Full size image We next examined how cholinergic activation of NSPCs engenders the proliferation in vivo by injecting adult zebrafish intraperitoneally with ACh (2 mM), muscarine (a selective agonist of muscarinic-ACh receptors, 50 \u03bcM), or nicotine (a selective agonist of nicotinic-ACh receptors, 300 \u03bcM) (Fig. 5b ). Activation of the ACh receptors significantly increased the number of BrdU + cells in the spinal cord (Fig. 5b ). Notably, nicotine administration resulted in a higher number of BrdU + cells than ACh, reflecting that extracellular acetylcholinesterases rapidly degrade ACh. In contrast, GABA (500 \u03bcM) or the GABA A receptor antagonist gabazine (100 \u03bcM; Fig. 5b ) caused a decrease in the number of BrdU + cells. Conversely, the number of BrdU + cells increased in animals treated with the GABA antagonist gabazine (Fig. 5b ). To verify that neurotransmitters acted directly on spinal cord receptors and not through a systemic effect, we treated isolated intact cords with cholinergic and GABAergic antagonists and agonists ex vivo (Supplementary Fig. 8a ). In accordance with the in vivo studies, we observed a significant increase in proliferation upon selective activation of cholinergic receptors (Supplementary Fig. 8b ). In contrast, GABA-treated adult spinal cords showed a significant decrease in the detected BrdU + cells, and gabazine caused a significant increase in proliferation (Supplementary Fig. 8b ). We also treated the isolated intact spinal cords with nicotine, GABA, and gabazine in the presence of the synaptic blocker TTX (Supplementary Fig. 9a ). We observed that activation of the nicotinic receptors and blockage of the GABA A receptors (gabazine) enhanced the proliferation (BrdU + cells), while GABA reduced the number of the newborn cells (Supplementary Fig. 9b ). Collectively, our data suggest that the observed changes in the proliferation resulted from the direct modulation of neurotransmitter receptors on spinal cord NSPCs. Next, we determined how simultaneous manipulation of nicotinic-Ach and GABA A receptors influenced proliferation. We found that activation of the nicotinic-ACh receptors along with the blockage of the GABA A receptors by co-administration of nicotine and gabazine did not produce more BrdU + cells than each manipulation separately (Fig. 5c ). When we performed nicotine and GABA co-injections, we observed that the number of the BrdU + cells was the same as in control (saline-injected) animals (Fig. 5c ). These results collectively show that direct activation of nicotinic receptors triggers NSPCs and induces the proliferative program that is counteracted by GABA A receptors. Adaptive regulation of NSPC GABA A receptors after training We determined whether physical training results in changes in NSPC receptors. We observed no changes in the frequency or amplitude of ACh-induced EPSCs in the NSPCs upon training (Fig. 5d ), suggesting that the number of cholinergic receptors remained unaltered. On the contrary, the responses observed by the NSPCs from trained animals after treatment with GABA (15 mM) had significantly lower amplitude than those observed in control (untrained) animals (Fig. 5e ). However, their duration was unaffected (Fig. 5e ), implying a reduction in GABA A receptors\u2019 abundance after prolonged physical activity. These findings collectively suggest that two distinct and complementary mechanisms regulate spinal cord proliferation after training: an increase in cholinergic neurotransmission (a synaptic\/network mechanism) and a reduction in the number of GABA A receptors (a self-regulatory mechanism) in the NSPCs. Manipulation of neurotransmitter receptors on NSPCs promotes neuronal regeneration and restoration of motor functions Spinal cord regeneration involves an extensive proliferation of NSPCs and subsequent neurogenesis. Indeed, following transection of the spinal cord at segment 15, we observed a significant increase in the number of BrdU + cells (Fig. 6a ). We asked whether pharmacological manipulation of nicotinic-ACh and GABA A receptors could promote regeneration (Fig. 6 ). Animals that received pharmacological treatment with either nicotine or gabazine produced higher number of BrdU + cells than the saline-treated fish (Fig. 6b, d ). The increased proliferation correlated with increased neurogenesis assayed by BrdU + mef-2 + (Fig. 6c, e ). These data suggest that pharmacological treatments can effectively bolster proliferation and neurogenesis in the injured animals. We found that both nicotine- and gabazine-treated fish recovered locomotion performance faster than the untreated animals after spinal cord transection, suggesting an accelerated regeneration process (Fig. 6f ). Thus, spinal cord regeneration could be promoted by either increasing the cholinergic signaling or blocking the GABAergic signaling to NSPCs. Fig. 6: Nicotine and gabazine promote neurogenesis and restoration of motor performance after spinal cord injury. a Pulse-chase experiment to assess proliferation, neurogenesis, and restoration of motor functions after pharmacological manipulation of the nicotinic-ACh and GABA A receptors in the adult zebrafish spinal cord. b Representative whole-mount confocal microphotographs showing BrdU + cells in the zebrafish spinal cord. c Representative whole-mount confocal images for immunodetection of BrdU + \/mef-2 + cells. Arrowheads indicate double-labeled cells. d Quantification of BrdU-incorporation after injury in control (saline) and pharmacologically treated animals (nicotine, gabazine). The dashed gray line represents the baseline (BrdU + cells in uninjured animals). e Quantification of the BrdU + cells express the neuronal marker mef-2 + . f Nicotine- and gabazine-treated animals swim faster than the control (saline) fish during the critical speed test. The dashed gray line represents the baseline (critical speed of the uninjured animals). Speed is normalized (BL\/s). BL, body length; BrdU, 5-bromo-2\u02b9-deoxyuridine; mef-2, myocyte enhancer factor-2; SCI, spinal cord injury. Data are presented as box plots showing the median with 25\/75 percentile (box and line) and minimum\u2013maximum (whiskers). * P < 0.05; ** P < 0.01. For detailed statistics, see Supplementary Table 1 . Full size image Discussion Here we revealed an adaptive mechanism by which physical activity dynamically modulates adult neurogenesis mediated by ACh and GABA neurotransmitters. Specifically, the locomotor CPG V2a-INs link motor functions to neurogenesis by contributing to regulation of the NSPCs\u2019 proliferation and subsequent neurogenesis. While activation of NSPCs relies on an increased synaptic cholinergic input and is independent of the number of cholinergic receptors, insensitivity to non-synaptic GABAergic signaling 3 , 8 is achieved by reducing the abundance of GABA A receptors (Fig. 7 ). Therefore, exercise-dependent neurogenesis involves two distinct and mutually antagonistic processes. Extending these findings to a spinal cord injury model, we found that activation of the nicotinic-ACh receptors and inhibition of the GABA A receptors increased the number of newborn neurons and promoted motor function restoration. Fig. 7: Proposed model for exercise controlled adult neurogenesis in the zebrafish spinal cord. The findings link the locomotor CPG network to adult neurogenesis. Spinal cholinergic interneurons, including the premotor V2a-IN population, increase their cholinergic release to NSPCs during training. ACh acts directly on the NSPCs via nicotinic and muscarinic cholinergic receptors to activate them. Activation of NSPCs leads to the downregulation of GABA A receptors. Full size image Previous studies have shown that several neurotransmitters can directly or indirectly regulate the activity of the NSPCs and neurogenesis in the nervous system 3 , 8 , 9 , 10 , 11 , 13 , 14 , 16 , 51 , 52 , 53 . Our results highlight the neurotransmitter ACh\u2019s pivotal role in mediating physical activity-induced proliferation and neurogenesis. The evolutionarily conserved role of ACh and GABA in regulating mammalian hippocampal neurogenesis 3 , 8 , 16 , 17 and spinal cord gliogenesis 13 , 15 , further indicates that these transmitters aim to control the generation of new cells without affecting their differentiation program. Indeed, it has been shown that after spinal cord injury, mammalian spinal cord progenitor\/stem\/progenitor cells exhibit a robust but abortive proliferative response that fails to generate mature neurons 54 , but rather produces the glial scar formation 33 , 55 . Conversely, zebrafish can fully regenerate the spinal cord and recover motor and sensory functions by activating the neurogenic program of the NSPCs around the central canal 34 , 56 . The antagonistic effects of ACh and GABA transmitter activity on NSPCs probably operate through a dynamic interplay with the molecular context provided by certain molecular factors, but the rules that govern this interaction still remain unclear. We also investigated the role of other neurotransmitters in the direct control of NSPCs. While serotonin is known to regulate proliferation and neurogenesis in various contexts 10 , 41 , 52 , 57 , 58 , its precise mechanism of action has remained unexplored 59 , 60 . Our results indicate that serotonin, glutamate, and glycine 22 are not directly involved in spinal NSPCs\u2019 proliferation. In support, our previous study showed that serotonergic inputs modulate NSPCs\u2019 proliferation in the brain indirectly through other signaling molecules, such as the brain-derived neurotrophic factor (BDNF) 41 , 61 . Similarly, glutamate has also been suggested to act on NSPCs indirectly, via modulation of neurotrophic factors, such as the BDNF and the nerve growth factor (NGF) and fibroblast growth factor (FGF) 10 , 62 , 63 , 64 , 65 . It is conceivable, indeed probable, that adaptation occurs within the spinal cord after training 31 . The exact mechanisms of this adaptation are unclear, and the present data generate several testable hypotheses. For example, neurotransmitter switching is a recently discovered form of plasticity 66 , 67 whereby neurons change their transmitter phenotypes in response to a sustained stimulus such as exercise 31 . As such, it is an activity-dependent adaptive mechanism 31 , 68 that could explain changes in neurotransmitter availability and equilibrium that occur in the nervous system under both physiological as well as pathophysiological conditions. Further studies could examine the extent to which increased cholinergic neurotransmission to stem cells is mediated through cholinergic re-specification of the spinal interneurons. The findings reported here show that the activity of the locomotor networks induces NSPCs to proliferate. We found that cholinergic V2a-INs are among the spinal interneurons interacting with the NSPCs, presumably using direct (monosynaptically) and indirect (polysynaptically) connectivity. We observed reliable, fast, and time-locked cholinergic reactions in the NSPCs triggered by V2a-IN spikes, a result that is commonly interpreted as reflecting direct monosynaptic input (Fig. 4f and Supplementary Fig. 6 ). Yet, the observed changes in the shape of the EPSCs after applying mephenesin also suggested the presence of a polysynaptic component (Supplementary Fig. 6 ). Even if a component of the observed action potential-mediated signals is transmitted through a downstream neuron to NSPCs, the critical reasoning of using spike-triggered approaches in V2a-INs through pair recordings and evaluating the downstream impact to NSPCs is to determine effective communication between the two cell types, which our data do certainly suggest. Future studies may uncover a remaining question regarding the extent of the physiological relevance between the direct and indirect communication in modulating the NSPC activity after training. V2a-INs are regarded as fundamental components of the locomotor CPG and are therefore essential for initiating and maintaining locomotor rhythm 19 , 20 , 21 , 43 , 46 , 47 and to provide the primary driving input to motoneurons during locomotion 43 . However, other local cholinergic interneurons, like the ones residing close to the central canal, exist in the spinal cord 45 , 69 , 70 , and we cannot rule out the possibility that their firing could produce value-related cholinergic input to NSPCs. Nevertheless, the data here link locomotor network activity 19 , 20 , 21 to spinal cord neurogenesis and demonstrate an essential non-motor\/non-neuronal function for the CPG. Methods Experimental animals All animals were raised and kept in a core zebrafish facility at the Karolinska Institute following established practices. Adult zebrafish of both sexes ( Danio rerio ; n = 343 animals; 8\u201310 weeks old; length: 15\u201320 mm; weight: 0.04\u20130.06 g), wild type (AB\/T\u00fcbingen), Tg( Chx10:GFP nns1 ), and Tg( her4.1:GFP ) lines. Zebrafish of both sexes were used in all experiments. No selection criteria and blinding procedures were used to allocate zebrafish to any experimental group. The local Animal Research Ethical Committee (at Karolinska Institutet), Stockholm (Ethical permit no. 9248-2017) approved all experimental protocols, and were implemented under EU directive for the care and use of laboratory animals (2010\/63\/EU). All efforts were made to utilize only the minimum number of experimental animals necessary to obtain reliable scientific data. Training protocol All animals used in the swim training paradigm had similar sizes (body length, BL; body depth, BD) and weights. Some of the designated animals ( n = 8 zebrafish) were randomly selected and subjected to the critical speed ( U CRIT ) test, which measures the highest sustainable swimming speed a fish can reach using a commercially available swim tunnel (5 L; Loligo systems, SW10050). After determining the critical speed, animals were selected for the exercise training protocol, in which exercised\/trained zebrafish (~25) swam at 60% of U CRIT for 6 h per day, 5 days per week. To study the effect of training on animal growth, fish were trained for 6 consecutive weeks. At each time point (every 7 days), the fish were anesthetized in 0.03% tricaine methane sulfonate (MS-222, Sigma-Aldrich, E10521), and images of the body size were obtained. For all the other experiments, zebrafish were trained for 2 consecutive weeks. After the exercise period, fish were randomly assigned to a short-term experimental group (training) or long-term (rest) group. Animals of the recovery group were kept under standard conditions for 2 weeks. Afterward, all animals (training\/rest) selected for anatomical investigations were anesthetized and processed for immunohistochemistry, as described in the \u201cImmunohistochemistry\u201d section. Trained animals for electrophysiological recordings were processed within the first three days after the end of the training. BrdU treatment Animals were treated with 5-bromo-2\u02b9-deoxyuridine (BrdU; Sigma-Aldrich, B5002) at a concentration of 0.7% in fish water for 2 h. BrdU is a nonradioactive analog of thymidine incorporated into proliferating cells\u2019 DNA during the S phase of mitosis. Fish were then allowed to survive for another 22 h (short-term survival) or 2 weeks (long-term survival) before being processed for BrdU immunodetection. For the acute treatment, animals were treated with BrdU at a concentration of 0.7% in fish water for 1 h before analysis. In acute experiments described in Fig. 5a , animals were injected intraperitoneally (volume: 2 \u03bcl) with either saline, nicotine (300 \u03bc\u039c; Sigma-Aldrich, SML1236), GABA (500 \u03bc\u039c; Sigma-Aldrich, A2129), or gabazine (100 \u03bc\u039c; Sigma-Aldrich, SR95531). Immediately after injection, the animals were treated with BrdU for 1 h, as described above. Descending neuron labeling Zebrafish were anesthetized in 0.03% tricaine methane sulfonate (MS-222, Sigma-Aldrich, E10521). Retrograde labeling of descending spinal cord neurons located in spinal segments 1\u20133 was achieved through dye injections with biotinylated dextran (3000 MW; ThermoFisher, D7135) into segment 16 or 17. Animals were kept alive for at least 24 h after injection to allow retrograde transport of the tracer, deeply anesthetized with 0.1% MS-222, and the spinal cords were dissected and fixed in 4% paraformaldehyde (PFA) and 5% saturated picric acid (Sigma-Aldrich, P6744) in phosphate-buffered saline (PBS; 0.01 M, pH = 7.4; Santa Cruz Biotechnology, Inc., CAS30525-89-4) at 4 \u00b0C for 4\u201310 h. The tissue was then washed extensively with PBS and incubated in streptavidin conjugated to Alexa Fluor 488 (dilution 1:500, ThermoFisher, S32354), Alexa Fluor 555 (1:500, ThermoFisher, S32355), or Alexa Fluor 647 (dilution 1:500, ThermoFisher, S32357) overnight at 4 \u00b0C. Primary and secondary antibodies were applied as described in the \u201cImmunohistochemistry\u201d section. After thorough buffer rinses, the tissue was mounted on gelatin-coated microscope slides and cover-slipped with an anti-fade fluorescent mounting medium (Vectashield Hard Set, VectorLabs; H-1400). Pharmacology For the experiments conducted to evaluate the impact of pharmacological agents on proliferation and neurogenesis in vivo, animals were anesthetized using 0.03% tricaine methane sulfonate (MS-222; Sigma-Aldrich, E10521) in fish water and injected intraperitoneally (volume: 2 \u03bcl) with saline, ACh (2 mM; Sigma-Aldrich, A6625), muscarine (50 \u03bc\u039c; Sigma-Aldrich, M104), nicotine (300 \u03bc\u039c; Sigma-Aldrich, SML1236), GABA (500 \u03bc\u039c; Sigma-Aldrich, A2129), or gabazine (100 \u03bc\u039c; Sigma-Aldrich, SR95531). Immediately after injection, the animals were treated with BrdU as described above (see \u201cBrdU treatment\u201d section). For ex vivo evaluation of NSPC receptor activation\u2019s contribution to proliferation, animals were anesthetized and dissected as for the electrophysiological recordings. Isolated intact spinal cords were then transferred to a continuously aired chamber containing the pharmacological agents, and BrdU diluted in the extracellular solution used for electrophysiological recording. In some experiments the extracellular solution contained TTX (1 \u03bcM) to abolish the synaptic transmission in the spinal cord networks. After the pharmacological treatments, the animals and tissues were processed for immunodetection of the incorporated BrdU. Immunohistochemistry All animals were deeply anesthetized with tricaine methane sulfonate (MS-222, Sigma-Aldrich, E10521). The spinal cords were then extracted and fixed in 4% paraformaldehyde (PFA) and 5% saturated picric acid (Sigma-Aldrich, P6744) in phosphate-buffered saline (PBS) (0.01 M; pH = 7.4, Santa Cruz Biotechnology, Inc., CAS30525-89-4) at 4 \u00b0C for 2\u201314 h. We performed immunolabeling in both whole-mount spinal cords and cryosections. For sections, the tissue was removed carefully and cryoprotected overnight in 30% (w\/v) sucrose in PBS at 4 \u00b0C, embedded in Cryomount (Histolab, 45830) sectioning medium, rapidly frozen in dry-ice-cooled isopentane (2-methylbutane; Sigma-Aldrich, 277258) at approximately \u201335 \u00b0C, and stored at \u221280 \u00b0C until use. Transverse coronal plane cryosections (thickness: 20\u201325 \u03bcm) of the tissue were collected and processed for immunohistochemistry. For all sample types (whole-mount and cryosections), the tissue was washed 3 times for 5 min each in PBS. Nonspecific protein binding sites were blocked with 4% normal donkey serum (NDS; Sigma-Aldrich, D9663) with 1% bovine serum albumin (BSA; Sigma-Aldrich, A2153) and 1% Triton X-100 (Sigma-Aldrich, T8787) in PBS for 1 h at room temperature (RT). Primary antibodies (Supplementary Table 2 ) were diluted in 1% of the blocking solution and applied for 1\u20133 days at 4 \u00b0C. After thorough buffer rinses, the tissues were then incubated with the appropriate secondary antibodies (Supplementary Table 2 ) diluted 1:500 or with streptavidin conjugated to Alexa Fluor 488 (1:500, ThermoFisher, S32354), Alexa Fluor 555 (1:500, ThermoFisher, S32355), or Alexa Fluor 647 (1:500, ThermoFisher, S32357) in 1% Triton X-100 (Sigma-Aldrich, T8787) in PBS overnight at 4 \u00b0C. Finally, the tissue was thoroughly rinsed in PBS and cover-slipped with a hard fluorescent medium (VectorLabs; H-1400). To visualize the incorporated BrdU, DNA denaturation was performed by incubating the tissue in 2 N HCl for 30 min (sections) or 75 min (whole mounts) at 37 \u00b0C, followed by thorough washing in PBS. The standard immunodetection procedure described above was then applied. Electrophysiology Adult zebrafish were cold-anesthetized in a slush of a frozen extracellular solution containing MS-222. The skin and muscles were removed to allow access to the spinal cord. The spinal cord was dissected out carefully and transferred to a recording chamber that was continuously perfused with an extracellular solution containing 135.2 mM NaCl, 2.9 mM KCl, 2.1 mM CaCl2, 10 mM HEPES, and 10 mM glucose at pH 7.8 (adjusted with NaOH) and an osmolarity of 290 mOsm. For whole-cell intracellular recordings of NSPCs in voltage-clamp mode, electrodes (resistance, 3\u20135 M\u03a9) were pulled from borosilicate glass (outer diameter, 1.5 mm; inner diameter, 0.87 mm; Hilgenberg) on a micropipette puller (model P-97, Sutter Instruments) and filled with an intracellular solution containing 120 mM K-gluconate, 5 mM KCl, 10 mM HEPES, 4 mM Mg2ATP, 0.3 mM Na4GTP, and 10 mM Na-phosphocreatine at pH 7.4 (adjusted with KOH) and an osmolarity of 275 mOsm. Cells were visualized using a microscope (LNscope; Luigs & Neumann) equipped with a CCD camera (Lumenera) and explicitly targeted. Intracellular patch-clamp electrodes were advanced to the stem\/progenitor cells using a motorized micromanipulator (Luigs & Neumann) while applying constant positive pressure. Intracellular signals were amplified with a MultiClamp 700B intracellular amplifier (Molecular Devices). All cells were clamped at \u201370 mV throughout all voltage-clamp recordings. All experiments were performed at RT (23 \u00b0C). The following drugs (prepared by diluting stock solutions in distilled water) were added (singly or in combinations mentioned in the text) to the physiological solution: acetylcholine (ACh, 100 \u03bcM or 5 mM; Sigma-Aldrich, A6625), GABA (\u03b3-aminobutyric acid, 1 or 15 mM; Sigma-Aldrich, A2129), gabazine (10 \u03bcM; Sigma-Aldrich, SR95531), glutamate (5 mM; Sigma-Aldrich), glycine (1 mM; Sigma-Aldrich, G2879), methyllycaconitine (MLA, 10 \u03bc\u039c; Sigma-Aldrich, M168), muscarine (500 \u03bc\u039c; Sigma-Aldrich, M104), nicotine (100 \u03bcM; Sigma-Aldrich, N3876 and SML1236), N -methyl- D -aspartate (NMDA, 100 \u03bcM; Sigma-Aldrich, M3262), serotonin (1 mM; Sigma-Aldrich, H9523), and tetrodotoxin (TTX, 1 \u03bc\u039c; Sigma-Aldrich, T8024). For the evaluation of the activity of the NSPCs during fictive locomotion (Fig. 2a ), we used the adult zebrafish ex vivo preparation 42 , 71 . Extracellular recordings were performed from the motor nerves. Activation of the locomotion was induced by extracellular stimulation (using a train of 10 pulses: 1 Hz) applied via a glass pipette placed at the junction between the brain and the spinal cord. Recordings were made from both ipsilateral and contralateral located NSPCs and motor nerves (Fig. 2b ). The local spinal neuron activation was triggered by extracellular stimulation (using a train of 10 pulses: 20 Hz) applied via a glass pipette (Fig. 3c ). Activation of the long descending spinal neurons was induced by extracellular stimulation (using a train of 10 pulses: 20 Hz) applied via a glass pipette placed 5\u20136 segments rostral to whole-cell NSPC recording the adult zebrafish spinal cord (Fig. 4i ). To attenuate and potentially block the polysynaptic transmission, we used 2.5x HiDi (a high concentration of divalent cations) solution or mephenesin (1 mM; Sigma-Aldrich, 286567) a possible polysynaptic blocker applied for at least 20 min before all the recordings. For dual whole-cell recordings of V2a-INs and progenitors\/stem cells, two patch-clamp electrodes were advanced from opposite directions into the spinal cord to record cells separated by at least five segments (inter-segmental recordings) or from the same spinal segment (intra-segmental recordings). Single and multiple short-duration (0.5 ms) suprathreshold and subthreshold current pulses were used to stimulate presynaptic V2a interneurons and record responses in stem\/progenitor cells. All dual whole-cell recordings are presented as averages of 20\u201335 sweeps. Only NSPCs (GFP + ) that had stable resting membrane potentials at or below \u221260 mV did not fire action potentials upon strong depolarizations (>0 mV) and showed minimal changes in resistance (<5%) were included in this study. In all recordings, the EPSC events were detected and analyzed in a semi-automatic (supervised) fashion after baseline subtraction using AxoGraph (version X 1.5.4; AxoGraph Scientific, Sydney, Australia; RRID: SCR_014284) or Clampfit (version 10.6; Molecular Devices). The EPSC amplitude was calculated as the difference between the baseline and the peak of the event. Spinal cord injury Adult zebrafish were anesthetized in 0.03% tricaine methane sulfonate (MS- 222; Sigma-Aldrich) before subjection to the spinal cord injury, which involved complete transection of the spinal cord segment 15 with a micro knife (10318-14; Fine Science Tools) under constant visual control. Once the lesion was completed, the spinally transected animals were injected intraperitoneally (volume: 2 \u03bcl) with BrdU solution (Sigma; 0.2 mg\/g body weight) containing either saline (control), nicotine (300 \u03bc\u039c; Sigma-Aldrich, SML1236), or gabazine (100 \u03bc\u039c; Sigma-Aldrich, SR95531). All animals were kept in freshwater under standard conditions and received two additional intraperitoneal injections of the drugs every 4 days, as described in Fig. 6a . Critical speed test All post-injured animals (12 days after injury) were subjected to the critical speed ( U CRIT ) test using a commercially available swim tunnel (5 L; Loligo systems, SW10050). Critical speed ( U CRIT ) is a measure of the highest sustainable swimming speed that a fish can reach. The zebrafish were subjected to time intervals (2 min) of increased water flow velocity (increments of 4.5 cm\/s) until the fish could not swim against the water current 31 . The critical speed was normalized to the experimental animals\u2019 body length (BL) and is given as BL\/s. Analysis Morphometric analysis of the adult zebrafish was performed in images acquired with an HD camera (MC120, Leica) attached to a stereomicroscope (M60, Leica). The body size (total length, TL) of each animal was quantified using ImageJ. All immunodetections of whole-mount images of the adult zebrafish spinal cord preparations were acquired using an LSM 800 laser scanning confocal microscope (Zeiss) with a 40x objective (oil immersion). Each examined whole-mount spinal cord hemisegment was scanned from the ipsilateral side to the contralateral side at the contralateral primary motoneurons level to ensure the central canal region\u2019s acquisition, generating a z-stack (z-step size = 0.3\u20130.5 \u03bcm). All neurons, including her4 :GFP + stem\/progenitor cells and newborn cells, were counted in spinal hemisegment 15 or 16. The neurons\u2019 somata and cells\u2019 relative positions ( XYZ coordinates) within the spinal cord were determined (using the lateral, dorsal, and ventral edges of the cord as landmarks) using ImageJ (cell counter plugin). Soma sizes and numbers were also measured using ImageJ. All whole-mount data are presented as the number of cells or neurons in each analyzed hemisegment. The central canal region was defined as the area in the midline between the contralateral primary motoneurons. Analysis and quantifications of the BrdU + cell differentiation profiles were performed using 6 coronal sections (20 \u03bcm thick, 20 \u03bcm intervals) of the spinal cord segments 14\u201316. All quantifications in the injured animals were performed in an area of 150 \u03bcm length, located 50\u201360 \u03bcm rostrally from the injured site. The probability matrix of the V2a-IN processes in the central canal area was generated from multiple data (~7 sections\/animal; n = 15 zebrafish) using Origin 8 (OriginLab, Northampton, MA, USA). To enhance visualization of our data, most of the whole-mount images presented here were prepared by merging subsets of the original z-stacks. Most single channel images showing the BrdU + were inverted to allow better visualization. Colocalizations were detected by visual identification of structures whose color reflects the combined contribution of two or more antibodies in the merged image. Most of the presented traces were low-pass filtered (Gaussian, 11-21 coefficients) using Clampfit (version 11.0; Molecular Devices). All figures and graphs were prepared with Adobe Photoshop and Adobe Illustrator (Adobe Systems Inc., San Jose, CA, USA). Digital modifications of the images (brightness and contrast) were minimal to diminish the potential distortion of biological information. All double-labeled immunofluorescence images were converted to magenta-green to improve visualization of the results for color-blind readers. Statistics and reproducibility The significance of differences between the means in experimental groups and conditions was analyzed using parametric tests such as the two-tailed unpaired or paired Student\u2019s t -test and one-way ANOVA (ordinary) followed by post hoc Tukey\u2019s test or Dunnett\u2019s multiple comparison test, using Prism (GraphPad Software Inc.). Significance levels indicated in all figures are as follows: * P < 0.05, ** P < 0.01, *** P < 0.001, **** P < 0.0001. All data are presented as mean \u00b1 s.e.m. (standard error of mean) or as box plots showing the median, 25th, and 75th percentile (box and line) and minimal and maximal values (whiskers). Finally, the n values indicate the final number of validated animals per group, cells, or events that were evaluated and presented in detail in Supplementary Table 1 . All experiments were carried out independently 2\u20135 times from different investigators. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability All data used for the analyses presented in this study are included in Supplementary Table 1 . Source data are provided with this paper. ","News_Body":"Researchers at Karolinska Institutet, the German Center for Neurodegenerative Diseases (DZNE) and Columbia University Irving Medical Center have found an unexpected link between spinal locomotor network activity and adult neurogenesis in the adult zebrafish spinal cord. The study has recently been published in Nature Communications. Since the first demonstration of spinal central pattern generators (CPGs) in the early '70s, the activity of neurons involved in the central pattern generator networks has been considered only in terms of their contribution to locomotion. \"We can now reveal an unforeseen yet central non-motor function of spinal locomotor neurons and demonstrate how they dynamically regulate neurogenesis and regeneration following spinal cord injury,\" says Konstantinos Ampatzis, researcher at the Department of Neuroscience and corresponding author. What does your study show? \"In this study, we identify the direct contribution of the spinal locomotor neurons in activating the spinal cord stem cell population, glial cells that can generate new neurons in the adult zebrafish. Therefore, during prolonged locomotion, as we see after training, the stem cells receive excessive synaptic input that allows them to exit their quiescent state and proliferate.\" The researchers revealed that acetylcholine and GABA are the two neurotransmitters that can directly affect the stem cells in the adult zebrafish spinal cord; however, they act antagonistically to each other. \"To identify the neurons that provide the cholinergic input to activate the stem cells was among the most unexpected findings. We found that a particular type of spinal locomotor interneurons, named V2a's, is among the neurons that link locomotion and stem cell activation,\" Konstantinos Ampatzis continues. How might your findings be put to use? \"The overall outcome is a comprehensive understanding of the plasticity and adaptations (mechanisms, structural changes) that develop in response to physical activity and how these adaptive phenomena underlie pathogenicity after injury and\/or regeneration of spinal networks. The results are expected to have a substantial impact because they lay the groundwork for developing new, more effective targeted treatments for restoration of the spinal cord after injury,\" Konstantinos Ampatzis explains. The study involved a set of different methodologies in neuroscience, such as anatomy, electrophysiology, pharmacology, and behavior in the adult zebrafish. In their experiments, the researchers took advantage of the experimental amenability of the adult zebrafish. \"This model animal is ideal for these studies. It has an anatomically simple nervous system yet possesses all vertebrate features. It offers unprecedented access to neuronal circuits in behaving animals, and it has a rare ability to regenerate after injury.\" What is your next step? \"Our next step is to identify the type of neurons that are born under homeostasis, training and spinal cord injury. We need to identify if the new neurons replace the existing ones or if they act as add-ons on the spinal cord networks,\" says Konstantinos Ampatzis. ","News_Title":"Unexpected functions of the spinal locomotor network","Topic":"Biology"}
{"Paper_Body":"Abstract Group 3 innate lymphoid cells (ILC3s) are major regulators of inflammation, infection, microbiota composition and metabolism 1 . ILC3s and neuronal cells have been shown to interact at discrete mucosal locations to steer mucosal defence 2 , 3 . Nevertheless, it is unclear whether neuroimmune circuits operate at an organismal level, integrating extrinsic environmental signals to orchestrate ILC3 responses. Here we show that light-entrained and brain-tuned circadian circuits regulate enteric ILC3s, intestinal homeostasis, gut defence and host lipid metabolism in mice. We found that enteric ILC3s display circadian expression of clock genes and ILC3-related transcription factors. ILC3-autonomous ablation of the circadian regulator Arntl led to disrupted gut ILC3 homeostasis, impaired epithelial reactivity, a deregulated microbiome, increased susceptibility to bowel infection and disrupted lipid metabolism. Loss of ILC3-intrinsic Arntl shaped the gut \u2018postcode receptors\u2019 of ILC3s. Strikingly, light\u2013dark cycles, feeding rhythms and microbial cues differentially regulated ILC3 clocks, with light signals being the major entraining cues of ILC3s. Accordingly, surgically or genetically induced deregulation of brain rhythmicity led to disrupted circadian ILC3 oscillations, a deregulated microbiome and altered lipid metabolism. Our work reveals a circadian circuitry that translates environmental light cues into enteric ILC3s, shaping intestinal health, metabolism and organismal homeostasis.     Main ILC3s have been shown to be part of discrete mucosal neuroimmune cell units 2 , 3 , 4 , 5 , raising the hypothesis that ILC3s may also integrate systemic neuroimmune circuits to regulate tissue integrity and organismic homeostasis. Circadian rhythms rely on local and systemic cues to coordinate mammalian physiology and are genetically encoded by molecular clocks that allow organisms to anticipate and adapt to extrinsic environmental changes 6 , 7 . The circadian clock machinery consists of an autoregulatory network of feedback loops primarily driven by the activators ARNTL and CLOCK and the repressors PER1\u2013PER3, CRY1 and CRY2, amongst others 6 , 7 . Analysis of subsets of intestinal ILCs and their bone marrow progenitors revealed that mature ILC3s express high levels of circadian clock genes (Fig. 1a\u2013c , Extended Data Fig. 1a\u2013d ). Notably, ILC3s displayed a circadian pattern of Per1 Venus expression (Fig. 1b ) and transcriptional analysis of ILC3 revealed circadian expression of master clock regulators and ILC3-related transcription factors (Fig. 1c ). To test whether ILC3s are regulated in a circadian manner, we investigated whether intestinal ILC3s require intrinsic clock signals. Thus, we interfered with the expression of the master circadian activator Arntl . Arntl fl mice were bred to Vav1 Cre mice, allowing conditional deletion of Arntl in all haematopoietic cells ( Arntl \u0394Vav1 mice). Although Arntl \u0394Vav1 mice displayed normal numbers of intestinal natural killer (NK) cells and enteric group 1 and 2 ILCs, gut ILC3s were severely and selectively reduced in these mice when compared to their wild-type littermate controls (Fig. 1d, e , Extended Data Fig. 2a, b ). To more precisely define ILC3-intrinsic effects, we generated mixed bone marrow chimaeras by transferring Arntl -competent ( Arntl fl ) or Arntl -deficient ( Arntl \u0394Vav1 ) bone marrow against a third-party wild-type competitor into alymphoid hosts (Fig. 1f ). Analysis of such chimaeras confirmed cell-autonomous circadian regulation of ILC3s, while their innate and adaptive counterparts were unperturbed (Fig. 1g , Extended Data Fig. 2c ). Fig. 1: Intestinal ILC3s are controlled in a circadian manner. a , Gene expression in CLPs, ILCPs and intestinal ILC3s. CLP and ILCP n = 4; ILC3 n = 6. b , PER1\u2013VENUS mean fluorescence intensity (MFI). CLP and ILCP n = 6; ILC3 n = 4. c , Circadian gene expression in enteric ILC3s; n = 5. d , Intestinal ILC subsets in Arntl fl and Arntl \u0394Vav1 mice; n = 4. e , Cell numbers of intestinal ILC3s and IL-17- and IL-22-producing ILC3 subsets in Arntl fl and Arntl \u0394Vav1 mice; n = 4. f , Generation of mixed bone marrow chimaeras. g , Percentage of donor cells and cell numbers of ILC3s, IL-17 and IL-22-producing ILC3 subsets in the gut from mixed bone marrow chimaeras. Arntl fl n = 5, Arntl \u0394Vav1 n = 7. b , c , White and grey represent light and dark periods, respectively. Data are representative of three independent experiments. n represents biologically independent samples ( a , c ) or animals ( b , d \u2013 g ). Data shown as mean \u00b1 s.e.m. a , Two-way ANOVA and Tukey\u2019s test; b , c , cosinor analysis; d , e , g , Two-tailed Mann\u2013Whitney U test. * P < 0.05; ** P < 0.01; *** P < 0.001; NS, not significant. Source Data . Full size image To investigate the functional effect of ILC3-intrinsic circadian signals, we deleted Arntl in ROR\u03b3t-expressing cells by breeding Rorgt Cre mice (also known as Rorc Cre ) to Arntl fl mice ( Arntl \u0394Rorgt mice). When compared to their wild-type littermate controls, Arntl \u0394Rorgt mice showed a selective reduction of ILC3 subsets and IL-17- and IL-22-producing ILC3s (Fig. 2a, b , Extended Data Fig. 3a\u2013j ). Notably, independent deletion of Nr1d1 also perturbed subsets of enteric ILC3s, further supporting a role of the clock machinery in ILC3s (Extended Data Fig. 4a\u2013e ). ILC3s have been shown to regulate the expression of genes related to epithelial reactivity and microbial composition 1 . Analysis of Arntl fl and Arntl \u0394Rorgt mice revealed a profound reduction in the expression of reactivity genes in the Arntl \u0394Rorgt intestinal epithelium; notably, Reg3b , Reg3g , Muc3 and Muc13 were consistently reduced in Arntl -deficient mice (Fig. 2c ). Furthermore, Arntl \u0394Rorgt mice displayed altered diurnal patterns of Proteobacteria and Bacteroidetes (Fig. 2d , Extended Data Fig. 3j ). To investigate whether disruption of ILC3-intrinsic ARNTL affected enteric defence, we tested how Arntl \u0394Rorgt mice responded to intestinal infection. To this end, we bred Arntl \u0394Rorgt mice to Rag1 \u2212\/\u2212 mice to exclude putative T cell effects (Extended Data Fig. 3g\u2013i ). Rag1 \u2212\/\u2212 Arntl \u0394Rorgt mice were infected with the attaching and effacing bacteria Citrobacter rodentium 2 . When compared to their wild-type littermate controls, Rag1 \u2212\/\u2212 Arntl \u0394Rorgt mice had marked gut inflammation, fewer IL-22-producing ILC3s, increased C. rodentium infection and bacterial translocation, reduced expression of epithelial reactivity genes, increased weight loss and reduced survival (Fig. 2e\u2013j , Extended Data Fig. 5a\u2013j ). These results indicate that cell-intrinsic circadian signals selectively control intestinal ILC3s and shape gut epithelial reactivity, microbial communities and enteric defence. Previous studies indicated that ILC3s regulate host lipid metabolism 8 . When compared to their wild-type littermate controls, the epithelium of Arntl \u0394Rorgt mice revealed a marked increase in mRNA that codes for key lipid epithelial transporters, including Fabp1 , Fabp2 , Scd1, Cd36 and Apoe (Fig. 2k ). Accordingly, these changes were associated with increased gonadal and subcutaneous accumulation of fat in Arntl \u0394Rorgt mice when compared to their wild-type littermate controls (Fig. 2l , Extended Data Fig. 5k\u2013n ). Thus, ILC3-intrinsic circadian signals shape epithelial lipid transport and body fat composition. Fig. 2: ILC3-intrinsic Arntl regulates gut homeostasis and defence. a , Enteric ILC3s and subtypes in Arntl fl and Arntl \u0394Rorgt mice; n = 4. b , Gut T helper cells in Arntl fl and Arntl \u0394Rorgt mice; n = 5. c , Expression of epithelial reactivity genes in Arntl \u0394Rorgt mice compared with Arntl fl mice; n = 5. d , qPCR analysis of Proteobacteria in stools from Arntl fl and Arntl \u0394Rorgt mice (see Methods ). Arntl fl n = 5; Arntl \u0394Rorgt n = 6. e\u2013j , Data from C. rodentium -infected Rag1 \u2212\/\u2212 Arntl fl and Rag1 \u2212\/\u2212 Arntl \u0394Rorgt mice. e , Histopathology of colon sections; n = 5. f , Colitis score; n = 5. g , Colon length; n = 5. h , Infection burden; Rag1 \u2212\/\u2212 Arntl fl n = 6, Rag1 \u2212\/\u2212 Arntl \u0394Rorgt n = 7. i , Bacterial translocation to the spleen; Rag1 \u2212\/\u2212 Arntl fl n = 6, Rag1 \u2212\/\u2212 Arntl \u0394Rorgt n = 7. j , Survival; n = 5. k , Expression of epithelial lipid transporter genes in Arntl fl ( n = 4) and Arntl \u0394Rorgt ( n = 5) mice. l , Gonadal and subcutaneous adipose tissue in Arntl fl and Arntl \u0394Rorgt mice; n = 5. d , White and grey represent light and dark periods, respectively. Scale bars, 250 \u03bcm. Data are representative of at least three independent experiments; n represents biologically independent animals. Data shown as mean \u00b1 s.e.m. a , b , f , g , i , k , Two-tailed Mann\u2013Whitney U test; d , cosinor analysis; h , two-way ANOVA and Sidak\u2019s test; j , log-rank test; l , two-tailed unpaired Student\u2019s t -test. * P < 0.05; ** P < 0.01; *** P < 0.001; NS, not significant. Source Data . Full size image To further investigate how cell-intrinsic Arntl controls intestinal ILC3 homeostasis, initially we studied the diurnal oscillations of the ILC3 clock machinery. When compared to their wild-type littermate controls, Arntl \u0394Rorgt ILC3s displayed a disrupted diurnal pattern of activator and repressor circadian genes (Fig. 3a ). Sequentially, we used genome-wide transcriptional profiling of Arntl- sufficient and -deficient ILC3s to interrogate the effect of a deregulated circadian machinery. Diurnal analysis of the genetic signature associated with ILC3 identity 1 demonstrated that the vast majority of those genes were unperturbed in Arntl -deficient ILC3s, suggesting that ARNTL is dispensable to ILC3 lineage commitment (Fig. 3b , Extended Data Fig. 6a\u2013c ). To test this hypothesis, we first studied the effect of ablation of Arntl in ILC3 progenitors. Arntl \u0394Vav1 mice had unperturbed numbers of common lymphoid progenitors (CLPs) and innate lymphoid cell progenitors (ILCPs; Fig. 3c , Extended Data Fig. 6d ). Sequentially, we analysed the effects of Arntl ablation in ILC3s in other organs. Compared to their littermate controls, Arntl \u0394Rorgt mice had normal numbers of ILC3s in the spleen, lungs and blood, in contrast to their pronounced reduction in the intestine (Figs. 2a , 3d, e , Extended Data Fig. 6e ). Notably, enteric Arntl \u0394Rorgt ILC3s showed unperturbed proliferation and apoptosis-related genetic signatures (Extended Data Fig. 6b, c ), suggesting that Arntl \u0394Rorgt ILC3s may show altered migration to the intestinal mucosa 9 . When compared to their wild-type littermate controls, ILC3s in Arntl \u0394Rorgt mice showed a marked reduction in gut postcode molecules\u2014which are essential receptors for intestinal lamina propria homing\u2014and accumulated in mesenteric lymph nodes 9 (Extended Data Fig. 6f ). Notably, the expression of the integrin and chemokine receptors CCR9, \u03b14\u03b27 and CXCR4 was selectively and hierarchically reduced in Arntl \u0394Rorgt ILC3s (Fig. 3f\u2013h , Extended Data Fig. 6g\u2013m ). To investigate whether ARNTL could directly regulate expression of Ccr9 , we performed chromatin immunoprecipitation (ChIP). Binding of ARNTL to the Ccr9 locus in ILC3s followed a diurnal pattern, with increased binding at Zeitgeber time (ZT) 5 (Fig. 3i ). Thus, ARNTL can contribute directly to the expression of Ccr9 in ILC3s, although additional factors may also regulate this gene. In conclusion, while a fully operational ILC3-intrinsic circadian machinery is not required for lineage commitment and development of ILC3s, cell-intrinsic clock signals are required for a functional ILC3 gut receptor postcode. Fig. 3: ILC3-intrinsic circadian signals regulate an enteric receptor postcode. a , Relative expression of circadian genes in enteric ILC3s from Arntl fl and Arntl \u0394Rorgt mice; n = 3. b , RNA sequencing (RNA-seq) analysis of gut ILC3s from Arntl fl and Arntl \u0394Rorgt mice at ZT5 and ZT23; n = 3. c , Numbers of CLPs and ILCPs in Arntl fl and Arntl \u0394Vav1 mice; n = 4. d , e , ILC3s in spleen ( d , n = 3) and lung ( e , n = 6) of Arntl fl and Arntl \u0394Rorgt mice. f , g , Expression of \u03b14\u03b27 ( f ) and CCR9 ( g ) by gut ILC3s in Arntl fl and Arntl \u0394Rorgt mice; n = 4. h , Circadian variation in expression of CCR9 by intestinal ILC3s in Arntl fl and Arntl \u0394Rorgt mice; n = 4. i , ChIP analysis of binding of ARNTL to the Ccr9 locus in enteric ILC3s; n = 3. A\u2013J denote putative ARNTL DNA-binding sites. Data are representative of three independent experiments. n represents biologically independent animals ( a , c \u2013 h ) or samples ( b , i ). a , h , White and grey represent light and dark periods, respectively. Data shown as mean \u00b1 s.e.m. a , Two-way ANOVA; c \u2013 g , two-tailed Mann\u2013Whitney U test; h , cosinor analysis; i , two-tailed unpaired Student\u2019s t -test. * P < 0.05; ** P < 0.01; *** P < 0.001; NS, not significant. Source Data . Full size image Circadian rhythms allow organisms to adapt to extrinsic environmental changes. Microbial cues can alter the rhythms of intestinal cells 10 , 11 , and feeding regimens are major circadian entraining cues for peripheral organs, such as the liver 12 . In order to define the environmental cues that entrain circadian oscillations of ILC3, we initially investigated whether microbial signals affect the oscillations of ILC3s. Treatment of Per1 Venus reporter mice with antibiotics did not alter the amplitude of circadian oscillations, but did induce a minute shift in the acrophase (timing of the peak of the cycle; Fig. 4a ). We then tested whether feeding regimens, which are major entraining cues of oscillations in the liver, pancreas, kidney, and heart 12 , could alter ILC3 rhythms. To this end, we restricted food access to a 12-h interval and compared Per1 Venus oscillations to those observed in mice with inverted feeding regimens 12 . Inverted feeding had a small effect on the amplitude of ILC3 oscillations but did not invert the acrophase of ILC3s (Fig. 4b , Extended Data Fig. 7a ), in contrast to the full inversion of the acrophase of hepatocytes 12 (Extended Data Fig. 7b ). As these local intestinal cues could not invert the acrophase of ILC3s, we hypothesize that light\u2013dark cycles are major regulators of enteric ILC3 oscillations 6 . To test this hypothesis, we placed Per1 Venus mice in light-tight cabinets on two opposing 12-h light\u2013dark cycles. Inversion of light\u2013dark cycles had a profound effect on the circadian oscillations of ILC3s (Fig. 4c ). Notably, and in contrast to microbiota and feeding regimens, light cycles fully inverted the acrophase of Per1 Venus oscillations in ILC3s (Fig. 4c , Extended Data Fig. 7c ). Furthermore, light\u2013dark cycles entrained ILC3 oscillations, as revealed by their maintenance upon removal of light (constant darkness; Fig. 4d , Extended Data Fig. 7d ), confirming that light is a major environmental entraining signal for ILC3 intrinsic oscillations. Together, these data indicate that ILC3s integrate systemic and local cues hierarchically; while microbiota and feeding regimens locally adjust the ILC3 clock, light\u2013dark cycles are major entraining cues of ILC3s, fully setting and entraining their intrinsic oscillatory clock. Fig. 4: Light-entrained and brain-tuned cues shape intestinal ILC3s. a \u2013 d , PER1\u2013VENUS MFI in gut ILC3s from mice treated with or without antibiotics (Ab) ( a ; n = 3); with restricted or inverted feeding ( b ; n = 3); with opposing light\u2013dark cycles ( c ; n = 3); and with opposing light\u2013dark cycles followed by constant darkness ( d ; n = 3). e , Magnetic resonance imaging of sham- and SCN-ablated (xSCN) mice; n = 11. White arrows indicate location of lesion. f , PER1\u2013VENUS MFI in enteric ILC3s from sham- or SCN-ablated mice; n = 3. g , Expression of circadian genes in enteric ILC3s from Arntl fl and Arntl \u0394Camk2a mice; n = 3. h , CCR9 expression in gut ILC3s from Arntl fl and Arntl \u0394Camk2a mice; n = 3. i , Expression of epithelial reactivity genes in the small intestine from Arntl fl and Arntl \u0394Camk2a mice; n = 3. j , qPCR analysis of Proteobacteria in stools from Arntl fl and Arntl \u0394Camk2a mice; n = 4. k , Expression of lipid transporter genes in the epithelium of the small intestine in Arntl fl and Arntl \u0394Camk2a mice; n = 3. l , Gonadal and subcutaneous adipose tissue in Arntl fl mice ( n = 5) and Arntl \u0394Camk2a mice ( n = 4). a , b , White and grey represent light and dark periods, respectively. Data shown as mean \u00b1 s.e.m. n represents biologically independent animals. a \u2013 d , f \u2013 k , Cosinor analysis; f \u2013 k , cosine fitted curves; amplitude (Amp) and acrophase (Acro) were extracted from the cosinor model. l , Two-tailed unpaired Student\u2019s t -test. * P < 0.05; ** P < 0.01; *** P < 0.001; NS, not significant. Source Data . Full size image The suprachiasmatic nuclei (SCN) in the hypothalamus are main integrators of light signals 6 , suggesting that brain cues may regulate ILC3s. To assess the influence of the master circadian pacemaker on ILC3s, while excluding confounding light-induced, SCN-independent effects 13 , 14 , we performed SCN ablation by electrolytic lesion in Per1 Venus mice using stereotaxic brain surgery 15 . Strikingly, whereas sham-operated mice displayed circadian Per1 Venus oscillations in ILC3s, ILC3s in SCN-ablated mice lost the circadian rhythmicity of Per1 Venus and other circadian genes (Fig. 4e, f , Extended Data Fig. 8a\u2013d ). Because electrolytic lesions of the SCN may cause scission of afferent and efferent fibres in the SCN, we further confirmed that brain SCN-derived cues control ILC3s by genetic ablation of Arntl in the SCN 14 . Arntl fl mice were bred to Camk2a Cre mice to allow forebrain- and SCN-specific deletion of Arntl ( Arntl \u0394Camk2a ) 14 . When compared to their control counterparts, ILC3s from Arntl \u0394Camk2a mice showed severe arrhythmicity of circadian regulatory genes and of the enteric postcode molecule CCR9 (Fig. 4g, h , Extended Data Fig. 9a\u2013f ). In addition, Arntl \u0394Camk2a mice showed alterations in epithelial reactivity genes and microbial communities, particularly Proteobacteria and Bacteroidetes (Fig. 4i, j , Extended Data Fig. 9g\u2013i ). Finally, the intestinal epithelium of Arntl \u0394Camk2a mice showed disrupted circadian expression of lipid epithelial transporters, and these changes were associated with increased gonadal and subcutaneous fat accumulation (Fig. 4k, l ). Together, these data indicate that light-entrained and brain-tuned circuits regulate enteric ILC3s, controlling microbial communities, lipid metabolism and body composition. Deciphering the mechanisms by which neuroimmune circuits operate to integrate extrinsic and systemic signals is essential for understanding tissue and organ homeostasis. We found that light cues are major extrinsic entraining cues of ILC3 circadian rhythms, and surgically or genetically induced deregulation of brain rhythmicity resulted in altered ILC3 regulation. In turn, the ILC3-intrinsic circadian machinery controlled the gut receptor postcode of ILC3s, shaping enteric ILC3s and host homeostasis. Our data reveal that ILC3s display diurnal oscillations that are genetically encoded, cell-autonomous and entrained by light cues. While microbiota and feeding regimens could locally induce small adjustments to ILC3 oscillations, light\u2013dark cycles were major entraining cues of the ILC3 circadian clock. Whether the effects of photonic signals on ILC3s are immediate or rely on other peripheral clocks remains to be elucidated 16 , 17 . Nevertheless, cell-intrinsic ablation of important endocrine and peripheral neural signals in ILC3s did not affect gut ILC3 numbers (Extended Data Fig. 10a-i ). Our work indicates that ILC3s integrate local and systemic entraining cues in a distinct hierarchical manner, establishing an organismal circuitry that is an essential link between the extrinsic environment, enteric ILC3s, gut defence, lipid metabolism and host homeostasis (Extended Data Fig. 10j ). Previous studies demonstrated that ILCs integrate tissue microenvironmental signals, including cytokines, micronutrients and neuroregulators 3 , 4 , 18 , 19 . Here we show that ILC3s have a cell-intrinsic circadian clock that integrates extrinsic light-entrained and brain-tuned signals. Coupling light cues to ILC3 circadian regulation may have ensured efficient and integrated multi-system anticipatory responses to environmental changes. Notably, the regulation of ILC3 activity by systemic circadian circuits may have evolved to maximize metabolic homeostasis, gut defence and efficient symbiosis with commensal organisms that have been evolutionary partners of mammals. Finally, our current data may also contribute to a better understanding of how circadian disruptions in humans are associated with metabolic diseases, bowel inflammatory conditions and cancer 20 . Methods Mice Nod scid gamma (NSG) mice were purchased from Jackson Laboratories. C57BL\/6J Ly5.1 mice were purchased from Jackson Laboratories and bred with C57BL\/6J mice to obtain C57BL\/6 Ly5.1\/Ly5.2 (CD45.1\/CD45.2). Mouse lines used were: Rag1 \u2212\/\u2212 (ref. 21 ), Rag2 \u2212\/\u2212 Il2rg \u2212\/\u2212 (ref. 22 , 23 ), Vav1 Cre (ref. 24 ), Rorgt Cre (ref. 25 ), Camk2a Cre (ref. 26 ), Il7ra Cre (ref. 27 ), Per1 Venus (ref. 28 ), Ret GFP (ref. 29 ), Rosa26 RFP (ref. 30 ), Nr1d1 \u2212\/\u2212 (ref. 31 ), Arntl fl (ref. 32 ), Nr3c1 fl (ref. 33 ) and Adrb2 fl (ref. 34 ). All mouse lines were on a full C57BL\/6J background. All lines were bred and maintained at Champalimaud Centre for the Unknown (CCU) animal facility under specific pathogen-free conditions. Male and female mice were used at 8\u201314 weeks old, unless stated otherwise. Sex- and age-matched mice were used for analysis of small intestine epithelium lipid transporters and quantification of white adipose tissue. Mice were maintained in 12-h light\u2013dark cycles, with ad libitum access to food and water, if not specified otherwise. For light inversion experiments mice were housed in ventilated, light-tight cabinets on defined 12-h light\u2013dark cycles (Ternox). Camk2a Cre Arntl fl ( Arntl \u0394Camk2a ) mice and wild-type littermate controls were maintained in constant darkness as previously described 14 . Mice were systematically compared with co-housed littermate controls unless stated otherwise. Power analysis was performed to estimate the number of experimental mice required. All animal experiments were approved by national and local institutional review boards (IRBs), Dire\u00e7\u00e3o Geral de Veterin\u00e1ria and CCU ethical committees. Randomization and blinding were not used unless stated otherwise. Cell isolation Isolation of small intestine and colonic lamina propria cells was as previously described 2 . In brief, intestines and colons were thoroughly rinsed with cold PBS1\u00d7, Peyer patches were removed from the small intestine, and intestines and colons were cut into 1-cm pieces and shaken for 30 min in PBS containing 2% FBS, 1% HEPES and 5 mM EDTA to remove intraepithelial and epithelial cells. Intestines and colons were then digested with collagenase D (0.5 mg\/ml; Roche) and DNase I (20 U\/ml; Roche) in complete RPMI for 30 min at 37 \u00b0C, under gentle agitation. Cells were passed through a 100-\u03bcm cell strainer and purified by centrifugation for 30 min at 2,400 rpm in a 40\/80 Percoll (GE Healthcare) gradient. Lungs were finely minced and digested in complete RPMI supplemented with collagenase D (0.1 mg\/ml; Roche) and DNase I (20 U\/ml; Roche) for 1 h at 37 \u00b0C under gentle agitation. Cells were passed through a 100-\u03bcm cell strainer and purified by centrifugation for 30 min at 2,400 rpm in a 40\/80 Percoll (GE Healthcare) gradient. Spleen and mesenteric lymph node cell suspensions were obtained using 70-\u03bcm strainers. Bone marrow cells were collected by either flushing or crushing bones and filtered using 70-\u03bcm strainers. Erythrocytes from small intestine, colon, lung, spleen and bone marrow preparations were lysed with RBC lysis buffer (eBioscience). Leukocytes from blood were isolated by treatment with Ficoll (GE Healthcare). Flow cytometry analysis and cell sorting For cytokine analysis ex vivo, cells were incubated with PMA (phorbol 12-myristate 13-acetate; 50 ng\/ml) and ionomycin (500 ng\/ml) (Sigma-Aldrich) in the presence of brefeldin A (eBioscience) for 4 h before intracellular staining. Intracellular staining for cytokines and transcription factors analysis was performed using IC fixation and Staining Buffer Set (eBioscience). Cell sorting was performed using FACSFusion (BD Biosciences). Sorted populations were >95% pure. Flow cytometry analysis was performed on LSRFortessa X-20 (BD Biosciences). Data were analysed using FlowJo 8.8.7 software (Tree Star). Cell populations were gated in live cells, both for sorting and flow cytometry analysis. Cell populations Cell populations were defined as: bone marrow (BM) common lymphoid progenitor (CLP): Lin \u2212 CD127 + Flt3 + Sca1 int c-Kit int ; BM innate lymphoid cell progenitor (ILCP): Lin \u2212 CD127 + Flt3 \u2212 CD25 \u2212 c-Kit + \u03b14\u03b27 high ; BM ILC2 progenitor (ILC2P): Lin \u2212 CD127 + Flt3 \u2212 Sca1 + CD25 + ; small intestine (SI) NK: CD45 + Lin \u2212 NK1.1 + NKp46 + CD27 + CD49b + CD127 \u2212 EOMES + or CD45 + Lin \u2212 NK1.1 + NKp46 + CD27 + CD49b + CD127 \u2212 ; small intestine ILC1: CD45 + Lin \u2212 NK1.1 + NKp46 + CD27 + CD49b \u2212 CD127 + Tbet + or CD45 + Lin \u2212 NK1.1 + NKp46 + CD27 + CD49b \u2212 CD127 + ; small intestine ILC2: CD45 + Lin \u2212 Thy1.2 + KLRG1 + GATA3 + or CD45 + Lin \u2212 Thy1.2 + KLRG1 + Sca-1 + CD25 + ; lamina propria, spleen, mesenteric lymph node and lung ILC3: CD45 + Lin \u2212 Thy1.2 high ROR\u03b3t + or CD45 + Lin \u2212 Thy1.2 high KLRG1 \u2212 ; ILC3-IL-17 + : CD45 + Lin \u2212 Thy1.2 high ROR\u03b3t + IL-17 + ; ILC3-IL-22 + : CD45 + Lin \u2212 Thy1.2 high ROR\u03b3t + IL-22 + ; for ILC3 subsets additional markers were used: ILC3-NCR \u2212 CD4 \u2212 : NKp46 \u2212 CD4 \u2212 ; ILC3-LTi CD4 + : NKp46 \u2212 CD4 + ; ILC3-CCR6 \u2212 NCR \u2212 : CCR6 \u2212 NKp46 \u2212 ; ILC3-LTi-like: CCR6 + NKp46 \u2212 ; ILC3-NCR + : NKp46 + ; SI Th17 cells: CD45 + Lin + Thy1.2 + CD4 + ROR\u03b3t + ; colon Tregs: CD45 + CD3 + Thy1.2 + CD4 + CD25 + FOXP3 + ; colon Tregs ROR\u03b3t + : CD45 + CD3 + Thy1.2 + CD4 + CD25 + FOXP3 + ROR\u03b3t + . The lineage cocktail for BM, lung, small intestine lamina propria, spleen and mesenteric lymph nodes included CD3\u025b, CD8\u03b1, CD19, B220, CD11c, CD11b, Ter119, Gr1, TCR\u03b2, TCR\u03b3\u03b4 and NK1.1. For NK and ILC1 staining in the small intestine, NK1.1 and CD11b were not added to the lineage cocktail. Antibody list Cell suspensions were stained with: anti-CD45 (30-F11); anti-CD45.1 (A20); anti-CD45.2 (104); anti-CD11c (N418); anti-CD11b (Mi\/70); anti-CD127 (IL7R\u03b1; A7R34); anti-CD27(LG.7F9); anti-CD8\u03b1 (53-6.7); anti-CD19 (eBio1D3); anti-CXCR4(L276F12); anti-NK1.1 (PK136); anti-CD3\u025b (eBio500A2); anti-TER119 (TER-119); anti-Gr1 (RB6-8C5); anti-CD4 (RM4-5); anti-CD25 (PC61); anti-CD117 (c-Kit; 2B8); anti-CD90.2 (Thy1.2; 53-2.1); anti-TCR\u03b2 (H57-595); anti-TCR\u03b3\u03b4 (GL3); anti-B220 (RA3-6B2); anti-KLRG1 (2F1\/KLRG1); anti-Ly-6A\/E (Sca1; D7); anti-CCR9 (CW-1.2); anti-IL-17 (TC11-18H10.1); anti-rat IgG1k isotype control (RTK2071); anti-streptavidin fluorochrome conjugates from Biolegend; anti-\u03b14\u03b27 (DATK32); anti-Flt3 (A2F10); anti-NKp46 (29A1.4); anti-CD49b (DX5); anti-Ki67 (SolA15); anti-rat IgG2ak isotype control (eBR2a); anti-IL-22 (1H8PWSR); anti-rat IgG1k isotype control (eBRG1); anti-EOMES (Dan11mag); anti-Tbet (eBio4B10); anti-FOPX3 (FJK-16s); anti-GATA3 (TWAJ); anti-CD16\/CD32 (93); 7AAD viability dye from eBiosciences; anti-CD196 (CCR6; 140706) from BD Biosciences; anti-ROR\u03b3t (Q31-378) and anti-mouse IgG2ak isotype control (G155-178) from BD Pharmingen. LIVE\/DEAD Fixable Aqua Dead Cell Stain Kit was purchased from Invitrogen. Bone marrow transplantation Bone marrow CD3 \u2212 cells were FACS sorted from Arntl fl , Vav1 Cre Arntl fl , Rag1 \u2212\/\u2212 Arntl fl , Rag1 \u2212\/\u2212 Rorgt Cre Arntl fl , Nr1d1 +\/+ , Nr1d1 \u2212\/\u2212 and C57BL\/6 Ly5.1\/Ly5.2 mice. Sorted cells (2 \u00d7 10 5 ) from Arntl - or Nr1d1 -deficient and -competent wild-type littermate controls were intravenously injected in direct competition with a third-party wild-type competitor (CD45.1\/CD45.2), in a 1:1 ratio, into non-lethally irradiated NSG (150cGy) or Rag2 \u2212\/\u2212 Il2rg \u2212\/\u2212 (500cGy) mice (CD45.1). Recipients were analysed 8 weeks after transplantation. Quantitative RT\u2013PCR RNA from sorted cells was extracted using RNeasy micro kit (Qiagen) according to the manufacturer\u2019s protocol. Liver, small intestine (ileum) and colon epithelium was collected for RNA extraction using Trizol (Invitrogen) and zirconia\/silica beads (BioSpec) in a bead beater (MIDSCI). RNA concentration was determined using Nanodrop Spectrophotometer (Nanodrop Technologies). For TaqMan assays (Applied Biosystems) RNA was retro-transcribed using a High Capacity RNA-to-cDNA Kit (Applied Biosystems), followed by a pre-amplification PCR using TaqMan PreAmp Master Mix (Applied Biosystems). TaqMan Gene Expression Master Mix (Applied Biosystems) was used in real-time PCR. Real-time PCR analysis was performed using StepOne and QuantStudio 5 Real-Time PCR systems (Applied Biosystems). Hprt , Gapdh and Eef1a1 were used as housekeeping genes. When multiple endogenous controls were used, these were treated as a single population and the reference value calculated by arithmetic mean of their CT values. The mRNA analysis was performed as previously described 35 . In brief, we used the comparative C T method (2 \u2212\u0394CT ), in which \u0394 C T(gene of interest) = C T(gene of interest) \u2212 C T(housekeeping reference value) . When fold change comparison between samples was required, the comparative \u0394 C T method (2 \u2212\u0394\u0394CT ) was applied. TaqMan gene expression assays TaqMan Gene Expression Assays (Applied Biosystems) were the following: Hprt Mm00446968_m1; Gapdh Mm99999915_g1; Eef1a1 Mm01973893_g1; Arntl Mm00500223_m1; Clock Mm00455950_m1; Nr1d1 Mm00520708_m1; Nr1d2 Mm01310356_g1; Per1 Mm00501813_m1; Per2 00478113_m1; Cry1 Mm00500223_m1; Cry2 Mm01331539_m1; Runx1 Mm01213404_m1 ; Tox Mm00455231_m1; Rorgt Mm01261022_m1; Ahr Mm00478932_m1; Rora Mm01173766_m1; Ccr9 Mm02528165_s1; Reg3a Mm01181787_m1; Reg3b Mm00440616_g1; Reg3g Mm00441127_m1; Muc1 Mm00449604_m1; Muc2 Mm01276696_m1; Muc3 Mm01207064_m1; Muc13 Mm00495397_m1; S100a8 Mm01276696_m1; S100a9 Mm00656925_m1; Epcam Mm00493214_m1; Apoe Mm01307193_g1; Cd36 Mm01307193_g1; Fabp1 Mm00444340_m1; Fabp2 Mm00433188_m1; and Scd1 Mm00772290_m1. Quantitative PCR analysis of bacteria in stools at the phylum level DNA from faecal pellets of female mice was isolated with ZR Fecal DNA MicroPrep (Zymo Research). Quantification of bacteria was determined from standard curves established by qPCR as previously described 2 . qPCRs were performed with NZY qPCR Green Master Mix (Nzytech) and different primer sets using a QuantStudio 5 Real-Time PCR System (Applied Biosystems) thermocycler. Samples were normalized to 16S rDNA and reported according to the 2 \u2212\u0394CT method. Primer sequences were: 16S rDNA, F-ACTCCTACGGGAGGCAGCAGT and R-ATTACCGCGGCTGCTGGC; Bacteroidetes, F-GAGAGGAAGGTCCCCCAC and R-CGCTACTTGGCTGGTTCAG; Proteobacteria, F-GGTTCTGAGAGGAGGTCCC and R-GCTGGCTCCCGTAGGAGT; Firmicutes, F-GGAGCATGTGGTTTAATTCGAAGCA and R-AGCTGACGACAACCATGCAC. C. rodentium infection Infection with C. rodentium ICC180 (derived from DBS100 strain) 36 was performed at ZT6 by gavage inoculation with 10 9 colony-forming units (CFUs) 36 , 37 . Acquisition and quantification of luciferase signal was performed in an IVIS Lumina III System (Perkin Elmer). Throughout infection, weight loss, diarrhoea and bloody stools were monitored daily. CFU measurement Bacterial translocation was determined in the spleen, liver, and mesenteric lymph nodes, taking in account total bacteria and luciferase-positive C. rodentium . Organs were removed, weighed and brought into suspension. Bacterial CFUs from organ samples were determined via serial dilutions on Luria broth (LB) agar (Invitrogen) and MacConkey agar (Sigma-Aldrich). Colonies were counted after 2 days of culture at 37 \u00b0C. Luciferase-positive C. rodentium was quantified on MacConkey agar plates using an IVIS Lumina III System (Perkin Elmer). CFUs were determined per volume (ml) for each organ. Antibiotic and dexamethasone treatment Pregnant females and newborn mice were treated with streptomycin (5 g\/l), ampicillin (1 g\/l) and colistin (1 g\/l) (Sigma-Aldrich) in drinking water with 3% sucrose. Control mice were given 3% sucrose in drinking water as previously described 38 . Dexamethasone 21-phosphate disodium salt (200 \u03bcg) (Sigma) or PBS was injected intraperitoneally at ZT0. After 4, 8, 12 and 23 h (ZT 4, 8, 12 and 23) mice were killed and analysed. ChIP assay Enteric ILC3s from adult C57BL\/6J mice were isolated by flow cytometry. Cells were fixed, cross-linked and lysed, and chromosomal DNA\u2013protein complexes were sonicated to generate DNA fragments ranging from 200 to 400 base pairs as previously described 2 . DNA\u2013protein complexes were immunoprecipitated using LowCell# ChIP kit (Diagenode), with 1 \u03bcg of antibody against ARNTL (Abcam) and IgG isotype control (Abcam). Immunoprecipitates were uncrosslinked and analysed by qPCR using primer pairs flanking ARNTL putative sites (E-boxes) in the Ccr9 locus (determined by computational analysis using TFBS tools and Jaspar 2018). Results were normalized to input intensity and control IgG. Primer sequences were: A: F-CATTTCATAGCTTAGGCTGGCATGG; R-CTAGCTAACTGGTCTCAAAGTCCTC; B: F-GCCTCCCTTGTACTACCTG AAGC; R-TCCCAACACCAGGCCGAGTA; C: F-AGGGTCAATTTCTT AGGGCGACA; R-GCCAAGTGTTCGGTCCCAC; D: F-TCTGGCTTCT CACCATGACCACT; R-TCTAAGGCGTCACCACTGTTCTC, E: F-TTTGG GGAATCATCTTACAGC AGAG; R-ATTCATCCTGGCCCTTTCCTTCTTA; F: F-GCTCCACCTCATAGTTGTCTGG; R-CCATGAGCACGTGGAGAGAAAG; G: F-GGTCGAATACCGCGTGGGTT; R-CCCGGTAGAGGCTGCAAGAAA; H: F-AGGCAAATCTGGGCCTATCC; R-GGCCCAGTACAGAGGGGTCT; I: F-GGCTCAGGCTAGCAGGTCTC; R-TGTTTGGCCAGCATCCTCCA; J: F-ACTCAGAGGTGCTGTGACTCC; R-AGCTTTAGGACCACAATGGGCA. Food restriction (inverted feeding) Per1 Venus mice fed during the night received food from 21:00 to 9:00 (control group), whereas mice fed during the day had access to food from 9:00 to 21:00 (inverted group). Food restriction was performed during nine consecutive days as previously described 12 . For food restriction in constant darkness, Per1 Venus mice were housed in constant darkness with ad libitum access to food and water for 2 weeks. Then, access to food was restricted to the subjective day or night, for 12 days, in constant darkness. Inverted light\u2013dark cycles To induce changes in light regime, Per1 Venus mice were placed in ventilated, light-tight cabinets on a 12-h light\u2013dark cycle (Ternox). After acclimation, light cycles were changed for mice in the inverted group for 3 weeks to completely establish an inverse light cycle, while they remained the same for mice in the control group, as previously described 39 . For inverted light\u2013dark cycle experiments followed by constant darkness, after establishing an inverse light\u2013dark cycle, mice were transferred into constant darkness for 3 weeks. SCN lesions Bilateral ablation of the SCN was performed in 9\u201312-week-old Per1 Venus males by electrolytic lesion using stereotaxic brain surgery, as described previously 15 . Mice were kept under deep anaesthesia using a mixture of isoflurane and oxygen (1\u20133% isoflurane at 1 l\/min). Surgeries were performed using a stereotaxic device (Kopf). After identification of the bregma, a hole was drilled through which the lesion electrode was inserted into the brain. Electrodes were made by isolating a 0.25-mm stainless steel insect pin with a heat shrink polyester tubing, except for 0.2 mm at the tip. The electrode tip was aimed at the SCN, 0.3 mm anterior to bregma, 0.20 mm lateral to the midline, and 5.8 mm ventral to the surface of the cortex, according to the Paxinos Mouse Brain Atlas, 2001. Bilateral SCN lesions were made by passing a 1-mA current through the electrode for 6 s, in the left and right SCN separately. Sham-lesioned mice underwent the same procedure, but no current was passed through the electrode. After surgery animals were housed individually under constant dark conditions with ad libitum food and water and were allowed to recover for 1 week before behavioural analysis. Successfully SCN-lesioned mice were selected by magnetic resonance imaging (MRI), arrhythmic behaviour and histopathology analysis. Magnetic resonance imaging Screening of SCN ablated mice was performed using a Bruker ICON scanner (Bruker, Karlsruhe, Germany). RARE (Rapid Acquisition with Refocused Echoes) sequence was used to acquire coronal, sagittal and axial slices (five slices in each orientation) with the following parameters: RARE factor = 8, TE = 85 ms, TR = 2,500 ms, resolution = 156 \u00d7 156 \u00d7 500 \u00b5m 3 (30 averages). For high-quality images, a 9.4-T BioSpec scanner (Bruker, Karlsruhe, Germany) was used. This operates with Paravision 6.0.1 software and is interfaced with an Avance IIIHD console. Anatomical images (16 axial and 13 sagittal slices) were acquired using a RARE sequence with RARE factor = 8, TE = 36 ms, TR = 2,200 ms and resolution of 80 \u00d7 80 \u00d7 500 \u00b5m 3 (12 averages). Behavioural analysis Sham-operated and SCN-ablated mice were individually housed and after a 24-h acclimation period their movement was recorded for 72 h, in constant darkness, using the automated animal behaviour CleverSys system. Data were auto scored by the CleverSys software. Videos and scoring were visually validated. Circadian rhythmicity was evaluated using the cosinor regression model 40 , 41 . Histopathology analysis Mice infected with C. rodentium were killed by CO 2 narcosis, the gastrointestinal tract was isolated, and the full length of caecum and colon was collected and fixed in 10% neutral buffered formalin. Colon was trimmed in multiple transverse and cross-sections and caecum in one cross-section 42 , and all were processed for paraffin embedding. Sections (3\u20134 \u03bcm) were stained with haematoxylin and eosin and lesions were scored by a pathologist blinded to experimental groups, according to previously published criteria 43 , 44 , 45 . In brief, lesions were individually scored (0\u20134 increasing severity) for: mucosal loss; mucosal epithelial hyperplasia; degree of inflammation; extent of the section affected in any manner; and extent of the section affected in the most severe manner, as previously described 45 . The score was derived by summing the individual lesion and extent scores. Mesenteric (mesocolic) inflammation was noted but not scored. Liver, gonadal and subcutaneous fat from Arntl \u2206Rorgt mice was collected, fixed in 10% neutral buffered formalin, processed for paraffin embedding, sectioned into 3-\u03bcm-thick sections and stained with haematoxylin and eosin. The presence of inflammatory infiltrates was analysed by a pathologist blinded to experimental groups. For the SCN lesions experiment, mice were killed with CO 2 narcosis, necropsy was performed and brain was harvested and fixed in 4% PFA. Coronal sections of 50-\u00b5m thickness were prepared with a vibratome (Leica VT1000 S), from 0.6 to \u20131.3 relative to the bregma, collected on Superfrost Plus slides (Menzel-Gl\u00e4ser) and allowed to dry overnight before Nissl staining. Stained slides were hydrated in distilled water for a few seconds and incubated in Cresyl Violet stain solution (Sigma-Aldrich) for 30 min. Slides were dehydrated in graded ethanol and mounted with CV Mount (Leica). Coronal sections were analysed for the presence or absence of an SCN lesion (partial versus total ablation, unilateral versus bilateral) in a Leica DM200 microscope coupled to a Leica MC170HD camera (Leica Microsystems, Wetzlar, Germany). Microscopy Adult intestines from Ret GFP mice were flushed with cold PBS (Gibco) and opened longitudinally. Mucus and epithelium were removed, and intestines were fixed in 4% PFA (Sigma-Aldrich) at room temperature for 10 min and incubated in blocking\/permeabilizing buffer solution (PBS containing 2% BSA, 2% goat serum, 0.6% Triton X-100). Samples were cleared with benzyl alcohol-benzyl benzoate (Sigma-Aldrich) prior to dehydration in methanol 18 , 46 . Whole-mount samples were incubated overnight or for 2 days at 4 \u00b0C using the following antibodies: anti-tyrosine hydroxylase (TH) (Pel-Freez Biologicals) and anti-GFP (Aves Labs). Alexa Fluor 488 goat anti-chicken and Alexa Fluor 568 goat anti-rabbit (Invitrogen) were used as secondary antibodies overnight at room temperature. For SCN imaging, RFP \u0394Camk2a and RFP \u0394Rorgt mice were anaesthetized and perfused intracardially with PBS followed by 4% paraformaldehyde (pH 7.4, Sigma-Aldrich). The brains were removed and post-fixed for 24 h in 4% paraformaldehyde and transferred to phosphate buffer. Coronal sections (50 \u00b5m) were collected through the entire SCN using a Leica vibratome (VT1000s) into phosphate buffer and processed free-floating. Sections were incubated with neurotrace 500\/525 (Invitrogen, N21480) diluted 1\/200 and mounted using Mowiol. Samples were acquired on a Zeiss LSM710 confocal microscope using EC Plan-Neofluar 10\u00d7\/0.30 M27, Plan Apochromat 20\u00d7\/0.8 M27 and EC Plan-Neofluar 40\u00d7\/1.30 objectives. RNA sequencing and data analysis RNA was extracted and purified from sorted small intestinal lamina propria cells isolated at ZT5 and ZT23. RNA quality was assessed using an Agilent 2100 Bioanalyzer. SMART-SeqII (ultra-low input RNA) libraries were prepared using Nextera XT DNA sample preparation kit (Illumina). Sequencing was performed on an Illumina HiSeq4000 platform, PE100. Global quality of FASTQ files with raw RNA-seq reads was analysed using fastqc (ver 0.11.5) (  ). Vast-tools 47 (version 2.0.0) aligning and read processing software was used for quantification of gene expression in read counts from FASTQ files using VASTD-DB 47 transcript annotation for mouse genome assembly mm9. Only the 8,443 genes with read count information in all 12 samples and an average greater than 1.25 reads per sample were considered informative enough for subsequent analyses. Preprocessing of read count data, namely transforming them to log 2 (counts per million) (logCPM), was performed with voom 48 , included in the Bioconductor 49 package limma 50 (version 3.38.3) for the statistical software environment R (version 3.5.1). Linear models and empirical Bayes statistics were used for differential gene expression analysis, using limma. For heat maps, normalized RNA-seq data were plotted using the pheatmap (v1.0.10) R package (  ). Heat-map genes were clustered using Euclidean distance as metric. Statistics Results are shown as mean \u00b1 s.e.m. Statistical analysis was performed using GraphPad Prism software (version 6.01). Comparisons between two samples were performed using Mann\u2013Whitney U test or unpaired Student\u2019s t -test. Two-way ANOVA analysis was used for multiple group comparisons, followed by Tukey\u2019s post hoc test or Sidak\u2019s multiple comparisons test. Circadian rhythmicity was evaluated using the cosinor regression model 40 , 41 , 51 , using the cosinor (v1.1) R package. A single-component cosinor fits one cosine curve by least squares to the data. The circadian period was assumed to be 24 h for all analysis and the significance of the circadian fit was assessed by a zero-amplitude test with 95% confidence. A single-component cosinor yields estimates and defines standard errors with 95% confidence limits for amplitude and acrophase using Taylor\u2019s series expansion 51 . The latter were compared using two-tailed Student\u2019s t -test where indicated. Results were considered significant at * P < 0.05, ** P < 0.01, *** P < 0.001. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this paper. Data availability Source data for quantifications shown in all graphs plotted in the Figures and Extended Data Figures are available in the online version of the paper. The datasets generated in this study are also available from the corresponding author upon reasonable request. RNA-seq datasets analysed are publicly available in the Gene Expression Omnibus repository with accession number GSE135235 . Change history 22 November 2019 An Amendment to this paper has been published and can be accessed via a link at the top of the paper. ","News_Body":"It is well known that individuals who work night shifts or travel often across different time zones have a higher tendency to become overweight and suffer from gut inflammation. The underlying cause for this robust phenomenon has been the subject of many studies that tried to relate physiological processes with the activity of the brain's circadian clock, which is generated in response to the daylight cycle. Now, the group of Henrique Veiga-Fernandes, at the Champalimaud Centre for the Unknown in Lisbon, Portugal, discovered that the function of a group of immune cells, which are known to be strong contributors to gut health, is directly controlled by the brain's circadian clock. Their findings were published today in the scientific journal Nature. \"Sleep deprivation, or altered sleep habits, can have dramatic health consequences, resulting in a range of diseases that frequently have an immune component, such as bowel inflammatory conditions,\" says Veiga-Fernandes, the principal investigator. \"To understand why this happens, we started by asking whether immune cells in the gut are influenced by the circadian clock.\" The big clock and the little clock Almost all cells in the body have an internal genetic machinery that follows the circadian rhythm through the expression of what are commonly known as \"clock genes.\" The clock genes work like little clocks that inform cells of the time of day and thereby help the organs and systems that the cells make up together, anticipate what is going to happen, for instance if it's time to eat or sleep. Even though these cell clocks are autonomous, they still need to be synchronized in order to make sure that \"everyone is on the same page.\" \"The cells inside the body don't have direct information about external light, which means that individual cell clocks can be off,\" Veiga-Fernandes explains. \"The job of the brain's clock, which receives direct information about daylight, is to synchronize all of these little clocks inside the body so that all systems are in synch, which is absolutely crucial for our wellbeing.\" Among the variety of immune cells that are present in the intestine, the team discovered that Type 3 Innate Lymphoid Cells (ILC3s) were particularly susceptible to perturbations of their clock genes. \"These cells fulfill important functions in the gut: they fight infection, control the integrity of the gut epithelium and instruct lipid absorption,\" explains Veiga-Fernandes. \"When we disrupted their clocks, we found that the number of ILC3s in the gut was significantly reduced. This resulted in severe inflammation, breaching of the gut barrier, and increased fat accumulation.\" These robust results drove the team to investigate why is the number of ILC3s in the gut affected so strongly by the brain's circadian clock. The answer to this question ended up being the missing link they were searching for. It's all about being in the right place at the right time When the team analyzed how disrupting the brain's circadian clock influenced the expression of different genes in ILC3s, they found that it resulted in a very specific problem: the molecular zip-code was missing! It so happens that in order to localize to the intestine, ILC3s need to express a protein on their membrane that works as a molecular zip-code. This 'tag' instructs ILC3s, which are transient residents in the gut, where to migrate. In the absence of the brain's circadian inputs, ILC3s failed to express this tag, which meant they were unable to reach their destination. According to Veiga-Fernandes, these results are very exciting, because they clarify why gut health becomes compromised in individuals who are routinely active during the night. \"This mechanism is a beautiful example of evolutionary adaptation,\" says Veiga-Fernandes. \"During the day's active period, which is when you feed, the brain's circadian clock reduces the activity of ILC3s in order to promote healthy lipid metabolism. But then, the gut could be damaged during feeding. So after the feeding period is over, the brain's circadian clock instructs ILC3s to come back into the gut, where they are now needed to fight against invaders and promote regeneration of the epithelium.\" \"It comes as no surprise then,\" he continues, \"that people who work at night can suffer from inflammatory intestinal disorders. It has all to do with the fact that this specific neuro-immune axis is so well-regulated by the brain's clock that any changes in our habits have an immediate impact on these important, ancient immune cells.\" This study joins a series of groundbreaking discoveries produced by Veiga-Fernandes and his team, all drawing new links between the immune and nervous systems. \"The concept that the nervous system can coordinate the function of the immune system is entirely novel. It has been a very inspiring journey; the more we learn about this link, the more we understand how important it is for our wellbeing and we are looking forward to seeing what we will find next,\" he concludes. ","News_Title":"How sleepless nights compromise the health of your gut","Topic":"Medicine"}
{"Paper_Body":"Abstract A small, male cockroach (7 mm in length) in Dominican amber is described as Supella dominicana sp. n. (Blattida: Ectobiidae = Blattellidae). The dark tegmina, which are equal to the length of the abdomen, have a yellow cross bar and a central stripe giving the illusion that the body is divided into two halves. The pronotum is partially triangular in outline, with rounded edges and unusually flat surface. The fore femora contain two short apical terminal spines and a series of short wide-spaced marginal spines. The fore tarsus has the first article surpassing the others combined. The 7-segmented cerci are longer than wide. The arolia are well developed and the tarsal claws are symmetrical, of equal length, each with a blunt tooth. The two styles are small, equal in shape and with a branched seta. Developing spermatids are present at the tip of the abdomen. This fossil, which is the first ectobiid cockroach described from Dominican amber, provides some new features of the genus Supella Shelford, 1911. Access provided by MPDL Services gGmbH c\/o Max Planck Digital Library Working on a manuscript? Avoid the common mistakes Introduction Dominican amber contains a wealth of information on the biodiversity, ecology, biogeography, speciation and extinction of plants and animals in the mid-Cenozoic (Poinar 1999 , 2010 ). It contains valuable information on the parasites and pathogens of past organisms, including malaria-carrying mosquitoes and nematomorph-infected cockroaches. Regarding cockroaches, we remember those maintained in laboratory cultures at schools and universities for various physiological and behavioral experiments. Yet, these represented only a small fraction with the majority of species occurring throughout tropical and subtropical regions around the world (Hinkelman 2021a , b , 2022 ; Wappler and Vr\u0161ansk\u00fd 2021 ). Habits of cockroaches vary considerably and not all live around human habitations. Some live in caves while others prefer stumps or under bark and some genera are soil-burrowing (see Sendi 2021 ; Vr\u0161ansk\u00fd et al. 2019a ; Song et al. 2021 ). Some are active during the day while others come out at night. Body size can also vary greatly, ranging from 2 mm long in myrmecophilic cockroaches (Bohn et al. 2021 ) to 66 mm for the Neotropical species Megaloblatta blaberoides (Walker, 1871) (Bell et al. 2007 ). This species can have an overall length of over 120 mm. Cockroaches are considered medically important insects since they are carriers of human pathogens, including bacteria that cause gastroenteritis infections such as salmonella, staphylococcus and streptococcus, resulting in diarrhea, fever, and vomiting. They can also carry viruses (Vr\u0161ansk\u00fd et al. 2019b ). Among those closely associated with human habitations around the world are the German cockroach ( Blatella germanica Linnaeus, 1767), the oriental cockroach ( Blatta orientalis Linnaeus, 1758), the brown-banded cockroach ( Supella longipalpa (Fabricius, 1798)) and the American cockroach ( Periplaneta americana Linnaeus, 1758). Aside from these insects spreading pathogens and causing allergic reactions, just their presence and inability to eliminate them from homes can result in psychological stress and low morale (Bowles et al. 2018 ). Currently, the genus Supella contains three subgenera (Rehn 1947 ) and ten species. Nine of them occur in the Ethiopian zoogeographical region (Princis 1969 ; Roth 1985 , 1991 ) and one species in the Saharo - Sindian regional zone (specifically the Arabian Peninsula \u2013 Grandcolas 1994 ). The present paper describes a cockroach of the genus Supella Shelford, 1911 from Dominican amber, thus revealing another insect taxon which has no native species remaining in Hispaniola (see also Lewis et al. 1990 ) or even in the entire Nearctic and Neotropics. Materials and methods The fossil originated from La Toca amber mine in the northern mountain range (Cordillera Septentrional) of the Dominican Republic between Puerto Plata and Santiago (Poinar 1991 ; Donnelly 1988 ). Amber from mines in this region was produced by Hymenaea protera Poinar, 1991 (Poinar 1991 ) (Fabaceae). Dating of Dominican amber is controversial, with the youngest proposed age of 20\u201315 Mya based on Foraminifera (Iturralde-Vinent and MacPhee 1996 ) and the oldest of 45\u201330 Mya based on coccoliths (Cepek in Schlee 1990). These dates are based on microfossils in the strata containing the amber that is secondarily deposited in turbiditic sandstones of the Upper Eocene to Lower Miocene Mamey Group (Draper et al. 1994 ). Dilcher et al. ( 1992 ) stated that \u201c\u2026the amber clasts, from all physical characteristics, were already matured amber at the time of re-deposition into marine basins. Therefore, the age of the amber is greater than Miocene and quite likely is as early as late Eocene\u201d. The issue is further complicated by the discovery of Early Oligocene amber in Puerto Rico and Maastrichtian-Paleocene amber in Jamaica (Iturralde-Vinent, 2001 ) showing that amber from a range of deposits occurs in the Greater Antilles. Observations and photographs were made with a Nikon SMZ-10 R stereoscopic microscope and Nikon Optiphot compound microscope with magnifications up to 600 X. Helicon Focus Pro X64 was used to stack photos for better clarity and depth of field. Systematic paleontology Order: Blattida Latreille, 1810 (typified Blattariae Latreille, 1810) Family: Ectobiidae Brunner von Wattenwyl, 1865 = Blatellidae Karny, 1908 Subfamily: Pseudophyllodromiinae Hebard, 1929 Genus Supella Shelford, 1911 Supella dominicana sp. n. Zoobank code: E64B2B5E-CC3C-423 A-9813-00B4D238724A. Diagnosis (based on a complete adult male holotype) Small body (7 mm long); tegmina dark, with yellow cross bar and central stripe giving the illusion that the body is divided into two halves, external border of tegmina pale; pronotum triangular to trapezoidal in outline, with rounded edges and flat dorsum; tegmina short, not surpassing abdomen; twice as long as wide, apicies rounded; marginal field of tegmina extends to 43% of wing length; forewing costo-radial field with veins numerous (16), oblique. Fore femora with two apical spines and a series of short marginal spines. Foretarsus with first article surpassing the other articles combined; pulvillus only on fourth tarsomere, arolia well developed, tarsal claws symmetrical, blunt toothed; styles small, equal in shape, with branched setae. Description ( Figs. 1 , 2 , 3 , 4 , 5 and 6 ) . Fig. 1 Dorsal view of holotype of Supella dominicana in Dominican amber. Scale bar = 1.5 mm Full size image Fig. 2 Ventral view of holotype of Supella dominicana in Dominican amber. Scale bar = 1.4 mm Full size image Fig. 3 Lateral view of holotype of Supella dominicana in Dominican amber. Scale bar = 1.4 mm Full size image Fig. 4 Holotype of Supella dominicana in Dominican amber. a Fields of tegmen. A = Anal field; D = Discoidal field; S = Scapular field; M = Marginal field. Scale bar = 1.4 mm. b Face. A= antenna; Ai= antennal insertion; C= clypeus; G= gena; L= labrum; M= maxillary palp; O= ocellar spot. Scale bar = 90 \u03bcm Full size image Fig. 5 Holotype of Supella dominicana in Dominican amber. a Terminal femoral spines Scale bar = 210 \u03bcm. b Fore tarsus. Scale bar = 240 \u03bcm Full size image Fig. 6 Holotype of Supella dominicana in Dominican amber. a Arolium (arrow) on hind leg. Note small basal tooth on claws. Scale bar = 75 \u03bcm. b Style. Scale bar = 90 \u03bcm. c Cercus. Scale bar = 210 \u03bcm. d Male genital hook (sclerite HLA). Scale bar = 210 \u03bcm. e Acrosomes (dark spots) on sperm cells. Scale bar = 200 \u03bcm Full size image Well-preserved male with body complete except for distorted left antennae formed during burial (Figs. 1 , 2 and 3 ). Small robust species (L = 7 mm; width = 2.8 mm). Body elliptical in outline; tegmina dark (L = 5.0 mm), short, not surpassing abdomen, with yellow cross bar and central stripe giving the illusion that the body is divided into two halves; marginal field extends to 43% of wing length; costal veins numerous, oblique (16) (Fig. 4 a). No vein deformity (sensu Vr\u0161ansk\u00fd 2005 ) recorded. Head small, triangular in shape (Fig. 4 b), with very fine antennae almost as long as body, two lateral ocelli (30 \u03bcm in diameter) present; pronotum broadly triangular in outline, with rounded edges, dorsum flat, pale yellow with incomplete wide, dark reddish border, length, 1.4 mm, width, 1.8 mm; legs slender, yellowish brown, cursorial; lower front margin of fore femur armed with 2 long apical spines (210 \u03bcm in length) (Fig. 5 a), followed by 4 medium-long spines with a row of very short spines (setae) between them; mid and hind tibia armed with a series of widely separated long, robust spines; all tarsi 5-segmented and terminated with claws and well-developed arolia; fore tarsus with first tarsomere surpassing the other articles combined (Fig. 5 b); tarsal claws of equal length (105 \u03bcm), each bearing a blunt basal tooth (Fig. 6 a); pulvilli reduced, only noticeable on fourth tarsomere; abdominal sternites distinct, yellowish brown; styles arising from sockets (105 \u03bcm wide at base) (Fig. 6 b); style seta 5\u2013branched, central seta 315 \u03bcm in length; cerci of medium length (L = 1.3 mm), with 7 segments, all longer than wide (Fig. 6 c). Male genital hook yellowish, 0.7 mm in length (Fig. 6 d); tip of abdomen shows sperm bundle (spermatodesm) containing spermatozoa with dark acrosomes (Fig. 6 e) that are surrounded by mucopolysaccharides (gelatinous material). Derivation of name the specific epithet indicates the place of origin of the fossil. Type material Holotype (O\u2013 2\u201343) deposited in the Poinar amber collection maintained at Oregon State University. Type locality La Toca mine in the northern mountain ranges of the Dominican Republic. Systematical remarks Placement in Supella is based on body shape, coloration and features of the tegmina as outlined by Rehn ( 1947 ). While the new species shares several features with Supella miocenica Vr\u0161ansk\u00fd, Cifuentes-Ruiz, Vidli\u010dka, \u010campor, Jr. et Vega, 2011 from Mexican amber (Vr\u0161ansky et al. 2011), the two species can be easily separated by color patterns alone. S. miocenica has two distinct triangular patches on the pronotum, while the pronotum of S. dominicana is basally yellow except for a reddish brown partial margin (these color differences could represent sexual dimorphisms). Also the 13 cercomeres of S. miocenica are wider that long, while the 7 cercomeres of S. dominicana are longer than wide. In addition, the pronotum is significantly vaulted in S. miocenica but essentially flat in S. dominicana . Another Mexican amber cockroach is Anaplecta vega Barna, \u0160m\u00eddov\u00e1 et Jos\u00e9, 2019 (Barna et al. 2019 ). This species, which is assigned to the (sub)family Anaplectinae is under 5 mm in length, has prolonged mouthparts, long maxillary palps with a flattened terminal palpomere, and large eyes, which separates it from S. dominicana . S. dominicana is homoplasically very similar to Anaplecta calosoma Shelford, 1912, but Anaplecta Burmeister, 1838 has different cerci with long setae. Other ectobiid genera in the Pseudophyllodromiinae with a similar size range have various characters that separate them from the fossil (Blatchley 1920 ; Li et al. 2017 , 2020 ; Guti\u00e9rrez 2002a , b , 2005 , 2009; Guti\u00e9rrez and P\u00e9rez-Gelabert 2000 , 2001 ; Greenwalt and Vidli\u010dka 2015 ; Vr\u0161ansk\u00fd et al. 2021 ). There are no extant native members of Supella in Hispaniola presently (Guti\u00e9rrez, 2002a , b , 2005 , 2006 ; Guti\u00e9rrez and P\u00e9rez-Gelabert, 2001 ). Discussion This is the first ectobiid cockroach described from Dominican amber. While Guti\u00e9rrez and P\u00e9rez-Gelabert ( 2000 ) provided a list of the genera of fossil cockroaches in Dominican amber, which included Anaplecta, Cariblatta Hebard, 1916, Euthlastoblatta Hebard, 1917, Holocompsa Burmeister, 1838, Plectoptera Saussure, 1864 and Pseudosymploce Rehn et Hebard, 1927, none have been described at that time (Arillo and Ortu\u00f1o 2005 ). Later (Gorochov 2007 ) described Erucoblatta semicaeca, Holocompsa nigra and H. abbreviata . Poinar ( 1999 ) described hairworms (Nematomorpha) emerging from a cockroach in or near the genus Supella in Dominican amber (Bell et al., 2007 ). Aside from S. longipalpa (Fabricius, 1798) which is cosmopolitan in distribution, S. miocenica and S. dominicana are the only known Neotropical members of the genus while a number of living species occur in Africa (Rehn 1947 ; Vr\u0161ansk\u00fd et al. 2011 ). It is well known that many insect lineages in Dominican amber, such as Mastotermes Froggatt, 1897 termites (Isoptera: Mastotermitidae) and Leptomyrmex Mayr, 1862 ants (Hymenaea: Formicidae), are now absent in Hispaniola. Also the closest living species of the tree that produced Dominican amber ( Hymenaea protera Poinar, 1991) is Hymenaea verrucosa Gaertner, 1830 in East Africa (Poinar 1991 , 1992 , 1999 , 2010 ). Vr\u0161ansk\u00fd et al. ( 2011 ) discussed the same phenomenon with Supella miocenica in Mexican amber and lists its extant African and Asian con-subgeners. He postulated that general extinctions in Americas probably took place during the time between deposition of Dominican and Chiapas amber. Nevertheless, this discovery with presence of cosmopolitan fauna in American (Dominican) amber might support his later hypothesis of much later American extinctions approaching 3.92 Ma (Vr\u0161ansk\u00fd 2005 ; Vr\u0161ansk\u00fd et al. 2017 ). Other explanation would be that the age of Dominican amber is comparable or even older than that of the Chiapas amber (see Material and Methods). Differences separating S. dominicana from the African and Asian members of the subgenus are color patterns and size (lengths vary from 16.0 to 25.5 mm in S. mirabilis (Shelford, 1908), S. gemma Rehn, 1947, S. tchadiana Roth, 1987 and S. occidentalis Princis, 1963 but only 7.0 mm in S. dominicana ). At the very tip of the abdomen is a portion of a sperm bundle (spermatodesm) containing spermatozoa with dark acrosomes. The spermatodesm is surrounded by mucopolysaccharides (gelatinous material). The sperm would have been set free (become motile) in the seminal receptacle of a female. This appears to be the first record of fossilized cockroach sperm. ","News_Body":"The cockroach, reviled around the world for its sickness-causing potential and general creepiness, now occupies an important position in the study of amber fossils thanks to research by an Oregon State University scientist. George Poinar Jr., professor emeritus in the OSU College of Science, has identified a new cockroach species. The male specimen, which Poinar named Supella dominicana, is encased in Dominican amber and is the first fossil cockroach to be found with sperm cells. \"It is well preserved with a yellow cross bar across the wings and a central, vertical, yellow stripe that appears to divide the body into two parts,\" he said. \"It has long spines, used for defense, on its legs, especially the hind legs. Also of interest is the sperm bundle containing spermatozoa with dark acrosomes, structures covering the head of the sperm, since fossil sperm are rare.\" The specimen, about 30 million years old, is also the only cockroach of its variety, ectobiid, to be discovered in amber from the Dominican Republic, though it has no living descendants in the Dominican or anywhere in the West Indies. As is the case with another Supella cockroach described earlier from Mexican amber, S. dominicana's closest living relatives are in Africa and Asia. Credit: Oregon State University \"So what caused these cockroaches to become extinct when it is so difficult to get rid of them today?\" wondered Poinar, an international expert in using plant and animal life forms preserved in amber to learn about the biology and ecology of the distant past. There are more than 4,000 species of cockroaches crawling around multiple habitats all over the Earth, but only about 30 types of roaches share habitat with humans, and just a handful of those are regarded as pests. But they are highly regarded as such, Poinar notes. Ancient, primitive and extraordinarily resilient, cockroaches can survive in temperatures well below freezing and can withstand pressures of up to 900 times their body weight, he said\u2014which means if you try to kill one by stepping on it, you probably won't succeed. Cockroaches are so tough that they can live for a week after being decapitated, he added, and they can scuttle at a lightning pace\u2014their speed to body length ratio is equivalent to a human running at about 200 mph. Since it doesn't bother cockroaches to walk through sewage or decaying matter, they'll potentially contaminate whatever surface they touch in your home as they search for food in the form of grease, crumbs, pantry items, even book bindings and cardboard. Credit: Oregon State University \"They are considered medically important insects since they are carriers of human pathogens, including bacteria that cause salmonella, staphylococcus and streptococcus,\" Poinar said. \"They also harbor viruses. And in addition to spreading pathogens and causing allergic reactions, just their presence is very unsettling.\" Prodigiously reproductive, able to squeeze into tiny hiding places and equipped with enzymes that protect them from toxic substances, cockroaches are not easily evicted once they show up somewhere, he said. There's also growing evidence that they're developing resistance to many insecticides. \"The difficulty in eliminating them from homes once they've taken up residence can cause a lot of stress,\" Poinar said. \"Many might say that the best place for a cockroach is entombed in amber.\" Poinar's identification of the new species was published in the journal Biologia. ","News_Title":"Amber researcher finds new species of cockroach, first fossilized roach sperm","Topic":"Biology"}
{"Paper_Body":"Abstract A deeper understanding of early disease mechanisms occurring in Parkinson\u2019s disease (PD) is needed to reveal restorative targets. Here we report that human induced pluripotent stem cell (iPSC)-derived dopaminergic neurons (DAn) obtained from healthy individuals or patients harboring LRRK2 PD-causing mutation can create highly complex networks with evident signs of functional maturation over time. Compared to control neuronal networks, LRRK2 PD patients\u2019 networks displayed an elevated bursting behavior, in the absence of neurodegeneration. By combining functional calcium imaging, biophysical modeling, and DAn-lineage tracing, we found a decrease in DAn neurite density that triggered overall functional alterations in PD neuronal networks. Our data implicate early dysfunction as a prime focus that may contribute to the initiation of downstream degenerative pathways preceding DAn loss in PD, highlighting a potential window of opportunity for pre-symptomatic assessment of chronic degenerative diseases. Introduction Parkinson\u2019s disease (PD) is the most common neurodegenerative movement disorder, with an estimated prevalence in industrialized countries of 0.3% in the general population, which increases to 1.0% in people older than 60 years and to 3.0% in people older than 80 years 1 . Clinically, PD is characterized by classical motor syndrome linked to a progressive loss of dopamine-containing neurons (DAn) in the substantia nigra pars compacta, and disabling non-motor symptoms related to extranigral lesions. Current therapies for PD are symptomatic and do not limit the progression of disability with time. It has been proposed that early intervention might slow down or even stop disease progression, by preserving neurons from the undergoing irreversible neurodegeneration 1 , 2 . However, early treatment relies on early diagnosis, which unfortunately is especially complicated in the case of PD. Current diagnostic modalities in PD are based on the presence of motor symptoms, a stage at which up to 70% of DAn have been lost 3 . Even though pre-motor symptoms are known to precede clinical diagnosis of PD by as much as a decade, they are rather unspecific and unsuitable as stand-alone biomarkers of the disease 4 . Therefore, the identification of early diagnostic or progression markers of PD represents an urgent medical need. Although the majority of PD cases are of unknown cause, so-called idiopathic PD, around 5% have been shown to have a genetic basis, with mutations in the LRRK2 gene accounting for the largest number of patients of familial PD 5 . Interestingly, LRRK2 polymorphisms are also considered a relevant genetic determinant for sporadic PD 6 , and LRRK2 function appears dysregulated in sporadic cases of PD 7 , even in the absence of LRRK2 mutations\/polymorphisms. These findings, together with the fact that PD associated with mutations in LRRK2 (L2-PD) is clinically indistinguishable from sporadic PD, position LRRK2 as an essential player for understanding both genetic and idiopathic PD 8 . LRRK2 is a highly complex protein with multiple enzymatic domains, involved in a variety of intracellular signaling pathways and cellular processes such as cytoskeleton dynamics, vesicle trafficking and endocytosis, autophagy, reactive oxygen species, mitochondrial metabolism, and function of immune cells. However, the exact physiological role of LRRK2 and its implication for PD pathogenesis remains unknown 8 . Of especial relevance for the investigation of early disease markers, transgenic mouse models of L2-PD display, before any events of neurodegeneration, an abnormally elevated excitatory activity and altered spine morphology in dorsal striatal spiny projection neurons 9 . Moreover, experimental models for other neurodegenerative conditions such as Alzheimer\u2019s disease 10 , 11 and amyotrophic lateral sclerosis 12 , have been shown to exhibit neuron hyperexcitability before the disease onset. It has also been demonstrated that the combination of PD with dementia often correlates with a disruption of both functional and effective connectivity in the cortex 13 . In contrast, the association of PD with depression correlates with disrupted functional connectivity between the median cingulate cortex and the prefrontal cortex and cerebellum 14 , 15 . The development of induced pluripotent stem cell (iPSC) technologies enables the generation of patient-specific, disease-relevant, cell-based experimental models of human diseases. Importantly, iPSC-based models can recapitulate some of the earliest signs of disease, even at pre-symptomatic stages 12 , 16 . In this study, we used an experimental platform based on DAn-enriched neuronal cultures derived from L2-PD patients, their gene-edited isogenic counterparts, or from healthy individuals. Such cultures formed active neuronal networks, the functionality of which was analyzed by calcium imaging. After multiple iterations of experimental characterization and biophysical modeling of neuronal network behavior, we could identify early alterations in PD neuronal function that were not present in control networks, and that predated the onset of neuron degeneration. Results Generation and characterization of iPSC-derived DA neurons A total of seven iPSC lines representing L2-PD patients and healthy aged-matched controls, along with gene-edited counterparts and fluorescent TH reporters, were used for the current studies (see Table 1 and \u201cMethods\u201d for further details). Some of these iPSC lines have been previously generated and fully characterized in our laboratories 17 , 18 , 19 , whereas two additional TH reporter lines were generated for this study (Supplementary Fig. 1 ). Table 1 Summary of the healthy controls and patients used in this study. Full size table iPSC differentiation toward DAn fate was performed using a modified version of the previously established midbrain floor-plate protocol 20 , which enabled the maintenance of differentiated cells for up to 10 weeks 21 . Briefly, we first cultured iPSCs on Matrigel with mTeSR medium until they reached 80% confluence, then we induced specification toward the ventral midbrain (VM) fate using a combination of knockout serum medium and neural induction medium (Fig. 1a ). At day 12 post-plating (D12) the cells exhibited a homogeneous morphology and marker profile of VM floor-plate progenitors, expressing FOXA2+\/LMX1A+ and the VM NPC markers such as OTX2 and EN1 paired with the neuroectodermal stem cell marker Nestin (Fig. 1b ). These progenitors were then cultured in neuronal differentiation medium supplemented with growth factors including BDNF, GNDF, TGF-\u03b2, and DAPT, with the aim to foster neuronal differentiation and survival (Fig. 1a ). At D50, the majority of the cells were positive for the dendritic marker MAP2, and quantitative immunolabeling for tyrosine hydroxylase (TH) and FOXA2 revealed that 30\u201340% of them were also committed to DA neuronal fate (Fig. 1c, d ). Fig. 1: Generation of ventral midbrain (VM) dopaminergic neurons from human iPSCs. a Scheme showing VM dopaminergic neurons differentiation protocol. b Representative immunofluorescence (IF) images of CTR (SP11) and PD1 (SP12) NPCs at day 12 of the differentiation process. iPSC-derived neural cultures express floor plate progenitor markers, such as Lmx1A, FoxA2, Otx2, Nestin, and EN1. c Representative IF images of CTR (SP11) and PD1 (SP12) differentiated neuronal cultures expressing neuronal markers (MAP2, TH) and midbrain-type DA markers (FoxA2 and Girk2) at day 50. Scale bar is 50 \u03bcm. d Percentage of TH\/FoxA2 and TH\/Girk2 vmDAn at day 50 of all the lines [CTR (SP11), CTR TH (SP11 TH), gene-edited isoPD1 (SP12 wt\/wt), PD1 (SP12), PD1 TH (SP12 TH), PD2 (SP13)]. A number of independent experiments n = 3. e \u2013 f Representative images of neuronal cultures at D50 expressing specific markers of maturation: e DAT (dopamine transporter) (scale bar, 50 \u03bcm); and f PSD95 (post-synaptic marker, red) and synapsin (synaptic marker, green). Orthogonal views show colocalization (scale bar, 10 \u03bcm). g Representative images of CTR (SP11) and PD1 (SP12) neuronal culture at D35-D50 and D80 showing the expression of TH and MAP2 markers at the different timepoints. h Percentage of TH+ cells over DAPI at the three different timepoints (D35-D50 and D80). i Heatmap of gene expression profiles of neuronal cultures of CTR (SP11), PD1 (SP12), PD2 (SP13), and gene-edited isogenic PD1 line (isoPD1) at D50, with dendrograms showing the strong similarities between independent experiments and the absence of clustering between control (CTR and isoPD lines, light brown) and PD (PD1 and PD2 lines, blue) conditions. Shown are transcripts with \u22652-fold change in expression, grouped as upregulated (50 transcripts, green bar) or downregulated (107 transcripts, purple bar). No statistically significant differences were found at p-Adj \u2264 0.1 when comparing control and PD conditions. Each type of culture was analyzed a minimum of 2 independent times, except for the gene-edited isogenic PD1 line, for which only one RNA preparation passed the quality control and could be analyzed. Full size image By the same day of differentiation (D50), about 35% of TH+ neurons also expressed the A9 domain-specific marker G protein-activated inward rectifier potassium channel 2 (GIRK2) (Fig. 1c, d ), and displayed Dopamine Transporter (DAT), the essential marker of mature DAn (Fig. 1e ). TH+ neurons also showed expression of pre-synaptic marker Synapsin and post-synaptic marker PSD95, indicating their capability to form synapses (Fig. 1f ). Under these conditions, controls (CTR) and PD-iPSCs gave rise to VM DAn that were morphologically homogeneous and showed the expected features of mature VM DAn, including complex dendritic arborization (Fig. 1g ). Although the protocol used was comparably effective in all iPSC lines analyzed, we found some variability in the number of VM DAn across lines at D35, ranging from 10 to 20% VM DAn ratio concerning all differentiated cells. This ratio did not depend on the presence or type of disease (Fig. 1h ). Instead, it seemed related to the specific iPSC clone used and to the evolution at the early stages of differentiation. After establishing equivalent DAn-enriched cultures from both control and patient iPSC lines, we evaluated whether there were any differences in cell viability between DAn derived from controls and patient iPSC lines. We first noted that control and L2-PD iPSC-derived DAn were indistinguishable, based on immunofluorescence (IF) images (Fig. 1g ) up to 80 days in culture, the latest timepoint investigated. By counting the number of TH-expressing cells (Fig. 1h ) we found no decline when cultured over time up to D80, strongly suggesting that DAn are not degenerating under these conditions (Fig. 1g, h ). We also found no differences in the percentage of cells with pyknotic nuclei in patient lines compared to control lines (data not shown; D50: 12\u201315% in all the lines; D80: 15\u201320% in all the lines). We next analyzed whether changes in transcriptome profiles suggestive of neurodegeneration appeared under these conditions. For this purpose, we measured the expression of a panel of 770 genes relevant for this process (NanoString Human Neuropathology Panel) in neuronal cultures differentiated from control and L2-PD iPSC at D50. Independent differentiation experiments from the same iPSC lines displayed highly similar gene expression profiles (Fig. 1i , Supplementary Fig. 2a ), highlighting the robustness of the differentiation protocol. A comparison of gene expression levels between control and L2-PD cultures identified 157 differentially-expressed transcripts with \u22652-fold change (50 upregulated and 107 downregulated), but no differences reached statistical significance with p-Adj \u2264 0.1 ( Supplementary Dataset ), nor did they correlate with PD-related hallmarks (Supplementary Fig. 2b\u2013d ). Thus, even though previous studies have found overt signs of neurodegeneration upon long-term culture of DAn differentiated from L2-PD iPSC 17 , 18 , 22 , 23 , under the culture conditions used in the present work (that include neurotrophic factors), they appeared to form overall healthy neuronal cultures similar to the ones generated by control iPSC. Characterization of neuronal activity Calcium activity recordings were then performed across all the seven independent lines to examine the functional maturity of the iPSC-derived neuronal networks after 35, 50, and 80 days of differentiation (D35-D50-D80) (Fig. 2a ). We noted that our calcium fluorescence assay enabled tracking the behavior of ~500 neurons in the field of view with a high spatial and temporal resolution, allowing us to resolve single cells and their dynamic interactions. Fig. 2: Network dynamics of vmDA neurons. a Representative image of a bright field recording at D80 (isoPD1) using the calcium imaging assay. Scale bar is 100 \u03bcm. Regions of interest were manually selected for each neuron (diameter 10 \u03bcm; color circles) to obtain the normalized calcium fluorescence time series of spontaneous activity, DFF (%) \\(\\equiv 100 \\cdot(F - F_0)\/F_0\\) , with F 0 the fluorescence signal of the neuron at rest. The green box illustrates the fast rise of fluorescence upon activation that procures the spiking onset time (arrowhead). The dashed black boxes illustrate coordinated neuronal activity that shape network bursts when several neurons are involved. b Average neuronal activity along maturation for the different cell lines. Data points are expressed as mean\u00b1SD. Trend lines are linear regressions. The number of cultures used in each condition and timepoint were: D35 (CTR: n = 3; isoPD1: 3; PD1: 4; PD2: 4); D50 (3; 9; 9; 5); D80 (3; 9; 8; 5). c Representative raster plots (top) and global network activity (GNA, bottom) for CTR (SP11), isoPD1, and PD1 (SP12) neuronal cultures at D80. Each plot shows 5 min of recording. Peaks in the GNA reveal network bursts (blue dots). Extreme bursting events (red dots) are those that are above a threshold (red dotted line) set as 95% confidence interval of CTR bursts\u2019 distribution. CTR and isoPD1 networks show a relative low percentage of extreme events, which contrasts with the rich abundance of them in PD1 networks. d Ratio of extreme events for all studied cell lines at D50 and D80. The colored boxes are a guide to visualize the distributions, which show the mean \u00b1 SD and the individual realizations (dots). For panels ( b ) and ( d ), the number of independent experiments for each condition and timepoint are D35 (CTR: n = 3, isoPD1: 3, PD1: 4, PD2: 4); D50 (3, 9, 9, 5); D80 (3, 9, 8, 5). *** p < 0.001 (ANOVA with multiple comparison analysis). Full size image Sharp increases in the fluorescence traces (Fig. 2a ) revealed spontaneous neuronal activations, which were analyzed to extract the onset times of elicited action potentials. With these data we first computed the average neuronal activity along culture time (Fig. 2b ) and observed that all lines evolved similarly, indicating that any anomalies in PD networks would depend on the structure of the activity patterns, rather than on the strength of activity itself. Thus, we next computed the Global Network Activity (GNA), defined as the fraction of neurons in the network that coactivated in a time window of 1 s. As shown in Fig. 2c , the GNA captures the level of neuronal synchronization present in the raster plot. For control cultures and also for the rescued isogenic PD line (left and central panels), collective activity encompassed synchronous activations of moderate size, between 10 and 40% of the network. In contrast, PD neuronal cultures (Fig. 2c , right panels) displayed a two-state dynamics, with strong whole-network synchronous events combined with quiescent intervals. Importantly, the distinct GNA patterns of control and PD neuronal cultures hint at the existence of intrinsically different network mechanisms in the two systems, which orchestrate a markedly different collective behavior. The GNA also informed that average neuronal activity by itself was not sufficiently informative to reveal alterations due to disease. To quantify the differences between cell lines exposed by the GNA analysis, we next analyzed the amplitude of the GNA events and extracted those that exceeded a given threshold (Fig. 2c , red dotted line), termed \u2018extreme event\u2019. We observed that the \u2018ratio of extreme events\u2019, i.e., the number of large bursting episodes relative to all detected episodes, was much higher in PD lines than in CTR or in genetically-corrected isogenic control (isoPD) lines, particularly at late stages of maturation (D80) (Fig. 2d ), which could render them more susceptible to stress by creating an overly rigid, synchrony-locked network despite their continued viability in culture. Functional connectivity of control and PD neuronal networks The different structure of global activity of CTR, isoPD, and PD lines hints at the existence of distinct functional connectivity traits between them. To shed light on network functionality, we used transfer entropy (see \u201cMethods\u201d for details) to compute the functional connectivity among all pairs of active neurons in a given network. As shown in Fig. 3a (top panels), the functional networks displayed abundant connections, with a combination of short-range and long-range links that extended across the network. Even though the functional networks seemed similar in spatial organization, we observed significant differences between control and PD networks. The first difference was a lower density of connections in the PD line, suggesting an overall degradation of functionality. A second difference concerns the structure of functional communities, i.e., groups of neurons that tend to connect among themselves more strongly than with the rest of the network. As shown in the bottom panels of Fig. 3a , functional communities were abundant in CTR and isoPD cultures (highlighted blue boxes), indicating cross-talking of neurons in small groups, in agreement with the moderate size of collective activations. For PD cultures, however, the communities were much larger (orange boxes), indicating not only a failed formation of functional microcircuits, but a tendency toward excessively strong network synchronicity, as also seen in the raster plots. Fig. 3: Functional connectivity of CTR, isoPD, and PD lines. a Top, Representative functional connectivity maps at D80 for CTR, isoPD1, and PD1 cultures. Dots are neurons and lines functional connections. The diameter of a dot and its opacity is proportional to the connectivity of the neuron it represents. Bottom, corresponding functional connectivity matrices. Black points are functional connections and colored boxes are functional communities. b Comparison of the number of communities and community statistic Q for the three lines at D50 and D80. PD1 networks are excessively integrated, with a relatively small number of communities strongly interconnected (low Q) as compared to CTR and isoPD1 networks. The colored boxes are a guide to visualize the distributions, which show the mean\u00b1SD and the individual realizations (dots). Number of independent experiments: D50 (CTR: n = 3, isoPD1: 4, PD1: 6); D80 (3, 5, 6). **** p < 0.001; ** p < 0.01; * p < 0.05 (ANOVA with multiple comparison analysis). c Sketch of functional differences between normal and PD networks according to the data, with PD networks displaying on average a lower connectivity combined with fewer and excessively connected communities. d Cumulative distribution functions (CDF) of functional connections (in % of network) at D50 for CTR, PD1, PD2, and gene-edited isogenic PD1 line (isoPD1). The CDFs of the two PD lines exhibit a trend toward a lower connectivity as compared to CTR and isoPD1 ones, which are similar. e Corresponding distributions at D80. The two PD lines maintain their low connectivity profiles and strengthen their differences relative to CTR and isoPD1, which remain similar. The number of independent experiments used in panels ( d ) and ( e ) are D50 (CTR: n = 3, isoPD1: 4, PD1: 6, PD2: 5); D80 (3, 5, 6, 5). Full size image These results were reproducible across realizations. As shown in Fig. 3b , PD networks exhibited a clear trend toward a small number of communities, which was accompanied by a tendency toward a stronger bond between these communities, i.e., an abnormal excessive integration. The latter was captured by the \u2018community statistic Q\u2019. The lower the Q value, the stronger the integration in the network. These differences appeared at D50 and substantially strengthened at D80. While CTR and isoPD cultures showed a moderate integration, PD cultures exhibited an abnormally strong integration. The combination of these results is summarized in Fig. 3c , which depicts two toy networks with the same number of neurons but with different functional organization. The normal, healthy network is characterized by a high average connectivity and well-defined small communities, while the PD network is characterized by a lower average connectivity and large, strongly linked communities that effectually almost shape a unique structure. To complete the functional analysis, we compared in more detail the statistics of functional connections. Figure 3d, e shows, for D50 and D80 stages of development, the cumulative distribution function of connections, CDF(k), for control, isoPD, and PD lines. This distribution portrays the probability that a neuron in the network has a number of connections less or equal than k. For D50, all distributions were similar and relatively close to one another, illustrating that they originated from similar dynamics, namely a combination of individual activity and network bursting. The distributions also showed a subtle growth, indicating that weakly connected neurons were rare. However, the PD distributions at D50 revealed a tendency toward a lower connectivity. Indeed, although the distributions were similar in shape, an analysis of the distance between distributions (Kullback\u2013Leibler divergence and Kolmogorov\u2013Smirnov test, Supp. Fig. 3 ) showed statistical differences among them, indicating that D50 could be the characteristic timepoint at which functional alterations in PD lines start to be detectable. The differences among distributions accentuated at D80, with PD cultures exhibiting a more pronounced trend toward a lower connectivity (a sharp increase of CDF(k) at low k values) that markedly departed from CTR and isoPD cultures. Interestingly, there were no statistically significant differences between CTR and isoPD cultures at this timepoint, a result that suggests the successful rescue of affected cell lines through correction of the LRRK2 mutation by gene edition. In light of these results, we considered the control network as the reference for a healthy development and function, and the departure from it as a signature of the pathology. Thus, we hypothesize that the LRRK2 mutation undermines the development of neuronal circuitry to such an extent that it alters collective network activity and functional organization. Contrasting dynamics of TH and non-TH neurons in control and PD lines To investigate the origin of the functional impairment found in PD DAn, we took advantage of the genetic TH-reporter tool created in our lab 19 . This reporter allows identifying TH and non-TH neuronal populations in the networks and analyzing their functional characteristics separately. Figure 4a, b exemplifies such a construction for representative isoPD and PD networks at D80, in which two neuronal layers, one corresponding to TH neurons (red) and another one corresponding to non-TH neurons (blue), interact functionally. The spontaneous activity of each subpopulation of neurons at D80 is shown in the accompanying raster plots. A comparison of the activity patterns between isoPD and PD cultures reveals the strong contrast in their dynamics. While the non-TH population in isoPD cultures shows a sustained activity with weak collective events, PD cultures show strong synchrony episodes that extend to both subpopulations. Fig. 4: Functional connectivity and dynamics of TH+ and non-TH+ neurons. a Left, representative functional network of an isoPD1 culture at D80, signaling the location of TH+ neurons (red) and non-TH+ neurons (blue). The diameter of a dot is proportional to the connectivity of the neuron it represents. Only 10% of the connections are shown for clarity. Right, corresponding raster plots of the two populations. b Corresponding analysis for a PD1 culture, in which the non-TH+ subpopulation exhibits a much stronger synchronous behavior as compared to controls. c Ratio of extreme events for each subpopulation of neurons at two maturation stages, D50 and D80. PD cultures show in general a higher ratio of extreme events as compared to controls. The non-TH+ population in the PD network at D50 shows a strong variability in the ratio of extreme events across realizations, indicating the onset of malfunction. The same population at D80 is dominated by extreme events that reflect the strong synchronous behavior. The colored boxes are a guide to visualize the distributions, which show the mean \u00b1 SD and the individual realizations (dots). The number of independent experiments in panel ( c ) are D50 (CTR: n = 3, isoPD1: 5, PD1: 5); D80 (3, 5, 5). *** p < 0.001; ** p < 0.01; * p < 0.05 (ANOVA with multiple comparison analysis). Full size image In addition, as shown in Fig. 4c , a comparison of the ratio of extreme events for TH and non-TH subpopulations indicates that, on average, PD cultures exhibited a higher number of extreme events in both subpopulations when compared to controls. However, along development, TH and non-TH ratios for controls were similar at D50 and D80, whereas the ratios for the PD line switched from mostly TH at D50 to mostly non-TH at D80. Altogether, these results unfold an abnormal subpopulation dynamics in PD networks, with an overall excessive bursting and a reversing leadership of TH and non-TH subpopulations along development. We also noted that PD cultures at D50 showed important variability among realizations, with extreme events in the non-TH population being absent in some experiments and abundant in others. Given this variability, we hypothesize that the D50 timepoint might mark the onset of structural alterations that later translate into dynamic and functional deficits. Since about 70% of the neurons in our networks are non-TH (Fig. 1g, h ), the above results suggest that, in healthy control cultures, non-TH neurons drive spontaneous activity in the network. In contrast, TH neurons play a regulatory yet essential role by facilitating the coexistence of small neuronal coactivations and whole-network bursting events. This regulatory role appears to be lost in PD cultures, in which TH neurons frame spontaneous activity patterns with excessive synchrony that translate into an abundance of extreme bursting episodes. We, therefore, hypothesize that the LRRK2 mutation alters the balance between neuronal subpopulations by degrading the physical coupling among neuronal types. An in silico model captures the dynamic alterations in PD networks Next, we carried out numerical simulations to better understand the impact of TH+ cells structural failure on PD network dynamics. Specifically, we evaluated whether a reduction in TH cells connectivity was sufficient to switch global network dynamics from balanced to excessively synchronous. We first reproduced the behavior of control networks. As shown in Fig. 5a , we used the same neuronal spatial arrangement of the cultured networks and considered a mixed population of 55% excitatory non-TH neurons, 25% excitatory TH+ neurons, and 20% inhibitory neurons. These values were selected to concord with both the typical 80% excitation of cortical circuits in vitro 24 and the TH\/DAPI ratio found in our cultures (Fig. 1 ). Neuronal dendritic trees and axons were then incorporated according to realistic biological rules, so that a connection was established whenever an axon crossed the dendritic tree of a neuron (Fig. 5b , top). Once the network structure was set, dynamics was incorporated through an extended Hodgkin\u2013Huxley model with parameters adjusted to replicate the activity in controls. As shown in the top panels of Fig. 5c , normal network dynamics was qualitatively similar to the experiments shown in Fig. 2c (left and center) and characterized by an intense background activity in combination with coordinated activity episodes. The inspection of the different populations (Fig. 5c , top, right panels) shows that non-TH neurons were also the drivers of network dynamics. It should be noted that inhibition was necessary in the simulations to ensure a sufficiently high spontaneous activity. Fig. 5: Numerical simulations of normal and PD networks. a Representative CTR culture at day 80, used as a reference for the spatial allocation of neurons in the numerical model. The enlarged area depicts a detail of the neuronal spatial arrangement. 300 neurons are used in the simulation, and are randomly assigned as dopaminergic (DA, red, 25% of network), inhibitory (green, 20%), and excitatory (blue, 55%). b Random pruning algorithm, in which PD damage is simulated by a shortening of either axons (red curve) or dendritic arbor (dotted red circle) in DA neurons (red hexagons). In the sketch, normal connectivity is established when the dendritic arbor of any neuron (dotted circles) intersects an axon. Neurite pruning disconnects neurons either because of axonal shortening (neuron #1) or dendritic loss (neuron #2) in DAn. c Raster plots of simulated normal and PD networks, with the latter corresponding to an axonal pruning on 10% of DA neurons, each one losing about 80% of connections. In the raster plots, the left panels show the dynamics of the entire network, whereas the right panels show the dynamics in each subpopulation. All raster plots range from 0 to 300, but the number of active neurons in each plot vary according to the population monitored. PD simulations show a markedly synchronous behavior of the excitatory population that translates onto the entire network. d Ratio of extreme events for different percentages of pruned DAn subpopulation, showing that the presence of extreme bursting events increases as network affectation progresses. The colored boxes are a guide to visualize the distributions, which show the mean \u00b1 SD and the individual realizations (dots). The number of individual simulations is n = 4 for each condition. *** p < 0.001; * p < 0.05 (ANOVA with multiple comparison analysis). e Cumulative distribution of functional connections (CDF) for two PD networks (at 10 and 30% pruning) and a normal, non-pruned network. The distributions show the same trend as in the experiments, with PD departing from normal toward a low degree functional connectivity scenario. Full size image To explore PD dysfunction, we used the same control networks and modeled the shortening of neurites in TH neurons, effectually reducing the connectivity probability in the TH population (Fig. 5b , bottom). Such a construction signified that a single TH cell lost about 80% of its connections. We then explored the minimum fraction of TH cells that had to be affected to observe an impact on the network dynamic. As shown in the bottom panels of Fig. 5c , PD simulations were characterized by an exceedingly synchronous behavior that captured well the experimental observations of PD cultures (Fig. 2c , right), with a much lower background activity and a similar dynamic in both TH and non-TH populations. Simulations also demonstrated that the affectation of ~10% TH cells sufficed to drive the networks toward a chronic bursting behavior with an abundance of extreme, whole-network synchronous events (Fig. 5c ). As shown in Fig. 5d , we compared three different ratios of affected TH neurons. An increase in this ratio mimics disease progression, i.e., developmental time in the experiments. The results show that the ratio and occurrence of extreme events are similar for different damage rates, possibly because the remaining excitatory and inhibitory neurons are sufficiently large to maintain activity. Interestingly, these results also indicate that a small damage suffices to drive the network toward an excessively synchronous scenario. Finally, an analysis of functional connectivity showed that the model also reproduces well the experimental observations, with a tendency for simulated PD cultures to shift toward lower connectivity states (Fig. 5e ). TH+ neurons carrying LRRK2 mutation feature a lower number of dendrites To verify the prediction of the in silico model above and expose physical alterations in the circuitry of PD lines, we investigated the morphology of TH neurons in CTR, isoPD, and PD neuronal cultures at D50 and D80 (Fig. 6a and Supplementary Fig. 4a ). We found that DAn differentiated from PD iPSC lines showed a lower number of TH neurites compared to those derived from CTR or isoPD lines (1.2 \u00b1 0.1 neurites for PD vs . 4.5 \u00b1 0.2 for CTR and 4.1 \u00b1 0.3 for isoPD, Fig. 6b ). In addition, the number of neurites in CTR and isoPD cultures increased along with development while it decreased in PD cultures, indicating a progressive deterioration of network structure for the latter (Fig. 6b ). In contrast, the number of neurites in MAP2+ neurons from control, isoPD, and PD lines did not show any significant differences (Fig. 6c ), confirming that the dynamic and functional deficits of PD lines are localized to the TH subpopulation, and that these cells experience a gradual structural failure in the form of neurite loss. Fig. 6: Biological confirmation of the in silico hypothesis. a Gene-edited isoPD1 (SP12 wt\/wt) and PD1 (SP12) TH+ neuronal processes are traced (orange) to determine the number and the structure of neurites at D50 (left panels) and D80 (right panels). The top panels for each cell line show the original image of TH+ neurons. A blue triangle highlights a representative traced neuron shown in the middle panels. The bottom panels for each cell line show MAP2+ neuronal processes. Scale bar is 50 \u03bcm in all images. b The quantification of the number of TH neurites shows significant differences between CTR and PD1 both at D50 and D80. c No differences are detected on the number of MAP2 neurites. d Levels of released DA measured in culture media from CTR (SP11), PD1 (SP12), PD2 (SP13), and isoPD1 (SP12 wt\/wt) at D50 and D80 of differentiation, relative to CTR levels. * p -value < 0.05; ** p -value < 0.01; *** p -value < 0.001 (ANOVA with multiple comparison analysis). Number of independent experiments n = 3. Full size image Next, we evaluated DA release in the media collected from CTR, isoPD, and PD neuronal cultures. Supernatants of PD cultures revealed decreased dopamine levels at D50 and D80 compared with those of CTR and isoPD cultures (Fig. 6d ), indicating that the reduction in TH+ neurites has functional consequences on DA release. Finally, to investigate whether the early sign of overall alteration in the network structure and dynamics could contribute to neurodegeneration, we maintained iPSC-derived DAn over culture times longer than D80. Interestingly, in aged (110 days, latest timepoint analyzed) cultures, PD DAn showed morphological alterations, including reduced number and length of neurites, and significantly decreased cell survival compared with isoPD DAn (Supplementary Fig. 5a\u2013d ). In contrast, neuronal degeneration was not evident in non-TH+ cells, as judged by the percentage of MAP2+\/TH\u2212 neurons in the neuronal cultures (Supplementary Fig. 5e ). Discussion Since early diagnosis of PD is expected to dramatically improve the outcome of therapies under current development, in the present study we interrogated a human neuronal cell-based model of PD for the earliest detectable functional alterations. We found that DA neurons derived from iPSC representing healthy individuals or PD patients harboring LRRK2 mutation developed appropriate physiological characteristics forming complex and mature networks during the differentiation process. However, PD neuronal networks developed abnormal hypersynchrony at the latest timepoint analyzed (D80), in contrast with healthy or gene-edited isogenic PD networks. These new data from PD-affected human DAn indicate that early dysfunction may contribute to the initiation of downstream degenerative pathways that ultimately lead to DAn loss in PD. A general limitation of human iPSC-based disease modeling strategies that should be taken into account when interpreting the results of our studies is the notorious variability described among iPSC lines and clones 25 . To rule out the impact of interline\/interclone variability in our findings, we used several iPSC lines and iPSC clones to represent each experimental condition. Specifically, controls included two independent iPSC lines generated from two independent individuals, and a total of four iPSC clones, whereas the PD condition was similarly represented by two independent iPSC lines generated from two independent individuals, and a total of three iPSC clones (Table 1 ). Moreover, the confirmation of our findings in neuronal cultures differentiated from gene-edited isogenic controls of PD iPSC provides further reassurance that the alterations identified in PD samples in our studies are indeed related to the disease condition, rather than reflecting the specific iPSC line\/clone utilized. Our previous work showed that PD patient-specific iPSC-derived DAn are particularly susceptible to undergo neurodegeneration upon long-term culture 17 , 18 , 23 , 26 . In those studies, DAn were differentiated from iPSC and then maintained in culture in the absence of neurotrophic factors, which resulted in evident signs of DAn neurodegeneration only in PD samples, after 75 days in culture. For the present study, we used a different protocol for the generation and maintenance of iPSC-derived DAn in culture, which included neurotrophic factors and allowed comparing the function of mature DAn from PD and control iPSC in the absence of neurodegeneration for up to 80 days. Following these conditions, we used a calcium fluorescence imaging assay to monitor the functional neuronal activity of control and L2-PD iPSC-derived DAn at different timepoints of differentiation. We found that DAn derived from control iPSC exhibited an increase in both the number of spontaneous activity events and the number of bursting episodes, i.e., network-spanning synchronous activations. Such a trend is the one that could be expected from maturing healthy iPSC-derived neurons in vitro 27 . Conversely, L2-PD iPSC-derived DAn showed a two-state dynamic completely different from controls, characterized by strong synchronous events combined with quiescent intervals. The dynamics of PD cultures suggests that the structure of collective activation \u2014but not the average individual neuronal activity\u2014was the critical feature of PD malfunctioning neuronal behavior. This apparent sign of functional alterations was only displayed by the diseased neurons at a late timepoint. Early in development, both control and PD neuronal cultures showed similar functional behavior, indicating that there is a defined period of time in which PD network development starts to degrade and functional deficiencies emerge. The proportion of control iPSC-derived DAn in each of the cell lines remained unchanged along the ten weeks of culturing. Thus, despite remaining viable in culture, PD-affected DAn failed at procuring the necessary structural support for a rich collective dynamic and brought the network toward an excessively synchronous state. We introduced the connectivity probability of the TH population into a biophysical model to address the role of neurite connectivity. Numerical simulations of the model revealed that neurite density loss in DAn was among the first causes of dynamic and functional alterations. A random neurite loss in 10% of DAn sufficed to render a bursting-dominated dynamic and an excessively connected functional network. Our experimental data confirmed the reduction of neurites specifically in DAn of PD neuronal cultures. Thus, even though the culture conditions used in the present studies prevented the appearance of overt signs of PD DAn neurodegeneration observed in previous works using more standard culture conditions 17 , 22 , 28 , they could not prevent a slight reduction in PD DAn neurites. Therefore, it is important to note that this phenotype appears to be independent from differentiation and culture methods, depends on the presence of the LRRK2 G2019S mutation (inasmuch as it is absent in gene-corrected isogenic PD DAn), and underlies the functional alterations in overall network dynamics. It is tempting to speculate that this observation might be related to the recent finding that the LRRK2-G2019S mutated protein interferes with microtubule-based motors 29 . Previous studies of PD patients 15 and animal models of the disease 9 have reported early hyperexcitability of DAn and corticospinal neurons of the motor cortex. In our study, we demonstrate that DAn derived from patient iPSC harboring LRRK2 mutations exhibit hyperexcitability in culture. This increased activity might contribute to trigger a cascade of excitotoxic disease mechanisms involving pathological changes in Ca 2+ handling and the eventual activation of cell death pathways. In contrast, recent work supports a link between hyperexcitability and neuroprotection 30 , 31 , in particular when hyperexcitability is induced via reduction of neurite density and a consequent lack in connections and communication between neurons. This connectivity loss may trigger compensatory mechanisms in which the neurons create an aberrant structural and functional connectivity that can be pointed as an early marker of pathology 32 , 33 . To determine the actual role of hyperexcitability in PD, it will be necessary for future studies to examine the effects of finely controlled manipulations of excitability on human DAn. Thus, dysfunction of DAn physiology appears to precede the functional alterations that are then spread in the overall culture, placing DAn dysfunction as an early sign of overall alteration and neurodegeneration. Moreover, we directly connected this alteration to a reduction in the neurite arborization using both experimental data and in silico analysis. Taken together, these findings highlight the importance of addressing early changes in mechanisms underlying spike generation at the DAn soma when considering disease pathogenesis and potential treatment strategies for PD. Furthermore, our findings show the usefulness of sensitive physiological studies of human iPSC-derived DAn for future work aiming to develop new diagnostic tools and therapeutics for PD. Methods iPSC lines and gene editing The parental iPSC lines used in our studies were previously generated and fully characterized 17 , 18 , 19 . The generation and use of human iPSCs in this work were approved by the Spanish competent authorities (Commission on Guarantees concerning the Donation and Use of Human Tissues and Cells of the Carlos III National Institute of Health). All procedure adhered to internal and EU guidelines for research involving derivation of pluripotent cell line. All subjects gave informed consent for the study using forms approved by the ethical Committee on the Use of Human Subjects in Research at Hospital Clinic in Barcelona. The iPSC lines used in this study include one iPSC line obtained from a healthy donor (SP11) and two lines obtained from PD patients carrying the LRRK2 G2019S mutation (SP12 and SP13). From these original lines, isogenic controls solely differing in the presence of the LRRK2 G2019S mutation were obtained by correcting the LRRK2 mutation in the SP12 iPSC line. Expanded subject information, cell characterization, and technical details of the original iPSCs are described in Table 1 . Generation of CRISPR\/Cas9 plasmids and donor template for homology-directed repair For correcting the LRRK2 mutation, LRRK2 G2019S mutant SP12 iPSC were edited using TALENs. The CRISPR\/Cas9 plasmid pSpCas9(BB)-2A-GFP (PX458) was a gift from Dr. Feng Zhang (Broad Institute, MIT; Addgene plasmid #12345) 34 . The original pCbh promoter was exchanged for the full-length pCAGGS promoter to achieve higher expression levels in hiPSCs. Custom guide RNAs were cloned into the BbsI sites as annealed oligonucleotides. The donor template for HDR was generated using standard molecular cloning procedures. Briefly, for TH donor template, homology arms were amplified from genomic DNA and verified by Sanger sequencing. Resulting sequences matched those of the reference genome GRCh38. The homology arms were inserted into the KpnI-ApaI (5\u2032HA) and SpeI-XbaI (3\u2032HA) sites of pBS-SK ( \u2212 ). The sequence coding for the P2A peptide was added to mOrange with the primers used to amplify the gene and the PCR product was inserted into the ApaI-XhoI sites of the pBS-5\u2032HA-3\u2032HA plasmid. Finally, LoxP-pRex1-Neo-SV40-LoxP was amplified from the aMHC-eGFP-Rex-Neo plasmid (gift from Dr. Mark Mercola; Addgene plasmid #21229) 35 and inserted between the XhoI-SpeI sites of the plasmid. For LRRK2 donor template, homology arms were amplified from genomic DNA of a wild-type donor and verified by Sanger sequencing. The homology arms were inserted in the KpnI-XhoI (5\u2032HA) and SpeI-NotI (3\u2032HA) sites of pBS-SK ( \u2212 ). LoxP-pRex1-Neo-SV40-LoxP was inserted in a second cloning step between the SalI-BamHI sites. Gene edition in iPSC To generate the TH-mOrange hiPSC reporter cell line, cells were transfected with the HDR template and a Cas9- and gRNA-encoding plasmid; the latter overlapping the TH gene stop codon. In total, 800,000 iPSCs were seeded in 10-cm plates the day before transfection. iPSCs were co-transfected with 6 \u00b5g of CRISPR\/Cas9 plasmid and 9 \u00b5g HDR template using FuGENE HD (Promega) at a 1:3 DNA to reagent ratio. Cells were plated in selection medium containing 50 \u00b5g\/mL G418 (Melford Laboratories Ltd., Ipswich, UK) and maintained for 2 weeks until resistant colonies could be screened. At that time, one-half of each resistant colony was manually picked and site-specific integration was verified by PCR. To correct the LRRK2 G2019S mutation in the SP_12 iPSC line, cells were transfected with the wild-type HDR template and a Cas9- and gRNA-encoding plasmid whose gRNA overlapped the insertion site for the selection cassette. Transfection, clone selection, and subsequent screening were conducted as described above. To excise the selection cassette, edited iPSCs were transfected with a CRE recombinase-expressing plasmid, gifted from Dr. Michel Sadelain (Sloan Kettering Institute; Addgene plasmid #27546) 36 . At 48 h post-transfection, cells were dissociated and seeded at clonal density on a feeder layer of irradiated human fibroblasts. When colonies attained a certain size, they were picked and subcultured in independent Matrigel-coated wells. Cells were sampled and checked for cassette excision by PCR and Sanger sequencing. Those clones in which the cassette was excised were expanded, cryopreserved, and karyotyped. Expanded information regarding oligonucleotides used during gene editing procedures are listed in Supplementary Table 3 in Supplementary Information . iPSC differentiation into vm DA lineage Directed differentiation of iPSC onto ventral dopaminergic neurons (DAn) was carried out following a previously published protocol 20 , with minor modifications. Briefly, iPSCs were cultured in mTeSR commercial medium until they reached 80% confluence. Ventral midbrain induction was then forced by switching to SRM medium (KO-DMEM,15% KO serum, 1% P\/S, 1% glutamine, 1%NEAA, 0.1% bet- mercaptoethanol) with SB Tocris 1614 (a selective inhibitor of the growth factor TGF-b), LDN193189, Stemgent 04-0074 (BMP inhibitor) to inhibit the dual SMAD pathway, SAG and Purmorphamine, Calbiochem 540220 (SHH pathway activators) to induce neuroepithelial stem cells formation and proliferation. The medium was next changed to Neurobasal with 1% P\/S, 1% N2, and 2% B27-VitA and CHIR99021, Stemgent 04-0004 (CHIR), a potent GSK3B inhibitor known to strongly activate WNT signaling that induces LMX1A in FOXA2 ventral midbrain (VM) dopaminergic neurons precursors. The best co-expression of LMX1A and FOXA2, crucial factors for inducing ventral midbrain fate, was obtained at day 12 of differentiation. After generating and characterizing the VM precursors, these cells were cultured in Neurobasal medium, 1% P\/S and 2% B27-VitA with neurotrophic factors: 20 ng\/ml of BDNF (450-02, Peprotech), 20 ng\/ml of GDNF (450-10, Peprotech), 1 ng\/ml of TGF-B3, (R&D Systems 243-B3), Ascorbic acid (Sigma A-4034), 0.5 mM of dbcAMP (D0627-25MG, SIGMA), and 5 \u00b5M of DAPT (565770; Calbiochem). On day 20, the precursors were split into wells previously coated with Poly Ornithine (15 \u00b5g\/ml)\/human Laminin (1 g\/mL) and Fibronectin (2 \u00b5g\/mL). Cells were differentiated for additional 15, 30, and 60 days, finally providing the timepoints states in the main text of 35, 50, and 80 days. Studied cultures were fixed with PFA 4% and characterized for VM specificity. Immunocytochemistry The differentiated cultures were fixed with 4% PFA (15 min), washed three times with DPBS (15 min), then washed with either TBS1x (low triton protocol for vesicles specific antibodies) or with TBS1+ (for standard protein immunocytochemistry) 3 times for 15 min and then blocked for 2 h with TBS++ with or without low triton. Primary antibodies were incubated for 48 h at 4 \u00b0C. Samples were then washed with TBS 1x\/TBS+ (15 min) three times. The blocking was next repeated for 1 h at room temperature followed by 2 h incubation with the secondary antibodies (all at 1:200 dilution). The antibodies used are listed in Supplementary Table 4 in Supplementary Information . The samples were then washed with TBS 1x (15 min) three times and then incubated with nuclear staining DAPI (Invitrogen, dilution 1:5000) for 10 min. After washing twice the DAPI with TBX1x, samples were mounted with PVA:DABCO, dried for 2 h at room temperature, and stored at 4 \u00b0C until imaged. Samples were imaged using an SP5 confocal microscope (Leica\u00ae) and analyzed with FIJI\u00ae is Just ImageJTM\u00ae. Gene expression analysis using Human Neuropathology Panel 50 ng of total RNA per sample was prepared for analysis with a NanoString Human Neuropathology Panel chip. The assay was performed on an nCounter SPRINT Analysis System (Sanford Consortium for Regenerative Medicine Stem Cell Genomics Core, La Jolla) according to the manufacturer\u2019s instructions. The nSolver software by NanoString was used to normalize gene expression data. ROSALIND software (OnRamp Bioinformatics,  ) was then used to interpret targeted gene expression data and to create heatmaps. Data was then analyzed by ROSALIND\u00ae (  ), with a HyperScale architecture developed by OnRamp BioInformatics, Inc. (San Diego, CA) to interpret targeted gene expression data and to create heatmaps. Read Distribution percentages, violin plots, identity heatmaps, and sample MDS plots were generated as part of the QC step. The limma R library 37 was used to calculate fold changes and p -values. Clustering of genes for the final heatmap of differentially expressed genes was done using the PAM (Partitioning Around Medoids) method using the fpc R library (  ) that takes into consideration the direction and type of all signals on a pathway, the position, role, and type of every gene, etc. Hypergeometric distribution was used to analyze the enrichment of pathways, gene ontology, domain structure, and other ontologies. Functional enrichment analysis of pathways, gene ontology, domain structure, and other ontologies was performed using HOMER 38 . Several database sources were referenced for enrichment analysis, including Interpro 39 , NCBI 40 , KEG 41 , 42 , MSigDB 43 , REACTOME 44 , and WikiPathways 45 . Enrichment was calculated relative to a set of background genes, relevant to the experiment. The transcriptomic data has been deposited in Gene Expression Omnibus (GEO) of the National Center for Biotechnology Information and are accessible through GEO Series accession number SE167335 . Neuronal quantification during differentiation Immunostaining images were analyzed using Fiji\u00ae software to quantify the percentage of TH\/DAPI at day 35, 50, and 80, and TH\/FOXA2 and TH\/GIRK at day 50 and the presence of pyknotic nuclei at day 50 and 80. An average of 5 images was quantified for each ratio, and each differentiation was performed at least three times. Neurite quantification Immunostaining images were analyzed with NeuronJ\u00ae software to quantify number of neurites per neuron and neurite length for TH+ cells. Each neuron was analyzed in NeuronJ\u00ae and each trace was automatically measured and organized in order to obtain information for every single cell. Immunostaining images were analyzed with NeuronJ\u00ae software to quantify number of neurites per neuron for MAP2+ cells. An average of five images and ten neurons per image were analyzed at each timepoint for TH+ and MAP2+ data for every iPSC-derived line. Calcium imaging assay We used calcium fluorescence imaging 46 , 47 , 48 , 49 to evaluate the differences in spontaneous activity between healthy and PD neurons. Calcium imaging allowed the monitoring of a large population of neurons, simultaneously and non-invasively (Fig. 2 ). Living neurons were incubated for 30 min in a solution that contained 3 ml of the recording medium (EM, consisting of 128 mM NaCl, 1 mM CaCl 2 , 1 mM MgCl 2 , 45 mM sucrose, 10 mM glucose, and 0.01 M Hepes; pH 7.4) and 4 \u00b5g\/ml of the cell-permeant calcium-sensitive dye Fluo-8-AM. At the end of incubation, cultures were washed with 2 ml of fresh EM to remove residual Fluo-8 and transferred to a glass-bottom dish (Mattek) filled with EM for imaging. The dish was mounted on a Zeiss inverted microscope equipped with a CMOS camera (Hamamatsu Orca Flash 2.8) and an arc lamp for fluorescence. Greyscale images of spontaneous neuronal activity were acquired at 20 Hz for 15 min in a field of view of 2.8 \u00d7 2.1 mm 2 that contained between 300 and 700 neurons. A bright-field image of the monitored region was taken at the end of the recording session for easier cell identification. Data was then analyzed with the custom software NETCAL, run on MatLab\u00ae, to extract the trains of neuronal activations, as follows. Regions of Interest (ROIs) corresponding to cell bodies that exhibited prototypical neuronal morphology were manually drawn on the bright-field images, and their fluorescence intensity as a function of time extracted (Fig. 2a ). These fluorescence traces were then inspected to remove non-neuronal signals (either undifferentiated cells or glia). A Schmitt-trigger method 49 was next used on the fluorescence traces to identify the timing of neuronal activations, finally procuring the set of spike trains for each neuron. The resulting data of neuronal network activity was visualized in the form of raster plots. Collective episodes of coherent activity (network bursts) appeared as the synchronous activation of a large fraction of the neurons in the network in a short time window. Determination of DA release levels The supernatant collected from DA neurons differentiated from CTR (SP11), PD1 (SP12), PD2 (SP13), and isoPD1 (SP12 wt\/wt) for 50 and 80 days (D50 and D80), was harvested and kept directly at \u221280 \u00b0C until the moment of analysis. Before their analysis, medium samples were previously deproteinized with 50 \u00b5l of homogenization medium (100 mL miliQ H2O, 100 mg of sodium metabisulphite (S-1516, Sigma), 10 mg EDTA-Na (E5134, Sigma), 100 mg cysteine (C-4022, Sigma), and 3.5 mL de HClO4 concentrated (Scharlau, 70%); centrifuged at 15,000 rpm during 30\u2032 at 4 \u00b0C and the supernatant was filtered (Millex-HV 0.45 \u03bcm, Millipore) for a posterior HPLC injection. The concentration of dopamine (DA) in supernatant (SN) samples was determined by an HPLC system consistent of a Waters 717plus autosampler (Waters Cromatografia), a Waters 515 pump, a 5 \u03bcm particle size C18 column (100 \u00d7 46 mm, Kinetex EVO, Phenomenex), and a Waters 2465 amperometric detector set at an oxidation potential of 0.75 V. The mobile phase consisted of 0.15 M NaH 2 PO 4 .H 2 O, 0.57 mM 1-octane sulfonic acid, 0.5 mM EDTA (pH 2.8, adjusted with phosphoric acid), and 7.4% methanol and was pumped at 0.9 ml\/min. The total sample analysis time was 50 min and the DA retention time was 3.94 min. The detection limit was 2\u20133 fmol (injection volume 60 \u00b5l). Corresponding dopamine metabolite content was normalized to protein concentration determined previously by Bradford method detection. Average neuronal activity and global network activity (GNA) The average neuronal activity quantified the degree of spontaneous activity in the recordings and was determined by counting the number of activations per neuron and minute, averaging afterward across neurons and realizations of the same line and timepoint. The GNA quantified the capacity of the network to exhibit collective synchronous events ( bursts ), and was determined by, first, counting the neurons that activated together in a sliding window of 1 s in length (corresponding to 20 image frames) without repetition and, second, by normalizing the count with the number of active neurons in the network. GNA thus varied between 0 (no activity) and 1 (full network activation). Bursts appeared in the GNA data as sharp peaks. The higher the GNA amplitude, the higher the number of participating neurons in the burst (Fig. 2c ). Ratio of extreme events of network bursting They corresponded to those GNA episodes in which neuronal participation was much higher than average and relatively to control (CTR) cultures. To compute the number of extreme events, GNA data were analyzed as follows. First, for each recording, background activity was determined by iteratively removing all peaks in the GNA signal that exceed by two standard deviations the average GNA value, procuring a background activity that was typically around GNA \u2245 0.05 (5% of the network). The non-background peaks of the recording were then ascribed as truly bursting episodes, with amplitudes that were typically above GNA = 0.1. Second, all the peak amplitudes observed in CTR cultures were pooled together and the average peak amplitude A CTR and standard deviation SD CTR determined. Then, for each realization and condition (CTR, isoPD1, PD1, and PD2), those bursting peaks that were above A CTR + 2 SD CTR were considered as \u2018extreme events\u2019. The \u2018ratio of extreme events\u2019 R EE was then determined, for each realization, as the ratio between the number of extreme peaks and the total number of observed peaks. Since this definition sets the CTR cultures as reference, some of the CTR realizations exhibited R EE close to 0, while PD1 or PD2 realizations exhibited R EE close to 1. Effective connectivity Since the number of neurons varied among realizations, all connectivity analyses were carried out in networks with randomly chosen 340 neurons, the minimum population in all experiments. Also, to ensure that connectivity and network analyses reflected the impact of bursting behavior, only those recording with at least 10 bursting events were used for connectivity inference. Causal relationships among pairs of neuronal spike trains were inferred using a modified version of the Generalized Transfer Entropy (GTE) 50 . Briefly, given a pair of spike trains corresponding to neurons X and Y, an effective connection was established between X and Y whenever the information contained in X significantly increased the capacity to predict future states of Y (Granger causality). For the actual estimation of the effective connectivity, binarized time series (\u20181\u2019 for the presence of a spike, \u20180\u2019 for absence) were constructed and computed in the fast implementation of GTE run in MatLab. Instant feedback was present, and Markov Order was set as 2. The actual GTE estimate was then compared with those from the joint distribution of all inputs to Y and all outputs to X, setting a connection as significant whenever the GTE estimate exceeded the mean + 1 standard deviations of the joint distribution. This threshold was considered optimal to capture the effective interactions among neurons during bursting episodes, which is the key dynamic characteristic separating CTR and isoPD cultures from PD ones. All network measures and connectivity statistics were computed with this threshold condition. However, for visualization purposes only, the GTE data shown in the functional matrices and network maps were thresholded at mean + 2.5 standard deviations, which procured the top 10% strongest links. In either case, the GTE scores were finally set to 0 (absence of connection) or 1 (connection present), shaping directed yet unweighted connectivity matrices. In all network maps, the directionality of the connections was not shown for clarity. Also, for clarity of language, the term \u2018functional\u2019 was used instead of \u2018effective\u2019 throughout the description of results. Functional communities GTE connectivity data was analyzed, for each culture realization, to obtain the number of functional communities and their interrelation. A functional community corresponds to groups of neurons that are more connected within themselves than with the rest of network. They were detected using a fast implementation of the Louvain\u2019s algorithm on the most significant connected component (Brain Connectivity Toolbox) 51 . Communities were visualized as boxes along the diagonal of the functional connectivity matrices. The strength of a community, i.e., how isolated it is from the rest of the network, was asserted through the community statistic Q. The larger Q, the higher the tendency of the network to split into characteristic communities. Q = 0 corresponds to the situation in which the network is highly integrated and the only community is the network itself, while Q = 1 corresponds to the extreme case in which all neurons are disconnected from one another and there are as many communities as neurons. Cumulative distribution of connections and Kullback\u2013Leibler divergence The effective connectivity matrix was analyzed to extract the distribution of connections p(k), i.e., the normalized histogram of the number of neurons having k connections. This distribution was transformed into the \u2018cumulative distribution function\u2019 CDF(k), which provides the probability that a neuron has many connections less or equal than k. The divergence between two CDF distributions P and S was quantified through the Kullback\u2013Leibler divergence \\(D_{{\\mathrm{KL}}}\\left( {P||S} \\right) = \\mathop {\\sum }\\nolimits_i P\\left( i \\right)\\ln \\frac{{P\\left( i \\right)}}{{S\\left( i \\right)}}\\) , using the function KLDiv.m in MatLab. Significant statistical differences between P and Q were analyzed using the Kolmogorov\u2013Smirnov test. In silico model The model of Compte et al. 52 was used to simulate a network of excitatory, inhibitory, and dopaminergic neurons. The model incorporated soma and synaptic dynamics as well as noise in the form of Poissonian trains of excitatory pre-synaptic potentials. Network construction was set by placing on a bidimensional space the same neurons as the ones observed experimentally in the form of ROIs, but limited to a randomly chosen population of 300 neurons, and axons grew using biologically-realistic rules as described 53 , 54 . Simulations were run for the equivalent of 10 min in the experiments, and 4 realizations were carried out for each pruning condition (no pruning, 10%, 30, and 50% pruned neurites). Raster plots of network activity were analyzed identically as in the experiments to compute the ratio of extreme events, the effective connective, and CDFs. Full details of the model are provided in Supplementary Information . Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability The authors declare that the main data supporting the findings of this study are available within the article and its Supplementary Information files. Extra data are available from the corresponding author upon request. ","News_Body":"Researchers from IDIBELL and the University of Barcelona (UB) report that neurons derived from Parkinson's patients show impairments in their transmission before neurodegeneration. The study used dopaminergic neurons differentiated from patient stem cells as a model. Parkinson's is a neurodegenerative disease characterized by the death of dopaminergic neurons. This neuronal death leads to a series of motor manifestations characteristic of the disease, such as tremors, rigidity, slowness of movement, or postural instability. In most cases, the cause of the disease is unknown; however, mutations in the LRRK2 gene are responsible for 5% of cases. Current therapies against Parkinson's focus on alleviating symptoms, but do not stop its progression. It is thought that early interventions before the appearance of the first symptoms that prevent neuronal death could slow down or even stop the evolution of the disease. However, currently, the diagnosis is based on the appearance of symptoms, when 70% of the neurons have already been lost. A group of researchers from IDIBELL and the University of Barcelona (UB) has identified early functional deficiencies, before death, in neurons derived from patients with genetic Parkinson's. Dr.Antonella Consiglio says, \"These discoveries open the door to early diagnosis, which would allow us to carry out a premature intervention that would slow down neuronal death, and therefore, would stop the evolution of the disease.\" In this work, dopaminergic neurons, the most vulnerable in Parkinson's, differentiated from stem cells (iPSC) of healthy individuals and patients with genetic Parkinson's, have been used as a model. Researchers have observed that these dopaminergic neurons are capable of maturing and forming functional neural networks in culture, in both control and Parkinson's disease conditions. However, this work published in npj Parkinson's Disease shows that neurons from individuals with Parkinson's are more spontaneously active and present more explosion episodes in which, for example, the entire network is activated at the same time. All this occurs before the neurodegeneration. The researchers believe that this early neuronal dysfunction could be contributing to initiating the cascade of events responsible for the death of dopaminergic neurons, and consequently, Parkinson's disease. Furthermore, this work highlights the extraordinary window of opportunity provided by experimental models based on iPSC in the understanding and presymptomatic evaluation of neurodegenerative diseases. ","News_Title":"An early neuronal dysfunction in Parkinson's that could help early diagnosis","Topic":"Medicine"}
{"Paper_Body":"Abstract The degree to which species can rapidly adapt is key to survival in the face of climatic and other anthropogenic changes. For little brown bats ( Myotis lucifugus ), whose populations have experienced declines of over 90% because of the introduced fungal pathogen that causes white-nose syndrome (WNS), survival of the species may ultimately depend upon its capacity for adaptive change. Here, we present evidence of selectively driven change (adaptation), despite dramatic nonadaptive genomic shifts (genetic drift) associated with population declines. We compared the genetic makeups of wild survivors versus non-survivors of WNS, and found significant shifts in allele frequencies of genes associated with regulating arousal from hibernation (GABARB1), breakdown of fats (cGMP-PK1), and vocalizations (FOXP2). Changes at these genes are suggestive of evolutionary adaptation, given that WNS causes bats to arouse with unusual frequency from hibernation, contributing to premature depletion of fat reserves. However, whether these putatively adaptive shifts in allele frequencies translate into sufficient increases in survival for the species to rebound in the face of WNS is unknown. Introduction Events that kill large portions of populations, including naturally and anthropogenically induced disasters, increasingly threaten biodiversity 1 , 2 . Invasive species are a major trigger of these declines 3 , including invasive pathogens, against which native species can experience high mortality due to a lack of co-evolutionary defenses 4 , 5 , 6 . Introduced fungal pathogens can be particularly dangerous\u2014they can frequently survive in the environment for extended periods, affect a relatively broad range of hosts, and can be highly virulent 7 , thereby driving mass-mortalities of native species (e.g. amphibian chytrid 8 , snake fungal disease 9 , sea fan aspergillosis 10 , and others 11 , 12 , 13 ) as well as threatening agricultural crops 14 , 15 (e.g. rice blast disease 16 and Fusarium wilt in bananas 17 ). Although host mortalities may have little impact on fungal pathogens, the pathogens can exert incredibly strong selective pressures on their host populations 18 . A pressing conservation question is whether host populations can evolve resistance or tolerance during such epidemics\u2014a necessary first step towards preventing extinction. Strong selective pressures might theoretically lead to an evolutionary rescue effect if host populations adapt 19 . However, acute events that kill off most members of a species also reduce the genetic diversity upon which natural selection can act, thereby limiting the capacity for adaptive change 20 . White-nose syndrome (WNS) is a disease affecting bats, which is caused by the invasive fungus Pseudogymnoascus destructans 21 . This highly destructive pathogen has decimated populations of bats, with 12 North American species currently affected 22 , and some populations experiencing losses of 90\u2013100% 23 . The fungus was first inadvertently introduced to North America by humans in 2006 (in the northeastern U.S.) 24 , and is spreading across the continent, largely via infected bats 25 . The exact mechanism of death is not known, but bats apparently die from secondary physiological complications (e.g. depleted fat reserves) associated with too frequent arousals from hibernation 26 . Here, we conduct a genome scan to test for evidence of evolutionary changes in little brown bats ( Myotis lucifugus ) in response to WNS. The recent expansion of the fungus into our study area in 2014 combined with the staggering impact of WNS on the local population (roughly 78%) 27 provides an opportunity to study the initial evolutionary effects of this pathogen, which continues to spread throughout the continent. Eurasian bats within the genus Myotis \u2014in the native range of the pathogen\u2014tolerate fungal growths with no noticeable mortality 28 , 29 . In contrast, little brown bats were the most common bats in eastern North America prior to WNS, but due to population losses, the species has now been listed as endangered by the IUCN 30 and the federal government of Canada 31 , with a similar decision by the U.S. government pending 32 . Despite large observed declines, some individuals may have greater genetic-based tolerance or resistance to the disease, raising the potential for adaptive change in little brown bats via selective forces acting on standing genetic variation. However, dramatic population losses may confound the effectiveness of selection or purge potential adaptive variants via genetic drift. Information about these evolutionary processes can help inform the tempo and pace of management efforts for this species, by indicating which, if any, populations are adapting to the pathogen and what traits may be important for survival. Results In our tests for evolutionary changes in little brown bats, we compared the genetic makeup of \u201csurvivors\u201d and \u201cnon-survivors\u201d of the disease (see Fig. 1 ) in a genome-wide survey of 19,797 single nucleotide polymorphisms (SNPs) among 14,345 loci (140 bp segments) generated from a reduced representation library (ddRadSeq 33 ). We detected the effects of stochastic, non-adaptive genomic changes in otherwise neutral portions of the genome (genetic drift) reflective of the large numbers that have died from WNS in this species. Nevertheless, we also identified genetic changes (based on F ST -outlier analyses) that may have contributed to survival (as opposed to changes simply due to strong genetic drift), where the signature of selection can be detected by levels of genetic differentiation at a gene that exceeds background levels across the genome 34 , 35 . See methods for more details. Figure 1 Sampling locations of little brown bats. ( A ) Sequenced survivors ( n = 9, marked by stars) and non-survivors ( n = 29, crosses), jittered around similar collection sites (black dots); the size of the symbol indicates relative differences in the number of samples per site (see Table S1 for details). Survivors undertake short-distance migrations away from hibernacula in spring, which is reflected in their scattered collection locations. Non-survivors are closely associated with underground hibernation sites, with most ( B ) collected within hibernacula (~26 carcasses marked by circles on the floor of a mine), although some ( C ) leave these sites prematurely, like these dead bats on the outer screen of a house <1 km from a hibernaculum (note the snowy landscape). Photo credits A. Kurta (top) and C. Rockey (bottom). Full size image Non-adaptive evolution associated with large number of deaths caused by WNS To visualize the drift-induced changes that have occurred broadly across the genome, a PCA generated using the survivors, onto which the non-survivors were projected (Fig. 2 ), indicated the genomic makeup of survivors differs substantially from the non-survivors (which is robust to more stringent criteria for data filtering; Fig. S1 ). Quantification of the rate of evolutionary change from an inferred common ancestor showed the rate of drift is an order of magnitude higher in survivors (mean F = 0.04 \u00b1 SE 0.0001) relative to non-survivors ( F = 0.006 \u00b1 0.0003), using the F -model in Structure 36 , 37 . This amount of drift-induced genetic change (Fig. 2 ) is especially striking given that these changes have accumulated over, at most, three years (with most of our samples separated by just one year; Table S1 ), in a species that can live for well over 20 years 38 , 39 and in which females typically produce one pup per year 40 . Figure 2 Stochastic drift induced genetic change. ( A ) PCA of survivors of WNS, with non-survivors projected onto the PC axes; PC1 explained 27% and 66% of the variance among survivors and non-survivors, respectively, and PC2 explained 13% and 6% of the variance. ( B ) The estimated degree of genetic drift ( F , as estimated in Structure 36 , 37 ) is an order of magnitude greater for survivors compared to non-survivors, as illustrated by the contrasting branch lengths from an inferred common ancestor. Full size image Selective divergence putatively driven by WNS Quantification of locus-specific differentiation across the genome using F ST -outlier analyses identified nine SNP alleles that are significantly more common among survivors than non-survivors across all three outlier detection methods (Table S2 ; for details on individual genotypes see Table S3 ). These nine variable sites were the only outliers identified using the AMOVA-corrected F ST from STACKS (Fig. 3 ), and were also among the outliers recognized in the two other tests (see Figs. S2 and S3 ). Analyses with and without four non-survivors that were collected several years prior to other samples (in 2014; Table S1 ) confirmed the robustness of these results to different collection dates (Figs. S2 \u2013 S4 ). Figure 3 Putative loci under positive selection. AMOVA-corrected F ST -values of SNPs versus alignment position, highlighting the three genes that our SNPs map to, as well as an outlier SNP nearby to PLA2G7 (*), and the outlier SNP which is adjacent to CGMP-PK1 ( \u2020 ). The dashed line marks the significance threshold and alternating colors indicate different genomic scaffolds (1,214 in our dataset). Full size image Comparison of the nine top-candidate loci with the M. lucifugus reference genome (MYOLUC 2.0 41 ) indicates three mRNA-coding SNPs are located in introns of annotated genes (Table S2 ). These three genes are: the gamma-aminobutyric acid (GABA) receptor subunit beta-1 (GABRB1; Gene ID 102432079 in the reference genome), cyclic guanosine-3\u2032,5\u2032-monophosphate-dependent protein kinase 1 (cGMP-PK1; Gene ID 102431010), and the forkhead box P2 protein (FOXP2; Gene ID 102423801). Two other SNPs are close to annotated genes\u2014one was near the previously identified cGMP-PK1 gene in our dataset (3,387 bp away), and the other was near phospholipase A2 group VII (PLA2G7; Gene ID 19253; 2,747 bp away). The remaining four SNPs are relatively distant from any area of the reference genome with known function (>170,000 bp away on average). Discussion We studied the genetic differences between wild little brown bats that were survivors versus non-survivors of WNS, and found evidence that there is likely a genetic component to survivorship for individuals facing this disease. This apparent adaptation has occurred very quickly since the detected evolutionary changes took place after the WNS introduction in 2014, and survivors were sampled just a few years later. The putative selectively driven genetic changes we identify (Fig. 3 ) have also occurred despite dramatic nonadaptive genomic shifts (genetic drift; Fig. 2 ) associated with population declines due to the disease. Together, this suggests that the putative adaptive changes have resulted from very strong selective forces acting on standing genetic variation. Such rapid evolutionary changes are not unprecedented. For example, populations of the steelhead trout ( Oncorhynchus mykiss ) introduced to the central USA from coastal areas show signs of adaptation to freshwater conditions, despite small founder populations 42 . Likewise, extremely rapid phenotypic adaptation in Caribbean lizards followed a hurricane, with surviving lizards having larger toe pads which were presumably better at gripping surfaces during strong winds 43 . The putatively adaptive SNPs among the surviving bats in our study are located within or in close proximity to four genes (cGMP-PK1, FOXP2, GABARB1, and PLA2G7), which when mapped to the annotated reference genome suggest different ways adaptive shifts might contribute to survival. GABARB1 is a receptor for the neurotransmitter GABA, which is a major neural inhibitor in the brains of vertebrates, and has also long been suspected to be involved in regulating hibernation 44 . In addition to GABA, these receptors are also sensitive to histamines 45 , which similarly help regulate hibernation in mammals 46 and are released in response to tissue damage from WNS 47 . The importance of an individual\u2019s sensitivity to histamines is further hinted by PLA2G7, which regulates release of histamines from mast cells 48 . Because arousals account for 80\u201390% of bats\u2019 energy budget during hibernation 32 , genetic variation that contributes to even small changes in arousal frequencies could result in large differences in energy expenditures, making the difference between life and death (i.e., affecting susceptibility to WNS). We speculate that bats genetically predisposed to release fewer histamines, or be less prone to arousals induced by histamines, are better able to survive WNS through conservation of energy reserves. Links between metabolic demands and survival are further suggested by cGMP-PK1, which was implicated by two significant SNPs in our dataset (one within the gene and one nearby). This gene is part of pathways involving cellular metabolism and breakdown of fat, and allelic variants have been linked to obesity in mammals 49 , 50 , which might prove beneficial for WNS-infected bats facing premature depletion of winter fat reserves. In fact, a recent study documented a post-WNS phenotypic shift towards fatter bats of this species 51 . Although this may be due to a variety of potential mechanisms, including non-evolutionary ones (see discussion in 51 ), our findings suggest a genetic component to this shift. In contrast to the SNPs linked to physiological mechanisms during winter hibernation, a SNP within FOXP2 suggests behavioral differences might confer a selective advantage. Specifically, FOXP2 is associated with vocalizations in other vertebrates, and echolocation in bats 52 . Because variation in calls is closely associated with the type of prey and habitat bats must navigate, echolocation is an important functional trait, and potentially adaptive shifts might be related to hunting proficiency, speed of developing foraging abilities in juvenile bats, or subtle differences in prey preferences. These could affect the type and amount of fat that bats store for hibernation. In addition to echolocations for hunting, bats also emit social calls. Sociality may influence the impact of the disease in this species 53 , and due to the importance of FOXP2 in communication, the gene has been linked to variations in social behavior in other species 54 , 55 , 56 . A more detailed study is needed to test these hypotheses, and there are possibly alternative unknown functions of FOXP2 in bats. Interestingly, no individuals in our dataset were heterozygous for this SNP. Although outlier analyses can contain false positives, potentially inferring selectively driven differentiation when there is none 57 , we think it is unlikely the mRNA-coding SNPs we detected are statistical artifacts. The four genes we identify had putatively adaptive alleles that were entirely absent from our non-survivors (with the exception of a single allele copy in one individual). With the much greater sampling of non-survivors ( n = 29) this difference is also not due to limited sampling (Fig. 1 ). An alternative consideration is that genetic drift, not selection, explains the elevated differentiation in what we identified as putatively adaptive alleles among the survivors. With inter-locus contrasts, the genome serves as the expected background for differentiation caused by drift (i.e., the expected variance in F ST -values in this case; Fig. 3 ). However demographic processes can inflate the variance of the distribution of F ST -values (e.g. population structure such as isolation by distance or expansion; reviewed in Hoban et al . 57 ), potentially confounding the signals of selection and drift. Although we cannot rule out a role for non-selective processes, we note that annotation of the alleles suggests that selection is involved given that the functions are consistent with an adaptive response. Whether the putative adaptive changes described here reflect host resistance or tolerance to the fungal pathogen has consequences for evolutionary and ecological pressures, as well as management strategies. While our study does not explicitly test whether bats survive WNS via resistance versus tolerance mechanisms, and the genomic approach we used only looked at a small portion of the genome, we found putative selection acting on non-immune genes, which suggests disease tolerance 58 may be important. Specifically, the alleles we identify could assist some bats in \u201cholding out\u201d until spring, when they leave sites in which growth of the pathogen is restricted to. While infected bats do exhibit an immune response to the fungus 59 , they likely ultimately die due to secondary physiological complications linked to starvation while hibernating 26 , 60 . Such tolerance in little brown bats to WNS may be important for survival in both in intraspecific 61 and interspecific 62 contexts. However, others argue that resistance is the primary mechanism of survivorship 63 . Future work is needed to resolve this question. Conclusions What the outcome of the evolutionary change we report here might be and what it bodes for the future recovery of little brown bats is not clear\u2014it is too soon to claim that the species will be \u201csaved\u201d via an evolutionary rescue effect. There have been dramatic population declines, and low population sizes inherently make species vulnerable to further perturbations. Furthermore, the disease has only been present in North America for thirteen years at the time of this publication, and with little brown bats surviving to more than 20 years old in the wild 38 , 39 it will take time to determine whether surviving remnant populations have sufficient reproductive and recruitment levels to avoid extinction or extirpation. However, the functions of the genes we identify suggest that for this species, and possibly other bats effected by WNS, conservation of summer foraging habitat\u2014not just winter hibernation sites\u2014may promote population recovery, given that the selective advantages underlying shifts in FOXP2 would most likely manifest when bats are echolocating and hunting, and not in the hibernation sites where the bats are confronted with the fungus (Fig. 1 ). Other genes we identified are likely subject to strong selection during winter periods of infection, but could also be important year-round (cGMP-PK1, PLA2G7, and GABRB1), given their functions in cellular metabolism. With the limited representation of the genome, there may also be selective divergence in genes not studied here. Nevertheless, even without more extensive coverage of the genome, our work hints at the multifaceted nature of selection by identifying genes whose roles differ across habitats of highly seasonal environments, and are linked to both physiological and behavioral traits. Materials and Methods Study area We chose northern Michigan, USA, for our study because it represents a reasonably isolated population of little brown bats (Fig. 1A ); WNS is present throughout our study area, and was first detected there in early 2014. We sample non-survivors from hibernation sites during the winter and survivors during the summer (when they are no longer afflicted by the pathogen). However, because the species utilizes short distance seasonal migration (typically \u2264 500 km 64 ), during warmer periods they do not roost in the same sites in which they hibernate, thus the relative geographic isolation is important for assuring that bats sampled during both seasons were from the same population. Winter hibernation sites are concentrated in the northwestern portion of our study area (hibernation sites are lacking in the central and southern Michigan), and primarily consist of abandoned iron and copper mines. As a consequence, bats in our area (Fig. 1A ) are isolated from other populations by two factors: the Laurentian Great Lakes and the lack of suitable subterranean hibernation sites within migration range in central and southern Michigan. The seasonal sampling of bats is necessary because WNS non-survivors can only be documented in winter areas, and disease survivors can only be identified during summer. Sampling of focal species All sampled bats (Table S1 ) were categorized as either \u201csurvivors\u201d or \u201cnon-survivors\u201d of WNS. Survivors ( n = 9) were adult bats that had been born the previous year or earlier and thus had survived at least one hibernation period with the WNS pathogen (collected during summer s of 2016\u20132017, see Anthony 65 for aging methodology). Most individuals which succumb to the disease are found within the subterranean sites that afflicted species of bats rely upon in winter, and in which the fungus thrives, however some infected bats leave hibernation sites prematurely in winter in search of food or water, but quickly die due to lack of available resources and sub-freezing temperatures. Correspondingly, most non-survivors we sampled were bats found dead in or near hibernation sites during winter (collected in early 2016; n = 25; Fig. 1 ), although some tissue samples came from individuals with the pathogen that were euthanized during surveillance studies (i.e., they tested positive for the fungus; collected in early 2014; n = 4). Note that comparing survivors to this more general group of non-survivors makes tests for loci under selection more conservative, in that some of the euthanized bats categorized as non-survivors may not have died from WNS naturally. However, if non-survivors actually carried adaptive alleles, this would not produce a bias (i.e., make it more likely) to detect putatively selected alleles\u2014in fact it would make such detection more difficult. In addition, all analyses were repeated excluding the euthanized bats to confirm the robustness the results. Samples for most non-survivors ( n = 23) were from bat carcasses found during winter either in or proximal to the caves or mines in which they were hibernating. Prior to the introduction of WNS, it was uncommon to find dead bats at hibernacula, whereas conspicuous numbers of dead individuals are found in and around these sites post-introduction of the disease (Fig. 1B ), and all sites were WNS-positive at the time of collection. The accidental inclusion of bats which had died due to other causes would make it more difficult to detect adaptation in our analyses. To reduce disturbance to hibernating bats, dead bats were collected in conjunction with routine surveys by the Michigan Department of Natural Resources (MDNR) and Eastern Michigan University. Four samples were contributed by the U.S. Geological Survey National Wildlife Health Center; these bats were found during hibernation with the fungus growing on them, but were euthanized (as discussed above). Lastly, two samples of non-survivors came from the MDNR Wildlife Disease Laboratory (see details below). Among the survivors, collection methods varied (Table S1 ). Three survivors were captured during summer using mist-nets, and visual inspection confirmed evidence of recovering from WNS (i.e., the presence of healing wing lesions or scars). Tissue samples were collected via small biopsy punches (2 mm diameter, one punch for each wing, Premier Medical Products Company, Plymouth Meeting, Pennsylvania, USA), after which bats were immediately released. No individual was detained for longer than 30 minutes. Eight specimens were contributed by the MDNR Wildlife Disease Laboratory, which annually receives large numbers of bats for rabies testing after they are encountered by humans or pets 66 . All individuals used in this study tested negative for the rabies virus. Six of these were considered survivors because they were submitted for testing in summer or fall; during the summer this species uses structures such as houses in addition to trees 40 so there is no reason to believe that animals encountered by people during warmer periods were unhealthy. However, at the latitude of our study, little brown bats are not known to hibernate in buildings 40 . Consequently, any individual encountered by humans during sub-freezing periods is almost certainly on the cusp of dying from WNS. Individuals submitted to the MDNR Wildlife Disease laboratory in winter or early spring were therefore assigned to the non-survivor group ( n = 2 in this study). DNA sequencing and data processing DNA was extracted from membrane of wing tissue using DNeasy Blood and Tissue Kit (Qiagen, Valencia, CA, USA) and used to prepare a reduced representation genomic library for sequencing. Two restriction enzymes, Eco RI and Mse I, were used to digest extracted DNA (ddRadSeq 33 ), to which barcodes (unique tags 10 base-pairs long) and adapters for Illumina sequencing were then ligated. Ligation and amplification were done via polymerase chain reaction (PCR), and 350 to 450 bp long fragments were size selected using Pippin Prep (Sage Science, Beverly, Massachusetts, USA). The library of 38 samples was sequenced in one HiSeq. 2500 lane (Illumina, San Diego, CA, USA), at the Centre for Applied Genomics (Toronto, Ontario, CA). Genomic sequences were demultiplexed using the STACKS bioinformatics pipeline 67 (v. 2.1; specifically process rad-tags , gstacks , and populations ), and processed in conjunction with supporting programs. The first step, process radtags , allowed up to one mismatch in the adapter sequence and two mismatches in the barcode, with rescue of RAD-Tags allowed. A sliding window of 15% of the read length was used for an initial exclusion of any reads with a Phred score 68 below 10 within the window (note additional filters of a minimum Phred score of 30 were applied in downstream processing, as discussed below). Of 102,419,857 initial sequences, process radtags removed 1,144,865 reads containing the adapter sequence, 18,775,218 reads with ambiguous barcodes, 156,274 low quality reads, and 2,495,192 reads with ambiguous RAD-Tags. We then indexed a previously generated reference genome for the species, ftp:\/\/ftp.ncbi.nih.gov\/genomes\/Myotis_lucifugus (7x coverage; MYOLUC v. 2.0 41 ), and mapped our sequences to the genome using the Burrows-Wheeler Alignment Program (v. 7.17) indexing and MEM algorithms, respectively 69 , 70 . The resulting files were filtered (-F 0x804, -q 10, -m 100), converted to .bam files, and sorted using SAMtools 71 , 72 (v. 1.8-27). The reference-based method of gstacks (set to remove PCR duplicates) was run using the Marukilow model 73 , minimum Phred 68 score of 30, and alpha thresholds (for mean and variance) of 0.05 for discovering single nucleotide polymorphisms (SNPs). This resulted in 59,888,201 BAM records and 581,607 loci (8% of reads were excluded because they were excessively soft-clipped, and 3% had insufficient mapping qualities to be included). All remaining loci were genotyped, with a mean per-sample coverage of 10.5x \u00b1 7.1x, a mean of 138.5 bps per locus, and consistent phasing for 88.3% of diploid loci. Populations was then run with default settings and the resulting loci were filtered with a custom script in R 74 (v. 3.5.0) to remove loci and SNPs that may be artifacts of sequencing or alignment errors (Fig. S5 ) based on the number of SNPs per read position, resulting in exclusion of SNPs occurring in the last 2 bp of each read. Loci with unusually high levels of diversity were also removed from consideration (threshold \u03b8 > 0.026), leaving 273,261 unique loci. Using the list of vetted loci and SNPs, populations was then run again, retaining loci present in at least 56% of both survivors and non-survivors, ensuring a minimum sample size of at least six survivors; note the actual missing data was typically much lower (i.e., <15% in all but 7 individuals of survivors and non-survivors). This resulted in 40,963 loci (140-bp segments), of which were variable, containing 19,797 SNPs (our final SNPs), all of which had a minor allele frequency of >0.01. Minor allele thresholds of 0.01 and 0.05 were evaluated for downstream analyses, and when warranted the higher threshold was used (noted below). Mean genotyped sites per locus was 142.41 bp ( SE \u00b1 0.02). Because some loci contained more than one SNP, the robustness of downstream analysis to inclusion of multiple versus a single SNP per 140-bp fragment was evaluated. Main findings did not differ, thus we present analyses based on multiple SNPs per locus in the main text (see Fig. S6 for results based on a single SNP per locus). We also checked that the data were not biased due to different levels of genetic decomposition between the survivors and non-survivors by analyzing the Guanine-Cytosine (GC) content of each sample. Specifically, raw Illumina reads (immediately after process radtags ) of survivors were compared with the non-survivors using BBMap 75 (v. 38.01). The proportion of GC per individual per locus was averaged across all loci for each individual using a custom script in R. Mean GC content was 43% for survivors ( n = 9) and 42% ( n = 29) for non-survivors, which confirmed non-survivors were not biased towards higher GC because of decomposition. In addition, the relatedness of sampled individuals was evaluated in two ways: with related 76 in R 74 and using Plink 77 . Due to program constraints in related , 250 loci were randomly selected to simulate 100 pairs of individuals in each of four categories: parent-offspring, full sibling, half sibling, and unrelated. Application of the Ritland estimator of relatedness 78 to both the simulated and empirical dataset of 1,242 filtered SNPs (see Fig. S7 caption) indicated that none of the individuals in our dataset were related with the exception of two of the non-survivors, which may be half-siblings (Fig. S7 ). However, the Plink 77 analysis of 6,237 SNPs (restricted to a single SNP per locus and minor allele frequency >0.05, as per guidelines) indicated no related individuals within our dataset. We kept all individuals in downstream analyses, because the presence of a single pair of potential half siblings is not expected to influence estimates of allele frequencies or F ST , and removal of putatively related individuals can actually increase the error (for more details, see Waples & Anderson 79 ). Lastly, to confirm that individuals from different sampling sites within the study area could be considered one population, we used Structure 37 (v. 2.3.4 ) to evaluate if genome-wide differentiation indicated a single, panmictic population. We selected the ADMIXTURE model with \u2018Allele frequencies correlated\u2019 turned on and no prior information about sampling population and explored the best supported model, considering a range of genetic clusters (i.e., k = 1 to 5) with 10 repetitions for each k , for 500,000 Markov chain Monte Carlo iterations with a burn in of 50,000. Visual assessment was used to ascertain convergence by examining plots of F ST , alpha, and likelihood versus iterations, and to check for consistency among the ten iterations. No evidence of genetic subdivision based on geographic sampling locality was detected (see Fig. S8 ). Tests of genetic drift Given the large numbers that have died from WNS in this species, genetic differentiation between survivors and non-survivors may result because some alleles, just by chance, will increase or decrease in frequency. These stochastic, non-adaptive genomic changes in otherwise neutral portions of the genome (genetic drift) can be particularly great when only a small proportion of the population survives, sometimes causing population bottlenecks. To visualize the drift-induced changes that occurred broadly across the genome, we conducted a principal components analysis (PCA) of the survivors, and projected the non-survivors onto the estimated PC axes, and the degree of drift was quantified using the F -model 36 in STRUCTURE 37 . The PCA was calculated for the survivors, onto which the non-survivors were projected (by applying the same scaling and centering used for survivors to the non-survivors; see Lipson et al . 80 ). Generating a PCA in this manner is a method of visualizing differences when one group is a subset of the other (in terms of the proportion of variance), for example due to a series of founder events 80 .The PCA was performed in R 74 , in conjunction with the packages Adegenet 81 (v. 2.1.1) and Plyr 82 (v. 1.8.4) using the prcomp function. One survivor and four non-survivors were excluded from this analysis because of missing data (i.e., >50% missing loci), as were loci missing in >50% individuals (data were filtered using Plink v. 1.07 77 ; see Table S1 ). After this, the actual missing data was <15% for all individuals except one survivor and one non-survivor, with just under 50% missing data. Missing data were then replaced with the per locus mean value across all individuals. Only genomic sites with a minor allele frequency of \u22650.05 that were variable in both survivors and non-survivors were considered, for a total of 11,462 SNPs. The PCA was repeated to confirm the robustness of the results to missing data threshold, this time using a minimum data threshold of 8.7% missing data per individual and 19% per locus (mean missing data was 1.9%), which resulted in 13,666 loci and 31 individuals being included. We also directly estimated the amount of genetic drift between survivors and non-survivors in S tructure 37 using the F -model 36 (see also Harter et al. 83 .). The F -model accounts for differences in population sizes, and has been used to quantify differences in drift between groups of contrasting sample sizes that are similar in proportion to our own 83 . For our parameter of interest, F , we used a prior mean and SD of 0.10, which places similar probabilities on both large and small values of F . To implement this Bayesian approach, we preassigned individuals to one of the two groups (survivor or non-survivor), and used a burn-in of 50,000 followed by 500,000 reps. We fixed lambda at 1, and used a uniform prior from 1 to 10 for alpha, with a standard deviation of 0.025. Three iterations were run, with different random seeds for initiating the Markov Chains. Tests of loci under selection To identify genetic differences among individuals that might have contributed to their survival of WNS, we used F ST -outlier analyses, where the signature of selection can be detected by considering the proportional split of allelic variants between groups relative to background levels across the genome 34 , 35 . We identified candidate loci using three methods of outlier detection\u2014identification of outliers via (i) the number of standard deviations from the mean using an AMOVA-corrected F ST 84 , (ii) by assessing confidence intervals from bootstrap permutation across loci, and (iii) measuring departure from a chi-squared distribution (detailed below). Variable sites which met all three requirements were regarded as candidate loci apparently undergoing positive selection. All tests of selection were conducted with and without the four non-survivors sampled in 2014 (collected prior to the other specimens), to confirm that the results were robust. Note that the low number of sampled survivors reflects the devastating impact of WNS on this species; despite the small sample size, it is not beyond a size in which SNPs under selection can be detected with F ST -outlier analyses 85 . In our first approach, we used the AMOVA-corrected F ST 84 calculated by populations in STACKS 35 . SNPs with an F ST -value of greater than nine standard deviations from the mean (mean = 0.018 \u00b1 1 SD of 0.026) were considered outliers (similar to Willoughby et al . 42 ). A threshold of five standard deviations is often used in detection of outlier SNPs under positive selection 42 , 86 , 87 . We increased our threshold of significance to nine standard deviations to reduce the potential for false-positives. In the second approach, confidence intervals (95% CI) were estimated using diveRsity 88 . Using the diffCalc function, Weir and Cockerham\u2019s F ST 89 was calculated for all loci, with 1,000 bootstraps performed across loci. Only loci for which the lower limit of the CI remained five SD from the mean were considered outliers. In the third approach, outliers were identified with OutFLANK 90 , which estimates the expected neutral variation of F ST -values under a chi-squared distribution. As per the developer guidelines 90 , we excluded loci with low expected heterozygosity (<0.1), and visually adjusted the trim functions to best fit the observed distribution (LeftTrimFraction = 0.3 and RightTrimFraction = 0.05; Fig. S9 ). Significance was assessed using qvalue 91 in R 74 (v. 2.12). All results were visualized in R 74 , often in conjunction with the package ggplot2 92 . A custom script was used to identify SNPs which were identified as candidate loci under all three methods, and putatively selected sites were then cross-referenced with the species\u2019 annotated reference genome 41 to infer possible phenotypic function (see 93 , 94 for additional information on the reference genome and annotation). If the SNP\u2019s position was not within a gene, the nearest annotated areas in each direction were identified. Data availability Genomic data (raw reads) will be made available on GenBank (SRA accession PRJNA563655). All commands (STACKS, Structure ) and scripts (PCA, F ST ) used for analyses are available on GitHub (  ). Change history 24 March 2020 An amendment to this paper has been published and can be accessed via a link at the top of the paper. ","News_Body":"A new study from University of Michigan biologists presents the first genetic evidence of resistance in some bats to white-nose syndrome, a deadly fungal disease that has decimated some North American bat populations. The study involved northern Michigan populations of the little brown bat, one of the most common bats in eastern North America prior to the arrival of white-nose syndrome in 2006. Since then, some populations of the small, insect-eating bat have experienced declines of more than 90%. U-M researchers collected tissue samples from wild little brown bats that survived the disease, as well as individuals killed by the fungal pathogen. They compared the genetic makeup of the two groups and found differences in genes associated with regulating arousal from hibernation, the breakdown of fats and echolocation. \"Because we found differences in genes associated with regulating hibernation and breakdown of fats, it could be that bats that are genetically predisposed to be a little bit fatter or to sleep more deeply are less susceptible to the disease,\" said U-M's Giorgia Auteri, first author of a paper scheduled for publication Feb. 20 in the journal Scientific Reports. \"Changes at these genes are suggestive of evolutionary adaptation, given that white-nose syndrome causes bats to arouse with unusual frequency from winter hibernation, contributing to premature depletion of fat reserves,\" said Auteri, a doctoral student in the Department of Ecology and Evolutionary Biology who conducted the study for her dissertation. The other author of the Scientific Reports paper is U-M biologist Lacey Knowles, Auteri's faculty adviser. While the study was small\u2014involving tissue samples from 25 little brown bats killed by white-nose syndrome and nine bats that survived the disease\u2014the authors say their sample size is large enough to detect genetic changes driven by natural selection. A larger follow-up study is underway, expanding both the number of bats and the areas affected by the disease, to develop a fuller picture of adaptive change that may be key to the species' survival. The fungal pathogen that causes white-nose syndrome was inadvertently introduced in the northeastern United States in 2006 and is currently spreading across the continent. Thirteen species of North American bats are currently affected, with some populations experiencing losses of 90-100%. The disease is named for a distinctive fungal growth around the muzzles and on the wings of hibernating bats. The U-M team's study area is Michigan's northern Lower Peninsula and Upper Peninsula. White-nose syndrome fungus was first detected there in 2014, and its arrival allowed the researchers to study the pathogen's initial evolutionary impact. For the study, the U-M researchers collected tissue samples from dead little brown bats found in or near hibernation sites during the winter. The hibernation sites were concentrated in the western Upper Peninsula and primarily consisted of abandoned iron and copper mines. During the summer, they also collected small tissue samples from survivors that emerged successfully from hibernation despite exposure to the disease. Surviving bats had healing wing lesions or scars from the fungus. In the laboratory, DNA was extracted from the tissues and sequenced, and the sequences were mapped to a previously generated reference genome for the species. A genome scan was conducted to test for evidence of evolutionary changes in response to white-nose syndrome. The researchers found significant differences in three genes associated with arousal from hibernation (GABARB1), breakdown of fats (cGMP-PK1) and echolocation (FOXP2), as well as a fourth gene (PLA2G7) that regulates the release of histamines from mast cells. \"The function of one gene we identified hints that summer activities such as hunting via echolocation may be an important determinant of which individuals survive the winter infection period,\" Auteri said. \"This suggests that conservation of summer foraging habitat\u2014not just winter hibernation sites\u2014may promote population recovery in bats affected by white-nose syndrome.\" The observed genetic differences are suggestive of very rapid\u2014though not unprecedented\u2014evolutionary adaptation driven by natural selection, according to Auteri and Knowles. \"This apparent adaptation occurred very quickly, involves genes with a variety of functions which likely act across seasons in order to contribute to survivorship, and has taken place despite an observable reduction in genetic diversity associated with population declines,\" said Knowles, a professor in the Department of Ecology and Evolutionary Biology and a curator at the U-M Museum of Zoology. Auteri and Knowles said it's too soon to say how the evolutionary changes they uncovered are likely to affect the little brown bat's prospects. After all, these bats have suffered dramatic population declines, and low population sizes inherently make a species more vulnerable to further perturbations. \"But we're finding the hint that there could be these genetic changes that are occurring that might provide some type of survival in the future,\" Knowles said. \"So as these variants increase, there's some hope that these bats are not all going to die from the disease itself.\" Because little brown bats only have one pup per year, recovery of the species would likely take a long time, according to Auteri and Knowles. Due to population losses, little brown bats have been listed as endangered by the International Union for Conservation of Nature and by the federal government of Canada, with a similar decision by the U.S. government pending. ","News_Title":"First genetic evidence of resistance in some bats to white-nose syndrome, a devastating fungal disease","Topic":"Biology"}
{"Paper_Body":"Abstract The standard model of particle physics is remarkably successful because it is consistent with (almost) all experimental results. However, it fails to explain dark matter, dark energy and the imbalance between matter and antimatter in the Universe. Because discrepancies between standard-model predictions and experimental observations may provide evidence of new physics, an accurate evaluation of these predictions requires highly precise values of the fundamental physical constants. Among them, the fine-structure constant \u03b1 is of particular importance because it sets the strength of the electromagnetic interaction between light and charged elementary particles, such as the electron and the muon. Here we use matter-wave interferometry to measure the recoil velocity of a rubidium atom that absorbs a photon, and determine the fine-structure constant \u03b1 \u22121 = 137.035999206(11) with a relative accuracy of 81 parts per trillion. The accuracy of eleven digits in \u03b1 leads to an electron g factor 1 , 2 \u2014the most precise prediction of the standard model\u2014that has a greatly reduced uncertainty. Our value of the fine-structure constant differs by more than 5 standard deviations from the best available result from caesium recoil measurements 3 . Our result modifies the constraints on possible candidate dark-matter particles proposed to explain the anomalous decays of excited states of 8 Be nuclei 4 and paves the way for testing the discrepancy observed in the magnetic moment anomaly of the muon 5 in the electron sector 6 .     Main The fine-structure constant \u03b1 is the pillar of our system of fundamental constants. As the measure of the strength of the electromagnetic interaction in the low-energy limit, it has been measured using diverse physical phenomena: the quantum Hall effect, the Josephson effect, the atomic fine structure, atomic recoils and the electron magnetic moment anomaly 7 . Comparison of results across sub-fields of physics is a powerful test of the consistency between theory and experiment. In particular, the fine-structure constant is a crucial parameter for testing quantum electrodynamics (QED) and the standard model. This test relies on the comparison between the measured value of the electron gyromagnetic anomaly a e = ( g e \u2212 2)\/2 (where g e is the electron g factor) and its theoretical value. The standard-model prediction a e , SM is dominated by the QED term given by a perturbation series of \u03b1 \/\u03c0, and contains additional contributions from hadronic and weak interactions. Numerical and analytical evaluations of the coefficients of the QED series are firmly established up to the eighth order, and the accuracy of the tenth order has been improved over the past years 1 , 2 , 8 . Assuming that the prediction of the standard model is correct, comparison of the theory with the most accurate measurement of the electron magnetic moment 9 leads to a value of the fine-structure constant with a relative accuracy of 2.4 \u00d7 10 \u221210 dominated by experimental precision 9 (see Fig. 1 ). Fig. 1: Precision measurements of the fine-structure constant. Comparison of most precise determinations of the fine-structure constant so far. The red points are from g e \u2212 2 measurements and QED calculations, and the green and blue points are obtained from measurements of caesium and rubidium atomic recoils, respectively. Errors bars correspond to \u00b11 \u03c3 uncertainty. Previous data are from ref. 34 (Washington 1987), ref. 10 (Stanford 2002), ref. 18 (LKB 2011), ref. 9 (Harvard 2008), ref. 2 (RIKEN 2019) and ref. 3 (Berkeley 2018). Inset, magnification of the most accurate values of the fine-structure constant. Full size image From a different point of view, to test the prediction of the standard model, we need independent measurements of \u03b1 with a similar precision to evaluate a e , SM . The most successful independent approach is based on the measurement of the recoil velocity ( v r = \u0127k \/ m ) of an atom of mass m that absorbs a photon of momentum \u0127k (refs. 10 , 11 ). Here \u0127 is the reduced Planck constant ( \u0127 = h \/(2\u03c0)) and k = 2\u03c0\/ \u03bb is the photon wave vector, where \u03bb is the laser wavelength. Such a measurement yields the ratio h \/ m and then \u03b1 via the relation $${\\alpha }^{2}=\\frac{2{R}_{\\infty }}{c}\\times \\frac{m}{{m}_{{\\rm{e}}}}\\times \\frac{h}{m}.$$ (1) The Rydberg constant R \u221e is determined from hydrogen spectroscopy with an accuracy of 1.9 parts per trillion (ppt;  ). The atom-to-electron mass ratio m \/ m e is obtained from the ratio of the relative atomic mass of the atom A r ( m ) (known at 69 ppt for rubidium 12 , 13 ) and the relative atomic mass of the electron A r ( m e ) (known at 30 ppt) 14 . The speed of light in vacuum, c , has a fixed value. Here, we present a measurement of the recoil velocity on rubidium atoms. We measured h \/ m ( 87 Rb) = 4.59135925890(65) \u00d7 10 \u22129 m 2 s \u22121 . In the international system of units adopted in 2019, in which h has a fixed value, we obtain m ( 87 Rb) = 1.44316089776(21) \u00d7 10 \u221225 kg. This is the most accurate atomic mass measurement so far, to our knowledge. This results leads to a fine-structure constant \u03b1 of $${\\alpha }^{-1}=137.035999206(11).$$ The uncertainty contribution from the ratio h \/ m ( 87 Rb) is 2.4 \u00d7 10 \u221211 (statistical) and 6.8 \u00d7 10 \u221211 (systematic). Our result improves the accuracy on \u03b1 by a factor of 2.5 over the previous caesium recoil measurement 3 but, most notably, it reveals a 5.4 \u03c3 difference from this latest measurement. We built a dedicated experimental setup and implemented robust methods to control systematic effects. By accelerating atoms up to 6 m s \u22121 in 6 ms and using typical two-photon Raman transitions as beam splitters for the matter waves, we obtained a relative sensitivity on the recoil velocity of 0.6 ppb in 1 h of integration (0.3 ppb on \u03b1 ). This sensitivity is more than three times better than that obtained using the best atom interferometer based on multi-photon beam splitters 3 , although the latter technique is expected to provide a substantial gain in sensitivity with respect to Raman transitions 15 , 16 . The unprecedented sensitivity of our atom interferometer enables us to experimentally evaluate and mitigate several systematic biases. We recorded data with different experimental parameters, reinforcing the overall confidence of our error budget. We also implemented a Monte Carlo simulation that includes both the Ramsey\u2013Bord\u00e9 atom interferometer and the Bloch oscillations process. This code models precisely the underlying physics of our interferometer and provides an accurate evaluation of systematic effects, consistent with experimental results. Experiment Our experimental method is illustrated in Fig. 2 . The basic tools of our experiment are Bloch oscillations in an accelerated optical lattice, which enable the coherent transfer of a precise number of photon momenta to the atoms (typically 1,000 \u0127k ), and a matter-wave interferometer that measures the phase shift due to the change in velocity of the atoms. As in the optical domain, atom interferometry needs tools to split and recombine atomic wave packets; this is accomplished by a sequence of light pulses. The probability of detecting atoms in a given internal state at the output of the interferometer is a sinusoidal function of the accumulated phase difference along the two paths. Thus, the measurement of atomic populations enables the evaluation of the phase shift. Using the combination of the Ramsey\u2013Bord\u00e9 interferometer configuration and Bloch oscillations, the phase shift is proportional to the ratio h \/ m (ref. 17 ). Fig. 2: Experimental setup. a , Design of the vacuum chamber; the atom interferometer\u2014a 70-cm-long magnetically shielded tube\u2014is located in the upper area. b , Sequence of Bloch oscillations (B.O., red) and Raman pulses (yellow) used to control the trajectory of atoms before starting the atom interferometer. c , Atom interferometer light pulse sequence. The atomic trajectories for upward (blue) and downward (purple) accelerations are previously calculated to mitigate the gravity gradient effect. The separation between the two paths of each interferometer is exaggerated for clarity. Full size image We produce a cold rubidium sample using an optical molasses in the main chamber. Then, atoms are transported to the interferometry area, a 70-cm-long tube surrounded by a two-layer magnetic shield. The magnetic field is controlled to within 50 nT. To that end, we use an atomic elevator based on two Bloch oscillation pulses (acceleration\/deceleration) 17 . These are performed using two vertical counter-propagating laser beams, the frequency difference of which is swept to create an accelerated standing wave. Atomic trajectories are precisely adjusted by controlling this frequency difference. Between the two Bloch oscillation pulses of the elevator, we apply two Raman pulses to prepare atoms in a well defined atomic internal state (see Fig. 2b ). Raman transitions occur between the two hyperfine levels of the ground state of the rubidium atom and are also implemented using two vertical counter-propagating laser beams (with wave vectors k 1 = \u2212 k 2 and k R = k 1 \u2248 k 2 ). Their frequency difference \u03c9 R is controlled to compensate precisely the Doppler shift induced by the accelerations of the atoms. The atom interferometer is illustrated in Fig. 2c . It is implemented with two pairs of \u03c0\/2 Raman pulses. Each pulse acts as a beam splitter by transferring a momentum of 2 \u0127k R to an atom with a probability of 50%. The first pair creates a coherent superposition of two spatially separated wave packets in the same internal state with the same momentum. The second pair recombines the two wave packets. Between the second and third \u03c0\/2 pulses, a Bloch oscillation pulse transfers a momentum of 2 N B \u0127k B to both wave packets, where N B is the number of Bloch oscillations. The overall phase \u03a6 of the interferometer is given by $$\\varPhi ={T}_{{\\rm{R}}}\\left[{\\varepsilon }_{{\\rm{R}}}2{k}_{{\\rm{R}}}({\\varepsilon }_{{\\rm{B}}}\\frac{2{N}_{{\\rm{B}}}\\hbar {k}_{{\\rm{B}}}}{m}-gT)-\\delta {\\omega }_{{\\rm{R}}}\\right]+{\\varphi }_{{\\rm{L}}{\\rm{S}}},$$ (2) where T R is the time between the \u03c0\/2 pulses of each pair, T is the time between the first and the third \u03c0\/2 pulses, g is the gravitational acceleration, \u03d5 LS represents the phase corresponding to parasitic atomic level shifts and \u03b4 \u03c9 R is the difference of the Raman frequencies between the first and the third \u03c0\/2 pulses. \u03b5 R and \u03b5 B determine the orientation of Raman and Bloch lasers wave vectors, respectively. The fluorescence signal collected in the detection zone gives the number of atoms in each atomic level at the output of the interferometer. Atomic fringes are obtained by measuring the fraction of atoms in a given internal state for varying \u03b4 \u03c9 R . Using a mean-square adjustment, we calculate \u03b4 \u03c9 R,0 , the frequency for which \u03a6 = 0. Gravity is cancelled between upward ( \u03b5 B = 1) and downward ( \u03b5 B = \u22121) acceleration (see Fig. 2 ). Constant level shifts \u03d5 LS are mitigated by inverting the direction of the Raman beams ( \u03b5 R = \u00b11). The shot-to-shot parameters of the interferometer (\u03b4 \u03c9 R , \u03b5 R , \u03b5 B ) are applied randomly to avoid drifts. We record four spectra (Fig. 3a ) that yield $$\\frac{\\hbar }{m}=\\frac{1}{4}\\frac{{\\sum }_{{\\varepsilon }_{{\\rm{R}}},{\\varepsilon }_{{\\rm{B}}}}|{\\rm{\\delta }}{\\omega }_{{\\rm{R}},0}({\\varepsilon }_{{\\rm{R}}},{\\varepsilon }_{{\\rm{B}}})|}{4{N}_{{\\rm{B}}}{k}_{{\\rm{B}}}{k}_{{\\rm{R}}}}.$$ (3) Fig. 3: Data analysis. a , Typical set of four spectra recorded by inverting the directions of the Raman and Bloch beams for T R = 20 ms and N B = 500. Each spectrum displays the variation of the relative atomic population with respect to the parameter \u03b4 \u03c9 R . The lines are least-squares fits used to determine the position of the central fringe displayed on the top of each spectrum. b , Allan deviation \u03c3 \u03b1 of the measurement of the fine-structure constant \u03b1 at maximum sensitivity ( T R = 20 ms, N B = 500) as a function of the integration time \u03c4 . The line corresponds to \\({\\sigma }_{\\alpha }(\\tau )=3\\times {10}^{-10}\/\\sqrt{\\tau }\\) , with \u03c4 expressed in hours. Error bars indicate 1 \u03c3 uncertainties. c , Datasets used to determine the value of the fine-structure constant, \u03b1 . Data are obtained by changing the following experimental parameters: the pulse separation time, T R , the number of Bloch oscillations, N B , and their total duration, \u03c4 B . The circles and diamonds correspond to two different laser intensities during the \u03c0\/2 pulses of the interferometer. Error bars denote \u00b11 \u03c3 and are estimated from the standard deviation of the mean. The blue band represents the overall the \u00b11 \u03c3 standard deviation. The reduced \u03c7 2 for the combined data is 1.4. Full size image Data analysis For the conditions of Fig. 3a , the typical uncertainty on \u03b4 \u03c9 R,0 is 55 mHz. This leads to a statistical uncertainty on h \/ m of less than 2 ppb in 5 min. The behaviour of the Allan deviation calculated with a set of h \/ m measurements over 56 h (Fig. 3b ) shows that the data are independent (no correlations or long-term drift). It also indicates that the sensitivity of our setup on \u03b1 is 8 \u00d7 10 \u221211 in 14 h. Table 1 presents our error budget. Several systematic effects identified in our previous measurement 18 have been reduced by at least one order of magnitude. By controlling the experimental parameters of the atomic elevator, we are able to adjust precisely the altitude of atomic trajectories within 100 \u03bcm in such way that the gravity gradient cancels out between the configurations \u03b5 B = 1 and \u03b5 B = \u22121 (see Fig. 2c ). The effect of Earth\u2019s rotation is suppressed by continuously rotating one of the Raman beams during the interferometric pulse sequence 19 . The long-term drift of the beam alignment is corrected with an accuracy better than 4 \u03bcrad every 45 min by controlling the retro-reflection of the laser beams via a single-mode optical fibre. Our lasers are locked on a stabilized Fabry\u2013P\u00e9rot cavity and their frequencies are regularly measured using a frequency comb with an accuracy of less than 4 kHz. The low density of our atomic sample implies a reduction of the effects of the refraction index and atom\u2013atom interaction 20 to less than 1 ppt. Effects related to the geometrical parameters of the laser beams (Gouy phase and wave front curvature) are mitigated by using a 4.9-mm-waist beam passing through an apodizing filter and by adjusting the curvature with a shearing interferometer. Table 1 Error budget on \u03b1 Full size table Among the recently identified systematic effects, the most subtle one is related to correlations between the efficiency of the Bloch oscillations and short-scale spatial fluctuations in laser intensity. This effect raises the question of how to calculate the photon momentum in a distorted optical field. Relying on our previous work 21 , we reduce the contribution of this effect to the error budget to less than 0.02 ppb. Because of the expansion of the atomic cloud, there is a residual phase shift that is due to the variation of the intensity perceived by the atoms. This phase shift depends on the velocity distribution 22 , 23 . We implement a method to compensate for the mean intensity variation and use a Monte Carlo simulation to evaluate the residual bias due to this Raman phase shift. During the interferometer sequence, we apply a frequency ramp to compensate the Doppler shift induced by gravity. Nonlinearity in the delay of the optical phase-lock loop induces a residual phase shift that is measured and corrected for each spectrum. These systematic effects were not considered in our previous measurement 18 (see Fig. 1 ), which could explain the 2.4 \u03c3 discrepancy between that measurement and the present one. Unfortunately, we do not have available data to evaluate retrospectively the contributions of the phase shift in the Raman phase-lock loop and of short-scale fluctuations in the laser intensity to the 2011 measurement. Thus, we cannot firmly state that these two effects are the cause of the 2.4 \u03c3 discrepancy between our two measurements. Overall systematic errors contribute an uncertainty of 6.8 \u00d7 10 \u221211 . Figure 3c shows the data used for the determination of \u03b1 . Each point represents about 10 h of data. We took advantage of the sensitivity and reproducibility of our setup to study systematic effects by varying the experimental parameters (such as pulse-separation time, number of Bloch oscillations, duration of Bloch pulse, laser intensity and atomic trajectories). In parallel, we performed theoretical modelling and numerical simulations to interpret the experimental observations. The measurement campaign lasted one year and ended when consistent values were obtained for the different configurations. Using our measurement of the fine-structure constant, the standard-model prediction of the anomalous magnetic moment of the electron becomes $${a}_{{\\rm{e}}}({\\alpha }_{{\\rm{LKB2020}}})=\\frac{{g}_{{\\rm{e}}}-2}{2}=1\\hspace{-1pt},\\hspace{-1pt}159\\hspace{-1pt},\\hspace{-1pt}652\\hspace{-1pt},180.252\\,(95)\\times {10}^{-12}.$$ The relative uncertainty on g e is below 0.1 ppt, which is the most accurate prediction of the standard model. Comparison with the direct experimental measurement a e , exp (ref. 9 ) gives \u03b4 a e = a e,exp \u2212 a e ( \u03b1 LKB2020 ) = (4.8 \u00b1 3.0) \u00d7 10 \u221213 (+1.6 \u03c3 ), whereas comparison with caesium recoil measurements gives \u03b4\u2032 a e = a e,exp \u2212 a e ( \u03b1 Berkeley ) = (\u22128.8 \u00b1 3.6) \u00d7 10 \u221213 (\u22122.4 \u03c3 ). The uncertainty on \u03b4 a e is dominated by a e,exp . Discussion Our measurement sets additional limits on theories beyond the standard model that lead to a contribution to a e . Using a Bayes method 24 , our result implies that for a theory with positive \u03b4 a e , we can reject \u03b4 a e > 9.8 \u00d7 10 \u221213 with a 95% confidence level, and for a theory with negative \u03b4 a e , we can reject \u03b4 a e < \u22123.4 \u00d7 10 \u221213 with a 95% confidence level. For example, our result modifies the limits on a possible substructure within the electron. If the electron is composed of constituent particles of mass m * bound together by some unknown attraction, its natural size should be R = \u0127 \/( m * c ) and its magnetic moment would be modified by \u03b4 a e \u2248 m e \/ m * using the simplest analysis. According to the chirally invariant model 25 , our result excludes regions with m * < 520 GeV\/ c 2 or R > 4 \u00d7 10 \u221219 m with a confidence level of 95%. These are stringent limits set by low-energy experiments, although they are not yet at the limits of the Large Electron\u2013Positron collider (the largest electron\u2013positron collider available today) 26 . Moreover, our result sets the stage for testing whether the persistent discrepancy of 3.6 \u03c3 between the experimental value 5 and the standard-model prediction of the magnetic moment of the muon 27 , 28 ( a \u03bc ) exists for electrons. If this discrepancy (\u03b4 a \u03bc ) is the signature of new physics, similar effects could be observable for electrons. Using naive scaling, the effects on the electron would be of the order of ( m e \/ m \u03bc ) 2 \u03b4 a \u03bc (ref. 6 ), where m \u03bc is the mass of the muon. Figure 4a summarizes the overall contributions of experiments involved in the determination of \u03b4 a e . We also include the largest theoretical contributions from the fifth order of the QED series and the hadronic term. The dominant contribution comes from the direct measurement of the electron moment anomaly, a e , exp . For the first time, the contribution of the recoil measurement ( h \/ m ) is at the level of ( m e \/ m \u03bc ) 2 \u03b4a \u03bc \u2248 6.5 \u00d7 10 \u221214 , the value of \u03b4 a e deduced from the naive scaling (horizontal green bar). In the next years, improvement of one order of magnitude is expected for the accuracy of the measurement of a e , exp (ref. 29 ); it will then be possible to probe physics beyond the standard model with comparable information from both the electron and muon. Fig. 4: Impact on the test of the standard-model prediction of a e and limits on hypothetical X boson. a , Summary of contributions to the relative uncertainty on \u03b4 a e . The horizontal green line corresponds to the \u03b4 a e value obtained by taking into account the muon magnetic moment discrepancy and using a naive scaling model. Previous data from ref. 9 (Harvard 2008), ref. 18 (LKB 2011), ref. 3 (Berkeley 2018), ref. 13 (Atomic Mass Evaluation, AME 2016), ref. 14 (Max-Planck-Institut f\u00fcr Kernphysik, MPIK 2014) and ref. 2 (RIKEN 2019). Also shown are the 10th-order and hadronic contributions in the calculation of the electron moment anomaly. b , Exclusion area in ( \u03b5 , m X ) space for the X boson. The grey, blue and light purple regions are ruled out by the E141 31 , NA64 32 and BaBar 35 experiments, respectively. A test based on the magnetic moment of the electron rules out the orange region when using the Berkeley measurement 3 and the purple region when using the present result. Disregarding the Berkeley measurement, the remaining allowed range at 16.7 MeV is depicted by the thick red line. The zone favoured by \u03b4 a e > 0, as deduced from this work, is shown by grey dots. Full size image Finally, the anomaly reported in the angular distribution of positron\u2013electron pairs ( e + e \u2212 ) produced in 8 Be nuclear transitions 4 could be explained by the emission of a hypothetical protophobic gauge boson X with a mass of 16.7 MeV followed by the decay X \u2192 e + e \u2212 (ref. 30 ). The X boson is parameterized by a mixing strength \u03b5 with electrons and a non-zero mass m X . Figure 4b presents the exclusion space for those parameters. At 16.7 MeV, the upper limit of \u03b5 is set by the g e \u2212 2 value of the electron and its lower limit by electron beam dump experiments (E141 31 and NA64 32 collaborations). Recently, new results from the NA64 collaboration 33 excluded \u03b5 values lower than 6.8 \u00d7 10 \u22124 . Because vector coupling implies \u03b4 a e > 0, the result from a caesium recoil experiment imposes strong constraints on \u03b5 ; combined with the NA64 result, it rejects pure vector coupling of X (16.7 MeV) at 90% confidence level. By contrast, our measurement of \u03b1 gives \u03b4 a e > 0 and favours pure vector coupling with \u03b5 = (8 \u00b1 3) \u00d7 10 \u22124 , which could explain the 8 Be anomaly. Methods Experimental setup The design of the science chamber is shown on Fig. 2a . A three-dimensional magneto-optical trap (MOT) is loaded by a slow atomic beam generated in a two-dimensional MOT. An optical molasses is used to further cool down atoms to a temperature of 4 \u03bcK. The temperature of the atomic cloud is measured using Doppler-sensitive Raman transitions. After being released from the optical molasses ( t = 0), atoms are transported to a separate chamber in which the vacuum is controlled at the level of few 10 \u221211 mbar. The chamber consists of a long tube placed 50 cm above the centre of the MOT. One main difference with our previous setup 18 is that the atom interferometer is realized in this separate long tube, where the magnetic field is precisely controlled using a uniformly wound solenoid shielded by two layers of \u03bc-metal. Lasers for the Raman transitions are produced using second-harmonic generation from 1.56-\u03bcm lasers. These two lasers are phase-locked, and the scheme used to control the frequency difference between them during the interferometer sequence is shown in Extended Data Fig. 3a . The power used to drive Raman transitions is at maximum 70 mW per beam. The lasers are detuned with respect to the one-photon transition (Rb D2 line) by about 60 GHz. Laser beams for the Bloch oscillations are produced from a 1.56-\u03bcm fibre laser that is split into two. Each beam seeds an optical system (\u03bcQuans) in which it passes through an acousto-optic modulator to control the laser frequency, is then amplified and passes through a periodically poled lithium niobate crystal for second-harmonic generation (about 800 mW at 780 nm). The two Bloch beams are filtered through a Rb vapour cell to reduce the resonant component of the amplified spontaneous emission of the amplifiers 36 . The total power is 400 mW for a peak intensity of 530 mW cm \u22122 . The laser is blue-detuned by 40 GHz from Rb D2 line. The two Raman beams have linear and orthogonal polarizations. Together with one of the Bloch beams, they are transported with the same single-mode polarization-maintaining fibre at the top of the cell and pointing downwards (Extended Data Fig. 1a ). A polarizing beam splitter is placed at the bottom of the vacuum cell. It transmits one of the Raman beams, which is then retro-reflected on a horizontal mirror placed on a vibration isolation table to achieve the counter-propagating configuration. The second Raman beam and the Bloch beam are rejected by the polarizing beam splitter. The inversion of the Raman effective wave vector is performed by rotating the polarization of the Raman beams by 90\u00b0 before the fibre. The second Bloch beam is transported by an independent single-mode polarization-maintaining fibre at the bottom of the cell and points upwards. The waist of the beams at the output of the collimators is 4.9 mm. An apodizing filter is placed after each collimator 3 . Experimental sequence To transport atoms in the interferometry area, we use an atomic elevator based on two Bloch oscillation pulses (acceleration\/deceleration) 20 . By adjusting the parameters of the elevator (number of Bloch oscillations and delays), we can precisely choose the initial position z 0 and velocity v 0 of the cloud at the start of the interferometer t interf. . Between the two Bloch oscillations pulses of the elevator, we apply two Raman \u03c0 pulses with a blow-away pulse in between. With this sequence, atoms are prepared in the magnetically insensitive state, and by controlling the parameters of the first Raman \u03c0 pulse (intensity and duration) one can set the width of the vertical velocity distribution of the atomic cloud. Using a pulse duration of 189 \u03bcs, we obtain a velocity distribution with a full-width at half-maximum of 1.7 mm s \u22121 . After the preparation sequence, 500,000 atoms form the cloud. The interferometer consists of four \u03c0\/2 Raman pulses of the same duration arranged in two identical Ramsey sequences (delay T R ) separated by a duration T . The Bloch oscillation pulse is applied between the second and third Raman pulses (see Fig. 2c or Extended Data Fig. 1c for definitions of the pulse timing notation). To perform Bloch oscillations, we load the atoms at time t acc. in an optical lattice by adiabatically ramping up the laser intensity for \u03c4 adiab. = 500 \u03bcs. Then, we implement N B oscillations by accelerating the lattice during time \u03c4 B , which is proportional to N B and in our experiment corresponds to \u03c4 osc = 12 \u03bcs per oscillation unless otherwise specified. Finally, the lattice is adiabatically ramped down for another 500 \u03bcs. The detection scheme (Extended Data Fig. 1b ) is composed of three horizontal retro-reflected light sheets through which the atoms fall successively. The first light sheet is resonant with atoms in the state | F = 2 \u27e9 that emit fluorescence photons collected on a large-area photo-diode ( F , hyperfine quantum number). A cache placed at the bottom of the light sheet blocks the retro-reflection, leading to pushing the detected atoms away from the detection system. The remaining atoms in | F = 1 \u27e9 pass through a light sheet that repumps them in | F = 2 \u27e9 , and they are subsequently detected in a third light sheet similar to the first one. The relative population of atoms in each state is then obtained from the collected fluorescence signals. Theoretical phase shift at the output of the interferometer To maintain the resonance condition of the Raman transitions, the frequency difference \u03c9 R between the lasers that drive them is carefully adjusted. In addition to the frequency difference shift \u03b4 \u03c9 R between the first and third \u03c0\/2 pulses, we apply during the Ramsey sequences a ramp at rate \u03b2 to compensate for gravity. Thus, the effective wave vector of Raman transitions varies along the interferometer, which can induce a bias 37 . By treating this effect as a perturbation in the Lagrangian formalism 38 , we obtain a modified version of equation ( 2 ): $$\\begin{array}{l}\\varPhi ={T}_{{\\rm{R}}}\\left[{\\varepsilon }_{{\\rm{R}}}2{k}_{{\\rm{R}}}({\\varepsilon }_{{\\rm{B}}}\\frac{2{N}_{{\\rm{B}}}\\hbar {k}_{{\\rm{B}}}}{m}-gT)-\\delta {\\omega }_{{\\rm{R}}}\\right]+{\\varphi }_{{\\rm{L}}{\\rm{S}}}\\\\ \\,+\\frac{{T}_{{\\rm{R}}}}{c}\\{\\beta [gT\\left(\\frac{T}{2}+{T}_{{\\rm{R}}}\\right)-{v}_{0}T\\\\ \\,+{\\varepsilon }_{{\\rm{B}}}\\frac{2{N}_{{\\rm{B}}}\\hbar {k}_{{\\rm{B}}}}{m}\\left({t}_{{\\rm{a}}{\\rm{c}}{\\rm{c}}.}+{\\tau }_{{\\rm{a}}{\\rm{d}}{\\rm{i}}{\\rm{a}}{\\rm{b}}.}+\\frac{{\\tau }_{{\\rm{B}}}}{2}-{T}_{{\\rm{R}}}-T\\right)]\\\\ \\,+2{k}_{{\\rm{R}}}\\left(\\frac{2{N}_{{\\rm{B}}}\\hbar {k}_{{\\rm{B}}}}{m}-gT\\right)\\left(2{v}_{0}-\\frac{{T}_{{\\rm{R}}}g}{2}+{\\varepsilon }_{{\\rm{B}}}\\frac{2{N}_{{\\rm{B}}}\\hbar {k}_{{\\rm{B}}}}{m}-gT\\right)\\},\\end{array}$$ (4) where k R is defined as the effective wave vector when the laser frequency difference is set to address atoms at zero velocity. This formula must be used to compute h \/ m from the central frequency determinations of the four spectra. However, because the additional term (second and third lines in equation ( 4 )) is independent of the direction of the Raman beams, the determination of h \/ m from equation ( 3 ) remains valid, provided that the value of the Raman wave vector corresponds to the one resulting from addressing atoms at zero velocity. Because we use this value, there is no correction associated to this effect. Evaluation of uncertainty budgets Thanks to the high sensitivity of our atom interferometer, a wide range of systematic effects was investigated and evaluated experimentally. Furthermore, we performed the measurements of h \/ m with various experimental parameters ( N B , T R , \u03c4 B , Raman laser intensity). The parameters are listed in Extended Data Table 1 . Given that many systematic effects depend on the position or velocity of the atoms, we implemented a Monte Carlo simulation of the experiment to calculate such effects precisely. The trajectories of the atoms during the measurement sequence were precisely controlled by means of the atomic elevator. The Monte Carlo simulation was based on the calculation of atomic trajectories using the real-time sequence of the experiment. Quantities depending on the trajectory of the atoms (such as the contrast of Rabi oscillations or the efficiency of Bloch oscillations) were calculated and compared with experimental results to confirm the validity of the model. Calculation of the final uncertainty The final value of h \/ m was obtained from hundreds of individual measurements of h \/ m . For each measurement, an uncertainty was calculated. This uncertainty has several origins that may be unique to this measurement (for example, the uncertainty of the fit or the laser frequency measurement), that depend on the parameters of the measurement (for example, the light shift and the gravity gradient) or that are common to all measurements (for example, the beam parameters). The uncertainty package of Python (  ) was used to compute the weighted average value of h \/ m . The final uncertainty is a weighted quadratic sum of all the elementary sources of uncertainty. The error budget is obtained by combining those contributions according to their origin. Monte Carlo simulation In this simulation, each atom is described by an atomic wave packet with mean momentum p ( t ), a phase \u03d5 ( t ) at its mean position r ( t ), and the real amplitude a ( t ). The momentum p ( t ) and the position r ( t ) of the wave packet evolve using classical forces that act on the atom, and the phase is calculated along this path. The sequence is split into different stages in which the accumulated phase, the evolution of the trajectory and the amplitude are computed. Three different stages are considered: free fall in the gravity field, Raman transitions and Bloch oscillations. During free fall, the amplitude remains constant, the trajectory is given by classical physics and the phase is computed using the action along the classical trajectory. For Raman transitions, the evolution is calculated in an accelerated frame in which the Raman frequency is constant. Analytical solutions for a finite pulse duration in the momentum representation are used 39 , allowing us to compute the amplitude and the phase. The displacement is calculated from the derivative of the phase with respect to the momentum. For Bloch oscillations, the evolution is calculated in the frame of the lattice. In this frame, the evolution is periodic and no displacement of the wave packet occurs. The phase evolution depends on three terms: (i) the phase due to the absorption and stimulated emission of N B photons: \u03d5 ph = N B [ \u03d5 up ( x , t ) \u2212 \u03d5 down ( x , t )], where \u03d5 up and \u03d5 down are the phases of the two lasers of the lattice; (ii) the phase due to acceleration: \u03d5 acc = m ( g \u2212 \u03b3 ) \u03c4 B \/ \u0127 , where \u03b3 is the acceleration of the lattice and \u03c4 B is the total duration of the acceleration; and (iii) the phase due to the lattice, \u03d5 latt , which is calculated from the average energy of the atom in the first band in the tight-binding limit $${\\varphi }_{{\\rm{latt}}}=[2\\sqrt[4]{{E}_{{\\rm{r}}}^{2}{V}_{{\\rm{up}}}{V}_{{\\rm{down}}}}+{(\\sqrt{{V}_{{\\rm{up}}}}-\\sqrt{{V}_{{\\rm{down}}}})}^{2}]\\frac{{\\tau }_{{\\rm{B}}}}{\\hbar },$$ (5) where E r is the recoil energy and V up\/down is the potential (light shift) of each individual laser of the lattice. From this energy, a classical force that acts on the atom is also calculated. The amplitude is calculated independently: the efficiency of the Bloch oscillation, which depends on both the depth of the lattice and the magnitude of the acceleration, is taken from tables computed using an independent numerical simulation 40 , 41 . The analytical formulas for the Raman and Bloch beam evolution are obtained assuming that the laser beams used are plane waves. Generalization to other beams is obtained by using a formula with a plane wave that locally fits the phase of the laser (amplitude, phase and phase gradient). These local parameters are obtained analytically when the simulation is performed with Gaussian beams. In the case of an arbitrary beam, numerical values are obtained using plane-wave decomposition of the solution of the Helmholtz equation. We compute the Fourier transform \\(\\tilde{A}({k}_{x},{k}_{y},{z}_{0})\\) of the wavefront at position z 0 . At any position, the complex amplitude is calculated using $$A(x,y,z)=\\iint {{\\rm{e}}}^{{\\rm{i}}({k}_{x}x+{k}_{y}y+\\sqrt{{k}^{2}-{k}_{x}^{2}-{k}_{y}^{2}}z)}\\tilde{A}({k}_{x},{k}_{y},{z}_{0}){\\rm{d}}{k}_{x}{\\rm{d}}{k}_{y}$$ (6) and the recoil is determined using $$\\begin{array}{c}{k}_{z}(x,y,z)=\\\\ \\iint \\sqrt{{k}^{2}-{k}_{x}^{2}-{k}_{y}^{2}}{{\\rm{e}}}^{{\\rm{i}}({k}_{x}x+{k}_{y}y+\\sqrt{{k}^{2}-{k}_{x}^{2}-{k}_{y}^{2}}z)}\\tilde{A}({k}_{x},{k}_{y},{z}_{0}){\\rm{d}}{k}_{x}{\\rm{d}}{k}_{y}.\\end{array}$$ (7) The Monte Carlo simulation is performed as follows: an initial set of N wave packets (index i ) is randomly calculated with a Gaussian distribution for both position and velocity. For each wave packet, and for the two paths (labelled A and B) of the interferometer, the final amplitude \\({a}_{i}^{{\\rm{A}}\/{\\rm{B}}}\\) , position \\({{\\bf{r}}}_{i}^{{\\rm{A}}\/{\\rm{B}}}\\) , momentum \\({{\\bf{p}}}_{i}^{{\\rm{A}}\/{\\rm{B}}}\\) and phase \\({\\varphi }_{i}^{{\\rm{A}}\/{\\rm{B}}}\\) are calculated. The phase of the interferometer is then obtained from: $$\\varPhi =\\frac{1}{N}\\mathop{\\sum }\\limits_{i=1}^{N}{a}_{i}^{{\\rm{A}}}{a}_{i}^{{\\rm{B}}}\\,\\left[{\\varphi }_{i}^{{\\rm{A}}}-{\\varphi }_{i}^{{\\rm{B}}}+\\frac{({{\\bf{p}}}_{i}^{{\\rm{B}}}+{{\\bf{p}}}_{i}^{{\\rm{A}}})\\cdot ({{\\bf{r}}}_{i}^{{\\rm{B}}}-{{\\bf{r}}}_{i}^{{\\rm{A}}})}{2\\hbar }\\right].$$ (8) The simulation is run for each of the four spectra. The value of h \/ m is deduced using equation ( 3 ). Frequency measurement The Bloch laser and of one of the Raman lasers are locked to a Fabry\u2013P\u00e9rot cavity. The cavity is itself locked to the two-photon transition from 5S 1 \/2 ( F = 3) to 5D 5 \/2 ( F = 5) in 85 Rb (ref. 42 ). The frequencies of those two lasers are measured using a commercial frequency comb (MenloSystems), which is referenced by a 100-MHz signal synchronized with the French National ","News_Body":"The validation and application of theories in physics require the measurement of universal values known as fundamental constants. A team of French researchers has just conducted the most accurate measurement to date of the fine-structure constant, which characterizes the strength of interaction between light and charged elementary particles, such as electrons. This value has just been determined with an accuracy of 11 significant digits; improving the precision of the previous measurement by a factor of 3. The scientists achieved such precision by enhancing their experimental set-up, in an effort to reduce inaccuracies and to control effects that can create perturbations of the measurement. The experiment involves cold rubidium atoms with a temperature approaching absolute zero. When they absorb photons, these atoms recoil at a velocity that depends on their mass. The highly precise measurement of this phenomenon helps to improve the knowledge of the fine-structure constant. These results, which will appear in Nature on 3 December, open new prospects for testing the Standard Model's theoretical predictions. The use of more accurate constants can help to answer fundamental questions, such as the origin of dark matter in the universe. ","News_Title":"Researchers improve the measurement of a fundamental physical constant","Topic":"Physics"}
{"Paper_Body":"Abstract Use of immune checkpoint inhibitors that target programmed cell death-1 (PD-1) can lead to various autoimmune-related adverse events (irAEs) including psoriasis-like dermatitis. Our observations on human samples indicated enhanced epidermal infiltration of CD8 T cells, and the pathogenesis of which appears to be dependent on IL-6 in the PD-1 signal blockade-induced psoriasis-like dermatitis. By using a murine model of imiquimod-induced psoriasis-like dermatitis, we further demonstrated that PD-1 deficiency accelerates skin inflammation with activated cytotoxic CD8 T cells into the epidermis, which engage in pathogenic cross-talk with keratinocytes resulting in production of IL-6. Moreover, genetically modified mice lacking PD-1 expression only on CD8 T cells developed accelerated dermatitis, moreover, blockade of IL-6 signaling by anti-IL-6 receptor antibody could ameliorate the dermatitis. Collectively, PD-1 signal blockade-induced psoriasis-like dermatitis is mediated by PD-1 signaling on CD8 T cells, and furthermore, IL-6 is likely to be a therapeutic target for the dermatitis. Introduction For cancer immune therapies that regulate T cells to enhance immune responses, T cells must successfully recognize tumor antigens through their T-cell receptors (TCRs) and become activated in order to expel tumors 1 , 2 . In addition, a number of stimulatory and inhibitory receptor and ligand pairs expressed on T cells, antigen-presenting cells (APCs) or tumor cells, termed immune checkpoints, also play crucial roles for both T cell activation and inhibition 3 . Programmed cell death-1 (PD-1) is one of these immune checkpoint molecules, which was initially detected in activated murine T cells upon TCR engagement 4 and subsequently in exhausted T cells 5 . Its ligands, programmed cell death-ligand 1 (PD-L1) and PD-L2, are expressed on various cell types, including hematopoietic cells infiltrating tumors, including APCs, and on non-hematopoietic cells such as cancer cells 6 , 7 . The interaction between PD-1 and its ligands reduces T cell function by inducing exhaustion, apoptosis, anergy, and downregulation of cytokine production by T cells, leading to suppression of the antitumor immune response 8 , 9 . In melanoma, PD-1 expression is detected on tumor-infiltrating lymphocytes including tumor antigen\u2013specific T cells, which are functionally impaired. Moreover, the biological activity of these cells can be partially recovered by inhibiting the PD-1 pathway 10 , 11 , 12 . Indeed, anti-PD-1 blocking antibodies such as nivolumab and pembrolizumab function as immune checkpoint inhibitors, and have proven effective for the treatment of melanoma 13 , 14 . However, as the PD-1 pathway also maintains peripheral T cell tolerance and regulates inflammation 15 , inhibition of this pathway may lead to autoimmune manifestations referred to as immune-related adverse events (irAEs) 16 , 17 . Early clinical trials and reviews have reported that anti-PD-1 antibody-related irAEs occur in more than 70% of patients, and cutaneous irAEs are the most frequently observed (approximately 40%). Further, most cutaneous irAEs are mild (low-grade) and manageable with topical steroids 16 , 18 , 19 , 20 , 21 . On the other hand, it has also been recently reported that two-thirds of patients with cutaneous irAEs reportedly required systemic corticosteroids for the treatment of eruptions, and 19% of patients discontinued cancer-immunotherapy due to irAEs, even though 75% experienced antitumor responses with the therapy 22 . High-dose and\/or long-term use of systemic immunosuppressive therapies are required to control such irAEs 23 , potentially resulting in prolonged interruption of cancer treatment. Moreover, these immunosuppressive therapies may also abrogate the antitumor response by counteracting lymphocyte activation 20 , 24 . Therefore, more efficacious, systemic therapies that resolve the symptoms of irAEs while also enabling shorter interruptions of cancer treatments and do not interfere with their antitumor effects would be ideal. In addition, a recent American Society of Clinical Oncology guideline suggests that cutaneous irAEs are increasingly recognized as a contributing factor to treatment noncompliance, discontinuation, or dose modification 24 . Plausibly, such skin manifestations cause changes in appearance along with discomfort, which reduces patient quality of life and results in loss of treatment motivation. We previously reported a case of nivolumab-induced psoriasis-like dermatitis 25 , which has been reported to develop in patients treated with anti-PD-1\/PD-L1 antibody 25 , 26 . The latest post-marketing surveillance of nivolumab in Japan reports that 2,391 cases of cutaneous irAE occurred, of which 103 cases (4.3 %) were labeled as psoriasis. Notably, more than 18% (19 \/103) of those cases were reportedly severe 27 . Importantly, the mechanism by which psoriasis-like dermatitis occurs following PD-1\/PD-L1 inhibition remains unknown, and strategies to mitigate the occurrence of especially severe cases are yet to be identified. With the recent increase in use of anti-PD-1 antibody for patients with various types of cancers, clarification of the underlying mechanisms and development of more efficacious treatment for PD-1 signal blockade-induced psoriasis-like dermatitis is needed. Application of imiquimod (IMQ), a toll-like receptor 7\/8 agonist, is known to induce psoriasis-like dermatitis in both humans 28 and mice 29 . Furthermore, it has already been reported that both PD-1 genetic deficiency and blockade of PD-1 with a specific monoclonal antibody exacerbate IMQ-induced psoriasis-like dermatitis in mice 30 . Therefore, it is likely that the pathophysiological mechanism of PD-1 signal blockade-induced psoriasis-like dermatitis could be elucidated using this murine model. The present study aimed to elucidate the characteristics and mechanisms underlying psoriasis-like dermatitis induced by blocking PD-1 signaling, and to identify suitable treatments. The observations from human samples and further experiments using a preclinical murine model of IMQ-induced psoriasis-like dermatitis demonstrated that the dermatitis was accelerated by an increase of skin-infiltrating activated, cytotoxic CD8 T cells allowing pathogenic crosstalk with keratinocytes and subsequent production of IL-6. Moreover, blockade of interleukin (IL)-6 signaling by anti-IL-6 receptor blocking antibody (MR16-1) restrained the PD-1 signal blockade provoked by severe dermatitis by inhibiting both Th17 cell differentiation and cytotoxic CD8 T cell activation. Thus, this highlights the significance of IL-6 blockade therapy specifically for the regulation of PD-1 signal blockade-induced dermatitis. Results Increased CD8\/CD4 ratio of epidermal-infiltrating lymphocytes in cases of anti-PD-1 antibody-induced psoriasis-like dermatitis compared to cases of idiopathic psoriasis Immunohistochemical (IHC) evaluation of skin biopsy samples, as demonstrated in Fig. 1a , revealed that CD8\/CD4 ratios of epidermal-infiltrating mononuclear cells were significantly increased in cases of anti-PD-1 antibody-induced psoriasis-like dermatitis (median \u00b1 standard deviation [SD], 3.48 \u00b1 1.0) compared to that in cases of idiopathic psoriasis (1.06 \u00b1 0.19, P = 0.008 by Mann\u2013Whiney U test, Fig. 1b ). Fig. 1: Characteristics of anti-programmed cell death-1 (PD-1) antibody-induced psoriasis-like dermatitis. a Representative clinical images of patients with idiopathic psoriasis and anti-PD-1 antibody-induced psoriasis-like dermatitis. Both patients developed well-defined scaly plaques scattered over their trunks and extremities. b Representative hematoxylin and eosin (HE)-stained, and anti-CD8 or CD4 antibody-stained skin biopsy samples from patients with idiopathic psoriasis and anti-PD-1 antibody-induced psoriasis-like dermatitis. Scale bars = 50 \u03bcm. c CD8\/CD4 ratios of epidermal-infiltrating lymphocytes ( n = 6 and 7 in idiopathic psoriasis and anti-PD-1 antibody-induced psoriasis-like dermatitis, respectively). ** P < 0.01 by nonparametric 2-tailed Mann\u2013Whitney U test. d Profiles of serum interleukin (IL)-6 levels in serum samples from anti-PD-1 antibody-treated cancer patients who developed psoriasis-like dermatitis as an immune-related adverse event (irAE, n = 8) and those with no irAE ( n = 19). **** P < 0.0001 by nonparametric 2-tailed Mann\u2013Whitney U test. Full size image Elevated serum IL-6 correlates with the development of anti-PD-1 antibody-induced psoriasis-like dermatitis in humans We reported in our preliminary study that only increased serum levels of IL-6, but not those of IL-17A, interferon (IFN)-\u03b3 and IL-8, correlated with the development of anti-PD-1 antibody-induced psoriasis-like dermatitis in patients with malignant melanoma 25 . In order to validate this phenomenon, we analyzed the serum levels of IL-6 in eight cases of psoriasis-like dermatitis, and 19 cases without any irAEs. Cases of psoriasis-like dermatitis exhibited significantly higher serum IL-6 levels compared to those of IL-6 in cases without any irAEs ( P < 0.0001 by Mann\u2013Whiney U test, Fig. 1c ). Our additional analysis using the remaining samples showed that there was no significant difference in the serum levels of soluble IL-6 receptor alfa (sIL-6R\u03b1) between the two groups, six cases of psoriasis-like dermatitis and 18 cases without any irAEs (Supplemental Fig. 1 ). Collectively, these results suggest that the pathogenesis of anti-PD-1 antibody-induced psoriasis-like dermatitis may depend on IL-6. PD-1 \u2212\/\u2212 mice exhibit more severe IMQ-induced psoriasis-like dermatitis than WT mice PD-1 \u2212\/\u2212 mice developed significantly more severe IMQ-induced psoriasis-like dermatitis (Fig. 2a ) when compared to WT mice as revealed by clinical measurements including ear swelling (change from the baseline at day 7, 20.6 \u00b1 2.6 \u03bcm vs. 7.2 \u00b1 1.4 \u03bcm, P = 0.0014 by two-way ANOVA, Fig. 2b ) and PASI score, which represents the severity of erythema, scaling and skin thickness, (7.8 \u00b1 0.2 vs. 3.2 \u00b1 0.2, P < 0.0001 by two-way ANOVA, Fig. 2c ) at day 7. Moreover, pathological analysis, as shown in Fig. 2d , of epidermal thickness (61.5 \u00b1 7.9 \u03bcm vs. 35.6 \u00b1 3.2 \u03bcm, P = 0.008 by Mann\u2013Whitney U test, Fig. 2e ) and the number of epidermal neutrophilic micro-abscesses (3.2 \u00b1 0.58\/ear slide vs. WT 0.6 \u00b1 0.24\/ear slide, P = 0.008 by Mann\u2013Whitney U test, Fig. 2f ) at day 7 further indicates the protective role of PD-1 in IMQ-induced psoriasis-like dermatitis. Fig. 2: Comparison of clinical and histological appearance and cytokine mRNA expression in imiquimod (IMQ)-induced psoriasis-like dermatitis between PD-1 \u2212\/\u2212 mice and wild-type (WT) mice. a Representative clinical images at day 7 of IMQ-induced psoriasis-like dermatitis in WT and PD-1 \u2212\/\u2212 mice. Application of vehicle cream was used as a control. b , c The course of ear swelling ( b ) and PASI score ( c ) representing the severity of erythema, scaling, and skin thickness of WT and PD-1 \u2212\/\u2212 mice. ** P <0.01 and **** P <0.0001 by two-way ANOVA. d Representative images of HE-stained ear samples from IMQ-induced psoriasis-like dermatitis in WT and PD-1 \u2212\/\u2212 mice at day 7. Application of vehicle cream was used as a control. Scale bars, 100 \u03bcm. e , f Epidermal hyperplasia ( e ), and the number of epidermal neutrophilic micro-abscesses ( f ) in the ear samples from IMQ-applied WT or PD-1 \u2212\/\u2212 mice ( n =5 in each group). Data are shown as mean \u00b1 standard deviation (SD). Data are representative of three independent experiments. ** P <0.01 by nonparametric 2-tailed Mann\u2013Whitney U test. g Quantitative reverse transcriptase-polymerase chain reaction (qRT-PCR) analysis of psoriasis-related cytokines and the neutrophilic surface marker Ly6g in ear samples from IMQ- or vehicle-treated WT and PD-1 \u2212\/\u2212 mice at day 7 ( n =7\u20138 in each group). Fold changes in mRNA levels were calculated and normalized against GAPDH mRNA levels. Data are expressed as mean \u00b1 SD. Data are representative of two independent experiments. * P <0.05, ** P <0.01, and *** P <0.001 by nonparametric 2-tailed Mann\u2013Whitney U test. Full size image Moreover, we confirmed that mice treated with anti-PD-1 blocking monoclonal antibody developed clinically and histopathologically severe IMQ-induced psoriasis-like dermatitis compared to control mice treated with isotype IgG2a control (Ctrl) antibody (Supplemental Fig. 2A\u2013E ). The results corresponded to the experiments using PD-1 \u2212\/\u2212 mice. We also conducted an experiment using the B16 melanoma murine model, in which B16F10 melanoma cells were inoculated into backs of both WT and PD-1 \u2212\/\u2212 mice, to investigate whether the presence of cancer involves PD-1 blockade-induced psoriasis-like dermatitis. First, this model did not induce psoriasis-like dermatitis spontaneously nor under vehicle cream treatment (Supplemental Fig. 3A\u2013E ). Moreover, the presence of B16 melanoma did not lead to a significant difference in PASI score or ear swelling in WT or PD-1 \u2212\/\u2212 mice (Supplemental Fig. 3A\u2013H ). Taken together, these data suggest that PD-1 blockade, either by genetic knockout or antibody treatment, promotes IMQ-induced psoriasis-like dermatitis, and that PD-1 blockade in the context of cancer does not increase the severity of dermatitis. PD-1 deficiency in mice results in increased epidermal infiltration of CD8 T cells with enhanced production of IFN-\u03b3 and CXC chemokine ligand (CXCL)9 IHC analysis of murine ear skin samples revealed significantly increased numbers of CD8 T cells infiltrating into the epidermis of PD-1 \u2212\/\u2212 mice and anti-PD-1 antibody-treated mice when compared to control mice (Fig. 3a and b, P = 0.008 by Mann\u2013Whitney U test, and Supplemental Fig. 2F , P = 0.008 by Mann\u2013Whitney U test), similar to what was seen in the patients with anti-PD-1 antibody-induced psoriasis-like dermatitis. Next, qRT-PCR analysis revealed that PD-1 \u2212\/\u2212 mice have significantly higher CD8a and IFN-\u03b3 mRNA levels in CD45 + epidermal cells and CXCL9 in keratinocytes (CD45-negative epidermal cells) compared to that of WT mice (Fig. 3c , P = 0.008, P = 0.008 and P = 0.03 by Mann\u2013Whitney U test, respectively). Fig. 3: PD-1 deficient CD8 T cells display enhanced interferon (IFN)-\u03b3 production, and IFN-\u03b3-stimulated keratinocytes produce CXC chemokine ligand (CXCL)9 in IMQ-induced psoriasis-like dermatitis. a A schematic illustrating the protocol for processing of ear skin samples into keratinocyte, epidermal CD45 + cell and dermal cell populations. b Representative images of immunohistochemical (IHC) staining of CD8 T cells in IMQ-applied WT and PD-1 \u2212\/\u2212 mice at day 5. Scale bars, 20 \u03bcm. c The number of infiltrated CD8 T cells in the epidermis. d The qRT-PCR analysis of CD8a, IFN-\u03b3, and CXCL9. Data are from five mice per group, and are representative of two independent experiments. * P < 0.05 and ** P < 0.01 by nonparametric 2-tailed Mann\u2013Whitney U test. Full size image PD-1 on CD8 T cells regulates the development of IMQ-induced psoriasis-like dermatitis Following 7 days of daily IMQ application, PD-1-cKO (PD-1 fl\/fl CD8 Cre ) mice were found to have developed more severe IMQ-induced psoriasis-like dermatitis than littermate Ctrl (PD-1 fl\/+ CD8 Cre ) mice (Fig. 4c ), when evaluated clinically by the change in ear swelling from the baseline to day 7 (17 \u00b1 1.4 \u03bcm vs. 6.8 \u00b1 0.75 \u03bcm, P < 0.0001 by two-way ANOVA, Fig. 4d ), PASI score at day 7 (6.3 \u00b1 0.42 vs. 3.8 \u00b1 0.25, P = 0.0001 by two-way ANOVA, Fig. 4e ), and pathologically by epidermal thickness at day 7 (59.5 \u00b1 2.6 \u03bcm vs. 37.4 \u00b1 3.7 \u03bcm, P = 0.008 by Mann\u2013Whitney U test, Fig. 4f , g). qRT-PCR analysis revealed that PD-1-cKO mice showed significantly higher levels of both CD8a and IFN-\u03b3 mRNA in serum than that found in littermate Ctrl mice ( P = 0.015 and P = 0.015 by Mann\u2013Whitney U test, respectively, Fig. 4h ). The number of CD8 T cells (CD45 + CD3 + CD8a + cells) in draining lymph nodes (dLNs) was increased, and more CD8 T cells in PD-1-cKO mice produced IFN-\u03b3 and Gzm B than that of littermate Ctrl mice (Fig. 4i , j). Fig. 4: Clinical and histological evaluation of IMQ-induced psoriasis-like dermatitis in conditional knockout mice with PD-1 deficiency specifically in CD8 T cells. a An overview of the PD-1-floxed mouse and the breeding strategy for conditional mutation using loxP and cyclization recombinase (Cre) driving mouse lines. Specific 34bp DNA fragments representing the loxP (locus of x-over, P1) sites were inserted across the PD-1 gene (Top). Conditional knockout (cKO) mice were generated by breeding the CD8a-Cre knock-in mouse strain with the PD-1-floxed mouse strain (Bottom). b Specific deletion of PD-1 expression in CD8 T cell, but not in CD4 T cells or B cells, of IMQ-treated PD-1-cKO mice (red line) compared to IMQ-treated WT littermates (blue line). Fluorescence Minus One (FMO) was used as a control (gray area). Data represent three independent experiments. c Representative clinical images of psoriasis-like dermatitis in PD-1cKO (CD8 Cre PD-1 fl\/fl ) mice and littermate control (Ctrl, CD8 Cre PD-1 fl\/+ ) mice at day 7. d , e Ear swelling ( d ) and PASI score ( n = 4 in each group). Data are representative of three independent experiments. *** P <0.001 and **** P <0.0001 by two-way ANOVA. f Representative images of HE-stained ear skin samples from IMQ-treated littermate Ctrl mice and PD-1-cKO mice at day 7. Scale bars, 100 \u03bcm. g Epidermal thickness ( n = 5 in each group). Data are representative of three independent experiments. h qRT-PCR analysis of CD8a and IFN-\u03b3 mRNA levels in ear skin samples from IMQ-treated littermate Ctrl mice ( n = 4) and PD-1-cKO mice ( n = 5) at day 7. Fold changes in mRNAs levels were normalized against GAPDH mRNA levels. i Total numbers of CD8 T cells in draining lymph nodes (dLNs) from IMQ-treated littermate Ctrl mice and PD-1-cKO mice at day 7 ( n = 5 in each group). j Representative histograms of IFN-\u03b3 and Granzyme B (GzmB) production by CD8 T cells in the dLNs. The gray histograms represent negative controls. Graphs of median fluorescent intensities (MFIs) of IFN-\u03b3 and Gzm B. The results are presented as means \u00b1 SDs. Data are representative of two independent experiments. * P <0.05 and ** P <0.01 by nonparametric 2-tailed Mann\u2013Whitney U test. Full size image In summary, these in vivo results suggest that the PD-1 deficiency enhances the numbers of infiltrating activated cytotoxic CD8 T cells, resulting in acceleration of psoriasis-like dermatitis. Enhanced expression of cutaneous IL-6 via PD-1 deficiency in mice qRT-PCR analysis revealed that unstimulated ear skin from PD-1 \u2212\/\u2212 and WT mice contain similar low levels of psoriasis-related cytokines, IL-6, IL23-A, and IL-17A ( P = 0.95, P = 0.57 and P = 0.21 by Mann\u2013Whitney U test, respectively, Fig. 2g ). IMQ application significantly increased mRNA expression of IL-23A and IL-17A in both WT mice and PD-1 \u2212\/\u2212 mice ( P = 0.007 and P = 0.0002 in WT mice, and P = 0.0003 and P = 0.0003 in PD-1 \u2212\/\u2212 mice by Mann\u2013Whitney U test, respectively, Fig. 2g ). Notably, increased IL-6 mRNA expression induced by IMQ application was observed only in PD-1 \u2212\/\u2212 mice and not in WT mice ( P = 0.0006 and P = 0.27, respectively, by Mann\u2013Whitney U test, Fig. 2g ). Expression of Ly6g, a neutrophil surface marker, mRNA was undetectable in both groups after vehicle cream application, but were increased significantly in PD-1 \u2212\/\u2212 mice compared to WT mice after IMQ application ( P = 0.048 by Mann\u2013Whitney U test, Fig. 2g ). In addition, these results were also confirmed using PD-1-specific blocking antibody treatment (Supplemental Fig. 2G ). Further investigations revealed a significantly higher level of IL-6 mRNA expression in the CD45-positive epidermal cells, and an increased total number of CD45-positive epidermal cells with specific infiltration of neutrophils in PD-1 \u2212\/\u2212 mice compared to WT mice (Supplemental Fig. 4 ). Collectively, IL-6 expression related to expression of Th17 cytokines and infiltration of neutrophils correlates with PD-1 deficiency-enhanced IMQ-induced psoriasis-like dermatitis. Blockade of the IL-6R ameliorates PD-1 deficiency-exacerbated psoriasis-like dermatitis The increase of serum IL-6 levels post-treatment with anti-PD-1 blocking antibody implies that the pathogenesis of PD-1 signal blockade-induced psoriasis-like dermatitis is dependent on IL-6. Therefore, we employed blockade of IL-6 signaling using an anti-IL-6R blocking antibody (MR16-1) in order to assess the effects on IMQ-induced psoriasis-like dermatitis. The baseline serum levels of psoriasis-related cytokines (IL-6, IL-17A, and IL-23A) were the same between WT and PD-1 \u2212\/\u2212 mice ( n = 3). Induction of psoriasis-like dermatitis by IMQ elevated these cytokines in both IgG Ctrl-treated WT and PD-1 \u2212\/\u2212 mice, and markedly in mice with PD-1 deficiency (Fig. 5h ). MR16-1-treated PD-1 \u2212\/\u2212 mice showed significantly less IMQ-induced psoriasis-like dermatitis compared to IgG Ctrl-treated PD-1 \u2212\/\u2212 mice as evaluated by ear swelling (7.0 \u00b1 0.6 \u03bcm vs. 15.7 \u00b1 1.8 \u03bcm, P = 0.0013 by two-way ANOVA) and PASI score (4.5 \u00b1 0.6 vs. 7.3 \u00b1 0.8, P = 0.005 by two-way ANOVA) at day 7. Moreover, MR16-1-treated PD-1 \u2212\/\u2212 mice were clinically similar to IgG Ctrl-treated WT mice (ear swelling 7.8 \u00b1 0.6 \u03bcm and PASI 4.3 \u00b1 0.5; P = 0.56 and P = 0.10 by two-way ANOVA, respectively, Fig. 5a\u2013c ). Histological analyses also revealed that MR16-1-treated PD-1 \u2212\/\u2212 mice had less severe psoriasis than IgG Ctrl-treated PD-1 \u2212\/\u2212 mice with reduced epidermal hyperplasia (38.8 \u00b1 2.0 \u03bcm vs. 55.8 \u00b1 1.8 \u03bcm, P = 0.01 by Mann\u2013Whitney U test, Fig. 5d , e) and reduced numbers of epidermal neutrophilic micro-abscesses (3.2 \u00b1 0.4 vs. 6.2 \u00b1 0.6, P = 0.008 by Mann\u2013Whitney U test), which was the same as IgG Ctrl-treated WT mice (34.8 \u00b1 2.5 \u03bcm, P = 0.06; and 2.4 \u00b1 0.4, P = 0.32 by Mann\u2013Whitney U test, respectively, Fig. 5f ). Further, compared to IgG Ctrl-treated PD-1 \u2212\/\u2212 mice, MR16-1-treated PD-1 \u2212\/\u2212 mice presented significantly suppressed levels of psoriasis-related cytokine mRNAs IL-6, IL-17a, and IL-23a in the ear skin at day 7 ( P = 0.003, P = 0.03 and P = 0.02, respectively, by Mann\u2013Whitney U test, Fig. 5g ), and significantly decreased serum levels of IL-17A and IL-23A ( P = 0.03 by Mann\u2013Whitney U test) to the baseline level at day 7 (Fig. 5h ). These cytokine expression levels (IL-6, IL-17A, and IL-23A) in MR16-1-treated PD-1 \u2212\/\u2212 mice were the same as those seen in IgG Ctrl-treated WT mice ( P = 0.44, P = 0.21 and P = 0.66 in skin mRNA levels, respectively, and P = 0.57, P = 0.15 and P = 0.15 in serum levels, respectively, analyzed by Mann\u2013Whitney U test). Fig. 5: Characteristics of anti-IL-6 receptor (IL-6R) antibody-treated IMQ-induced psoriasis-like dermatitis in PD-1 \u2212\/\u2212 mice. a Representative clinical images of anti-IL-6R antibody (MR16-1)- or IgG Ctrl-treated IMQ-induced psoriasis-like dermatitis in PD-1 \u2212\/\u2212 mice compared to IgG Ctrl-treated WT mice at day 7. b Ear swelling. c PASI score ( n = 4 in each group). Data are representative of two independent experiments. * P <0.05 and ** P <0.01 by two-way ANOVA. d Representative HE staining of ear skin samples from IgG Ctrl-treated WT mice, IgG Ctrl- or MR16-1-treated PD-1 \u2212\/\u2212 mice at day 7 ( n = 5 in each group). Scale bars, 50 \u03bcm. e Epidermal thickness ( n = 5 in each group). f The number of epidermal, neutrophilic micro-abscess ( n = 5 in each group). g qRT-PCR analysis of mRNA expression levels of psoriasis-related cytokines, IL-6, IL-23a, and IL-17a, in ear skin samples from IgG Ctrl-treated WT mice ( n = 10), IgG Ctrl-treated PD-1 \u2212\/\u2212 mice ( n = 7), and MR16-1-treated PD-1 \u2212\/\u2212 mice ( n = 9) at day 7 for IMQ application. Fold changes in mRNA levels normalized to GAPDH mRNA levels. h Multiplex, bead-based analysis of serum levels of psoriasis-related cytokines, IL-6, IL-23A, and IL-17A in IgG Ctrl-treated WT mice ( n = 4), IgG Ctrl-treated PD-1 \u2212\/\u2212 mice ( n = 4) and MR16-1-treated PD-1 \u2212\/\u2212 mice ( n = 5) at day 7 for IMQ application. Baseline (B\/L) serum levels of these cytokines were also measured ( n = 3 each). In some samples, cytokines were not detected (ND). Data are expressed as mean \u00b1 SEM. Data are representative of two independent experiments. * P <0.05 and ** P <0.01 by nonparametric 2-tailed Mann\u2013Whitney U test. Full size image Furthermore, we employed blockade of IL-17A signaling using an anti-IL-17A neutralizing monoclonal antibody in order to compare the effect with anti-IL-6R antibody on IMQ-induced psoriasis-like dermatitis in PD-1 \u2212\/\u2212 mice. When evaluated both clinically and histologically at day 7, treatment with anti-IL-17A antibody improved the dermatitis in both PD-1 \u2212\/\u2212 and WT mice to a level equivalent to treatment with anti-IL-6R antibody (Supplemental Fig. 5A\u2013E ). However, anti-IL-17A antibody-treated PD-1 \u2212\/\u2212 mice showed more severe IMQ-induced psoriasis-like dermatitis than IgG Ctrl-treated WT mice, as evaluated by PASI score (4.8 \u00b1 0.8 vs. 3.6 \u00b1 0.5, P = 0.003 by two-way ANOVA) and ear swelling (7.4 \u00b1 1.6 \u03bcm vs. 5.2 \u00b1 1.4 \u03bcm, P = 0.053 by two-way ANOVA) at day 5 (Supplemental Fig. 5B ). In contrast, anti-IL-6R antibody treatment improved the dermatitis in PD-1 \u2212\/\u2212 mice earlier, at day 5 (Fig. 5b , c). These results indicate the delayed efficacy of anti-IL-17A neutralizing antibody treatment, compared to treatment with anti-IL-6R antibody, for PD-1 signal blockade-induced psoriasis-like dermatitis. Taken together, blockade of IL-6 signaling with an anti-IL-6R antibody is a potential therapeutic approach to resolve psoriasis-like dermatitis caused by inhibition of PD-1. Moreover, the treatment kinetics of anti-IL-6R antibody appear to be shorter than that of anti-IL-17A antibody treatment. Accelerated psoriasis-like dermatitis due to PD-1 deficiency on CD8 T cells can be ameliorated by the treatment with anti-IL-6R blocking antibody To investigate whether blockade of IL-6 with anti-IL-6R antibody could restrain PD-1 deficiency-induced activation of CD8 T cells, both PD-1-cKO mice and their littermate Ctrl mice were treated with either anti-IL-6R antibody (MR16-1) or isotype IgG control. MR16-1-treated PD-1-cKO mice exhibited significant improvement in clinical manifestations of IMQ-induced psoriasis-like dermatitis compared to IgG Ctrl-treated PD-1-cKO mice (ear swelling change from the baseline on day 7, 7.8 \u00b1 1.5 \u03bcm vs. 16.3 \u00b1 1.4 \u03bcm, P = 0.0012 by two-way ANOVA; and PASI score at day 7, 4.3 \u00b1 0.2 vs. 8.2 \u00b1 0.7, P < 0.0001 by two-way ANOVA). Moreover, this was to a similar level as that of IgG Ctrl-treated littermate Ctrl mice (ear swelling 6.8 \u00b1 1.4 \u03bcm, P = 0.85 by two-way ANOVA; and PASI 3.6 \u00b1 0.4, P = 0.66 by two-way ANOVA, Fig. 6a\u2013c ). Histological evaluation also indicated that MR16-1-treated PD-1-cKO mice had less epidermal hyperplasia and reduced numbers of epidermis-infiltrating CD8 T cells than did IgG Ctrl-treated PD-1-cKO mice (48.7 \u00b1 3.1 \u03bcm vs. 74.0 \u00b1 3.7 \u03bcm, P = 0.002 by Mann\u2013Whitney U test, Fig. 6d , E; 20.8 \u00b1 6.9 vs. 73.8 \u00b1 15.2, P = 0.015 by Mann\u2013Whitney U test). In fact, the response in MR16-1-treated PD-1-cKO mice occurred at the same level as that of IgG Ctrl-treated littermate Ctrl mice (43.7 \u00b1 1.0 \u03bcm, P = 0.25 by Mann\u2013Whitney U test; and 8.4 \u00b1 2.8, P = 0.16 by Mann\u2013Whitney U test, Fig. 6d, e ). Furthermore, MR16-1-treated PD-1-cKO mice displayed significantly suppressed numbers of CD8 T cells in the dLNs and reduced CD8a and IFN-\u03b3 mRNA levels in the ear skin samples at day 7 when compared to IgG Ctrl-treated PD-1-cKO mice ( P = 0.004, P = 0.03 and P = 0.09, respectively, by Mann\u2013Whitney U test, Fig. 5g, h ). Fig. 6: Characteristics of anti-IL-6R antibody-treated IMQ-induced psoriasis-like dermatitis in cKO mice with PD-1 deficiency specifically in CD8 T cells. Representative clinical images of IgG Ctrl- or MR16-1-treated IMQ-induced psoriasis-like dermatitis in littermate Ctrl mice or PD-1-cKO mice. b Ear swelling. c PASI score. Data are representative of two independent experiments. ** P < 0.01 and **** P < 0.0001 by two-way ANOVA. d Representative histological images of HE-stained ear skin samples from these mice at day 7. Scale bars, 50 \u03bcm. e Epidermal thickness. f The number of epidermal, neutrophilic micro-abscess. g Total numbers of CD8 T cells in dLNs at day 7. h mRNA expression levels of CD8a and IFN-\u03b3 in ear skin samples at day 7. Fold changes in mRNAs levels were normalized to GAPDH mRNA levels. n = 5\u20136 in each group. Data are expressed as mean \u00b1 SEM. Data are representative of two independent experiments. * P < 0.05, ** P < 0.01 by nonparametric 2-tailed Mann\u2013Whitney U test. Full size image Importantly, there were not any differences between MR16-1-treatment and IgG Ctrl-treatment in littermate Ctrl mice, highlighting the significance of IL-6 blockade therapy for the regulation of PD-1 signal blockade-activated CD8 T cells in psoriasis-like dermatitis. Discussion The pathogenesis of cutaneous irAEs in patients treated with anti-PD-1 antibody has yet to be elucidated. However, previous reports suggest that activated proliferative intradermal CD8 T cells evoke cutaneous irAEs such as lichen planus-like dermatitis and eczematous reaction 31 , 32 . The present study highlights the importance of PD-1 expression on CD8 T cells for the regulation of psoriasis-like dermatitis. We found that CD8-positive lymphocyte infiltration into the epidermis was significantly increased in patients with anti-PD-1 antibody-induced psoriasis-like dermatitis compared to that in idiopathic psoriasis. A murine model of IMQ-induced psoriasis-like dermatitis clearly demonstrated that PD-1 deficiency accelerates infiltration of epidermal CD8 T cells with enhanced IFN-\u03b3 production of inflamed skin, and IFN-\u03b3-stimulated keratinocytes produced an IFN-\u03b3-inducible chemokine (CXCL9) for recruitment of T cells. Furthermore, the newly generated cKO mice with PD-1 deficiency specifically in CD8-positive cells demonstrated more severe IMQ-induced psoriasis-like dermatitis compared to the littermate control mice. These results suggest that PD-1 regulates skin-infiltrating CD8 T cells to engage in pathogenic crosstalk with PD-L1 expressed on various cells including keratinocytes 33 . In idiopathic psoriasis activation of conventional dendritic cells producing IL-23 lead to expansion and activation of autoreactive CD8 T cells in the dermis, which in turn acquire expression of \u03b11\u03b21-integrin and migrate into the epidermis. The epidermis has been identified as an ideal location for CD8 T cells to engage in pathogenic crosstalk with keratinocytes 34 , 35 . Furthermore, intra-epidermal CD8 T cells are shown to be highly pathogenic as the accumulation of epidermal T cells parallels the increase in proliferating keratinocytes in vivo 34 . Collectively, PD-1 signal blockade-induced activation of CD8 T cells is essential to induce and accelerate anti-PD-1 antibody-induced psoriasis-like dermatitis. We also found a significant increase in the serum levels of IL-6 in patients with anti-PD-1 antibody-induced psoriasis-like dermatitis, as we had shown in our preliminary study 25 , indicating that IL-6 could play an important role during disease development and thus, may be a suitable treatment target. As expected, a murine model of IMQ-induced psoriasis-like dermatitis enhanced via PD-1 deficiency was significantly improved by anti-IL-6R blocking antibody. These results clearly show the efficacy of IL-6\u2013targeting therapy for PD-1 deficiency abrogated psoriasis-like dermatitis. One essential role of IL-6 is in the promotion of T helper 17 (Th17) cell production 36 . Th17 cells were recently shown to be a main pathological cell population in idiopathic psoriasis, and blockade of IL-17A and IL-23 have been established as treatments 37 , although IL-6 was not established as a potential therapeutic target. Our results also demonstrate that increased production of Th17-related cytokines, such as IL-17A and IL-23A, was accelerated in PD-1 \u2212\/\u2212 mice and was significantly suppressed by IL-6 blockade at both the tissue mRNA and serum levels to the same level as control WT mice. IL-6 signals through the IL-6R\u03b1 and \u03b2 subunit glycoprotein 130 (gp130). However, as for cells that do not express IL-6R\u03b1 on their surface, such as CD8 T cells, trans-signaling, a process whereby IL-6 signaling occurs through a complex of IL-6 and a soluble form of the IL-6R\u03b1 binding to ubiquitously expressed gp130 38 , is believed to occur. Thus, IL-6 trans-signaling likely plays an important role for the development of cytotoxic CD8 T cell function 39 . Therefore, it is likely that increased levels of soluble IL-6 in PD-1 \u2212\/\u2212 mice promotes cytotoxic CD8 T cell function via IL-6 trans-signaling. Furthermore, our analysis of human samples from anti-PD-1 antibody-treated cancer patients revealed that the serum level of sIL-6 presented correlates with that of sIL-6R\u03b1 in patients with anti-PD-1 antibody-induced psoriasis-like dermatitis, which would result in enhanced epidermal infiltration of CD8 T cells. Therefore, blocking this trans-signaling process with anti-IL-6R antibody might decrease the inflammation seen during PD-1 signal inhibition-provoked psoriasis-like dermatitis by impairing the promotion of CD8 T cells. Indeed, mice with PD-1-deficiency specifically in CD8 T cells display severe psoriasis-like dermatitis, which can be restrained by blockade of IL-6 signaling. Collectively, this treatment strategy comprised of selective blockade of IL-6 signal with anti-IL-6R blocking antibody could be effective and ideal for treating PD-1 signal blockade-induced psoriasis-like dermatitis. In fact, there have been a few case reports and a retrospective cohort study showing successful treatment of steroid-refractory irAEs with one dose of an anti-human IL-6R antibody, tocilizumab 40 , 41 , 42 , even though its use in irAE has not yet been validated. Thus, the present study for the first time demonstrates the rationale for this treatment and the pathophysiology of IL-6 signaling in PD-1 signal inhibition-provoked autoimmunity. On the other hand, a synergistic antitumor effect has been demonstrated on combined blockade of both IL-6 signaling and PD-1\/PD-L1 pathways in tumor-bearing mice 43 , suggesting the efficacy of the dual signal blockade in terms of resolving the symptoms of irAEs without interfering antitumor effects. Even though psoriasis-like eruptions have been reported as a paradoxical phenomenon after use of tocilizumab 44 , our experiments and the previous clinical case report 42 demonstrate that IL-6 blockade therapy during the initial phase of PD-1 signal blockade-induced psoriasis-like dermatitis may rapidly reduce the severity of irAE and therefore, result in shorter interruptions of cancer treatments. Collectively, individuals with PD-1 signal blockade-induced psoriasis-like dermatitis can potentially benefit from IL-6-targeted therapeutic intervention, which is expected to inhibit both Th17 cell differentiation and cytotoxic CD8 T cell activation in the pathological mechanisms of irAE. The first and foremost possible limitation of the current study is its retrospective nature in human sample collection from a limited number of institutes. Therefore, potential biases, such as selection bias and reporting bias, cannot be excluded, and functional analysis of CD8 T cells in skin and blood has yet to be completed. In addition, there are potentially some differences in the pathogenic mechanisms of psoriasis-like dermatitis between PD-1-deficient mice and anti-PD-1 antibody-treated mice\/humans. These include our results that serum levels of IL-23 were significantly elevated in anti-PD-1 antibody-treated mice with psoriasis-like dermatitis compared to control mice, while the levels were equal between PD-1-deficient mice and WT mice. Further, our study did not directly addressed if PD-1 signal blockade on CD4 T cells could have some effects, as was reported in a murine model of virus infection where the effects on CD4 T cells alter CD8 T-cell function through PD-1 signal blockade 45 . Moreover, the exact source of IL-6 is yet to be determined, even though IL-6 mRNA levels in CD45-positive epidermal cells, a potential cell population identified in the current study, were significantly elevated in PD-1-deficient mice compared to WT mice. Further prospective studies are needed to clarify those findings. Despite the limitations, data from the current study highlighted the unique characteristics of PD-1 signal blockade-induced psoriasis-like dermatitis, most strikingly the significance of strong correlation between the enhanced IL-6 production and the dermatitis development, indicating the potential significance of IL-6-targeting for therapeutic intervention. In summary, IL-6 plays important roles during disease development of PD-1 signal blockade-induced psoriasis-like dermatitis. Moreover, PD-1 expressed on CD8 T cells is responsible for the regulation of skin inflammation. Blockade of IL-6 signaling decreases inflammation in PD-1 signal inhibition-provoked psoriasis-like dermatitis, and specifically, causes a reduction in the levels of Th17-related cytokines in a murine model of IMQ-induced psoriasis-like dermatitis. Thus, these findings highlight the potential significance of IL-6-targeting for therapeutic intervention of PD-1 signal blockade-induced psoriasis-like dermatitis in humans. Methods Human sample collection Formalin-fixed paraffin-embedded (FFPE) skin biopsy samples were obtained from melanoma ( n = 3), renal cell carcinoma ( n = 2), gastric cancer ( n = 1) and lung cancer ( n = 1) patients with anti-PD-1 antibody-induced psoriasis-like dermatitis ( n = 7, totally), and idiopathic psoriasis patients ( n = 6), who visited Tsukuba University Hospital (Japan) and Mito Saiseikai General Hospital (Japan) from 2014 to 2018. Serum samples were collected post-treatment from melanoma ( n = 25), renal cell carcinoma ( n = 1), and lung cancer ( n = 1) patients treated with anti-PD-1 antibody at Tsukuba University Hospital (Japan) from 2014 to 2019 ( n = 27), including eight patients who developed psoriasis-like dermatitis after the treatment. Patient clinical data were retrospectively reviewed from their medical records. Mice Wild-type (WT) C57BL\/6J male mice originally from the Jackson Laboratories were purchased from Charles River Japan. PD-1-knockout (PD-1 \u2212\/\u2212 ) mice were provided by Dr. Tasuku Honjo (Kyoto University, Japan). We generated mice with a PD-1 allele mutated by the insertion of two loxP sites flanking parts of the promoter region (PD-1 fl\/fl mice) using a CRISPR-Cas9 system at Laboratory Animal Resource Center, University of Tsukuba. PD-1 fl\/fl mice develop normally indicating that the insertion of the loxP sites does not significantly interfere with regulation of the PD-1 gene. Floxed heterozygous PD-1 fl\/+ and heterozygous CD8 cre mice (C57BL\/6-Tg(Cd8a-cre)1ltan\/J, Jackson Laboratories) were crossed to generate double heterozygous PD-1 fl\/+ ; CD8 cre mice, which were bred with homozygous PD-1 fl\/fl mice to produce conditional PD-1 homozygous (PD-1 fl\/fl CD8 cre ), PD-1 conditional knockout (PD-1-cKO), mice, and their PD-1 heterozygous (PD-1 fl\/+ CD8 cre ) littermates (Littermate Ctrl, Fig. 3 ). The primer sequences used for genotyping of CD8 cre , PD-1 \u2212\/\u2212, and PD-1 fl are listed in Supplemental Table 1 . We confirmed the complete deletion of PD-1 expression specifically in the CD8 T cell population (CD3 + CD8 + lymphocytes) in lymph nodes of PD-1-cKO mice by flow cytometry (Supplemental Fig. 1 ). C57BL\/6 background male mice, 8 to 12-weeks-old, were maintained in specific pathogen-free conditions and used for all experiments. Murine model of psoriasis-like dermatitis In order to replicate a modified murine model of IMQ-induced psoriasis-like dermatitis 30 , 3.5% IMQ cream diluted from 5% IMQ cream (Beselna \u00ae ; Mochida Pharmaceuticals) with vehicle control cream (Vanicream \u00ae ; Pharmaceutical Specialties) (62.5 mg IMQ total, which was a lower dose compared to the dose used in a conventional model of IMQ-induced psoriasis-like dermatitis) was applied topically on a daily basis to the shaved back and both ears for 5 or 7 consecutive days. Control mice were treated with the vehicle control cream only. Scoring system for evaluating the severity of skin inflammation To score the severity of inflammation of the back skin, an objective scoring system mimicking the Psoriasis Area and Severity Index (PASI) score for psoriasis patients was used as in a previous study 46 , in which independent scores of erythema, scaling, and thickening with a scale from 0 to 4 (0, none; 1, slight; 2, moderate; 3, marked; 4, very marked) were cumulated (ranged from 0 to 12). The ear thickness was measured using a micrometer (Mitutoyo). IL-6 blockade An anti-interleukin-6 receptor (anti-IL-6R) blocking antibody (MR16-1, Chugai Pharmaceuticals), which is a rat IgG1 monoclonal antibody against murine IL-6R\u03b1 chain, was injected intravenously at the dose of 2 mg per mouse prior to IMQ application at day 0. IgG isotype control (MP Biomedicals\u2122) was used as a control. Histological analysis All FFPE human skin biopsy samples and murine ear skin samples were sectioned into 2 and 4-\u03bcm-thick slides, respectively, and subsequently undergone hematoxylin-eosin (HE) staining. Human FFPE skin samples were stained immunohistologically with anti-human CD8 and anti-human CD4 monoclonal antibodies (clone C8\/144B and 4B12, respectively, Nichirei Biosciences) using an automatic slide stainer according to the manufacturer\u2019s instructions. The numbers of epidermal-infiltrating cells per sample (magnification, \u00d7400) were counted. Murine ear FFPE samples were stained immunohistochemically with primary anti-CD3 (clone SP7, diluted 1:100, Abcam) and anti-CD8a (clone 4SM15, diluted 1:400, eBioscience) monoclonal antibodies, fluorescent-labeled secondary antibodies (Alexa Fluor \u00ae 488-labeled goat anti-rabbit IgG, and Alexa Fluor \u00ae 555-labeled goat anti-rat IgG, Abcam), and 4\u2032,6-diamidino-2-phenylindole (DAPI) to detect the nucleus, by standard immunohistochemical staining techniques. A fluorescence microscope (BZ-X700, Keyence) was used for observation and to count the number of infiltrating cells per sample (magnification, \u00d7400). Blood sample assay system Murine blood samples were collected using the submandibular bleeding method and serum samples were subsequently isolated. Human and murine serum samples were immediately stored at \u2264 \u221220 \u00b0C for later use. In order to analyze cytokine (human and murine IL-6, murine IL-23A, and IL-17A) serum levels, the MILLIPLEX\u00ae MAP Kit (Merck Millipore) using Bio-Plex \u00ae Luminex 200 multiplex assay system (Bio-Rad) was employed according to the manufacturer\u2019s protocol. Human serum levels of sIL-6 and sIL-6R\u03b1 were measured using Enzyme-Linked Immuno Sorbent Assay (ELISA) kit (Duoset and Quantikine; R&D systems) according to the manufacturer\u2019s protocol. Quantitative reverse transcription-polymerase chain reaction (qRT-PCR) Total RNA was extracted from the murine ear samples using Trizol Reagent (Invitrogen). RNA concentrations were quantified and the OD 260\/230 and the OD 260\/280 ratio of the RNA samples were confirmed to be more than 1.8 and 1.6, respectively, with the NanoDrop ND-1000 (peqLab Biotechnologie GmbH). Complementary DNA (cDNA) was synthesized with a High-Capacity cDNA Reverse Transcription Kit (Thermo Fisher) according to the manufacturer\u2019s instructions. Messenger RNA (mRNA) expression levels were detected by PCR amplification of cDNA using the QuantStudio\u2122 5 Real-Time PCR Systems (Applied Biosystems) with PrimeTime \u00ae Gene Expression Master Mix and Prime Tim qPCR predesigned primers (Integrated DNA Technologies) listed in Supplemental Table 1 . All qRT-PCR analyses were performed in triplicate. Amplification products were quantified by the comparative CT method. The mRNA level of each gene was normalized to that of glyceraldehyde-3-phosphate dehydrogenase ( GAPDH ). Skin separation Murine ear skin samples were treated with 0.25% trypsin (FUJIFILM Wako Pure Chemical Corporation) solution for 40 minutes at 37 \u00b0C in order to separate the epidermis and dermis. After washing two times with phosphate-buffered saline without Ca 2+ and Mg 2+ and passing through a 70 \u00b5m cell strainer, dissociated epidermal cells were then separated into CD45 - single cells (keratinocytes) and CD45 + single cells using MACS \u00ae cell separation technology with CD45 MicroBeads beads (Miltenyi Biotec) according to the manufacturer\u2019s instructions. The positive selected fractions and the negative sorted fractions contained more than 95% and less than 1% of CD45-positive cells, respectively, by flow cytometry (data not shown). Flow cytometry Draining lymph nodes (dLNs) were harvested and single-cell suspensions were prepared. For the exclusion of dead cells, the Zombie fixable viability kit (BioLegend) was used. Cells were incubated in FACS staining buffer (PBS containing 1% BSA and 5 mM EDTA) with anti-Fc\u03b3III\/II receptor antibody (BD), and anti-CD45 (30-F11, BioLegend), anti-CD4 (Gk1.5, BioLegend), anti-CD8 (53\u20136.7, BioLegend), anti-CD3e (145-2C11, BioLegend), anti-B220 (RA3-6B2, eBioscience), and anti-PD-1 (29F.1A12, BioLegend) antibodies. For intracellular IFN-\u03b3 and Gzm B staining, cells were stimulated with 25 ng\/ml PMA and 1 \u00b5g\/ml Ionomycin in RPMI 1640 medium supplemented with 10% fetal bovine serum, 2 mM l -glutamine, 100 U\/ml penicillin, and 100 \u00b5g\/ml streptomycin (complete RPMI) with monensin (Golgi Stop, BD). After five hours of incubation, cell surface staining was followed by intracellular cytokine staining using the Fix\/Perm Kit (BD) in accordance with the manufacturer\u2019s instructions with anti\u2013IFN-\u03b3 (XMG1.2, BD) and anti-Gzm B (NGZB, eBioscience) antibodies. Fluorescence-minus-one controls were used as negative controls. Cells were acquired on the Gallios (Beckman-Coulter) and data were analyzed using the FlowJo software (v7.6.5). Statistics and reproducibility The differences between the groups were evaluated by Student\u2019s t test, Mann\u2013Whiney U test or two-way ANOVA using GraphPad Prism 7.0 Software. A value of P < 0.05 was considered to be statistically significant. We repeated at least twice experiments and the exact sample size (n) for each experiment appear in the figure legend. Study approval All patients provided written, informed consent in compliance with the approval by the Institutional Ethics Committee at the University of Tsukuba Hospital (number: H28-045 and H30-256). All animal experiments were approved by the Animal Experiment Committee of the University of Tsukuba (Permit Number: 17\u2013137), and performed in accordance with the Guide for the Care and Use of Laboratory Animals of the University of Tsukuba. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability Raw data for graphs can be found in Supplementary Data 1 . All other data are available within the manuscript files or from the corresponding author upon reasonable request. ","News_Body":"Using the body's immune system to fight cancer has great potential, but can also bring serious side effects, including itchy and painful skin reactions. But now, researchers from Japan have found how these skin reactions happen, potentially leading to a way to prevent them. In a study published this month in Communications Biology, researchers from the University of Tsukuba have determined that one unpleasant side effect of immunotherapy with PD-1 inhibitors, called \"anti-PD-1 antibody-induced psoriasis-like dermatitis,\" is caused by inflammation resulting from high levels of a specific protein. Cancer immunotherapies work through a process that allows the body's T cells to recognize and attack cancers. But because these same processes regulate inflammation, things can get out of balance. Therapies targeting PD-1 often lead to side effects called immune-related adverse events (irAEs), which happen in more than 70% of patients who take them. The most common of these is a skin reaction, and while some of these are mild and can be easily treated with steroid creams, other patients have itchy, painful, or scaly rashes requiring more intensive treatment. Nearly a fifth of patients receiving immunotherapy stop taking the treatment because of irAEs\u2014even though the treatment may be working well against their cancer. \"Inhibition of the PD-1 pathway is becoming front-line treatment for more and more cancers,\" says senior author Professor Naoko Okiyama. \"But it can't work if patients experience adverse events and discontinue treatment because of them. We hoped that by finding out exactly how PD-1 inhibitors cause dermatitis, we could also find a way to stop it.\" The new study builds on earlier research from the same team, who examined blood samples from cancer patients with this side effect, finding high levels of a cell signaling protein called IL-6. Testing this theoretical connection in mice, they found that PD-1 deficiency increased numbers of a specific type of white blood cells (called CD8 T cells) infiltrating the epidermis. CD8 T cells help the immune system kill viruses and bacteria as well as cancer cells. But when activated in large numbers, they can cause an excessive immune response leading to irAEs. The experiments in mice showed that PD-1 expressed on CD8 T cells regulates skin inflammation. The mice with PD-1 deficiency had high levels of IL-6 expression and subsequently developed dermatitis. As a final step, the researchers used an antibody to block IL-6 signaling in some of these mice\u2014and those mice developed significantly less dermatitis than the control group. \"Altogether, the results clearly show the efficacy of targeting IL-6 in mice,\" explains Professor Okiyama. \"With further study in humans, we may have a potential approach to resolving PD-1-related dermatitis.\" On the basis of these results, the researchers also propose that blockade of both IL-6 and PD-1 together could have an even better combined anti-cancer effect, though this has not yet been systematically studied. It's also unknown whether the approach will work as well in people as it does in mice. \"Our most striking finding is the importance of PD-1 expression on CD8 T cells in the development of dermatitis, showing real potential of IL-6 as a target for therapeutic intervention,\" says Professor Okiyama. \"But the hope is that we can implement this combined strategy without compromising the anti-tumor effects of the anti-PD-1 therapy.\" Immunotherapies for cancer treatment are still relatively new; therefore, limited information is available on their long-term side effects in comparison with older chemotherapy treatments. As increasing numbers of cancer patients are treated with anti-PD-1 immunotherapy, it will be ever more important to identify strategies to prevent or lessen these adverse events. ","News_Title":"Taking the itch out of cancer immunotherapy","Topic":"Medicine"}
{"Paper_Body":"Abstract Biochemical circuits made of rationally designed DNA molecules are proofs of concept for embedding control within complex molecular environments. They hold promise for transforming the current technologies in chemistry, biology, medicine and material science by introducing programmable and responsive behaviour to diverse molecular systems. As the transformative power of a technology depends on its accessibility, two main challenges are an automated design process and simple experimental procedures. Here we demonstrate the use of circuit design software, combined with the use of unpurified strands and simplified experimental procedures, for creating a complex DNA strand displacement circuit that consists of 78 distinct species. We develop a systematic procedure for overcoming the challenges involved in using unpurified DNA strands. We also develop a model that takes synthesis errors into consideration and semi-quantitatively reproduces the experimental data. Our methods now enable even novice researchers to successfully design and construct complex DNA strand displacement circuits. Introduction The success of computer engineering has inspired attempts to use hierarchical and systematic approaches for developing molecular devices with increasing complexity. To enable the design and construction of a wide range of functional molecular systems, we need software tools such as a compiler that can automatically translate high-level functions to low-level molecular implementations and provide models and simulations for predicting and debugging the behaviours of designed molecular systems. The mechanism of DNA strand displacement has been used to create a variety of synthetic molecular systems including circuits, motors and triggered assembly of structures 1 . Software tools have been developed for designing and analysing DNA strand displacement systems, capable of generating nucleic acid sequences from well-defined structures and molecular interactions 2 , 3 , calculating the thermodynamic 2 , 4 , 5 and kinetic 6 properties of designed molecules, and evaluating if the behaviours of the molecular systems agree with the higher-level designs 3 , 7 , 8 , 9 , 10 , 11 . There also exist a few molecular compilers that can translate abstract functions such as a logic function to DNA strand displacement implementations without requiring an understanding of the molecular level details 12 , 13 . However, there has been little independent experimental validation of these compilers, most of which were developed in parallel with or after experimental findings 12 , 14 . In addition to software tools that facilitate automated design and analysis of DNA strand displacement circuits, we also need to simplify the experimental procedures for creating these circuits in vitro , so that it is possible for researchers with diverse backgrounds to build their own circuits and explore potential applications. A great inspiration is DNA origami 15 , a technique that folds DNA into sophisticated structures. In just 10 years since its birth, DNA origami has become one of the most significant successes in the field of DNA nanotechnology. Over 170 research groups have contributed to advancing this technique or developing it for applications in a variety of research areas 16 , 17 , 18 , 19 . A fundamental reason why DNA origami was able to quickly spread around the world is that the experimental procedure is extremely simple and makes use of cheap, unpurified nucleic-acid strands. In contrast, other than a few very simple circuits with just one or two double-stranded components 20 , most DNA strand displacement circuits were constructed using strands that were purchased either purified or unpurified, but all followed by in-house polyacrylamide gel electrophoresis (PAGE) purification to reduce undesired products due to synthesis errors and stoichiometry errors 12 , 14 , 21 . Purified strands are approximately ten times more expensive than unpurified strands, which significantly increases the cost for building large-scale DNA circuits. In-house PAGE purification is both time consuming and labour intensive. In this work, we show that one can successfully build a complex DNA strand displacement circuit, using DNA sequences automatically generated from a molecular compiler. We also show that one can even do so using cheap, unpurified DNA strands, following simple and systematic experimental procedures. Results Circuit design A simple DNA strand displacement motif called the seesaw gate was developed to scale up the complexity of DNA circuits 22 and was used to demonstrate digital logic computation 12 and neural network computation 23 . The Seesaw Compiler 12 , 24 was developed to automatically translate an arbitrary feed forward digital logic circuit into its equivalent seesaw DNA circuit ( Fig. 1 ). The compiler takes an input file that describes a logic circuit with a list of input and output terminals, and a list of AND, OR, NOT, NAND and NOR gates with the connectivity of their terminals specified. First, a technique called dual-rail logic is applied to translate the original logic circuit into an equivalent circuit that contains AND and OR gates only 25 . This is because the NOT gate cannot be directly implemented in multi-layer use-once DNA circuits, if the OFF and ON state of a signal is represented by low and high concentration of a single DNA strand, respectively. If a NOT gate were implemented this way, then output molecules of the gate could be immediately produced in the absence of input. However, once this reaction reaches equilibrium it cannot be reversed, even if input molecules are added at a later point. With dual-rail logic, each terminal in the original circuit is replaced by two terminals, representing the OFF and ON states of a signal separately (for example, each input signal x i is replaced by and ). Thus, no reaction will take place until signal molecules on one of the two wires have arrived. With this representation, the NOT gate can be implemented by exchanging the two wires of an input and output signal. Each AND, OR, NAND and NOR gate in the original circuit is replaced by a pair of AND and OR gates. Figure 1: Automated circuit design steps using the Seesaw Compiler. A feedforward digital logic circuit is first translated into an equivalent dual-rail logic circuit and then translated into an equivalent seesaw DNA circuit. Visual DSD code and Mathematica code are generated for analysing and simulating the seesaw DNA circuit, and DNA sequences are generated for constructing the circuit. Bottom right diagram introduces the notations of seesaw circuits: black numbers indicate identities of nodes. The locations and values of red numbers indicate the identities of distinct DNA species and their relative initial concentrations, respectively. Full size image Next, the compiler translates the dual-rail logic circuit into an equivalent seesaw DNA circuit. In a seesaw DNA circuit, each signal is defined as a wire w j , i connecting seesaw nodes j and i , and implemented using a single-stranded DNA molecule. Each AND and OR gate in the dual-rail circuit is replaced by a seesaw AND and OR gate, respectively, which is defined as a pair of integrating and amplifying seesaw nodes connected with a set of input and output wires 12 . The seesaw nodes are composed of double-stranded threshold and gate:output molecules and single-stranded fuel molecules ( Fig. 1 , bottom right). We will explain how the seesaw logic gates work in the next section. Input fan-out gates are introduced to take an input signal that is used for multiple logic gates and produce the corresponding number of output signals. Reporters are introduced to take each output signal and generate a distinct fluorescence signal for readout. Finally, the compiler generates Visual DSD 3 , 26 code and Mathematica code for simulating and analysing the seesaw DNA circuit and a file that contains DNA sequences for all molecular species in the circuit. The Visual DSD code can be used to automatically produce diagrams of species, reactions and network graphs with domain-level representation of DNA and to simulate the circuit behaviour based on the network of chemical reactions. The Mathematica code provides more customized and efficient simulations of seesaw circuits. The simulation uses the CRNSimulator package 27 and models a specific set of side reactions in addition to the designed reactions in a seesaw network 12 . As a demonstration of using the Seesaw Compiler, we designed a single DNA strand displacement circuit that implements two distinct elementary cellular automata transition functions. An elementary cellular automaton (CA) is one of the simplest models of computation 28 . It consists of a one-dimensional grid of cells, collectively called a generation, where each cell has a binary state of 0 or 1. In each subsequent generation, the state for a cell C is determined by its current state and those of its left neighbour L and right neighbour R . A state transition rule maps each of the 2 3 =8 possible combinations of states for L , C and R to either 0 or 1. Thus, a length 8 binary string uniquely identifies one of the 2 8 possible transition functions that specify how an elementary CA will evolve between generations. The rule 110 elementary CA (binary number 01101110 written in decimal) is famously known to be Turing universal 29 . Another rule that is equally powerful is rule 124 (binary number 01111100 written in decimal), generated by applying the following mirror transformation: the new state of the centre cell for LCR = zyx in rule 124 is the same as the new state for LCR = xyz in rule 110. Our circuit was designed to compute a combined logic function of the two transition rules ( Fig. 2a ). It consists of five logic gates in two layers, including a three-input two-output NAND gate. It is noteworthy that we designed the circuit to demonstrate an interesting logic function associated with cellular automata and not to implement the actual cellular automata model. The circuit operates in a well-mixed test tube environment that does not involve spatial dynamics (that is, no geometry of cells). Figure 2: Design of a rule 110\u2013124 circuit using the Seesaw Compiler. ( a ) Gate diagram and truth table of a digital logic circuit that computes the transition rules 110 and 124 of elementary cellular automata. ( b ) Seesaw gate diagram of the equivalent DNA strand displacement circuit. Each seesaw node connected to a dual-rail input implements input fan-out. Each pair of seesaw nodes labelled and implements a dual-rail AND and OR gate, respectively. Each pair of dual-rail AND and OR gates implements an AND, OR or NAND gate in the original logic circuit. Each dual-rail output is converted to a fluorescence signal through a reporter, indicated as a half node with a zigzag arrow. Each circle and dot inside a seesaw node indicates a double-stranded threshold and gate molecule, respectively. Each dot on a wire indicates a single-stranded fuel molecule. ( c ) Simulations of the DNA strand displacement circuit using the previously developed model for purified seesaw circuits. Trajectories and their corresponding outputs have matching colours. Overlapping trajectories were shifted to be visible. Dotted and solid lines indicate dual-rail outputs that represent logic OFF and ON, respectively. For example, when input LCR =001, meaning L 0 , C 0 and R 1 were introduced at a high concentration and L 1 , C 1 and R 0 at a low concentration, two output trajectories R 124 0 and R 110 1 reached an ON state and the other two output trajectories R 124 1 and R 110 0 remained in an OFF state, indicating that the output was computed to be 0 and 1 for rule 124 and 110, respectively. Simulations were performed at 1 \u00d7 =50 nM\u2014the compiler recommended standard concentration for large-scale purified seesaw circuits. Full size image The DNA circuit generated by the Seesaw Compiler consisted of 6 layers and a total of 78 distinct initial DNA species ( Fig. 2b and Supplementary Fig. 1 ). Mathematica simulations of the DNA circuit predicted correct computation for all 8 possible input combinations under ideal experimental conditions ( Fig. 2c ). The next step was to construct the DNA circuit using strands that were purchased unpurified and with no additional in-house purification. We expected that the main challenges would be to understand how synthesis errors and stoichiometry errors affect the behaviours of DNA circuits and to explore solutions that restore the desired circuit behaviour. We took a bottom-up approach and began building the DNA circuit from the simplest functional component\u2014digital signal restoration. Calibrating effective concentrations Digital signal restoration is a process that pushes the intrinsically analog signal towards either the ideal ON or OFF state, therefore cleaning up the noise and compensating for the signal decay that occurs during circuit execution. In seesaw circuits, digital signal restoration is a component of every logic gate, and is implemented by an amplifying seesaw node with the following idealized input-output function: At the molecular level, the digital signal restoration process consists of two basic reactions: catalysis and thresholding. Catalysis is implemented with two toehold exchange pathways that release free output strands w i , k from double-stranded gate molecules G i : i , k , using the input strands w j , i as a catalyst ( Supplementary Fig. 2a ): Catalysis can be used for signal amplification, since a small amount of input can trigger the release of a much larger amount of output. Thresholding is implemented with double-stranded threshold molecules Th j , i : i consuming the input at a much faster rate ( k f \u226b k s ) than the input acting as a catalyst ( Supplementary Fig. 2b ): As shown in simulations generated using the Seesaw Compiler ( Fig. 3a ), when the concentration of the threshold molecule is 0.5 \u00d7 (where 1 \u00d7 is a standard concentration of 100 nM), we expect that input less than the threshold (for example, 0.3 \u00d7 ) should be cleaned up to an ideal OFF state via reaction 3 and input greater than the threshold (for example, 0.7 \u00d7 ) should be amplified to an ideal ON state via reaction 2. However, the observed circuit behaviour was different: when input=0.7 \u00d7 , the output signal was higher than an ideal OFF state, but did not reach an ideal ON state ( Fig. 3b ). This experimental result suggested that the input did not sufficiently exceed the threshold, which was an indication that the effective concentration of an unpurified threshold species, compared with that of an unpurified signal species, was higher than expected. Figure 3: Calibrating effective concentrations. ( a ) Simulations and ( b ) experimental data of digital signal restoration. ( c ) Estimating effective threshold concentration by fitting simulations to the data obtained. ( d ) OR and AND logic gates constructed using adjusted nominal threshold concentrations. ( e ) Estimating effective gate concentration. Data show steady-state fluorescence level. 1 \u00d7 =100 nM. Here and in later figures, all output signals in the data were normalized using the minimum fluorescence signal (the first data point) of an OFF trajectory as 0 and the maximum fluorescence signal (the average of the last five data points) of an ON trajectory as 1. Full size image The nominal concentration of a DNA species can be measured using ultraviolet absorbance, but it can be higher than the effective concentration, which is the concentration of the DNA species actually performing the desired reactions. If the sequences of the DNA strands are properly designed, the difference between nominal concentration and effective concentration is typically caused by synthesis errors including nucleotide insertion, deletion and mismatch. To calibrate the effective concentrations of unpurified DNA molecules, we defined the following ratio between effective (eff) and nominal (nom) concentrations of an arbitrary signal, threshold and gate species: The effective to nominal concentration of a DNA species cannot be measured in isolation. More importantly, the absolute values of \u03b1 , \u03b2 and \u03b3 should only affect the speed but not the correctness of computation, if the values remain comparable to each other. Thus, we chose to estimate the ratio between \u03b2 and \u03b1 for a threshold consuming a signal, by comparing simulation with experimental result of a signal restoration circuit. For example, manipulating the threshold value in simulation (sim) identified that agreed with the experimental data ( Fig. 3c ), which means the effective concentration of the threshold was similar to that of the signal for and . Thus, the threshold to signal ratio can be calculated as: A possible explanation for an unpurified threshold having a higher effective concentration than an unpurified signal, when the nominal concentrations are the same, is the following: the synthesis errors of an unpurified strand depend on the length of the strand, because in the process of chemical synthesis each nucleotide is attached to a growing chain of oligonucleotide one at a time and the coupling efficiency of each step is less than 100% (ref. 30 ). Threshold molecules are composed of shorter strands (15 and 25 nucleotides) than signal molecules (33 nucleotides) and thus may contain fewer synthesis errors. Additional signal restoration experiments suggested that the threshold to signal ratio \u03b2 \/ \u03b1 =1.4 was consistent for different threshold and signal molecules ( Supplementary Fig. 3 ). Thus, using this ratio, we can then calculate how to adjust the nominal thresholds for correctly computing logic AND and OR. Each seesaw logic gate has an integrating node upstream of an amplifying node. Ideally, an integrating node outputs the sum of all inputs: A two-input logic function can be computed as: Assuming that an ideal OFF state is [0, 0.2] and an ideal ON state is [0.8, 1], th =0.6 will compute logic OR and th =1.2 will compute logic AND, if the effective concentrations of the threshold and input signals are comparable to each other (that is, \u03b2 \/ \u03b1 =1). As \u03b2 \/ \u03b1 \u22601 for unpurified threshold and signal molecules, we can take this ratio into consideration while calculating the lower and upper bounds of the nominal threshold for an n -input logic gate: Using \u03b2 \/ \u03b1 =1.4, we chose a nominal threshold of 0.35 \u00d7 and 0.85 \u00d7 for two-input OR and AND gate, respectively, and 0.4 \u00d7 and 1.6 \u00d7 for three-input OR and AND gate. Experiments of the logic gates showed desired behaviours ( Fig. 3d and Supplementary Fig. 4 ). An alternative approach for adjusting the nominal threshold is to use the following equations: Compared with choosing a nominal threshold based on the lower and upper bounds, this approach is less flexible but simpler. Next, we can estimate the ratio between \u03b3 and \u03b1 for a gate releasing a signal, using an experiment that compares the fully triggered (tri) concentration of the gate with the signal when their nominal concentrations are the same. For example, the data in Fig. 3e showed that when . Thus, the gate to signal ratio can be calculated as: Additional gate calibration experiments suggested that the ratio \u03b3 \/ \u03b1 =0.8 was consistent for different gate and signal molecules ( Supplementary Fig. 5 ). We suspect that due to synthesis errors in gate molecules, not all gates can successfully release a signal, which is why an unpurified gate has a lower effective concentration compared to a signal. As signal restoration was built in within every logic gate to accept an ON state of [0.8, 1], we decided not to make any adjustment for nominal gate concentrations if \u03b3 \/ \u03b1 \u22650.8. Otherwise, nominal concentration of an amplifying gate and an n -input integrating gate can be adjusted as: Importantly, the values of \u03b1 , \u03b2 and \u03b3 should depend on the strand quality and thus could vary with different DNA synthesis providers, procedures and even batches. It is necessary to recalculate the ratios \u03b2 \/ \u03b1 and \u03b3 \/ \u03b1 , if these conditions change. Identifying outliers With calibrated logic gates, we investigated how well they compose together in larger circuits. We constructed a two-layer logic circuit that is part of the rule 124 sub-circuit and is composed of an AND gate and two upstream OR gates ( Fig. 4a ). The expected circuit behaviour is that the output should remain OFF when only one of the upstream OR gates is ON. However, the observed circuit behaviour showed that the output was reasonably OFF when one upstream OR gate was ON, but was half ON when the other upstream OR gate was ON. This experimental result suggested that the ON signals pushed onto the two input wires of the downstream AND gate (that is, the output wires of the two upstream OR gates) were significantly different from each other, which was an indication that the effective concentrations of the two unpurified gate species that released the output signals were different\u2014one of the gates must be an outlier with \u03b3 \/ \u03b1 \u22600.8. Figure 4: Identifying an outlier gate. ( a ) Logic circuit diagram, seesaw circuit diagram and experimental data of a two-layer logic circuit. ( b ) Measuring the effective concentrations of the gate species. Three independent circuits were used to measure the effective concentrations of two gates fully triggered by x 1 and x 2 , respectively, comparing with the effective concentration of x 3 (using signal strand w 18,53 ). ( c ) Experimental data of the two-layer logic circuit using adjusted nominal gate concentration. 1 \u00d7 =100 nM. Full size image Indeed, with a gate calibration experiment shown in Fig. 4b , we measured that \u03b3 18,53 \/ \u03b1 18,53 =0.8 \u00d7 for one gate and \u03b3 22,53 \/ \u03b1 22,53 =0.44 \u00d7 for another. A possible explanation is that the synthesis errors of unpurified strands somewhat depend on DNA sequences 30 and variations of effective concentrations may occur between different gate or threshold species. We suspect it was not a coincidence that the outlier gate had a lower effective concentration compared with other unpurified gates, because a particular DNA strand having much worse quality than average is probably more likely than it having much better quality. Once an outlier is identified, either a threshold or a gate, the nominal concentration can be adjusted using its own threshold to signal ratio (that is, \u03b2 \/ \u03b1 ) or gate to signal ratio (that is, \u03b3 \/ \u03b1 ), the common nominal concentration described in the previous section, and the common ratio for other thresholds and gates: We constructed the two-layer logic circuit using the adjusted nominal gate ( Fig. 4c ). The trajectories that compute logic ON reached an ideal high fluorescence state faster than the previous experiments shown in Fig. 4a and the trajectories that compute logic OFF remained at a lower fluorescence state that were roughly identical for all three input combinations, regardless of which upstream OR gate was ON. However, after identifying and adjusting the outlier gate, we still had a problem: the OFF trajectories were not at an ideal low fluorescence state. This led to the next tuning step that is necessary for unpurified seesaw circuits. Tuning circuit output Comparing the behaviour of the AND gate when it was in isolation ( Fig. 3d ) and that when it was connected with two upstream OR gates ( Fig. 4c ), the ON\/OFF separation was significantly decreased in the latter. These experimental results suggest that, compared with purified seesaw DNA circuits in which the ON\/OFF separations were roughly identical from a single logic gate to four-layer logic circuits 12 , unpurified circuits are much noisier and the behaviour becomes less robust with more than one layer. We suspect this is caused by the stoichiometry errors in unpurified gate species. The double-stranded gate molecules were annealed with the same amount of top and bottom strands, because both strands have combinations of toehold and branch migration domains that can cause undesired interactions with other circuit components and thus neither should be in excess. However, due to variations in the pipetting volume and in the accuracy of concentrations, the equal stoichiometry cannot be guaranteed. Without purification, a small excess of one strand or another in the gate species cannot be removed. Therefore, the excess of strands would result in undesired release of output signals in logic gates, even without input signals, and introduce extra noise to downstream logic gates. Fortunately, thanks to the thresholding function in every logic gate, we can tune the circuit output by increasing a threshold. A simple method for estimating how much threshold adjustment is needed is based on the ON\/OFF separation of the circuit output. Using experimental data of a logic circuit with different inputs, we can choose a trajectory that should compute logic ON and OFF, respectively, and calculate the difference ( \u03b4 ) between the observed OFF value and an ideal OFF value, when the ON trajectory reaches an ideal ON value. Considering 0.7 and 0.3 as the lower bound, and 0.9 and 0.1 as the upper bound for an ideal ON\/OFF separation, the range of \u03b4 can be determined as: The nominal threshold in the logic gate that produces the circuit output can then be adjusted accordingly: Using the data of the two-layer logic circuit shown in Fig. 4c , we chose the trajectory with input=01010 and 11100 as the reference ON and OFF trajectory, respectively, and calculated 0.08\u2264 \u03b4 \u22640.41. We then increased the threshold in the downstream AND gate to and repeated the experiment. The circuit behaviour was improved with a much better ON\/OFF separation ( Fig. 5a ). Figure 5: Tuning circuit output. Logic circuit diagram, seesaw circuit diagram and experimental data of a two-layer logic circuit with ( a ) two upstream OR gates connected to a downstream AND gate and ( b ) two upstream AND gates connected to a downstream OR gate. Nominal concentrations shown in grey and black indicate adjustments made in a previous step and in this step, respectively. Small insets of experimental data show the circuit behaviours before adjustments. 1 \u00d7 =100 nM. Full size image With the same method, we constructed another two-layer logic circuit that is composed of an OR gate and two upstream AND gates ( Fig. 5b ). In this case, using input=00011 and 01110 as the reference ON and OFF trajectories, we obtained a similar range of \u03b4 and decided to apply the same amount of increase to the threshold in the downstream OR gate. It is noteworthy that a rule of thumb is to choose the slowest ON trajectory and the fastest OFF trajectory as the references for threshold adjustment, but different choices can be made if one has the knowledge of which data set is experimentally more reliable. Also note that increasing the threshold not only suppresses the OFF trajectories but also slows down the ON trajectories and thus this method of tuning the circuit output is only applicable if all ON trajectories are significantly faster than all OFF trajectories (which should be true if the thresholds and gates are properly calibrated). Combining the two logic circuits shown in Fig. 5 and adding fan-out gates for input signals that are used in multiple logic gates, we successfully demonstrated the rule 124 sub-circuit consisting of 54 distinct DNA species ( Supplementary Fig. 6 ). We do not have evidence of how well unpurified circuits with multiple layers can be constructed, but we suspect that with the same amount of threshold increase (that is, \u03b4 \u00d7 \u03b1 \/ \u03b2 ) in all logic gates at layer two and above, undesired signals released from upstream gates can be effectively suppressed at every layer without accumulating over an increasing number of layers. Systematic procedure Starting from the calibration of effective concentrations for threshold and gate species in general, to the identification and adjustment of any outliers, and then to the final tuning of circuit output, we established three sequential steps for building unpurified seesaw circuits. To make these steps easy to follow, we now further describe a systematic procedure, and evaluate the procedure by constructing a new logic circuit from scratch\u2014the rule 110 sub-circuit. We summarized the procedure in a flowchart ( Fig. 6 ). It starts with constructing the simplest functional component, digital signal restoration, and estimating the effective threshold compared to a signal. If the threshold to signal ratio \u03b2 \/ \u03b1 >1.2, adjust the nominal thresholds in all logic gates. Next, construct a single logic gate. If it fails to compute correctly, it indicates that the threshold species in this logic gate is an outlier, and thus one needs to go back to the first step and repeat the process to calibrate this particular threshold. Otherwise, move on to gate calibration experiments. If the gate to signal ratio \u03b3 \/ \u03b1 <0.8, adjust all nominal gates. Figure 6: Flowchart for building seesaw DNA circuits using unpurified components. Insets show how the flowchart was used to construct the rule 110 sub-circuit. Y (yes) and N (no) highlighted in orange in the flowchart indicate the situations encountered and decisions made while building the rule 110 sub-circuit. 1 \u00d7 =100 nM. Full size image Then construct a two-layer logic circuit, and identify if there exists an outlier gate. If so, repeat the process to calibrate this particular gate. At this point, the circuit still may not exhibit desired ON\/OFF separation (for example, the OFF trajectories may be higher than 0.3 when the ON trajectories reach 0.7). However, if the ON trajectories are significantly faster than the OFF trajectories, increase the nominal threshold in the logic gate that directly produces the circuit output to tune the circuit behaviour. Continue to construct a larger circuit. If it fails to compute correctly, the most likely reason would be a new outlier gate. Identify the outlier based on cases where the ON\/OFF separation is worst, and repeat the steps for calibrating the gate accordingly. Following the flowchart, we completed the construction of the rule 110 sub-circuit in only 3 days ( Fig. 6 ). If all components were PAGE purified, incrementally building the circuit would require at least one additional day for each new experiment, assuming no experimental errors. The turnaround time would be significantly increased. Combining the components from both rule 110 and rule 124 sub-circuits, using shared input-fanout gates and a three-input NAND gate ( Fig. 2ab ), the full rule 110\u2013124 circuit consisting of 78 distinct DNA species was constructed in one test tube. The fluorescence kinetics experiments showed correct ON and OFF states of the two pairs of dual-rail outputs, for all eight possible inputs ( Fig. 7a ). To pictorially compare the ideal logic behaviour and the DNA circuit behaviour, we plotted each output into an array that represents eight cellular automata generations ( Fig. 7b ). The ideal logic circuit behaviour corresponds to four images of dogs. The DNA circuit behaviour yielded less contrast between the dogs and their backgrounds, but the patterns were still clearly recognizable. Figure 7: Implementing the rule 110\u2013124 full circuit. ( a ) Fluorescence kinetics data of the two pairs of dual-rail outputs. 1 \u00d7 =100 nM. All DNA sequences are listed in Supplementary Table 1 . ( b ) Comparing the ideal logic circuit behaviour (left) with the DNA circuit behaviour (right). Each of the circuit outputs is illustrated by an array of 7 \u00d7 8 cells, representative of eight cellular automata generations on a torus with starting configuration (0,0,0,1,0,0,0). The arrays for the DNA circuit were plotted using the output values at 24 h from the data. The ideal logic circuit behaviour corresponds to an image of a black dog with a white background for R 124 1 , an inverted image for R 124 0 and their mirror images for R 110 1 and R 110 0 , respectively. Full size image Modelling Despite that the experiments were performed at a higher concentration (that is, 1 \u00d7 =100 nM), the rule 110\u2013124 circuit computed much slower than what the simulations predicted for 1 \u00d7 =50 nM ( Fig. 2c ). We suspect that the difference was caused by the impurity of the molecules. To better predict the behaviour of seesaw circuits using unpurified components, we developed a model that takes synthesis errors into consideration. We first define the probability of having n errors in a chemically synthesized DNA strand of l bases, given that r is the probability of synthesis error per base: We then calculate the populations of signal, gate and threshold molecules with and without synthesis errors ( Fig. 8a ). To make the model simple enough, but accurate enough to describe reactions that involve molecules with synthesis errors at distinct locations, we treat the very small population of molecules with more than one synthesis error as non-reactive, and classify the remaining molecules containing a single synthesis error based on the domain where the error occurs. For example, a signal strand is composed of two branch migration domains flanking a toehold domain. Given that a branch migration domain has 15 bases and a toehold domain has 5 bases, the probability of a signal strand having s errors in a specific branch migration domain (and thus not in the other) and t errors in the toehold domain can be calculated as: Figure 8: A model for unpurified seesaw circuits. ( a ) Populations of signal, gate and threshold molecules without and with synthesis errors in the marked locations. r =0.01. ( b ) Example reactions that involve DNA strands without and with synthesis errors. \u2200 i , j , k , x and y . Full size image In a previous study on the robustness of a catalytic DNA strand displacement motif 21 , a single base mutation in an invading strand significantly slowed down (on the scale of 100 fold) a reversible strand displacement reaction that was designed with \u0394 G \u00b0\u22480, both when the mutation was in the toehold and when it was in the branch migration domain. In contrast, an irreversible strand displacement reaction was only slowed down significantly (also on the scale of 100-fold) when the mutation was in the toehold domain, but the reaction rate remained roughly unchanged when the mutation was in the branch migration domain. These observations lead us to the following interpretations: A synthesis error in the toehold domain can slow down strand displacement by increasing the disassociation rate of the toehold and thus decreasing the overall reaction rate. A synthesis error in the branch migration domain can also slow down strand displacement, but only when the energy change caused by the synthesis error is significant compared to the designed standard free energy of the reaction, and not when the reaction is already strongly favoured in one direction. Based on these interpretations, we estimated the rates of all five types of reactions in a seesaw network, involving all populations of defective molecules ( Fig. 8b and Supplementary Note 1 ). We first simulated the rule 110\u2013124 circuit assuming that all molecules do not have synthesis errors, at the concentrations used in the experiments ( Fig. 9a ). Using exactly the same concentrations for all species, and the same rate parameters for reactions that are not affected by synthesis errors, we then simulated the circuit with each species divided into multiple populations including synthesis errors ( Fig. 9b ). The results of these two simulations were dramatically different: only the latter exhibited a remarkable degree of agreement with the data shown in Fig. 7a . Figure 9: Simulations comparing the purified and unpurified models. ( a ) Simulations of the rule 110\u2013124 circuit using the previously developed model for purified seesaw circuits, predicting that the circuit should yield desired outputs in roughly 8 h (shown as dotted lines) and the undesired reactions will take over in 24 h. ( b ) Simulations using the new model for unpurified seesaw circuits, predicting that the circuit should yield desired outputs in roughly 24 h. k f =2 \u00d7 10 6 M \u22121 s \u22121 , k s =5 \u00d7 10 4 M \u22121 s \u22121 , k l =10 M \u22121 s \u22121 , k rf =26 s \u22121 , k rs =1.3 s \u22121 . 1 \u00d7 =100 nM. Full size image Discussion The biggest challenge that could prevent a molecular compiler from working in practice is that a new circuit may require new molecular components, which may not behave the same as the ones previously characterized. Thus, what made it possible to build a new complex circuit using the Seesaw Compiler? First, there are only three types of molecular components (signal, gate and threshold) for arbitrary feedforward logic circuits, which yield highly predictable circuit behaviour. Second, because of the simplicity of the molecules, there is minimal sequence design challenge. A three-letter code (A, T and C) for all signal strands is sufficient to eliminate undesired reactions. Finally, exact kinetics is not essential for qualitatively correct computation and thus small difference caused by DNA sequences should not affect the desired circuit behaviour. On the other hand, the biggest challenge that could prevent us from using unpurified DNA strands is that the synthesis errors may lead to completely unpredictable molecular behaviours. Thus, what made it possible to build a complex circuit using unpurified strands? First, the Seesaw Compiler provides simulations as a debugging tool and makes it straightforward to identify problems caused by the synthesis errors. Second, again because there are only three types of species, it is relatively easy to understand the behaviours of defective molecules, as we expect similar synthesis quality across distinct species of the same type. More importantly, the signal restoration built in to every logic gate allows simple tuning to restore desired circuit behaviour, compensating for the impurity of molecules. In general, there are several factors that we find important for the goals of producing a better molecular compiler, and implementing unpurified DNA circuits with more robust behaviours. Given that it is difficult to obtain fully predictable behaviour for newly designed molecular components, alternative architectures that enable arbitrary circuits to be created from a constant number of molecules will likely promote the development of compilers that work reliably in these contexts 31 . It is also necessary to eliminate leak reactions in DNA circuits 32 and to improve the building blocks such that they are substantially less sensitive to synthesis errors and stoichiometry errors. Nonetheless, with an experimental validation of the Seesaw Compiler and simplified experimental procedures using unpurified DNA strands described in this work, it is now possible to imagine a near future in which a molecular compiler can generate protocols from a high-level circuit function, and the protocols can then be executed by a liquid handling robot. Molecular engineers typing away on a computer to create biochemical circuits in a test tube is no longer just a distant dream. Methods DNA oligonucleotide synthesis DNA oligonucleotides were purchased from Integrated DNA Technologies (IDT). The DNA strands in gate, threshold and fuel species were purchased unpurified (standard desalting). The reporter strands with fluorophores and quenchers were purchased purified (HPLC). All strands were purchased at 100 \u03bcM in TE buffer pH 8.0 and stored at 4 \u00b0C. Annealing protocol and buffer condition Gate complexes were annealed together at 20 \u03bcM, with equal stoichiometry of top and bottom strands. Threshold and reporter complexes were annealed together at 20 \u03bcM with a 20% excess of top strands. All DNA complexes were annealed in 1 \u00d7 TE buffer with 12.5 mM Mg 2+ , prepared from 100 \u00d7 TE pH 8.0 (Fisher BioReagents) and 1 M MgCl 2 (Invitrogen). Annealing was performed in a thermal cycler (Eppendorf), first heating up to 90 \u00b0C for 2 min and then slowly cooling down to 20 \u00b0C at the rate of 6 s per 0.1 \u00b0C. All annealed complexes were stored at 4 \u00b0C. Fluorescence spectroscopy Fluorescence kinetics data in Figs 3 , 4 , 5 , 6 and Supplementary Figs 3\u20136 were collected every 2 min in a monochromator-based plate reader (Synergy H1M, BioTek). Experiments were performed with 100 \u03bcl reaction mixture per well, in 96-well microplates (black with clear flat bottom, polystyrene NBS, Corning 3651) at 25 \u00b0C. Clear adhesive sealing tapes (Thermo Scientific Nunc 232701) were used to prevent evaporation. The excitation\/emission wavelengths were set to 497\/527 nm for ATTO 488 and 597\/629 nm for ATTO 590. Fluorescence kinetics data in Fig. 7 were collected every 4 min in a spectrofluorimeter (Fluorolog-3, Horiba). Experiments were performed with 500 \u03bcl reaction mixture per cuvette, in fluorescence cuvettes (Hellma 115 F-QS) at 25 \u00b0C. The excitation\/emission wavelengths were set to 502\/522 nm for ATTO 488, 602\/624 nm for ATTO 590, 560\/575 nm for ATTO 550 and 649\/662 nm for ATTO 647. Both excitation and emission bandwidths were set to 2 nm and the integration time was 10 s for all experiments. Data analysis A Mathematica Notebook file for data analysis and example data files are available to download at the Seesaw Compiler website:  . Data availability Key data supporting the findings of this study are available to download at the Seesaw Compiler website and all other data are available from the corresponding author upon reasonable request. Additional information How to cite this article: Thubagere, A. J. et al . Compiler-aided systematic construction of large-scale DNA strand displacement circuits using unpurified components. Nat. Commun. 8, 14373 doi: 10.1038\/ncomms14373 (2017). Publisher\u2019s note : Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. ","News_Body":"Electronic circuits are found in almost everything from smartphones to spacecraft and are useful in a variety of computational problems from simple addition to determining the trajectories of interplanetary satellites. At Caltech, a group of researchers led by Assistant Professor of Bioengineering Lulu Qian is working to create circuits using not the usual silicon transistors but strands of DNA. The Qian group has made the technology of DNA circuits accessible to even novice researchers\u2014including undergraduate students\u2014using a software tool they developed called the Seesaw Compiler. Now, they have experimentally demonstrated that the tool can be used to quickly design DNA circuits that can then be built out of cheap \"unpurified\" DNA strands, following a systematic wet-lab procedure devised by Qian and colleagues. A paper describing the work appears in the February 23 issue of Nature Communications. Although DNA is best known as the molecule that encodes the genetic information of living things, they are also useful chemical building blocks. This is because the smaller molecules that make up a strand of DNA, called nucleotides, bind together only with very specific rules\u2014an A nucleotide binds to a T, and a C nucleotide binds to a G. A strand of DNA is a sequence of nucleotides and can become a double strand if it binds with a sequence of complementary nucleotides. DNA circuits are good at collecting information within a biochemical environment, processing the information locally and controlling the behavior of individual molecules. Circuits built out of DNA strands instead of silicon transistors can be used in completely different ways than electronic circuits. \"A DNA circuit could add 'smarts' to chemicals, medicines, or materials by making their functions responsive to the changes in their environments,\" Qian says. \"Importantly, these adaptive functions can be programmed by humans.\" To build a DNA circuit that can, for example, compute the square root of a number between 0 and 16, researchers first have to carefully design a mixture of single and partially double-stranded DNA that can chemically recognize a set of DNA strands whose concentrations represent the value of the original number. Mixing these together triggers a cascade of zipping and unzipping reactions, each reaction releasing a specific DNA strand upon binding. Once the reactions are complete, the identities of the resulting DNA strands reveal the answer to the problem. With the Seesaw Compiler, a researcher could tell a computer the desired function to be calculated and the computer would design the DNA sequences and mixtures needed. However, it was not clear how well these automatically designed DNA sequences and mixtures would work for building DNA circuits with new functions; for example, computing the rules that govern how a cell evolves by sensing neighboring cells. \"Constructing a circuit made of DNA has thus far been difficult for those who are not in this research area, because every circuit with a new function requires DNA strands with new sequences and there are no off-the-shelf DNA circuit components that can be purchased,\" says Chris Thachuk, senior postdoctoral scholar in computing and mathematical sciences and second author on the paper. \"Our circuit-design software is a step toward enabling researchers to just type in what they want to do or compute and having the software figure out all the DNA strands needed to perform the computation, together with simulations to predict the DNA circuit's behavior in a test tube. Even though these DNA strands are still not off-the-shelf products, we have now shown that they do work well for new circuits with user-designed functions.\" \"In the 1950s, only a few research labs that understood the physics of transistors could build early versions of electronic circuits and control their functions,\" says Qian. \"But today many software tools are available that use simple and human-friendly languages to design complex electronic circuits embedded in smart machines. Our software is kind of like that: it translates simple and human-friendly descriptions of computation to the design of complex DNA circuits.\" The Seesaw Compiler was put to the test in 2015 in a unique course at Caltech, taught by Qian and called \"Design and Construction of Programmable Molecular Systems\" (BE\/CS 196 ab). \"How do you evaluate the accessibility of a new technology? You give the technology to someone who is intellectually capable but has minimal prior background,\" Qian says. \"The students in this class were undergrads and first-year graduate students majoring in computer science and bioengineering,\" says Anupama Thubagere, a graduate student in biology and bioengineering and first author on the paper. \"I started working with them as a head teaching assistant and together we soon discovered that using the Seesaw Compiler to design a DNA circuit was easy for everyone.\" However, building the designed circuit in the wet lab was not so simple. Thus, with continued efforts after the class, the group set out to develop a systematic wet-lab procedure that could guide researchers\u2014even novices like undergraduate students\u2014through the process of building DNA circuits. \"Fortunately, we found a general solution to every challenge that we encountered, now making it easy for everyone to build their own DNA circuits,\" Thubagere says. The group showed that it was possible to use cheap, \"unpurified\" DNA strands in these circuits using the new process. This was only possible because steps in the systematic wet-lab procedure were designed to compensate for the lower synthesis quality of the DNA strands. \"We hope that this work will convince more computer scientists and researchers from other fields to join our community in developing increasingly powerful molecular machines and to explore a much wider range of applications that will eventually lead to the transformation of technology that has been promised by the invention of molecular computers,\" Qian says. The paper is titled, \"Compiler-aided systematic construction of large-scale DNA strand displacement circuits using unpurified components.\" ","News_Title":"Computing with biochemical circuits made easy","Topic":"Chemistry"}
{"Paper_Body":"Abstract Nutrient subsidies across ecotone boundaries can enhance productivity in the recipient ecosystem, especially if the nutrients are transferred from a nutrient rich to an oligotrophic ecosystem. This study demonstrates that seabird nutrients from islands are assimilated by endosymbionts in corals on fringing reefs and enhance growth of a dominant reef-building species, Acropora formosa . Nitrogen stable isotope ratios (\u03b4 15 N) of zooxanthellae were enriched in corals near seabird colonies and decreased linearly with distance from land, suggesting that ornithogenic nutrients were assimilated in corals. In a one-year reciprocal transplant experiment, A. formosa fragments grew up to four times faster near the seabird site than conspecifics grown without the influence of seabird nutrients. The corals influenced by elevated ornithogenic nutrients were located within a marine protected area with abundant herbivorous fish populations, which kept nuisance macroalgae to negligible levels despite high nutrient concentrations. In this pristine setting, seabird nutrients provide a beneficial nutrient subsidy that increases growth of the ecologically important branching corals. The findings highlight the importance of catchment\u2013to\u2013reef management, not only for ameliorating negative impacts from land but also to maintain beneficial nutrient subsidies, in this case seabird guano. Introduction Nutrient subsidies can transcend ecosystem boundaries where they can enhance productivity 1 and functional diversity 2 , alter food webs 3 , and increase stability 4 and persistence of recipient marine communities 5 . Allochthonous nutrients can transcend ecotones either passively, such as macroalgal detritus that washes up on coastlines 3 , 6 , 7 , or via active vectors including seabirds 1 , 8 . The ecological effects of these nutrient subsidies are particularly pronounced when the receiving ecosystem has low production 5 , 9 . A case in point is the Gulf of California islands where seabirds forage in highly productive marine waters and deposit guano around their roosting sites that enhance local productivity 1 and influence community structure in terrestrial desert ecosystems 4 , 10 . Nutrient enrichment from seabird colonies can also increase marine production via sea\u2013land\u2013sea transfer. For example, ornithogenic nutrients increased macroalgal production 11 and altered benthic community structure of a temperate intertidal rocky reef community 12 . In tropical ecosystems, seabird nutrients can enrich nitrogen inputs to soil on islands 8 , 13 and increase nutrient availability in adjacent pelagic 14 and benthic food webs 15 . Seabird-derived nutrients have been traced into coral holobionts 16 , however the ecological effects of these nutrients on reef-building (scleractinian) corals have not been demonstrated previously. This study assessed the influence of seabird nutrient subsidies on coral growth rates using a spatial gradient sampling scheme and a reciprocal transplant experiment. Coral reefs are among the most productive ecosystems yet occur in oligotrophic waters 17 . This paradigm is due largely to the tight coupling in nutrient cycling between the coral host and endosymbionts (commonly referred to as zooxanthellae), whereby inorganic nutrients excreted by the coral animal are assimilated by the symbiotic dinoflagellates of the family Symbiodiniaceae 18 to support photosynthesis 19 . In turn, the zooxanthellae translocate organic compounds to the coral animal to support metabolic demands 19 . Within the coral holobiont, endosymbionts can acquire inorganic nutrients from their host\u2019s waste metabolites or from surrounding seawater 20 . At a community level, the mutualistic association between the branching coral Sylophora pistillata and the coral obligate damselfish Dascyllus marginatus results in significantly higher growth rates of corals with resident damselfish due to nutrient subsidies from the fish waste 21 . Thus, external nutrients that elevate local nitrogen conditions in waters surrounding corals can increase zooxanthellae density, enhancing photosynthesis and coral growth rates 22 , 23 . However, there are environmental constraints and energetic costs associated with the maintenance of the mutualistic association between corals and endosymbionts with some studies showing that excessive nutrients can act like a stressor and cause a breakdown in the coral-algal symbiosis 24 . Elevated nutrient concentrations to coral reefs today are typically associated with anthropogenic sources including human sewage 25 , 26 , 27 , 28 and agricultural fertilizer 29 , 30 , where their effects are often considered detrimental to the coral reef ecosystem 31 . By contrast, nutrient subsidies from natural nutrient sources such as bird guano are principally excreted in an organic form of nitrogen 32 that undergoes speciation into various forms of nitrogen 33 and it remains to be shown whether it acts as a natural analogue to anthropogenic nutrient inputs. Nutrients generally increase cell densities of endosymbionts 22 , however the biochemical effect of this on corals is conflicting. Some studies show an increase in photosynthetic performance 34 and calcification 35 , 36 with increased nutritional supply. Conversely, other studies show a decrease in autotrophy caused by a chemical imbalance in the zooxanthellae 37 and a build-up of reactive oxygen species 38 , 39 which affects the stress tolerance of corals 40 . The relationship between nutrient availability and coral growth and photobiology is context-dependent, with exogenous factors like nutrient source likely a key determinant of the direction of the response at an individual coral level 41 . At the community level, excess nutrients can alter coral reproduction 42 and lead to loss of coral diversity and percent cover 43 . It can stimulate macroalgal growth and give algae a competitive advantage over slower-growing reef-building corals that once established, can create changes in chemical conditions on the reef 44 , 45 that maintain the reef in a macroalgal dominated state 46 . However, most studies on nutrient impacts on corals have been conducted on reefs that are already in a degraded state 47 or subject to multiple stressors in addition to excess nutrient availability 48 , including habitat transformation 49 and overfishing 50 . The reduction in numbers of herbivorous fishes, even at low levels of subsistence fishing 51 , together with increased nutrient delivery has been shown to erode resilience of coral reefs and cause transitions from healthy coral-dominated reefs to degraded algal-dominated systems 52 . By contrast, there are few studies on the effects of nutrient subsidies to coral reefs in less-disturbed ecosystems 14 , 16 , 53 , and no studies that have investigated the effects of seabird nutrients on coral growth rates. This study assessed whether seabird-derived nutrients assimilated by corals enhances coral growth rates. To investigate the spatial influence of seabird nutrients on one of the dominant reef-building corals in the Pacific, in hospite colonies of Acropora formosa were sampled every 20 m (from 20 m to 200 m) perpendicular to shore from Namenalailai (hereafter Namena), a remote island with abundant nesting seabirds and a large marine protected area. The zooxanthellae were extracted from these coral samples and analyzed for cell density and natural abundance stable isotope ratios of nitrogen (\u03b4 15 N). Since ornithogenic nitrogen is enriched in 15 N over background levels of nutrients 8 , 15 , 54 , 55 , 56 , \u03b4 15 N provides a natural tracer that can be used to assess the influence of seabird-derived nutrients in corals 16 . We expected a decreasing trend in \u03b4 15 N values and zooxanthellae densities in corals with increasing distance from the island consistent with a decreasing influence of seabird nutrient subsidies. To test whether seabird nutrients enhance growth rates of coral, we used a reciprocal transplant experiment for one year with fragments of A. formosa between Namena and Cousteau, the closest practical site with a similar physical environment but without nesting seabirds (Fig. 1 ). Cousteau is located on the island of Vanua Levu, it is also a marine protected area and had a few colonies of A. formosa at ca. 150 m offshore that were suitable for the transplant experiment. We hypothesized that growth rates of corals near the seabird roosting island would be greater than conspecifics from reefs without seabird colonies due to elevated nutrient availability from seabird guano. Figure 1 Location of study sites. Left: The Fiji archipelago (insert) and the position of the northern division, Vanua Levu, where the study sites are located. Right: ( a ) Namena island, with abundant populations of breeding seabirds, and ( b ) Cousteau on Vanua Levu. The spatial transect sites are shown as circles and the reciprocal transplant sites as stars. Map of Fiji and Vanua Levu created using Geographic Information System ArcGIS v.10.2 and the satellite images were obtained from Google Earth v.7.3.2. Full size image Results Nutrient characteristics of the two sites Dissolved inorganic nitrogen (DIN) concentrations were significantly elevated in the waters of the nearshore coral reef at Namena with DIN concentrations up to 12.7 \u00b5M compared to 1.8 \u00b5M at Cousteau (Table 1 ). Concentrations of nitrate (Wilcoxon W = 20, p = 0.025) and ammonia (Wilcoxon W = 21, p = 0.031) were significantly elevated at Namena relative to Cousteau. There was temporal variation in nutrient concentrations, with extremely high nitrate concentrations (up to 11.5 \u00b5M) measured in April 2013 at Namena. Phosphate concentrations also tended to be higher in April 2013 at both sites, although this was not significant. Phosphate concentrations were not statistically different (p > 0.05) between the transplant sites. The N:P in seawater was higher at Namena, with a ratio between 14\u201333 compared to Cousteau at 3\u20135. Table 1 Nutrient concentrations (mean \u00b1 S.E.) in the water column at the seabird-influenced marine protected area (MPA) site, Namena, and another MPA without seabirds, Cousteau. Full size table Spatial gradient of endosymbiont parameters at Namena The \u03b4 15 N values of extracted zooxanthellae decreased significantly with distance from land at Namena (F 2,28 = 177.4, p < 0.001) with an R 2 of 0.86 (Fig. 2 ). The mean \u03b4 15 N value for endosymbionts decreased from 7.7\u2030 at 20 m to 3.1\u2030 at 200 m from the island. Similarly, symbiont density was greater in coral colonies closer to land and decreased significantly with distance from shore at Namena (F 2,28 = 6.639, p = 0.016), although the relationship was weak (Fig. 3 ). When considering all sampled corals growing naturally within 200 m of the seabird roosting site at Namena island, the average density of zooxanthellae cells in corals was 1.7 \u00d7 10 6 cells.cm \u22122 host tissue (n = 30). Figure 2 Spatial transect. The stable nitrogen isotope values, \u03b4 15 N, of extracted endosymbionts with distance from shore (in meters) on the leeward side of Namena island, Fiji. Values are mean \u00b1 1 S.E. (n = 9) for the three Acropora formosa colonies sampled at 20 m intervals along the three transect lines perpendicular to the shore. Each colony is a pooled and homogenized sample of 3\u20135 fragments. Dashed line represents the linear regression, R 2 = 0.86, p < 0.001. Full size image Figure 3 Spatial transect. Cell density of endosymbionts (x10 6 cells per cm \u22122 host tissue) with distance from shore (in meters) on the leeward side of Namena island, Fiji. Values are mean \u00b1 1 S.E. (n = 9) for the three Acropora formosa colonies at 20 m intervals along the three transect lines perpendicular to the shore. Each colony is a pooled and homogenized sample of 3\u20135 fragments. Dashed line represents the linear regression, R 2 = 0.22, p = 0.016. Full size image Reciprocal transplant experiment Coral growth rates (measured as skeletal linear extension) were significantly different between coral nubbins grown at Namena and Cousteau (F 3,68 = 210.6, p < 0.001), with fragments maintained at Namena exhibiting up to four times greater linear extension rates than conspecifics transplanted to Cousteau (Figs 4 , 5 ). Tukey\u2019s post-hoc tests showed that the corals from Namena that were maintained at their natal site (N\u2013N) achieved significantly higher growth rates (mean 15.29 \u00b1 0.35 cm.y \u22121 ) than other nubbins. The next highest growth rates (mean 12.79 \u00b1 0.33 cm.y \u22121 ) were fragments from Cousteau that were transplanted to Namena (C\u2013N) for one year. By contrast, fragments outplanted at Cousteau that were collected from Cousteau (C\u2013C: mean 5.08 \u00b1 0.27 cm.y \u22121 ) or Namena (N\u2013C mean 3.75 \u00b1 0.20 cm.y \u22121 ) had significantly lower growth rates. There was no mortality during this experiment. Figure 4 Transplant experiment. Coral growth (linear extension in cm.y \u22121 ) of Acropora formosa fragments from the one-year reciprocal transplant experiment between Cousteau (C) and Namena (N), with the median and interquartile range shown in box-and-whisker plots. Treatments (left to right): C\u2013C = fragments from Cousteau and retained at their natal site; N\u2013C = fragments from Namena and transplanted to Cousteau; N\u2013N = fragments from Namena and retained at their natal site; C\u2013N fragments from Cousteau and transplanted to Namena. Significantly different treatments according to Tukey\u2019s Post Hoc tests are denoted by letters. Full size image Figure 5 Transplant experiment. ( a ) Individually-labelled fragments of Acropora formosa grown at Namena when the nubbins were created in January 2012, and ( b ) one year later in January 2013. ( c ) Examples of A. formosa fragments that originate from Namena and were transplanted to Cousteau (N\u2013C: three nubbins on left) or retained at Namena (N\u2013N: three fragments on right) after one year. Photographs: C. Savage. Full size image The water temperature averaged 28.1 \u00b0C at Cousteau (range: 23.3\u201330.7 \u00b0C) and 27.8 \u00b0C at Namena (range: 23.2\u201332.01 \u00b0C) during the four months for which data were reliably recorded with loggers. There was no significant difference in the average monthly temperature between the transplant sites (Welch Two-Sample t-test, t 1,6 = 0.482, p = 0.647). The average light environment at Namena tended to have slightly higher incident light (average PAR: 613 \u00b5mol photons m \u22122 s \u22121 ) compared to Cousteau (average PAR: 568 \u00b5mol photons m \u22122 s \u22121 ), however this was not significantly different (Welch Two-Sample t-test, t 1,6 = \u22120.220, p = 0.834). Discussion This is the first study to demonstrate a positive effect of seabird nutrient subsidies for corals, with significantly greater growth rates of a dominant branching coral near a seabird island. Elevated nutrients delivered to nearshore coral reefs adjacent to a breeding colony of seabirds provided a bottom-up nutrient subsidy that was assimilated by endosymbionts, as reflected by decreasing \u03b4 15 N values of zooxanthellae with distance from shore. Acropora formosa colonies growing in proximity to this elevated nutrient source and fragments transplanted from distant reefs to the area exhibited growth rates four times greater than conspecifics grown at the same depth on a coral reef without seabird-derived nutrients. Therefore, in contrast to excess anthropogenic nutrients, seabird guano can benefit coral reefs, which should be considered in catchment\u2013to\u2013reef management, particularly given the worldwide threat to seabirds. Seabird nutrients elevate nitrogen availability Nutrients were significantly elevated in seawater bathing the fringing reefs on the leeward side of Namena island, Fiji, where seabirds including 1000\u20133000 breeding pairs of red-footed boobies ( Sula sula ) roost year round 57 . The gradient of decreasing \u03b4 15 N values in extracted endosymbionts with distance from shore indicated that the elevated nitrogen source was most likely ornithogenic, since there are no rivers or point sources of nutrients on the island and seabird guano is enriched in 15 N relative to background nitrate \u03b4 15 N values 8 , 15 , 54 , 56 , 58 . Guano \u03b4 15 N values are >10\u2030 for seabirds 59 , with red-footed booby guano reported as 11\u2030 16 and decaying guano on another Fijian seabird island having \u03b4 15 N values as high as 50\u2030 8 . Local nitrate enrichment and elevated \u03b4 15 N values in corals have been linked with nesting seabirds, where ornithogenic nutrients can contribute 15\u201350% of the nitrogen requirements of the coral Pocillopora damicornis 16 . Thus, the findings of this study are consistent with seabird nutrients elevating nitrogen availability on local reefs and being assimilated by the endosymbionts. However previous studies have not assessed whether an ornithogenic nutrient subsidy has a direct effect on coral growth. This study shows that reef-building corals grown near a large seabird colony exhibited growth rates up to four times greater than conspecifics from the same area that were transplanted distant from seabird nutrients. Linear extension rates of 15 cm.y \u22121 at Namena are amongst the highest rates reported in the literature for comparable growth experiments of Acropora fragments 60 , 61 , 62 . The light conditions were above saturation levels for corals 63 and since there were no significant differences in the light or temperature conditions between the transplant sites and wave energy was similar with both sites north-facing and sheltered from the prevailing south-east trade winds, it suggests that inter-site differences in growth were mainly driven by the different nutrient conditions. Seabird guano elevates dissolved organic nitrogen 32 , inorganic nitrogen 15 , 16 and phosphate concentrations in seawater 15 , 58 . In this study, there was no significant difference in measured phosphate concentrations between Namena and Cousteau, despite differences in seabird populations. Phosphate fluxes may have been higher from seabird guano, however if this is assimilated readily by benthic organisms it would not show in the water column concentrations. Nevertheless, phosphate concentrations were elevated and not limiting at both sites 64 , 65 , suggesting that the endosymbionts were replete in phosphorus to support coral growth and metabolism 66 . Nitrogen concentrations in the water column, in comparison, were significantly different between sites with ammonia and nitrate significantly elevated at the seabird site (Namena) compared to Cousteau and other coastal sites in Fiji 64 . It should be cautioned that nutrients are temporally variable and this study reports on only two sampling occasions, however nitrate concentrations were significantly elevated for both sampling intervals at the seabird site. Large bird populations on small islands can result in extremely high nitrate concentrations in groundwater, which is advected into adjacent coastal lagoons 33 . In this study, despite nitrate concentrations above thresholds considered harmful to corals 64 , 65 , the A. formosa fragments growing near the seabird nesting island remained healthy during the experiment, grew vigorously and had endosymbiont cell densities considered optimal 67 for branching corals 68 , 69 , 70 to maintain photosynthetic performance 71 . The findings provide an interesting perspective on the contested issue of whether excess nutrients are harmful or beneficial to coral reefs 31 . The findings in this study suggest that natural sources of nutrient enrichment to the coast like seabird guano can have positive effects on acroporid corals in contrast to anthropogenic nutrient sources 41 . Guano nutrient subsidies have increased production of mangroves 72 and seagrass 73 and the current study shows that ornithogenic nutrients result in a nutrient-replete environment that can enhance coral production. The composition of seabird guano contains essential nutrients (nitrogen, phosphorus), including trace elements 58 and iron 74 , in sufficient amounts that biochemical functions remain stable 37 . Changes in nutrient stoichiometry can affect carbon acquisition and nutrient partitioning in the coral holobiont 37 , 75 . The seawater near the seabird colony had N:P which approximated Redfield ratio 76 in contrast to the site distant from seabirds. Thus the stoichiometric balance and nutrient source 41 is also important to consider along with input rates in determining the effect of nutrients on coral performance and production. An important caveat is that the reef where the study was done is located within a no-take marine protected area with abundant fish populations 77 , 78 . There are numerous studies that have documented phase shifts in benthic community composition from scleractinian corals to a degraded macroalgal-dominated state 46 , 79 following nutrient enrichment 26 , 80 , particularly with declines in herbivorous fishes 81 , 82 , 83 . In this study, the elevated nutrients from the seabirds didn\u2019t promote nuisance macroalgal blooms 84 despite the highly elevated DIN concentrations, most likely due to the presence of healthy fish populations, which would have maintained critical ecosystem functions like grazing and bioerosion 53 that prevents establishment of macroalgae. Conservation and management implications Marine conservation tends to focus on connectivity among reefs within a seascape to inform management decisions including where to locate marine protected areas 85 . However, catchment-to-reef connectivity can also be important to consider in marine management and conservation 86 , 87 , not only for taking into account the negative consequences from land, for example increased sediment inputs 29 , but also for positive gains when coral reefs are adjacent to pristine forested landscapes 14 . As shown in this study and other recent papers 14 , 15 , 16 , 53 , seabirds can provide important nutrient subsidies to the adjacent coast where seabird roosting sites are adjacent to coral reefs. Given that nearly one-third of seabird species are at risk of extinction globally 88 , conservation needs to consider possible effects of declines in this nutrient subsidy on coral growth around pristine remote atolls and reefs. To this end, Namena may provide an ideal before-and-after study system to investigate the effects of a decline or loss of guano for the adjacent coastal ecosystem as the island experienced severe destruction from hurricane Winston in February 2016 after this study was conducted and most seabird roosting sites were destroyed (pers. obs.). Apart from the direct effects of storm damage to the fringing coral reefs, investigating the indirect effects of a severe reduction in ornithogenic nutrients would advance our understanding of the role of allochthonous nutrient subsidies on productivity and recovery following disturbance. Methods Study site Namena is a ~0.5 km 2 island within the Kubulau District in northern Fiji that provides a model ecosystem to investigate the role of ornithogenic nutrient subsidies on coral growth without the confounding effects of other human stressors. Namena Marine Reserve is the largest (60.6 km 2 ) and oldest (established 1997) no-take marine protected area in Fiji 78 with high coral cover and abundant fish populations including healthy populations of top predators 77 . Namena\u2019s marine reserve is strictly no-take and compliance is self enforced by the local communities 78 . The island has an intact coastal forest with abundant populations of roosting seabirds, including an estimated 1000\u20133000 breeding pairs of red-footed boobies, Sula sula (population estimate: 1986\u20132008) 57 . The closest practical site without nesting seabirds for the transplant experiment is adjacent to the Cousteau resort on the island of Vanua Levu. While Cousteau had lower live coral cover than Namena that prevented comparative sampling along a spatial gradient every 20 m from shore, the focal species A. formosa was found ca. 150 m offshore, which enabled fragmentation to create transplant nubbins. Cousteau is a no-fishing marine protected area since 2000 and was extended in area in 2005. The physical environment is similar between Cousteau and Namena with comparable depth where the transplant corals were collected, water temperature and wave energy were similar and both sites were north-facing, thus protected from the prevailing south-east trade winds. Spatial transect sampling At Namena, samples of Acropora formosa colonies were collected for analyses of zooxanthellae density and nitrogen isotope ratios (\u03b4 15 N) in January 2012. Transect lines were conducted perpendicular to the shore and sampling done at 20 m intervals between 20 m from land to 200 m seaward. Fragments (ca. 5 cm) of A. formosa were collected from attached colonies by snorkeling along the transect line and collecting fragments at a depth of approximately 3 m. If A. formosa colonies were not available on the transect line, another colony within a 1.5 m radius of the transect line at the same distance from shore was sampled. Three transects were taken perpendicular to land approximately 50 m apart, and at each 20 m increment three separate A. formosa colonies were sampled by collecting between 3\u20135 fragments per colony (depending on availability). These fragments collected from a single colony were pooled and homogenized to get an averaged \u03b4 15 N value and zooxanthellae count per colony. The coral samples were immediately frozen and processed individually in the laboratory for endosymbiont density and stable isotope ratios. In total, nine samples were analyzed at each 20 m distance. These samples were collected with an approved permit (Fiji Immigration Research Permit 3273\/11). Transplant experiment A reciprocal transplant experiment was conducted between Namena and Cousteau. Coral fragments of A. formosa were created from visually healthy colonies at Namena and Cousteau between 08\u201316 January 2012 using established procedures 89 . The initial sizes of the fragments were comparable at the two sites, ranging between 3 cm and 10 cm, with fragment size determined by the size and shape of the colony from where they were collected. The nubbins were placed on individual, labeled (Hallprint\u00ae) concrete blocks using underwater epoxy and measured using calipers. They were left in aerated tanks under shade cloth for ca. 2 hours to establish on the bases before being planted out in situ . A total of 36 coral fragments were created at each site with half (n = 18) being retained at the natal site and half (n = 18) transplanted to the other site. Samples were transported in large containers with site seawater, under shade cloth and using battery-operated air bubblers to minimize stress during the 1-hour boat transport time between sites. At each site, the coral fragments were placed on a customized array at 3 m depth and elevated 50-cm off the seabed. The arrays were located ca. 150 m from land at both sites, on the leeward side of Namena Island, Fiji (17\u00b06\u203226.66\u2033S, 179\u00b06\u20326.21\u2033E) and at Cousteau (16\u00b048\u203243.57\u2033S, 179\u00b017\u203211.59\u2033E). These sites were chosen to be sufficiently close to land to be influenced by land-derived nutrient sources but deep enough to prevent wave damage or interference from snorkelers. The coral fragments were left to grow for 12 months, after which time the individual fragments were collected and measured to quantify growth using calipers. Growth was recorded as skeletal linear extension, including growth of side branches as well as the main axial branch of each fragment 60 . Samples of water column nutrient concentrations were collected mid-water (~2 m) above the transplant arrays at both transplant sites. The nutrient samples were taken in December 2011, 3 weeks before the spatial transect sampling at Namena and the initiation of the transplant experiment, and again in April 2013, after the reciprocal transplant experiment. The seawater samples were taken in acid-washed vials and immediately filtered through pre-combusted Whatman 0.45 \u03bcm GFF filters and stored on ice until frozen (within 2 h) at \u221220 \u00b0C. Samples were analyzed within 2 months of collection for dissolved inorganic ammonia (NH 4 + ), nitrite\/nitrate (NO 2 \u2212 \/NO 3 \u2212 ), and phosphorus (PO 4 2+ ) concentrations on a Lachat QuikChem 8500 series 2 Flow Injection Analysis autoanalyser. HOBO \u00ae pendant temperature\/light 64k data loggers (Onset) were deployed at the two transplant sites to measure the temperature and light environment at each of the arrays. Two loggers were attached on diagonally opposite corners of each array at the height of the coral fragments and set to log at 10-minute intervals. The HOBO light loggers record in Lux and were therefore calibrated by simultaneous recording underwater using a cosine corrected LI-COR \u00ae underwater sensor (LI-192 underwater quantum sensor coupled with a LI-250A light meter, LI-COR) and the data reported in PAR (\u00b5mol photons m \u22122 s \u22121 ) using a correction following established methods 90 . Laboratory analyses The zooxanthellae were extracted from the coral fragments using a waterpik 91 and 0.2 \u00b5m filtered site seawater. Zooxanthellae were separated from animal tissues using four centrifugation steps (2700 g for 10 min). The pellet containing the zooxanthellae was resuspended in 10 mL of 0.2 \u00b5m filtered sterile seawater and a known volume filtered onto pre-combusted GF\/F 0.45 \u00b5m filters and dried for stable isotope analyses. The filters were analyzed for nitrogen stable isotope ratios at Isotrace, Department of Chemistry, University of Otago, on a Europa Hydra mass spectrometer coupled to a Carlo Erba NC 2500 elemental analyser. The isotope ratios are reported in the delta notation: $${{\\rm{\\delta }}}^{{\\rm{15}}}{\\rm{N}}=[({{\\rm{R}}}_{{\\rm{sample}}}\/{{\\rm{R}}}_{{\\rm{standard}}})\\,-{\\rm{1}}]\\times {\\rm{1000}}$$ where R refers to the ratio 15 N: 14 N and all values are reported in the units, per mil (\u2030). Raw isotope ratios are normalized by three-point calibration to the international scales using two IAEA (International Atomic Energy Agency) reference materials (USGS-40 and USGS-41) and a laboratory standard (EDTA-OAS, Elemental Microanalysis Ltd, UK). EDTA-OAS has multi-year and multi-laboratory calibration records against IAEA reference materials and is used as a drift control material by assaying a pair of aliquots after every twelve samples of a batch. Precision for \u03b4 15 N is \u00b1 0.2\u2030. A second aliquot of the resuspended pellet was used to determine cell density. The cell density of endosymbionts was counted using a Scepter 2.0 handheld automated cell counter (Millipore) with a 40 \u00b5m sensor after diluting the extracted zooxanthellae samples 2:1 in phosphate-buffered saline (PBS) and checking for accuracy on select samples using a haemocytometer. The surface area of the coral fragment used was measured according to the paraffin wax dipping technique 92 , 93 and the symbiont density normalized as cells.cm \u22122 host tissue. Statistical analyses The isotope (\u03b4 15 N) and zooxanthellae density (cells.cm \u22122 ) data for the three replicate colonies along each transect line with distance from shore were averaged and analyzed using Generalised Linear Models (GLM) with distance from land a fixed factor and the measured symbiont parameters analyzed as continuous predictor variables. Growth of the coral fragments from the reciprocal transplant experiment were compared after testing for normality and homoscedasticity of variances using a one-way analysis of variance (ANOVA) and Tukey\u2019s Post Hoc tests. To test for differences in nutrient concentrations between the Namena and Cousteau transplant sites the nutrient concentrations (ammonia, nitrate, phosphate) were compared using a nonparametric Wilcoxon Signed-rank test, since the nutrients were collected at two time points and the data violated the assumptions of normality even after log-transformation. The temperature and light logger data from Namena and Cousteau were combined into monthly measurements. Since the two replicate loggers at each site were not significantly different (p > 0.05), these data were averaged for Namena and Cousteau sites, respectively. When the data were downloaded, the light readings were not reliable after four months due to biofouling, hence the data were filtered to the first four months of reliable data. The monthly average temperature and light conditions at Namena were compared to Cousteau using Welch\u2019s t tests following Shapiro Wilks tests for normal distribution of the data. Two measures of light conditions were analyzed: the total incident light and average light conditions monthly at each site. All statistical analyses were conducted using R Studio v3.0.1 94 . The datasets generated during and\/or analyzed during the current study are available from the corresponding author on reasonable request. ","News_Body":"A University of Otago study has shown the positive impact bird poo, or guano, has on coral growth in tropical seas. Published online in the respected scientific journal Scientific Reports, the study Seabird nutrients are assimilated by corals and enhance coral growth rates demonstrates that seabird nutrients can significantly boost coral growth rates, offering a positive news story in a decade that has documented dramatic declines in reef health and percentage cover. \"The findings have important implications for catchment-to-reef connectivity and demonstrate that coral conservation should also consider catchment management in addition to marine protection,\" says author Dr. Candida Savage, of Otago's Department of Marine Science. The research was conducted in two Fiji marine protected areas; one remote island (Namena) with an intact coastal forest with breeding seabirds, the other (Cousteau) is away from any seabirds and their associated guano. Natural chemical tracers in coral tissues showed that corals growing near the roosting seabirds took up seabird nutrients. A one-year growth experiment demonstrated that corals grew up to four times faster at the Namena reef compared to the Cousteau reef due to the presence of seabirds. \"Bird guano is known for its qualities as a fertiliser, however the impact it had on coral growth has been unknown until now. I was astounded at how much of a difference the presence of guano had in promoting coral growth,\" Dr. Savage says. The research shows that natural sources of nutrients like seabird guano may benefit coral reefs, in contrast to man-made nutrients from land that tend to degrade coral reefs. Comparison of staghorn corals grown for one year without the influence of seabird guano (three corals on left) with corals grown near a seabird colony (three corals on right). Credit: Dr Candida Savage Coral reefs face multiple global and local threats including excess nutrient runoff from land. Over the last decade, the percent of threatened reefs has increased by 30 per cent, with nearly 75 per cent of the world's reefs threatened today. Coral reefs are crucially important for biodiversity and people. Despite covering less than one per cent of the earth's surface, coral reefs are home to one-quarter of all marine fish species and countless invertebrates. Data obtained on the reefresilience website illustrates the importance of coral reefs for humans. At least five hundred million people rely on coral reefs for food, coastal protection, and livelihoods. In developing countries, coral reefs contribute about one-quarter of the total fish catch, providing food to an estimated one billion people in Asia alone. They form natural barriers that protect nearby shorelines from the eroding forces of the sea, thereby protecting coastal dwellings, agricultural land and beaches. Corals growing underwater at a site with roosting seabirds grew up to four times faster than corals grown distant from seabirds. Credit: Dr Candida Savage \"Given that nearly one-third of seabird species are at risk of extinction globally and now that we know how beneficial seabird subsidies are for coral growth, we should consider catchment-to-reef management to protect our marine ecosystems. This could be in the form of protection of established seabird nesting grounds or promoting new seabird habitats by enhancing natural vegetation on land alongside protecting marine areas. If the birds are there, the benefits of their droppings will be too,\" Dr. Savage says. ","News_Title":"Study proves importance of bird poo in enhancing coral growth","Topic":"Biology"}
{"Paper_Body":"Abstract Intramolecular motions in proteins are one of the important factors that determine their biological activity and interactions with molecules of biological importance. Magnetic relaxation of 15 N amide nuclei allows one to monitor motions of protein backbone over a wide range of time scales. 15 N{ 1 H} nuclear Overhauser effect is essential for the identification of fast backbone motions in proteins. Therefore, exact measurements of NOE values and their accuracies are critical for determining the picosecond time scale of protein backbone. Measurement of dynamic NOE allows for the determination of NOE values and their probable errors defined by any sound criterion of nonlinear regression methods. The dynamic NOE measurements can be readily applied for non-deuterated or deuterated proteins in both HSQC and TROSY-type experiments. Comparison of the dynamic NOE method with commonly implied steady-state NOE is presented in measurements performed at three magnetic field strengths. It is also shown that improperly set NOE measurement cannot be restored with correction factors reported in the literature. Working on a manuscript? Avoid the common mistakes Introduction Since its first use of magnetic relaxation measurements of 15 N nuclei applied to the protein, the staphylococcal nuclease (Kay et al. 1989 ), this method has become indispensable in the determination of molecular motions in biopolymers (Jarymowycz and Stone 2006 ; Kempf and Loria 2003 ; Palmer, III 2004 ; Reddy and Rayney 2010 ; Stetz et al. 2019 ). The canonical triad of relaxation parameters\u2014longitudinal ( R 1 ) and transverse ( R 2 ) relaxation rates accompanied by the 15 N{ 1 H} nuclear Overhauser effect (NOE)\u2014have been most often used in studies investigating the mobility of backbone in proteins. It is a common opinion that 15 N{ 1 H} NOE is unique among the mentioned three relaxation parameters because it is regarded as essential for the accurate estimation of the spectral density function at high frequencies (\u03c9 H \u00b1 \u03c9 N ), and it is crucial for the identification of fast backbone motions. (Idiyatullin et al. 2001 ; Gong and Ishima 2007 ; Ferrage et al. 2009 ). The most common method for the determination of X{ 1 H} NOE is a steady-state approach. It requires measurements of the longitudinal polarization at the thermal equilibrium of spin X system, S 0 , and the steady-state longitudinal X polarization under 1 H irradiation, S sat (Noggle and Schirmer 1971 ). Note that the nuclear Overhauser effect , defined as \\(\\varepsilon = {{S_{sat} } \\mathord{\\left\/ {\\vphantom {{S_{sat} } {S_{0} }}} \\right. \\kern-\\nulldelimiterspace} {S_{0} }}\\) , should not be mistaken with nuclear Overhauser enhancement , \\(\\eta = {{\\left( {S_{sat} - S_{0} } \\right)} \\mathord{\\left\/ {\\vphantom {{\\left( {S_{sat} - S_{0} } \\right)} {S_{0} = \\varepsilon - 1}}} \\right. \\kern-\\nulldelimiterspace} {S_{0} = \\varepsilon - 1}}\\) (Harris et al. 1997 ). It has to be pointed out that NOE measurements appear to be very demanding and artifact prone observations. One of severe obstacles in these experiments is their ca . tenfold lower sensitivity in comparison to R 1 N and R 2 N which is due to the fact that the NOE experiments with 1 H detection start with the equilibrium 15 N magnetization rather than 1 H. The steady-state 15 N{ 1 H} NOEs (ssNOE) are normally determined as a ratio of cross-peak intensities in two experiments\u2014with and without saturation of H N resonances. Such arrangement creates problems with computing statistically validated assessment of experimental errors. 15 N{ 1 H} NOE pulse sequence requires a very careful design as well. Properly chosen recycle delays between subsequent scans and saturation time of H N protons have to take into account the time needed to reach the equilibrium or stationary values of 15 N and H N magnetizations (Harris and Newman 1976 ; Canet 1976 ; Renner et al. 2002 ). Exchange of H N protons with the bulk water combined with the long longitudinal relaxation time of water protons leads to prolonged recycle delay in the spectrum acquired without saturation of H N resonances. Unintentional irradiation of the water resonance suppresses H N and other exchangeable signals owing to the saturation transfer and many non-exchangeable 1 H resonances via direct or indirect NOE with water (Grzesiek and Bax 1993 ) while interference of DD\/CSA relaxation mechanisms of 15 N amide nuclei disturbs the steady-state 15 N polarization during 1 H irradiation (Ferrage et al. 2009 ). All aforementioned processes depend directly or indirectly on the longitudinal relaxation rates of amide 1 H and 15 N nuclei R 1 H and R 1 N as well as the longitudinal relaxation rate of water protons, R 1 W , and the exchange rate between water and amide protons, k . In this study, the dynamic NMR experiment (DNOE), a forgotten method of the NOE determination in proteins, was experimentally tested, and the results were compared with independently performed steady-state NOE measurements at several magnetic fields for widely studied, small, globular protein ubiquitin. Additionally, several difficulties inherent in 15 N{ 1 H} NOEs and methods for overcoming or minimizing these difficulties are cautiously discussed. Experimental The uniformly labeled U-[ 15 N] human ubiquitin was obtained from Cambridge Isotope Laboratories, Inc in lyophilized powder form and dissolved to 0.8 mM protein concentration in buffer containing 10 mM sodium phosphate at pH 6.6 and 0.01% ( m \/ v ) NaN 3 . DSS- d 6 of 0.1% ( m \/ v ) in 99.9% D 2 O was placed in a sealed capillary inserted into the 5 mm NMR tube. Amide resonance assignments of ubiquitin were taken from BioMagResBank (BMRB) using the accession code 6457 (Cornilescu et al. 1998 ). NMR experiments were performed on three Bruker Avance NEO spectrometers operating at 1 H frequencies of 700, 800 and 950 MHz equipped with cryogenic TCI probes. The temperature was controlled before and after each measurement with an ethylene glycol reference sample (Rainford et al. 1979 ) and was set to 25 \u00b0C. The temperature was stable with maximum detected deviation of \u00b1 0.3 \u00b0C. Chemical shifts in the 1 H NMR spectra were reported with respect to external DSS- d 6 while chemical shifts of the 15 N signals were referenced indirectly using frequency ratio of 0.101329118 (Wishart et al. 1995 ). The spectral widths were set to 12 ppm and 22 ppm for 1 H and 15 N, respectively. The number of complex data points collected for 1 H and 15 N dimensions 2048 and 200, respectively. In each experiment, 8 scans were accumulated per FID. Double zero filling and a 90\u00b0-shifted squared sine-bell filter were applied prior to Fourier transformation. Data were processed using the program nmrPipe (Delaglio et al. 1995 ) and analyzed with the program SPARKY (Goddard and Kneller). Resonance intensities were used in calculating relaxation times and NOE values obtained from a nonlinear least-squares analysis performed using Fortran routines written in-house, based on the Newton\u2013Raphson algorithm (Press et al. 2007 ). The pulse programs used in this work were based on the HSQC-type R 1 ( 15 N) and 15 N{ 1 H} NOE experiments (Lakomek et al. 2012 ). The carrier frequency during 1 H saturation with 22 ms spaced 180\u00b0 hard pulses on 1 H was moved from water frequency to the centre of amide region (8.5 ppm). Evolution times in R 1 ( 15 N) and dynamic NOE experiments were collected in random order. Reproducibility of experiments was excellent. Therefore, the interleaved mode was not used since it could introduce instabilities of water magnetization (Renner et al. 2002 ). The list of delays applied in the experiments used in this work is given in Table S3. Results and discussion Dynamic NOE measurement\u2014introduction It can be concluded from the Solomon equations (Solomon 1955 ) that in the heteronuclear spin system X\u2013H, the heteronuclear Overhauser effect is built up with the rate R 1 (X) under the condition of proton saturation as shown for the 13 C- 1 H spin system (Kuhlmann et al. 1970 ; Kuhlmann and Grant 1971 ). As a consequence of this observation a dynamic NOE was employed for the simultaneous determination of R 1 ( 13 C) and 13 C{ 1 H} NOE using Eq. ( 1 ) $$S(t) = S_{0} [\\varepsilon + (1 - \\varepsilon )\\exp ( - R_{1} t)]$$ (1) Measurements of time dependent changes of signal intensities S ( t ) allow for the determination of \u03b5 , R 1 , and their probable errors, as defined by any standard criterion of nonlinear regression methods. The DNOE can be especially beneficial in studying nuclei with negative magnetogyric ratios since in unfavorable circumstances, nulling of the resonance in a proton saturated spectrum can occur. Therefore, the DNOE has been successfully used in relaxation studies of 29 Si (Kimber and Harris 1974 ; Ejchart et al. 1992 ) and 15 N (Levy et al. 1976 ) nuclei in organic molecules. The 15 N-DNOE has been also investigated in small protein (Zhukov and Ejchart 1999 ). This approach can be especially profitable in studies of medium to large size proteins displaying highly dynamic fragments. Time schedule of NOE measurement Both nitrogen polarizations, S sat and S 0 , depend on a number of physical processes in the vicinity of amide nitrogen nuclei. Dipolar interaction between 15 N and 1 H N brings about the nuclear Overhauser effect. Additional processes as chemical shift anisotropy relaxation mechanism of 15 N and its interference with 15 N\/ 1 H N dipolar interaction, direct NOE and saturation transfer from water to 1 H N protons due to chemical exchange influence both nitrogen polarizations, especially if the pulse sequence itself will result in non equilibrium state of water protons. Presaturation of the water resonance resulting in partial saturation of water magnetization attenuates 1 H N signal intensities mostly through the chemical exchange or through homonuclear NOE with water protons. (Grzesiek and Bax 1993 ; Lakomek et al. 2012 ). Therefore, evolution of the spin system towards S sat or S 0 nitrogen polarizations depends on the rates of the processes mentioned above, the longitudinal relaxation rates of 15 N, 1 H N , and water protons, R 1 N , R 1 H , and R 1 W , and the chemical exchange rate, k , between amide and water protons. These rates strongly determine the time schedule of NOE measurements, which is schematically shown in Fig. 1 . Hence, their knowledge is a prerequisite for choice of optimal delays. The numerical data of R 1 H and R 1 W for the sample studied here are given in Table 1 . Nevertheless, one should be aware that the R 1 W depends on temperature, pH, and protein concentration. Residue specific R 1 N values for the ubiquitin sample will be discussed further. Fig. 1 Steady-state NOE measurement is composed of two sequences: NOE and noNOE with saturated and unperturbed H N protons, respectively. Dynamic NOE measurement comprises several NOE type sequences with a set of different D sat values Full size image Table 1 Longitudinal relaxation rates of water protons R 1 W and averaged rates of amide protons R 1 H for ubiquitin sample at 25 \u00b0C Full size table In the noNOE reference measurement, 15 N nuclei have to reach the thermal equilibrium at the end of delay RD 1 . During the block denoted as measurement in Fig. 1 , the pulse sequence resulting in the 2D 15 N\/ 1 H spectrum with the desired cross peak intensities is executed. At the start of acquisition, several coupled relaxation processes take place, resulting in multi-exponential decay of 15 N, 1 H N , and water protons (Ferrage et al. 2008 ). Keeping in mind that R 1 W is much smaller than the rates of other processes, it can be reasonably assumed that R 1 W rate mainly defines RD 1 . Fulfillment of the condition $$\\exp ( - RD_{1} \\cdot R_{1W} ) < 0.02$$ (2) where factor 0.02 has been chosen to some extent arbitrarily, should properly determine RD 1 values in most of the cases. Still one has to be aware that the smallest decay rate resulting from the exact solution of full relaxation matrix can be smaller than R 1 W . In NOE measurement, the buildup of 15 N magnetization takes place with the rate R 1 N . 15 N relaxation rates can be, however, broadly dispersed if mobility of N\u2013H vectors in a studied molecule differ significantly. Therefore, to meet the condition $$\\exp ( - D_{sat} \\cdot R_{1N} ) < 0.02$$ (3) a compromise may be required (c.f. Table S1). Experiments of steady-state and dynamic NOE measurements differ in the RD 2 setting. In the case of steady-state NOE, the value RD 2 = 0 is adequate. Even if the nitrogen polarization displays a nonzero value at the beginning of the D sat period, it will still have enough time to reach the steady-state condition. In dynamic NOE, however, the nitrogen polarization has to start from closely controlled thermal equilibrium. Therefore, condition (2) with RD 1 replaced with RD 2 has to be fulfilled. The description ( RD 1 \u2013 RD 2 \u2013 D sat )\/ B 0 will be further adopted to characterize particular NOE experiments used in this work. Analysis of systematic errors resulting from an incorrect delay setting in NOE values, \u03b5 = S sat \/ S 0 , for nuclei with \u03b3 < 0 should take into account that these errors can be caused by false S 0 values and\/or S sat values. The apparent S 0, app value in not fully relaxed spectrum is always smaller than the S 0 of true equilibrium value. On the other hand, the non-equilibrium apparent S sat,app value is always larger than the S sat , equilibrium value, i.e. more positive for \u03b5 > 0 or less negative for \u03b5 < 0. The joint effect of erroneous S sat and S 0 , however, does not always result in the relation \u03b5 app > \u03b5 as could be hastily concluded. An attenuated S 0 value in conjunction with properly determined, negative S sat results in \u03b5 app < \u03b5, and this is experimentally confirmed by \u03b5 values observed for the C-terminal, mobile residue G76. Its values obtained in the measurements free of systematic errors (10-10-8)\/16.4 T and (10-10-5)\/22.3 T are equal to \u2212 0.812 and \u2212 0.246, respectively. Herein, both, S 0 and S sat values are expected to be error free. In the measurements (10-10-4)\/16.4 T and (10-10-1.3)\/22.3 T with proper S 0 value and S sat,app > S sat owing to too short D sat , \u03b5 app are equal to \u2212 0.738 and 0.162, respectively, while in (3-0-3)\/22.3 T with too short RD 1 and D sat delays, S 0, app < S 0 and \u03b5 app = \u2212 0.379 (cf. Figure 8 ). Such misleading behavior could be expected for mobile residues in flexible loops, unstructured termini, or intrinsically disordered proteins. Setup and data processing of DNOE measurement Relation between signal intensities and evolution times in a dynamic NOE experiment, D sat , depend on three parameters: nuclear Overhauser effect, \u03b5 , nitrogen longitudinal relaxation rate, R 1 N , and signal intensity at the thermal equilibrium, S 0 (Eq. 1 ). Provided that the longitudinal relaxation rates have been previously obtained in a separate experiment, their values can be entered in Eq. 1 , reducing the number of determined parameters in a computational task further denoted as a sequential one. Influence of the propagation of R 1 N errors on the \u03b5 values is usually negligible; variation of R 1 N values within the range \u00b1 \u03c3 (standard deviation) typically results in d\u03b5 changes smaller than 10 \u20135 except for residues exhibiting \u03b5 < 0.4 (Figs. S1, S2). In ubiquitin, such residues are located at the C-terminus. This behavior is attributed to the stronger correlation between \u03b5 and R 1 N parameters owing to the increased range of signal intensities for smaller \u03b5 values (Fig. 2 ). Another possibility of data processing, simultaneous use of dynamic NOE and relaxation rate data in one computational task, brings about results ( \u03b5 and d\u03b5 values) practically identical to those obtained in the sequential task. Fig. 2 Experimental data obtained in DNOE measurement at 16.4 T for D58 residue (brown circles), R74 (orange triangles), and G76 (light green squares). NOE values determined in the sequential task are: (D58) = 0.805, \u03b5 (R74) = 0.186, and \u03b5 (G76) = \u2212 0.813. Color-coded lines correspond to the nonlinear least-square fit of the Eq. ( 1 ) to the experimental data. Correlations between \u03b5 and R 1 , c ( \u03b5 , R 1 ), in the simultaneous task are: c (D58) = \u2212 0.003, c (R74) = 0.013, and c (G76) = 0.099. The larger range of intensities results in larger correlation c ( \u03b5 , R 1 ) between fitted parameters Full size image The dynamic NOE data can also be used without support from separate R 1 N data. Such data processing delivers the \u03b5 values and their errors close to those resulted from the sequential or simultaneous approach (Figs. S3, S5). On the other hand, derived R 1 relaxation rates are less accurate with errors an order of magnitude larger than those obtained in the dedicated R 1 experiment (Figs. S4, S6). Therefore, a dynamic NOE measurement cannot be regarded as a complete equivalence of a separate R 1 experiment. Numerical data for three different data processing methods of dynamic NOE at 22.3 T are given in the Table S2, and a comparison of the discussed numerical methods are presented in Table 2 using data acquired for ubiquitin at 16.4 and 22.3 T. The pairwise root-mean-square deviations (RMSD) for \u03b5 values are extremely small in all cases, while those for R 1 values are larger. Their values, together with average standard deviations, are given in Table 3 . Recently, an experimentally demanding TROSY-based pulse sequence dedicated to deuterated proteins has been invented for simultaneous measurement of R 1N relaxation rates and \u03b5 values. The accuracy of the proposed technique has been verified by comparison to the results of both relaxation parameters measured conventionally (O'Brien and Palmer III 2018 ). Table 2 Values of standard error ratios averaged over 70 amino acid residues of ubiquitin available from our experiments Full size table Table 3 The pairwise RMSDs and the mean values of standard deviations determined for three data reduction methods for dynamic NOE experiments Full size table Dynamic NOE measurements, as with relaxation rate experiments, require optimization of a number and length of saturation periods, D sat . One important assumption in the selection of D sat values is to sample a broad range of intensities I ( t ) ~ S ( t ) in a uniform manner. The shortest D sat equal to zero delivers I 0 ~ S 0 . The longest D sat should be as close to a value fulfilling the condition (2) as is practically feasible (c.f. Table S1). These assumptions were checked on the DNOE measurement comprising 11 delays. Next the number of delays was reduced to seven and then to 4 selected delays, and results were compared. Apparent NOE values and their standard deviations changed only slightly. Residue specific differences in \u03b5 values between the full experiment and each of the reduced ones were smaller than appropriate d\u03b5 values. They are compared in Fig. 3 , and the presented data assure that four correctly chosen D sat values do not deteriorate \u03b5 values and their accuracies. This conclusion allows us to state that DNOE measurement can require an acceptable amount of spectrometer time. Fig. 3 Residue specific differences with error bars between DNOE measurement at 22.3 T comprising 11 D sat values and curtailed DNOE measurements composed of four or seven D sat values (upper part and lower part, respectively). Horizontal, dashed lines represent averages of \u0394 \u03b5 values given in plots. Full set of D sat values [0.0, 0.11, 0.22, 0.35, 0.55, 0.66, 0.79, 1.10, 1.30, 3.00, 4.00]. Four values: 0.22, 0.66, 1.10, and 3.00 were rejected to get seven D sat value measurement. Further rejection of 0.11, 0.55, and 1.30 D sat values resulted in four-value set Full size image Error determination of NOE measurements The NOE errors are equally important to NOE values themselves. They are used to weigh the NOE data in the relaxation-based backbone protein dynamics calculation (Palmer et al. 1991 ; d\u2019Auvergne 2008 ; Jaremko et al. 2015 ). Inaccurate values of NOE errors can result in the erroneous estimation of protein backbone dynamics. Particularly, the overestimation of NOE leads to significant errors in the local dynamics parameters as evidenced by appropriate simulations (Ferrage et al. 2008 ). Occasionally, the average values of the NOE and standard errors in the mean have been determined from several separate NOE data sets (Stone et al. 1992 ; Renner et al. 2002 ). Nonetheless, it has been most often accepted to use signal-to-noise ratios ( SNR ) in the determination of steady-state NOE errors (Farrow et al. 1994 ; Tjandra et al. 1995 ; Fushman 2003 ). $$d\\varepsilon = \\left| \\varepsilon \\right|\\sqrt {SNR_{sat}^{ - 2} + SNR_{nonsat}^{ - 2} }$$ (4) The Eq. ( 4 ) is an approximation of exact formulation of experimental error determination since it takes into account only this part of experimental errors which arises from the thermal noise. It can be safely used if the thermal noise dominates other contributions to the total experimental error. A weak point in Eq. ( 4 ) arises also from the fact that amino acid residues located in flexible parts of macromolecules often display NOE values close to zero, which results in the underestimation of d\u03b5 , owing to the factor \\(\\left| \\varepsilon \\right|\\) as shown in Eq. ( 4 ). Justification of an SNR -based approach should comprise two issues: checking of the reliability of SNR determination delivered by commonly used processing tools and comparison of the SNR -determined errors with those obtained from the statistical analysis of a series of independent NOE measurements. To the best of authors' knowledge, such study has not been yet undertaken for 15 N nuclei in proteins and has only be performed once for 13 C nuclei (Bernatowicz et al. 2010 ). In our study, we found that SNR values automatically derived in the peak intensity determination differed from those obtained semi-manually; their larger part was overestimated. Therefore, automatically delivered SNR values concomitant cross peak intensities cannot be taken for granted. Description of the SNR issue is given in the Supporting Material (section: Determination of signal-to-noise ratio). In order to closely analyze the relevance of SNR -based NOE errors, a series of 10 NOE measurements was performed at 22.3 T using identical spectrometer setup. A comparison of standard deviations ( \u03c3 ) calculated for each of 70 residues of ubiquitin with corresponding means of SNR -based NOE errors is presented in Fig. 4 . It can be concluded from Fig. 4 that values of two presented sets of NOE errors are very similar, and their means are close to one another with a difference of 8\u22c510 \u22125 . Individual \u03b5 values for the residue A46 showing the largest NOE data dispersion are compared with the mean and the standard deviation in Fig. 5 . Examination of Figs. 4 and 5 allows us to conclude that properly determined SNR -based NOE errors are reliable and can be safely used in further applications. Fig. 4 Standard deviations (\u03c3) calculated for 70 residues of ubiquitin (brown circles) and their mean (solid horizontal brown line) determined for series of ten measurements. Means of ten SNR -based errors calculated for each residue (orange circles) and their mean (solid horizontal orange line) Full size image Fig. 5 The NOE values of A46 residue obtained in a series of 10 measurements with appropriate SNR -based errors (gray circles with SNR -based error bars) and their mean with standard deviation (red circle). Dashed red lines correspond to the mean \u00b1 \u03c3 Full size image Saturation of H N protons Originally, saturation of proton resonances was achieved by a train of 250\u00b0 pulses at 10 ms intervals (Markley et al. 1971 ). In protein relaxation studies, however, a train of 120\u00b0 pulses spaced 20 ms apart was commonly used for this purpose (Kay et al. 1989 ). In search of the optimal 1 H saturation scheme, different pulse lengths (120\u00b0, 180\u00b0, 250\u00b0) and different pulse spacings (5 ms, 10 ms, 20 ms) were employed (Renner et al. 2002 ). Finally, it was concluded that pulses of approximately 180\u00b0 at l0 ms intervals performed slightly better than other settings. Extensive experimental survey of H N proton saturation accompanied by theoretical calculations based on averaged Liouvillian theory was carried out on all components of saturation sequence (Ferrage et al. 2009 , 2010 ). It was concluded that the best results were obtained using the symmetric 180\u00b0 pulse train ( \u03c4 \/2 \u2212 180\u00b0 \u2212 \u03c4 \/2) n with \u03c4 = k \/ J NH , where n \u2014the integer determining length of saturation time ( D sat = n \u22c5 \u03c4 ) and k \u2014a small integer, usually k = 2, giving \u03c4 about 22 ms. It was also suggested to move the proton carrier frequency from water resonance to the center of the amide region and reduce the power of the 180\u00b0 pulses to minimize sample heating. Analysis of NOE experiments NOE experiments performed to analyze the influence of a particular sequence of parameters on the apparent nuclear Overhauser effects values, \u03b5 app , are presented in Table S3. Experiments, ssNOE(10-10-8)\/16.4, DNOE\/16.4, ssNOE(14-0-14)\/18.8, ssNOE(13-0-3)\/22.3, and DNOE\/22.3 can be expected to deliver the most accurate results. They are regarded as a kind of reference point for a selected magnetic field. The importance of using appropriate D sat values in steady-state NOE measurements is demonstrated by comparing NOEs in the experiments (14-0-4)\/18.8 and (14-0-14)\/18.8. The first displays a systematic increase of \u03b5 app owing to incomplete H N saturation during D sat . Residue specific differences between the mentioned experiments are shown in Fig. 6 . Residues G75 and G76 with negative \u03b5 values display decreased \u03b5 app as discussed earlier (section: Time schedule of NOE measurement). Fig. 6 NOE differences \u0394\u03b5 = \u03b5 app \u2212 \u03b5 obtained in measurements performed at 18.8 T with D sat = 4 s (\u03b5 app ) and D sat = 14 s (\u03b5). Average difference after rejection of G75 and G76 with \u03b5 < 0 is equal to 0.022 Full size image Calculation of factors \\(\\exp ( - D_{sat} \\cdot R_{1N} )\\) using residue specific R 1 N data is presented in Fig. 7 for D sat values utilized in the measurements performed at 22.3 T as listed in Table S3. The D sat = 3 s is sufficiently long for all residues except the last two C-terminal glycines, G75 and G76. In fact, even D sat = 4 s is not long enough for the observation of unperturbed G76. Therefore, it is not surprising that D sat = 1.3 s is much too short, and \u03b5 app values derived from the experiment (10-10-1.3)\/22.3 are significantly larger than those obtained at the longer period of D sat = 4 s (Fig. 8 ), on average, 0.0348. Fig. 7 Factor characterizing efficiencies of the saturation of nitrogen magnetization for different D sat values were calculated using residue specific R 1 N values determined at 22.3 T in a separate measurement. A common sense but arbitrary limit 0.02 is marked with a horizontal line Full size image Fig. 8 Nuclear Overhauser effect values obtained in steady-state NOE experiments with the saturation period D sat set to 1.3 s (red squares) or 4 s (blue circles) Full size image The effect of a very short RD 1 delay can be demonstrated by comparing experiments ssNOE(13-0-3)\/22.3, ssNOE(10-10-3)\/22.3, ssNOE(6-0-3)\/22.3, and ssNOE(3-0-3)\/22.3 (Fig. 9 ). The RD 1 = 3 s and RD 1 = 6 s result in the increase of \u03b5 magnitudes relative to the RD 1 = 13 s on average, 0.0544 and 0.0042, respectively. On the other hand, average difference between measurements with RD 1 = 13 s and RD 1 = 10 s is negligible \u2212 0.0007. This result gives evidence that RD 1 delay equal to 10 s allows to reach the equilibrium state of H N protons in the studies system. Fig. 9 NOE differences \u0394 \u03b5 = \u03b5 app \u2212 \u03b5 obtained for measurements performed at 22.3 T: ssNOE(13-0-3)\/22.3, ssNOE(10-10-3)\/22.3 (extracted from DNOE), ssNOE(6-0-3)\/22.3, and ssNOE(3-0-3)\/22.3. \u0394 for the RD 1 pair: 3 s and 13 s (brown circles), the RD 1 pair: 6 s and 13 s (orange triangles), the RD 1 pair: 10 s and 13 s(light green squares). Color coded average differences after rejection of G76 with \u03b5 < 0 are equal to 0.0544, 0.0042, and 0.0007 Full size image Concluding, comparison of the NOE values obtained at different settings of D sat or RD 1 highlights the importance of the correct choice of delays in the determination of accurate \u03b5 values. Correction factors As has been shown above, the effect of slow spin-lattice relaxation of water protons and the chemical exchange of amide protons with water combined with too short relaxation delays in the steady-state NOE experiments usually results in substantial systematic NOE errors owing to the incomplete relaxation towards the steady-state or equilibrium 15 N polarization. Therefore, several correction factors were introduced to compensate such errors using the following equation $$\\varepsilon = \\frac{{(1 - X)\\varepsilon_{app} }}{{1 - X\\varepsilon_{app} }}$$ (5) where \u03b5 and \u03b5 app are exact and apparent NOE values, respectively. It has been claimed that the effect of incomplete R 1 W recovery can be corrected by substituting the factor $$X = \\exp ( - RD \\cdot R_{1W} )$$ (5A) into Eq. 5 (Skelton et al. 1993 ). It has been also suggested that factor $$X = \\exp ( - RD \\cdot R_{1H} )$$ (5B) allows for the correction of the not sufficiently long relaxation delay RD with respect to R 1 H (Grzesiek and Bax 1993 ). Another correction that takes into consideration the inconsistency of both R 1 N and R 1 H with relaxation delays has also been recommended (Freedberg et al. 2002 ): $$X = \\frac{{R_{1N} }}{{R_{1N} - R_{1H} }}\\frac{{\\exp ( - RD \\cdot R_{1N} ) - \\exp ( - RD \\cdot R_{1H} )}}{{\\exp ( - RD \\cdot R_{1N} ) - 1}}$$ (5C) Efficiencies of all three corrections were checked on the NOE measurement with the intentionally too short delays: RD 1 = 3 s, RD 2 = 0, and D sat = 3 s, (3-0-3)\/22.3. As shown earlier (Fig. 9 ), all \u03b5 app in (3-0-3)\/22.3 measurement were larger than corresponding \u03b5 values in the correctly performed measurement (13-0-3)\/22.3. The mean of differences was equal to 0.054. None of these above-listed corrections was able to fully compensate the effect of wrong adjustment of RD 1 delay. Three corrections allowing for R 1 W (Eq. 5 ), R 1 H (Eq. 5B ), and R 1 H and R 1 N (Eq. 5C ) resulted in the means of absolute differences equal to 0.019, 0.048, and 0.036, respectively (Fig. 10 ). Therefore, these corrections have compensated for the delay missetting by 67%, 17%, and 38%, respectively . Obviously, the R 1 W effect is the most important factor for compensation. Fig. 10 Residue specific differences between corrected \u03b5 app and \u03b5 values obtained in (13-0-3)\/22.3 measurement. The \u03b5 app values were obtained from (3-0-3)\/22.3 experiment after compensation for R 1 W (Eq. 5A , brown circles), R 1 H (Eq. 5B , orange triangles), and R 1 H , R 1 N (Eq. 6 , light green squares). Horizontal color-coded lines correspond to appropriate means of difference magnitudes Full size image Compensation for a not long enough D sat period with properly chosen RD 1 is an easier task. The experiment (10-10-1.3)\/22.3 was discussed earlier, and its results were shown in Fig. 8 . Use of another correction, $$\\varepsilon = \\frac{{\\varepsilon_{app} - X}}{1 - X},{\\text{where}},X = \\exp ( - D_{sat} \\cdot R_{1N} )$$ (6) results in the corrected \u03b5 app values, which differ from the DNOE experiment by an average of 0.003 (Fig. S7). Nevertheless, in view of the above-mentioned results, it is obvious that none of the existing correction terms should be used as a substitute for a properly designed experiment. Conclusions In this study, it has been shown that dynamic NOE measurement is an efficient and accurate method for NOE determination. In particular, it presents its usefulness in cases of NOE values that are close to zero. This method provides a robust and more accurate alternative to widely used steady-state NOE measurement. The DNOE measurement allows for the determination of NOE values and their accuracies with standard nonlinear regression methods. If high accuracy longitudinal relaxation rates R 1 are not of great importance, they can be simultaneously obtained with a reduced accuracy as a \"by-product\" in the DNOE data processing without any significant reduction of the accuracy and precision of determined NOE values. It has been proven that commonly used methods of NOE accuracy based on the signal-to-noise ratio accompanying steady-state NOE measurements are reliable provided that root-mean-square noise has been determined correctly. It has to be stressed that in view of the results presented in this work, none of the existing correction terms are able to restore accurate NOE values in cases where measurements are improperly set up and performed. ","News_Body":"A fresh new look at an old technique in protein biochemistry has shown that it should be reintroduced to the spectroscopy toolkit. For decades, scientists have used nuclear magnetic resonance (NMR) spectroscopy to probe the molecular motions of proteins on various timescales. This technique has revealed aspects of enzyme reactions, protein folding and other biological processes, all on an atomic scale. Typically, spectroscopists will gage the rotation of NMR-active atoms in the protein backbone with and without proton irradiation to calculate a ratio known as a steady-state nuclear Overhauser effect (NOE); however, it was not always done this way. Before steady-state NOE experiments became the norm in biological investigations, scientists would often take a greater number of measurements over the course of an irradiation experiment. This method, termed \"dynamic\" NOE, might seem more complicated, but according to Ph.D. student Vladlena Kharchenko, it is no more time consuming than steady-state NOE, while dynamic NOE provides additional information about protein flexibility and is far more accurate to minute biological motions in proteins. \"It works for proteins and makes studying their dynamics even more accurate,\" says Kharchenko, a member of \u0141ukasz Jaremko's lab at KAUST. \"Our message to biological NMR spectroscopists is simple: 'Don't be afraid of dynamic NOE.'\" To prove the technique's worth, Kharchenko, Jaremko and their team performed a series of NMR experiments on ubiquitin, a globular protein that regulates a range of processes inside the cell. Working with Mariusz Jaremko, also from KAUST, and collaborators in Poland, the researchers collected both steady-state and dynamic NOE measurements and demonstrated that the dynamic approach is always preferable\u2014except under a few specific conditions, such as when instrument access is limited or when proteins degrade very rapidly. Notably, the steady-state approach proved especially prone to errors in regions of the ubiquitin protein that were flexible and disposed to moving around. The dynamic technique, in comparison, offered no such misleading results. In light of their findings, the KAUST team hopes that other scientists with an interest in atomic-level protein mechanics will now begin to adopt, or at least reconsider, dynamic NMR methods. Kharchenko says that sometimes, \"it's worth dusting off forgotten methods and checking if they fit to new emerging questions and systems of research interest.\" ","News_Title":"Old methods prove true for studying proteins","Topic":"Biology"}
{"Paper_Body":"Abstract Seahorses have a circum-global distribution in tropical to temperate coastal waters. Yet, seahorses show many adaptations for a sedentary, cryptic lifestyle: they require specific habitats, such as seagrass, kelp or coral reefs, lack pelvic and caudal fins, and give birth to directly developed offspring without pronounced pelagic larval stage, rendering long-range dispersal by conventional means inefficient. Here we investigate seahorses\u2019 worldwide dispersal and biogeographic patterns based on a de novo genome assembly of Hippocampus erectus as well as 358 re-sequenced genomes from 21 species. Seahorses evolved in the late Oligocene and subsequent circum-global colonization routes are identified and linked to changing dynamics in ocean currents and paleo-temporal seaway openings. Furthermore, the genetic basis of the recurring \u201cbony spines\u201d adaptive phenotype is linked to independent substitutions in a key developmental gene. Analyses thus suggest that rafting via ocean currents compensates for poor dispersal and rapid adaptation facilitates colonizing new habitats. Introduction Explaining mechanisms of marine biodiversification is challenging, owing to persistent paucity of information on patterns of speciation and phylogeography in marine ecosystems 1 , 2 , 3 . Major geological vicariance events, such as the closure of the Panama seaway 4 or the Tethys seaway 5 , 6 , have been suggested to impact patterns of marine biodiversification, particularly for organisms whose dispersal strategies rely on ocean currents transporting pelagic larvae or rafting individuals across large distances 7 . In such lineages, ecomorphological divergence and local adaptation after a colonization event can be slow even in the presence of strong divergent selective pressures 8 . Thus, comprehensive studies addressing spatio-temporal diversification patterns that include dynamics of geophysical processes, as well as knowledge of the genetic bases and developmental mechanisms of key adaptive traits, are required to understand the mechanisms that drive the evolution of marine biodiversity. The radiation of seahorses (Family Syngnathidae ) is a particularly iconic and suitable model system to investigate the effects that tectonic activity and ocean current dynamics can have on the dispersal and diversification of marine taxa due to the seahorses\u2019 dispersal by rafting 7 , 9 , as well as to study the rapid evolution of adaptive phenotypes in new environments. Seahorse genomes evolve under some of the highest mutation rates among teleosts 10 and have the greatest diversification rates within their family (Supplementary Fig. 1 , Figshare: Dataset 1 ). All seahorses are sedentary but exhibit specialized morphological and life-history traits 11 , 12 , 13 , such as a prehensile tail (and the lack of a caudal fin), an elongated snout, lack of pelvic fins, an armor of bony plates instead of scales, and a unique mode of male pregnancy whereby males give birth to developed juveniles 14 , 15 . Species of seahorses differ widely in body size, color patterns and other adaptive traits to their respective environments 11 , such as the presence or absence of bony spines, which are likely an adaption against predators 16 . Previous research revealed that the evolutionary origin of seahorses likely lies in the Late Oligocene\u2019s Indo-Pacific 17 , 18 , 19 from where different lineages dispersed around the globe despite the seahorses\u2019 poor endurance swimming abilities and their reliance on rafting as primary long-distance dispersal strategy 9 , 20 . Nonetheless, a comprehensive understanding of the seahorses\u2019 colonization routes is still missing as phylogenetic reconstructions were typically either derived only from relatively few species and\/or few genetic markers 18 , 21 , 22 , 23 . Here, we study the diversification patterns of these unique fishes based on the analysis of multiple sequenced seahorse genomes. By conducting comprehensive phylogenetic analyses, we infer their demographic history and clarify the role of seaway closures during their diversification as part of tracing the colonization routes from the origin of their common ancestor to their current distribution. Additionally, we address the adaptive phenotypic evolution of seahorses by studying the development of one of the most eye-catching traits within the genus: the presence or absence of bony spines. Results and discussion Global diversity of seahorses Using PacBio long-read sequencing (~115-fold coverage), Illumina short-read sequencing (~243-fold coverage), and Hi-C technology (~184-fold coverage) we de novo assembled the genome of a male Hippocampus erectus . With a contig N50 of 15.5 Mb, our chromosome-level assembly (total size 420.66 Mb; comprising 22 superscaffolds corresponding to the expected chromosome number) (Supplementary Figs. 2 \u2013 4 , Supplementary Tables 1\u2013 4 , and Supplementary Data 1 ) improved in sequence contiguity over previously available assemblies generated from Illumina short reads alone (contig N50: 14.57 kb) 10 , 24 . We re-sequenced the genomes (~16-fold coverage) of 358 seahorse specimens comprising 21 species reflecting Hippocampus \u2019 global distribution, with representatives of major seahorse lineages (Fig. 1a , Supplementary Fig. 5a , Supplementary Data 2 ). Fig. 1: Genetic diversity and phylogenetic relationships of 358 seahorse specimens. a Geographic sampling locations for sampled seahorses with patterns of nucleotide diversity ( \u03c0 ) of the 21 seahorse species across 22 chromosomes. Maps from Wessel et al. (2013) under GNU GPL license 91 . b Neighbor-joining tree constructed with genome-wide SNPs of 358 seahorses. Location pin symbols in ( a ) and branch background in ( b ) correspond to each other. Seahorses illustrations by Geng Qin. Source data are provided as a Source Data file. Full size image Our analysis identified each seahorse species as a monophyletic group in a neighbor-joining tree inferred from 41 million genome-wide single nucleotide polymorphisms (SNPs) (Fig. 1b , Supplementary Tables 5 \u2013 8 ), and they formed distinct clusters in a principal component analysis (Supplementary Fig. 5b ). Genetic diversity ( \u03b8\u03c0 and \u03b8\u03c9 ) varied substantially among species and chromosomes, as it was, for example, generally higher for seahorses in the North Atlantic Ocean biome than in the South Atlantic Ocean biome (Fig. 1a , Supplementary Figs. 6 , 7 , Figshare: Dataset 2 ). The time-calibrated tree estimated that the common ancestor of all extant seahorses lived ~20\u201325 Ma (million years ago) (Fig. 2a , Supplementary Figs. 8 , 9 , Figshare: Datasets 3 \u2013 6 ), which coincides with the beginning of a period of explosive diversification in most modern marine fish and coral lineages 25 , 26 . The Indo-Australian Archipelago was identified as the center of origin of the genus Hippocampus , in line with previous studies 18 , 19 (Fig. 2b , Supplementary Fig. 9 ). Subsequently, seahorses diversified and spread globally, with their colonization routes and dynamics strongly linked to prevalent oceanic currents and tectonic events (see Supplementary Text) 27 . Our species tree based on 2,000 loci suggests that H. abdominalis is the sister-lineage to a clade containing all other seahorses, and the latter are subdivided into two major phylogenetic clades: clade I comprises eight species exclusively inhabiting the Indo-Pacific Ocean, while clade II includes six species inhabiting the Atlantic Ocean, one from the East Pacific Ocean, and five from the Indo-Pacific Ocean (Fig. 2a , Supplementary Fig. 9 ). A more detailed description of clade II exemplifies the seahorses\u2019 dependence on ocean currents as a means of far-distance dispersal and showcases how temporal seaways can boost or limit diversification and dispersal. Fig. 2: Colonization and demographic history of seahorses. a Phylogenetic tree and divergence time estimates for 21 seahorse species. The branch line thickness corresponds to the population size estimates ( N e ) and colors indicate different lineages. Symbols I\u2013III indicate calibration points. b \u2013 d Predicted colonization routes (colored arrows) of seahorses based on divergence time, distribution, vicariance events, and ocean currents (white arrows). Maps modified from Ron Blakey \u00a9 2016 Colorado Plateau Geosystems Inc (License # 60519). b The Indo-Australian Archipelago was the center of origin (red marking) of the genus Hippocampus before seahorses diversified and dispersed globally 18\u201323 Ma. c Seahorses initially colonized the Atlantic Ocean through the opening Tethyan seaway, which, after its closure (Terminal Event during 7\u201313 Ma), separated this Tethyan lineage from its Indian Ocean sister lineage. The latter, subsequently rapidly diversified (yellow marking) in the Arabian Sea, establishing a second center of seahorse diversification. d A second seahorse colonization event of the Atlantic Ocean occurred from the Indian Ocean about 5 Ma by passing the South African tip, and finally arriving in the East Pacific Ocean through the still open Panama seaway approximately 3.6 Ma. Source data are available at Figshare (Datasets 4\u20136). Full size image Rapid diversification and colonization routes of clade II After separating from clade I by dispersing into the West Indian Ocean around 18.2 Ma, the ancestors of the South Atlantic and North Atlantic lineages diverged from each other approximately 15.2 Ma (Fig. 2a ). The North Atlantic lineage followed north-westward oceanic currents and passed through the Tethys Sea a few million years before the initial closure of the East Tethys Seaway due to tectonic shifts about 14 Ma 6 , 28 . Consistent with this colonization route for the North Atlantic lineage a strong genetic bottleneck in their ancestral population was detected (supporting the notion that founder dispersal is particularly common in seahorses 21 ), however, a rapid population expansions was detected after crossing the Atlantic Ocean in the mid Miocene (Fig. 2a, b , Supplementary Fig. 10 ). As previously proposed 22 , ancestors of H. hippocampus diverged from the North American lineages likely by back-crossing the Atlantic via the Gulf Stream (a dispersal route still effective today 29 ), and colonized the East Atlantic in the Pliocene. For many marine animal taxa inhabiting the shallow areas of the Arabian Sea, the closure of the East Tethys Seaway led to an increased biodiversity 6 , as it did for seahorses, leading to a second center of biodiversity in this group. For instance, about 13 Ma the ancestors of H. kelloggi and H. spinosissimus emerged as a new lineage by dispersing back into the Indo-Australian Archipelago. This event may had been facilitated by a reinforced Equatorial Counter Current in the Indian Ocean after the closure of the Tethys Seaway 30 , and thus further contributed to the high diversity in the original center of seahorse biodiversity (Fig. 2c, d ). The South Atlantic seahorse lineage split and dispersed from the Arabian Sea southernly, along the East of the African continent. The closure of the Tethys Seaway may have enhanced the East African coast current and the Agulhas Current, which potentially assisted in this southward long-distance migration 30 . This lineage passed the Cape of Good Hope, a potentially severe dispersal bottleneck reflected in the extremely low effective population size of this lineage ~4.8\u20133.6 Ma, and colonized the Southern and Western African coastlines ( H. capensis and H. algiricus , respectively). Following this second invasion of the Atlantic in the early Pliocene, ancestors of the South American lineages crossed the Atlantic and colonized the South American coastlines, with H. ingens emerging from an early lineage that colonized the north of South America. In line with previous studies 4 , 21 , we also found that this lineage crossed the Panama Seaway before its final closure 4 , where it thrived as indicated by a large average effective population size (Fig. 2a, d ). Subsequently, a second lineage successfully crossed the South Atlantic approximately 700k years ago and colonized the northern coast of South America and the Caribbean, from which H. reidi evolved. Average effective population sizes of this lineage remained relatively small, possibly as it was not able to spread into the East Pacific due to the prior closure of the Panama Seaway and the competitive disadvantage as its habitat likely overlapped with those of other seahorse species, such as H. erectus (Fig. 2d ). Repeated crossings of the South Atlantic via rafting along the Benguela & South Equatorial Current have been proposed before 21 , 22 . Indeed, ongoing gene flow from the West-African H. algiricus into the South American H. reidi population with much less pronounced gene flow in the opposite direction supports the notion that rafting along these ocean currents facilitated this colonization route (Fig. 3a ). Fig. 3: Gene flow and fluctuations in the effective population size. a Gene flow detected between species inhabiting the South Atlantic Ocean. Gene flow is shown nearby the white lines as migration rate deduced by G-PhoCS. Thickness and direction of the arrows correspond to rates and direction of gene flow, respectively. Maps modified from Ron Blakey \u00a9 2016 Colorado Plateau Geosystems Inc (License # 60519). Source Data are provided in Supplementary Table 10 . b Fluctuations in effective population size by PSMC. The x axis represents time in years before present while the y axis represents the effective population size. The charts are organized mainly according to the geographic distribution for each of the species with different distribution areas. Source data are provided as a Source Data file. c Sea level change during the past 1 million years in meters 33 . The yellow line indicates the last global interglacial peak while the cyan shade indicates the last glacial maximum period. Full size image The global diversification of seahorses thus involved long-distance dispersal and has been facilitated by paleo-seaway dynamics and changing ocean currents. Specifically, our analyses finally confirm that Indo-Pacific seahorses colonized the eastern coastline of America via two distinct routes and in two waves, a topic previously under debate 19 , 22 : firstly, by colonizing the still open Tethys seaway and subsequent crossing of the Atlantic Ocean, and later by passing the South African Cape of Good Hope. Interestingly, the second wave occurred only in the early Pliocene, potentially facilitated by a change in the South Atlantic and Caribbean ocean current dynamics driven by the ongoing closure of the Panama Seaway 27 , 31 . These findings contradict a recent study that suggested only one colonization via the South Africa route 22 and thus emphasizes the importance of a wide species representation in biogeography studies. As outlined above, tectonic shifts and subsequent changes in ocean current dynamics likely facilitated some of the major dispersal and diversification events in seahorses, however, more short-term changes in seawater levels can also drastically affect the evolution of marine organisms inhabiting shallow water, for example by changing the amount of suitable habitat in a given area or change its structure 32 . Fluctuations in effective population sizes ( N e s ) were estimated back up to 1 million years ago (Fig. 3b , Figshare: Dataset 7 ). When such fluctuations since the last glacial peak (~120 k years ago to ~10k years ago) were compared to fluctuations in seawater levels, which are primarily driven by variations in global temperature (via glaciations) 33 , the patterns suggest a complex effect of seawater levels on N e (Fig. 3c ). Several seahorses\u2019 effective population sizes appeared to be positively associated with warm climate and thus high seawater levels, as suggested by local maxima in effective population sizes following a warm period ~115 k years ago with a delay of several thousand years. These species include H. hippocampus (the sole European species considered), H. casscsio , H. fuscus (both lineages have restricted distribution ranges in and south of the Red Sea and East African coast), and H. subelongatus (only found at the West Australian coast). Effective population sizes of multiple other species show a more negative association with seawater levels with a local maximum in N e coinciding with a local minimum in sea level. These species include H. ingens (the only species considered distributed along the Pacific side of the American continent), H. spinosissimus , and H. trimaculatus , two species broadly distributed across the Sundaic region. However, several species show no peak in N e sizes likely associated with high or low seawater levels, and other factors might have a stronger influence on population sizes. For instance, species inhabiting the North Atlantic biome ( H. erectus , H. hippocampus & H. zosterae ) show generally larger N e than most other lineages (e.g., those inhabiting the South Atlantic) suggesting that the biome type can affect species N e s . Furthermore, some species might be more resilient against seawater level fluctuations or glaciation induced habitat loss than others as a result of increased dispersal abilities (e.g., via rafting 7 ), or because regional refugia from glaciations were available 34 . Convergent evolution of adaptive phenotypes During their worldwide diversification, seahorses had to adapt to diverse combinations of abiotic and biotic factors leading to unique adaptive phenotypes 24 . Adult seahorses have only relatively few predators due to their excellent camouflage and unappetizing bony plates and spines 11 . Spines, which were derived from L-type plates covering the surface of seahorses just under the skin, are morphologically similar to the diamond-shaped dermal spines covering the skin surface in pufferfishes, which are the extreme-scale derivatives 35 . Vertebrates possess a huge diversity of skin-derived structures, including teleost fish scales, reptilian scales, avian feathers, and mammalian hair 36 . Although the skin structures are not structurally homologous, they seem to be controlled by highly conserved genetic mechanisms between the different vertebrate clades 37 , 38 , 39 . Previous studies have shown that Hh, Fgf, Bmp, Wnt\/\u03b2-catenin, and Eda pathways were involved in teleost scale development 40 , 41 , 42 , 43 , 44 , 45 . It is likely that teleost skin structures (even when strongly modified), share common elements of these core signaling pathways known to underpin skin structure development throughout diverse vertebrate groups. Seahorses have also evolved variations in the degree of body coverage by spines, which may enable them to adapt to diverse ecological niches. Interestingly, species exhibiting bony spines were found to not be closely related by our species tree: H. spinosissimus , H. jayakari , H. histrix , and H. barbouri . This confirms previous findings 18 and suggests that some lineages were exposed to similar environmental pressures, such as specific predator types, have evolved similar phenotypes independently (Fig. 4 and Supplementary Fig. 11 ). Spiny seahorses inhabiting the north and west Indian Ocean split from their sister lineage 8.7 and 7.8 Ma, respectively, while spiny seahorses inhabiting the Pacific Ocean diverged from their sister lineage 14.7 and 6.8 Ma (Supplementary Fig. 11 ). Fig. 4: The evolution of spines. a Left, Species tree displaying the independent evolution of spines in seahorses. The branch length indicates number of substitutions per site. Four spiny seahorse species are highlighted in blue. Thicker branches correspond to higher rates of nonsynonymous-to-synonymous substitutions (d N\/ d S ) for bmp3 gene. Canonical and generalized McDonald and Kreitman test (MKT) for bmp3 gene was performed for three pairwise sister species with divergent spiny and non-spiny features highlighted by background colors, whose significance levels were indicated by p value with blue and red font, respectively. Right, comparison of amino acid substitutions in bmp3 protein, polymorphic and fixed substitutions in spiny seahorses are indicated with red and blue circles, respectively. b Distribution of d N \/d S values in bmp3 in spiny seahorses compared to non-spiny species. c Independent evolution in the phylogenetic tree reconstructed for the protein encoded by bmp3 . Seahorses illustrations by Geng Qin. d Whole-mount in situ hybridization of bmp3 in Hippocampus erectus . In situ photos of seahorses by Ralf F. Schneider. Source data are provided as a Source Data file. Full size image To investigate the molecular basis of this repeatedly evolved adaptive phenotype, we performed a positive selection analysis to investigate whether accelerated nonsynonymous\/synonymous mutation rate ratios (d N \/d S ) can be detected on the branches of spiny seahorses compared to non-spiny lineages. Using the codeml program in PAML we identified 37 genes putatively under positive selection with signals of accelerated d N \/d S in spiny seahorses ( p < 0.001, Supplementary Data 3 , Figshare: Dataset 8 ). Protein trees obtained from the amino acid sequences of all 37 genes showed that the four spiny seahorses are not closely related to each other (Fig. 4 , Figshare: Dataset 9 ), indicating that the spiny phenotype likely evolved independently. Specifically, the four spiny seahorse lineages exhibit independent amino acid changes in the bone morphogenetic protein 3 ( bmp3 ) gene (Fig. 4a ), and canonical and generalized McDonald and Kreitman tests (MKT) showed that bmp3 evolved under positive selection (neutrality index < 1, Chi-square test p < 0.05) (Fig. 4a, b , Supplementary Data 4 ). Spines emerge in many syngnathid species\u2019 embryos (including H. erectus ) and are lost in some species secondarily during maturation. Although the spiny phenotype likely has a polygenic basis, whole-mount in situ hybridizations demonstrate bmp3 expression in seahorse spines\u2019 early developmental stages in H. erectus , a species whose adult stages do not have well-developed spines (Fig. 4d , Supplementary Fig. 12a ). Being a transcription factor, bmp3 was shown to negatively regulate osteoblast differentiation (and thus bone mass) in mammals 46 , 47 , suggesting that divergent sites in this gene between spiny and non-spiny seahorses may affect its regulatory interaction with downstream genes and thus contribute to spine outgrowth in those species with derived peptide sequences. Moreover, a knockout experiment using CRISPR\/Cas9 in zebrafish showed that mutants have a series of significant scale defects, such as decrements in scale numbers, rearrangements, and irregular shapes, confirming that bmp3 plays a role in the formation of dermal bones in teleosts, and thus likely also spines (Supplementary Fig. 12b, c ). The independent evolution of complex adaptive phenotypes, such as the spine phenotype, suggests that seahorses have a generally high evolvability, in concordance with the high rates of nucleotide evolution already reported 10 and the high diversification rates of Hippocampus we reported here (Supplementary Fig. 1 , Figshare: Dataset 1 ). Thus, the ability to rapidly adapt to new environments and respond to changed selection regimes may, in addition to their unorthodox means of dispersal by rafting along oceanic currents, account for some of the evolutionary success seahorses had while diversifying globally. In conclusion, we report that seahorses dispersed over surprisingly long distances, and diversification was assisted by changing ocean currents and tectonic events. These include two independent invasions of the Atlantic Ocean from the West Indian Ocean, one of them facilitated by the last opening of the East Tethys Seaway and the other by passing the Cape of Good Hope and, finally, the colonization of the East Pacific Ocean through the Panama seaway. Convergent evolution of adaptive traits, such as in the case of repeatedly evolved protective dermal spines suggests that developmental-genetic pathways were recruited several times independently and presumably in response to predation pressure. Methods Diversification rate estimation in the Syngnathidae DNA sequences of 138 species of the Syngnathidae family and one outgroup were obtained from previous studies 48 , 49 . After sequence alignment using Clustal Omega (v1.2.4) 50 , a concatenated phylogenetic tree was obtained with RAxML (v8) using a best-scoring maximum likelihood tree search method (option -a) using a GTRGAMMA model and including 1,000 bootstrap replicates 51 . Relative divergence was estimated with the wLogDate python program 52 . Diversification rates (i.e., speciation minus extinction) were estimated using BAMM 2.5 48 . We accounted for non-random incomplete taxon sampling by including the proportion of missing taxa per genus (sample probabilities in Supplementary Data 5 ) as well as the overall sampled genera (=0.84). Priors were generated using setBAMMpriors in BAMMtools 48 . Analyses were run for 5 \u00d7 10 6 generations, sampling every 1000 generations and with a 25% burn-in. DNA sequences and the estimated phylogenetic tree are available at Figshare (Dataset 1). Long-read sequencing and assembly of the Hippocampus erectus genome A mature, male H. erectus bred in the aquatic farm in Fujian province, China, was used for the de novo genome assembly. Genomic DNA was extracted from tail muscles using a standard phenol\/chloroform extraction protocol. Single-Molecule, Real-Time (SMRT) sequencing was performed using a total of 5 \u03bcg of genomic DNA to generate a 20 kb library according to the manufacturer\u2019s instructions (Pacific Biosciences, USA). Subreads were obtained after size selection on a BluePippin system (Sage Science, USA). SMRT genome sequencing was performed on a PacBio Sequel platform (Pacific Biosciences, USA) to an approximate coverage of 113-fold. Reads with the quality lower than 0.75 and length shorter than 500 bp were excluded and 6.01 M subreads comprising a total of 47.88 Gb were retained for the assembly (longest subread = 71.17 kb, average length = 7.97 kb). The draft genome was assembled using WTDBG (  ). Sequence contigs were then error-corrected using Pilon 53 . Evaluation of the integrity of assembled sequences, genome size estimation, transposable element predictions and genome annotation are described in the Supplementary Information (Supplementary Methods, Supplementary Tables 2 \u2013 4 , Supplementary Data 1 ). High-throughput chromosome conformation capture (Hi-C) based genome scaffolding An adult farmed male H. erectus was used for the Hi-C analysis. The library was prepared following a standard in situ Hi-C protocol for blood samples 54 , using DpnII (NEB, Ipswich, USA) as the restriction enzyme. A standard circularization step was carried out, followed by DNA nanoballs (DNB) preparation according to the standard protocol of the BGISEQ-500 sequencing platform 55 . The library was then sequenced with a PE100 strategy using the BGISEQ-500 platform. Quality control and library evaluation is described in the Supplementary Methods. For Hi-C alignment and chromosome orientation, we first constructed an interaction matrix based on the valid reads. Then, the ICE software was used to correct for any preference of the enzyme-cut loci due to an uneven distribution in GC content 56 . The retrieved valid pairs (319,356,098) were then used to orientate and anchor the PacBio contigs into superscaffolds (chromosomes) applying the 3D-DNA pipeline with the key parameter of \u2018-m haploid -s 4 -c 22\u2019 57 . The contact maps were subsequently generated with the Juicer pipeline 58 , and the boundaries for each chromosome were manually rectified by visualizing the inter.hic file in Juicebox 59 , combining linkage information from the agp file. Re-sequencing sample preparation, mapping, and variant calling We sampled a total of 358 seahorse specimens from 21 species representing the major lineages of the genus Hippocampus (Fig. 1a , Supplementary Data 2 ), including 13 to 22 individuals per species, except for H. cassisio , H. capensis , and H. camelopardalis , represented by 8, 7, and 2 individuals, respectively. The classification of each specimen was based on morphological and genetic evidence 16 . Genomic DNA was extracted from tail muscles using a standard phenol\/chloroform extraction method and used to construct an approximately 350 bp-insert-size sequencing library. Paired-end libraries were sequenced on an Illumina HiSeq 4000 platform. One random sample for each species was sequenced at ~20-fold coverage, and the rest were sequenced at ~10-fold coverage. After the removal of adapters and low-quality reads (Supplementary Methods), clean reads for each individual were mapped to both the PacBio genome sequence of H. erectus and the Illumina genome sequence of H. comes using BWA-MEM with default parameters (v0.7.17) 60 . We calculated mapping rates, depth, and genome coverage using SAMtools (v1.6) after sorting and removal of duplicates 61 . The assembled Hippocampus erectus PacBio genome was then used as the reference genome. By assigning 21 species, we then performed variant calling for all 358 individuals using FreeBayes v9.9.2 62 . Mapping and base quality filters were used as default in FreeBayes (\u2013standard-filters flag). Details are shown in Supplementary Methods. The filtered dataset was then annotated according to the H. erectus genome using the package ANNOVAR 63 . Analysis of genetic diversity and divergence Inter-species genomic divergence was calculated for each pair of the 21 seahorse species, using the specimen with the highest sequencing fold coverage per species. We also calculated pairwise genetic distances among all 358 specimens using PLINK (v1.9) with the main parameter \u2018\u2013distance 1-ibs flat-missing\u2019 64 . A neighbor-joining (NJ) tree was then constructed using MEGA7 65 . Principal component analyses (PCA) were performed using smartPCA program within EIGENSOFT (v6.1.4) 66 . We furthermore analyzed intra-specific nucleotide diversity using ANGSD (v 0.924) 67 using sliding-window approach as stated in Supplementary Methods. Both Watterson ( \u03b8w ) 68 and pairwise ( \u03b8\u03c0 ) 69 estimators of theta were used for nucleotide diversity analysis (Figshare: Dataset 2 ). R packages \u2018vioplot\u2019 70 and \u2018circlize\u2019 71 were employed to explore nucleotide diversity among the different species and chromosomes. Global colonization patterns For our phylogenetic analyses, first gene families for Syngnathus scovelli , H. erectus , and H. comes were identified using Treefam 72 . After filtering low-quality genes with a premature termination codon or in which the base number of the coding region was not a multiple of three, gene family analyses were carried out and identified 5,475 single-copy orthologs 10 . Pair-wise alignments for H. erectus and S. scovelli were conducted using prank v.140603 73 and CDS sequences for 2,000 orthologs (randomly selected from the above mentioned) were then extracted for each specimen based on the SNP dataset (Figshare: Dataset 3 ). A coalescent-based phylogenetic tree was constructed using ASTRAL-III v5.6.1 74 , 75 , with a total of 2,000 independent gene trees and including one to five specimens for each of the species (103 specimens in total). Loci selected have an average length of 1,548 (\u00b1 1,325), average segregating sites of 18% (\u00b1 4%) and average missing data of 1%. Gene trees were generated using RAxML (v8) using the rapid bootstrap analysis and searched for the best-scoring maximum likelihood tree (option a) under a GTR + G substitution model and including 100 bootstrap replicates 51 . The DNA matrices, gene trees, and ASTRAL inference are available at Figshare (Dataset 4). To obtain divergence time estimates of the nodes in the Hippocampus species tree, 100 loci were randomly subsampled for the same one to five individuals per species (from the above list; 103 individuals in total), using the package starBEAST2 implemented in BEAST v2.4 76 . Loci selected for this analysis had an average of 1,579 bp (\u00b1 1,060), average segregating sites of 18% (\u00b1 4%) and 1% missing data. For calibration points, we used data from the paleontological work of Hippocampus 77 and other related groups of pygmy pipehorses and pipehorses 77 , 78 , 79 . Thus, using a lognormal distribution as hyperprior, we first calibrated the origin of Hippocampus genus to the youngest possible age of 11.6 Ma for which Hippocampus fossils were recorded as well as the existence of pipehorses and pygmy pipehorses has been shown 77 , 78 , 79 (Supplementary Table 9 ). Thus, this prior assumes that Hippocampus genus originated before the occurrence of the oldest known fossil of Hippocampus ( H. samarticus ) 77 , and we also relax a wide 95% HPD interval to accommodate uncertainty (95% HPD: 14.4-31.8). Second, we incorporated the information of the H. sarmaticus fossil from the Miocene as an ancestor of H. trimaculatus using a lognormal distribution with a mean 11.8 Ma to lead the median close to 11.6 Ma and the standard deviation was set to model uncertainty, covering the complete Middle Miocene upper bound to the Late Miocene (95% HPD: 8.32-16.1 Ma) 77 . Finally, following Teske and Beheregaray 17 , we set the divergence between H. reidi and H. ingens to a minimum of 2.8 Ma, in correspondence to the last connection between the Caribbean and the Pacific Ocean 80 . Although it has been argued that Colombian sediments supported the existence of Miocene temporal closures of the Panama seaway 80 , 81 , for this study we used a conservative prior by setting the minimal possible divergence time between these lineages to 2.8 Ma and also allowed the hyperprior to cover older dates, with 95% HPD: 3.07-4.64 Ma, given that O\u2019Dea et al. suggested that a connection between the Atlantic and Pacific Oceans allowing gene flow likely existed until 3.2 Ma (gradually reduced in time) 80 . All remaining settings were used as default, including unlinked strict clocks and unlinked JC69 substitution models among loci. We fixed the ASTRAL tree topology and ran two independent analyses during the 160 \u00d710 8 steps of the MCMC chain and sampled at every 80,000 generations. Convergence was diagnosed using Tracer v1.7 82 . The two independent runs were combined using LogCombiner (included in BEAST v2.4 package) with a 10% burn-in. The maximum credibility tree was obtained using TreeAnnotator (also included in BEAST v2.4 package). The DNA matrices and BEAST xml input file and outputs are available at Figshare (Dataset 5). The topology and branch lengths (divergence times) of the species tree were used to reconstruct the geographic diversification under two different models of diversification in space: diffusion 83 , 84 and heterogeneous landscape 85 . Both models were run in BEAST v2.4, using a lognormal clock and tip coordinates matching the sampling points and current distribution of the species. For the heterogeneous model, we included a deformation in the continental areas by increasing the friction in an external kml file, to decrease the probability of migration through continents and nearby seaways. We ran different values of friction and deformation, including deformation = 10, 20, 50, and 100 and valued each polygon at 2 (higher deformation, higher friction). Due to the high similarity in the results, we only presented the results with deformation = 20; value = 2 (Supplementary Fig. 9a ). Convergence was diagnosed using Tracer, and Tree Annotator was used to export the final tree with a 10% burn-in. Finally, we used SPREAD (v1.0.6) 83 to generate a kml file and Google Earth Pro to plot and animate the diversification of the Hippocampus genus in space and time. The BEAST xml input file and outputs are available at Figshare (Dataset 6). Demographic inference with G-PhoCS A total of 102 representative specimens (2-5 specimens for each species) were used to infer the demographic history of seahorses. Neutral loci were used to run the demographic analysis 86 . The filtering strategy is summarized in Supplementary Methods. 52.2% of the genome remained after filtering, from which we selected 6102 \u2018neutral loci\u2019 by identifying contiguous intervals of 1 kb that passed the filters. We used the default settings chosen by Gronau et al . 86 : a Gamma distribution (\u03b1 = 1.0, \u03b2 = 10,000) for the mutation-scaled population sizes (\u03b8) and divergence times (\u03c4), and a Gamma (\u03b1 = 0.002, \u03b2 = 0.00001) prior for the mutation-scaled migration rates ( m ). The Markov Chains exploring the space of parameter values were run for 100,000 burn-in iterations with an additional 200,000 iterations. The mean sampled value and the 95% Bayesian credible interval of each parameter were calculated by Tracer v1.7.1 82 . We assumed an average mutation rate ( \u03bc ) of 4.33 \u00d7 10 \u221210 per nucleotide per generation 10 and an average generation time of one year for the Hippocampus species. The population size estimates ( Ne ) were obtained from the mutation-scaled samples ( \u03b8 ) based on the formula Ne = \u03b8 \/ 4 \u03bc . Gene flow was measured by the total migration rate, which is the per-generation rate times the number of generations in which migration was allowed (Fig. 3a , Supplementary Table 10 ). Inference of demographic history from PSMC analysis Pairwise sequentially Markovian coalescence analyses (PSMC) 87 were used for one individual (with the highest genome coverage) per species for interspecific comparisons. Genotype information of the selected individual was retrieved from the alignment BAM files using SAMTOOLS 61 . Variants with sequencing depth less than a third of the average depth or greater than 2.5 times were removed. The program fq2psmcfa was used to convert the diploid consensus sequence to a FASTA-like format where the characters indicated heterozygous positions in consecutive bins of 100 bp. The program psmc was then used to infer the population size history 87 , where the parameters were set as -N 30 -t 15 -r 5 -p 4 + 25*2 + 4 + 6. We assumed a generation time of 1 year and a mutation rate ( \u03bc ) of 4.33\u00d710 \u221210 per nucleotide per generation 10 . The genetic basis for the spine trait Four seahorse species used in this study, including H. spinosissimus , H. jayakari , H. histrix , and H. barbouri , typically show well developed spines 16 (Fig. 3a , Supplementary Fig. 11 ). To detect positively selected genes (PSGs) potentially related to bony spines, we reconstructed gene sequences for 20 seahorse species (excluding H. camelopardalis with extremely low sequencing depth) using both SNPs and invariant sites ( H. erectus genome as reference). The aligned codon sequences for each gene were further analyzed using codeml program in PAML 88 to calculate the d N \/d S and we detected positive selection on particular branches considering the phylogenetic relationships among these 20 species (obtained using ASTRAL; described in Phylogenetic analysis Section). The \u2018one-ratio\u2019 and \u2018two-ratio\u2019 codon substitution models were considered. \u2018One-ratio\u2019 model assumes the same d N \/d S across all the branches in the phylogeny of species, which was termed as the \u2018null hypothesis\u2019. The \u2018Two-ratio\u2019 model presumes diverged d N \/d S for the branches of spiny and non-spiny lineages, as \u2018alternative hypothesis\u2019. Likelihood ratio tests were conducted to compare the above-mentioned models by calculating the corresponding likelihoods, \u03c7 2 critical values, and p values for each gene. We adopted a relatively strict threshold of 0.001 for the original p values to initially obtain a set of 37 putative genes under positive selection with significantly accelerated d N \/d S on the branches of spiny seahorse lineages (Supplementary Data 3 ). To further characterize the functional genes potentially relevant to spine development for the 37 candidate genes, we performed canonical and generalized MKT to detect the signature of natural selection based on population genomic sequences. For the canonical MKT, the number of nonsynonymous (d N ) and synonymous (d S ) variants between three pairwise sister species with divergent spiny and non-spiny features, containing H. spinosissimus and H. kelloggi , H. jayakari and H. mohnikei , and H. barbouri and H. comes , and those nonsynonymous (Pn) and synonymous (Ps) variants within species were estimated, where H. kuda and H. kuda & H. histrix were considered as outgroups, respectively. According to these tests, the neutrality index (NI = (Pn\/Ps)\/(d N \/d S )) were calculated, and a Chi-square test was implemented. NI < 1 indicated high divergence between species due to positive selection. We performed generalized MKTs, where, d N and d S , were estimated as the derived nonsynonymous and synonymous variations for one of the sister species with divergent spine status contrasted with the ancestral and sister species, which were then compared to the Pn and Ps, putatively neutral, in this lineage. We also implemented the \u2018Free-ratio model\u2019 to estimate the variable d N \/d S ratio on each phylogenetic branch based on the aligned codon sequences for each of the 37 genes through the maximum likelihood method using CODEML in PAML 88 . Distribution of d N \/d S values of the 37 putative PSGs in 20 seahorse species are available at Figshare (Dataset 8). By integrating the results from abovementioned analyses, the genes simultaneously showing significance in PAML and MKT, and consistently presenting accelerated d N \/d S from \u2018Free-ratio model\u2019 on the branches of spiny seahorse lineages in comparison with those of non-spiny lineages, especially with those of sister non-spiny lineages, were considered as confident candidates for further experimental confirmation. Additionally, we reconstructed the CDS sequences of 37 PSGs for 21 seahorse species (one specimen with the highest sequence fold coverage for each species) with the filtered SNP dataset and translated them into protein sequences using in-house scripts. We estimated the protein trees using RAxML (v8) 51 using the rapid bootstrap analysis and search of best-scoring maximum likelihood tree (option a) under a PROTGAMMAGTR substitution model and including 100 bootstrap replicates. The protein trees of these 37 PSGs from 21 seahorse species are deposited at Figshare (Dataset 9). Multiple sequence alignment analysis was then performed for bmp3 based on the generated protein sequences. Only private amino acid substitutions that were polymorphic or fixed in spiny seahorses were retrieved. Private, polymorphic substitutions refer to amino acid substitutions that were segregating exclusively in one or more of the four spiny seahorses, while private, fixed substations refer to amino acid substitutions that were fixed exclusively in one or more of the four spiny seahorses. Whole-mount in situ hybridization of bmp3 was performed with embryos of the lined seahorse H. erectus at different developmental stages, including approximately four, three, two, and one day prior to birth (for the latter, three independent replicates were performed with coinciding expression patterns). Embryos were dissected in RNase-free 1X phosphate-buffered saline and fixed in 4% paraformaldehyde (PFA) at 4 \u00b0C overnight. For bmp3 , a specific antisense RNA probe was synthesized 87 : digoxigenin-labeled UTPs (Roche, item-nr. 11277073910) and SP6 RNA Polymerase (Roche, item-nr. RPOLSP6-RO) were used to synthesize antisense RNA probes from plasmids in which a bmp3 PCR fragment was cloned behind a SP6 RNA Polymerase promoter (Supplementary Table 11 ). Hybridization procedures mostly followed previously described protocols 89 : firstly, embryos were bleached and cleared in 1.5% H 2 O 2 in 1% KOH until pigmentation was removed (was only done for the sample presented in Fig. 4 ), then permeabilized using 10 \u00b5g\/ml proteinase K in Tris-buffered saline with 0.1% Tween-20 (TBS-T) for 15-20 min, then endogenous alkaline phosphatase (AP) activity was deactivated using a solution of 0.2 M triethanolamine (pH 7.5) with 2.5% acetic anhydride added directly before treatment (for 20 min), and a refixation using 4% PFA for 20 min was performed. In between steps, washes were performed with TBS-T. Subsequently, samples were equilibrated with the hybridization mix at 68 \u00b0C for 4 h, followed by overnight hybridization using hybridization mix with 100 ng probe\/ml at 68 \u00b0C. Samples were then repeatedly washed using a mix from 5x saline sodium citrate (SSC), 50% formamide and 2%Tween-20 at 68 \u00b0C, followed by washes in 2x SSC with 0.2% Tween-20 at room temperature. After washes with TBS-T, samples were blocked using blocking buffer for 1.5 h, and then treated with the anti-DIG-AP antibody (1:4000 concentration; Roche, item-nr. 11093274910) in blocking buffer for 5 h at room temperature. After repeated washing of samples for 2 days with maleic acid buffer, they were kept in AP buffer (with Levamisol) for 20 min, after which they were moved to BM-Purple until desired color intensity was reached (Roche, item-nr. 11442074001), and finally photographed. To investigate the phenotypic consequences of bmp3 loss in a teleost, we used a CRISPR\/Cas9 strategy to generate a bmp3 mutant zebrafish line according to Miguel et al. 90 . The bmp3 guide RNA (gRNA) was designed online (  ) targeting the first exon of zebrafish bmp3 . The gRNAs was constructed by overlapping PCR. This method requires a target-specific DNA oligo (top-strand oligo) and a generic DNA oligo for the guide RNA (Supplementary Table 11 ). The target-specific oligo contains a T7 promoter, the target sequence and finally a 20-nt sequence complementary to the guide RNA (Supplementary Table 11 ). The two oligos are annealed and extended with DNA polymerase, and the resulting product serves as a template for in vitro transcription using the mMESSAGE mMACHINE\u2122 T7 Transcription Kit (Thermo Fischer Scientific AM1344) and the transcripted production was purified using the RNA Clean & Concentrator\u2122-5 (Zymo Research R1014). The pT3TS-nCas9n vector was synthesized using the XbaI restriction enzyme (NEB R0145S) and performed in vitro transcription and purification using mMESSAGE mMACHINE\u2122 T3 Transcription Kit (Thermo Fischer Scientific AM1348). The transgenic zebrafish parent labeled with green fluorescent protein for osteoblast-specific transcription factor (Osterix GFP) used in this experiment were cultured at 26\u201328 \u00b0C under a controlled light cycle (14 h light, 10 h dark) to induce spawning. Purified sgRNAs (80 ng\/\u03bcl) were co-injected with Cas9 mRNA (400 ng\/\u03bcl) into zebrafish embryos at the one-cell stage. These founders (F0) fish were raised to maturity and the genotyping primers (Supplementary Table 11 ) were used to screen out F0 with site mutations by the fin clipping, DNA extraction, PCR spanning the target site and sequencing. The adult F0 with mutation were outcrossed with wild-type fish to obtain F1 fish, which were subsequently genotyped. The F1 fishes with the same mutant genotype transmitting a frameshift mutation were inbred to obtain homozygous F2 fish, which were used for further phenotypic observation. Osterix GFP-labeled mutant and wild specimens were observed and photographed under a Leica M205 FA Fluorescent Stereo Microscope (Wetzlar, Germany). All experiments were performed in accordance with approved Institutional Animal Care and Use Committee protocols of the scientific ethic committee of the Huazhong Agricultural University (HZAUFI-2018-018). As results, we didn\u2019t observe allele mutation for dre-bmp3-gRNA1, so no stable line was generated for this CRISPR. But for dre-bmp3-gRNA2, two bmp3 nonsense alleles with 14 bp insertion ( bmp3 +14 ) and 2 bp deletion ( bmp3 \u22122 ) in the first exon were generated (Supplementary Fig. 12b ), which both caused frame-shift mutations at the 69th AA, and premature transcription termination event at the 161th and 94th AA, respectively. In the F2 mutant bmp3 fish, we observed a series of scale defects, such as decrements in scale numbers, rearrangements, and irregular shapes. The F2 bmp3 +14 mutant fishes gave 4\/29 fish with scale defects whereas 3\/31 had scale defects for F2 bmp3 \u22122 mutant fish. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability All sequencing data generated in this project are available at NCBI under BioProjects PRJNA613175 (PacBio,  ), PRJNA613176 (Hi-C,  ) and PRJNA612146 (Re-sequencing,  ). In addition, processed datasets including custom codes (Datasets 1 \u2013 9 ) are available at Figshare (  ). Source data are provided with this paper. Code availability Custom scripts employed for the analysis of the sequencing data are available at Figshare (  ). ","News_Body":"Seahorses are extremely poor swimmers. Surprisingly, however, they can be found in all of the world's oceans. On the basis of almost 360 different seahorse genomes, a group of researchers studied how these special fish were able to spread so suc-cessfully worldwide. Based on an evolutionary tree of 21 species it was possible to reconstruct the dispersal routes of seahorses worldwide and to explain where and when new species emerged. The international research collaboration involving the research team led by evolutionary biologist Professor Axel Meyer at the University of Konstanz and researchers from China and Singapore was able to identify factors that led to the success of the seahorse from a developmental biology perspective: its quickness to adapt by, for example, repeatedly evolving spines in the skin and its fast genetic rates of evolution. The results will be published on 17 February 2021 in Nature Communications. Seahorses of the genus Hippocampus emerged about 25 million years ago in the Indo-Pacific region from pipefish, their closest relatives. And while the latter usually swim fairly well, seahorses lack their pelvic and tail fins and evolved a prehensile tail instead that can be used, for example, to hold on to seaweed or corals. Early on, they split into two main groups. \"One group stayed mainly in the same place, while the other spread all over the world,\" says Dr. Ralf Schneider, who is now a postdoc-toral researcher at the GEOMAR Helmholtz Centre for Ocean Research Kiel, and participated in the study while working as a doctoral researcher in Axel Meyer's re-search team. In their original home waters of the Indo-Pacific, the remaining species diversified in a unique island environment, while the other group made its way into the Pacific Ocean via Africa, Europe and the Americas. Traveling the world by raft The particularly large amount of data collected for the study enabled the research team to create an especially reliable seahorse tree showing the relationships be-tween species and the global dispersal routes of the seahorse. Evolutionary biologist, Dr. Schneider, says: \"If you compare the relationships between the species to the ocean currents, you notice that seahorses were transported across the oceans.\" If, for example, they were carried out to sea during storms, they used their grasping tail to hold on to anything they could find, like a piece of algae or a tree trunk. These are places where the animals could survive for a long time. The currents often swept these \"rafts\" hundreds of kilometers across the ocean before they landed someplace where the seahorses could hop off and find a new home. Since seahorses have been around for more than 25 million years, it was important to factor in that ocean currents have changed over time as tectonic plates have shift-ed. For example, about 15 million years ago, the Tethys Ocean was almost as large as today's Mediterranean Sea. On the west side, where the Strait of Gibraltar is lo-cated today, it connected to the Atlantic Ocean. On the east side, where the Arabian Peninsula is today, it led to the Indian Ocean. Tectonic shifts change ocean currents The researchers were able to underscore, for example, that the seahorses were able to colonize the Tethys Ocean via the Arabian Sea just before the tectonic plates shifted and sealed off the eastern connection. The resulting current flowing westward towards the Atlantic Ocean brought seahorses to North America. A few million years later, this western connection also closed and the entire Tethys Ocean dried out. Ralf Schneider: \"Until now it was unclear whether seahorses in the Atlantic all traced their lineage to species from the Arabian Sea that had traveled south along the east coast of Africa, around the Cape of Good Hope and across the southern Atlantic Ocean to reach South America. We found out that a second lineage of seahorses had done just that, albeit later.\" Since the research team gathered 20 animal samples from each habitat, it was also possible to measure the genetic variation between individuals. And this generally revealed: The greater the variation, the larger the population. \"We can reconstruct the age of a variation based on its type. This makes it possible to calculate the size of the population at different points in time,\" the evolutionary biologist explains. This calculation reveals that the population that crossed the Atlantic Ocean to North America was very small, supporting the hypothesis that it have come from just a few animals brought there by the ocean's currents while holding on to a raft. The same data also showed that, even today, seahorses from Africa cross the southern Atlantic Ocean and introduce their genetic material into the South American population. Fast and flexible adaptation Seahorses not only spread around the world by traveling with the ocean currents, but they were also surprisingly good at settling in new habitats. Seahorses have greatly modified genomes and, throughout their evolution, they have lost many genes, emerged with new ones or gained duplicates. This means: Seahorses change very quickly in comparison to other fish. This is probably why different types of \"bony spines\" evolved quickly and independently of each other that protect seahorses from predation in some habitats. Some of the genes have been identified that exhibit particular modifications for cer-tain species, but they are not the same for all species. Multiple fast and independent selections led to the development of spines, and although the same genes play a role in this development, different mutations were responsible. This shows that the slower, sessile seahorses were particularly able to adapt quickly to their environments. This is one of the main reasons the research team gives for seahorses being so successful in colonizing new habitats. ","News_Title":"How sessile seahorses speciated and dispersed across the world's oceans in 25 million years","Topic":"Biology"}
{"Paper_Body":"Abstract Topological insulators are a new class of quantum materials that are characterized by robust topological surface states (TSSs) inside the bulk insulating gap 1 , 2 , which hold great potential for applications in quantum information and spintronics as well as thermoelectrics. One major obstacle is the relatively small size of the bulk bandgap, which is typically around 0.3 eV for the known topological insulator materials (ref. 3 and references therein). Here we demonstrate through ab initio calculations that a known superconductor BaBiO 3 (BBO) with a T c of nearly 30 K (refs 4 , 5 ) emerges as a topological insulator in the electron-doped region. BBO exhibits a large topological energy gap of 0.7 eV, inside which a Dirac type of TSSs exists. As the first oxide topological insulator, BBO is naturally stable against surface oxidization and degradation, distinct from chalcogenide topological insulators 6 , 7 , 8 . An extra advantage of BBO lies in its ability to serve as an interface between TSSs and superconductors to realize Majorana fermions for future applications in quantum computation 9 . Main Mixed-valent perovskite oxides based on BBO (refs 4 , 5 ) are, like cuprates, well-known superconductors. The parent compound BBO crystallizes in a mononclinic lattice 10 that is distorted from the perovskite structure, and this distortion is attributed to the coexistence of two valence states, Bi 3+ (6 s 2 ) and Bi 5+ (6 s 0 ), due to charge disproportion of the formal Bi 4+ . Octahedral BiO 6 breathes out and in for Bi 3+ and Bi 5+ , respectively 10 . Under hole-doping conditions, such as in Ba 1\u2212 x K x BiO 3 ( x \u223c 0.4; ref. 5 ) and BaBi 1\u2212 x Pb x O 3 ( x \u223c 0.3; refs 4 , 11 ), the breathing distortion is suppressed, resulting in a simple perovskite lattice 12 in which superconductivity emerges. Recent ab initio calculations 13 have assigned the higher T c superconductivity to a correlation-enhanced electron\u2013phonon coupling mechanism, stimulating the prediction and synthesis of new superconductor candidates among mixed-valent thallium perovskites 14 , 15 , 16 . The existing superconductivity has meant that research has mainly focused on hole-doped compounds, leaving electron-doped compounds relatively unexplored. In addition, the spin\u2013orbit coupling (SOC) effect was not taken into account in previous theoretical study (ref. 13 and references therein), because the electronic states in the superconducting (hole-doped) region mainly result from Bi- 6 s and O- 2 p orbitals whose SOC effect is usually negligible. By including the SOC effect in density-functional theory (DFT) calculations of the BBO band structure, we discovered a band inversion between the first (Bi- 6 s state) and second (Bi- 6 p state) conduction bands, which is stable against lattice distortions. This inversion indicates that BBO is a three-dimensional topological insulator with a large indirect energy gap of 0.7 eV when doped by electrons instead of holes. The band structure of ideal cubic BBO reveals that the conduction bands are modified markedly when SOC is included owing to the presence of the Bi- 6 p states, as illustrated in Fig. 1a . The first conduction band crossing the Fermi energy ( E F ) has a considerable Bi- 6 s contribution over the whole Brillouin zone, except at the R momentum point where the Bi- 6 p contribution is dominant with the Bi- 6 s lying above it. Although one can see an inversion between Bi- 6 p and 6 s states here, there is a zero energy gap at R without SOC because of the degeneracy of the p states. In previous literature that did not employ SOC, actually, this feature was already revealed. When SOC is included, we found that the | p , j = 3\/2\u3009 and | p , j = 1\/2\u3009 states split, which results in the large indirect energy gap of 0.7 eV in the vicinity of the R point. We point out that the band inversion strength is as large as nearly 2 eV, which is the energy difference between Bi- 6 s and | p , j = 1\/2\u3009 states at the R point, as shown in Fig. 1b . Unlike bulk HgTe (ref. 17 ), a well-known topological insulator, this inversion occurs between the | s , j = 1\/2\u3009 state and the | p , j = 1\/2\u3009 state, rather than the | p , j = 3\/2\u3009 state. As the Bi atom is the inversion centre of the perovskite lattice, the Bi- 6 s and Bi- 6 p states have + and \u2212 parities, respectively. Thus, a topological insulator state can be obtained if E F is shifted up into this energy gap. The parities of all the valence bands below this gap were also calculated at all time-reversal invariant momenta, \u0393, X, M and R, which yielded Z 2 topological invariants (1;111), confirming the topological non-trivial feature according to the parity criteria 18 . This is also consistent with a previous study of a topological insulator phase with Z 2 (1;111) in the perovskite lattice based on the model Hamiltonian 19 . At a doping rate of one electron per formula unit, E F shifts inside the s \u2013 p inversion gap, and all the Bi ions become Bi 3+ . Consequently, a cubic phase appears when the BiO 6 breathing distortion is suppressed, similar to the hole-doping case 12 . When the lone-pair Bi- 6 s state is fully occupied, we found that the new cubic lattice expands slightly in comparison with the undoped lattice. Although the s band becomes narrower in this case, the band inversion remains owing to the large s \u2013 p inversion strength (see Supplementary Fig. S1 ). Figure 1: Crystal structures and band structures of BBO. a , Ideal cubic perovskite lattice with the cubic Brillouin zone, and the band inversion process. The Bi atom is represented by the purple ball, O atoms by red balls and Ba atoms by green balls. Without SOC the Bi- s and Bi- p states are already inverted, resulting in two degenerate Bi- p bands at the R point. Subsequently, SOC splits this degeneracy and opens a large energy gap. b , Bulk band structure of the cubic lattice. The dispersions are shown along high-symmetry lines \u0393(0 0 0)\u2013X(0.5 0 0)\u2013M(0.5 0.5 0)\u2013R(0.5 0.5 0.5)\u2013 \u0393, as labelled in the cubic Brillouin zone at the top of a . The Fermi energy is shifted to zero. The red and green dots indicate the Bi- s and Bi- p states, respectively, and corresponding parities are labelled. c , Surface band structure of an electron-doped BBO. The surface normal is along the (001) direction. Dispersions are along (0 0)\u2013 (0.5 0.5)\u2013 (0.5 0) in the surface Brillouin zone. The red lines highlight the topological surface states inside the bulk bandgap. On the right of c is the three-dimensional plot of the surface Dirac cone near the point with helical spin textures. d , Bulk band structure and lattice structure of the monoclinic BiBaO 3 . The original R point of a cubic Brillouin zone in a is projected to the \u0393 point of the monoclinic Brillouin zone in d . Thus the band inversion exists at the \u0393 point in the monoclinic band structure, where Bi- s and Bi- p states are indicated. The effect of SOC is included in band structures b \u2013 d . Full size image To illustrate the TSSs, we calculated the surface band structure using a slab model based on the Wannier functions extracted from the electron-doped bulk band structures. As an example, we take the surface to be oriented along the (001) direction on which the bulk R point is projected onto the point (0.5,0.5,0) of the surface Brillouin zone. The slab is 30 BBO units thick with the outermost atomic layers being Ba\u2013O. The TSSs, shown in Fig. 1c , exhibit a simple Dirac-cone-like energy dispersion. The Dirac cone exhibits square warping at higher energies due to the cubic symmetry of the lattice. The Fermi surface below the Dirac point exhibits a right-hand helical spin-texture on the top surface, similar to that of Bi 2 Se 3 -type topological insulator materials 20 . The spin polarization orients dominantly inside the surface plane with negligible out-of-plane components. The Fermi velocity near the Dirac point is estimated to be approximately 0.75\u00d710 5 m s \u22121 , and inside the large bulk energy gap, the TSSs are well localized on the surface atomic layers to about two BBO units or around 1 nm in thickness. On the other hand, to obtain the minimal effective model of the band topology, we derive a four-band Hamiltonian similar to that for Bi 2 Se 3 (ref. 6 ) in the basis of | p ; j = 1\/2, m j = +1\/2\u3009,| s ; j = 1\/2, m j = +1\/2\u3009,| p ; j = 1\/2, m j = \u22121\/2\u3009, and | s ; j = 1\/2, m j = \u22121\/2\u3009: where k = k 0 \u2212 k R (0.5, 0.5, 0.5) is centred at the R point, and k \u00b1 = k x \u00b1 i k y , \u03b5 0 ( k ) = C + D k 2 and . The main difference from that of the Bi 2 Se 3 Hamiltonian is that equation (1) is isotropic to k owing to the cubic symmetry. We obtain the parameters of equation (1) by fitting the energy spectrum of the effective Hamiltonian to that of the ab initio calculations for the electron-doped cubic BBO using M = \u22120.625 eV, A = 2.5 eV \u00c5, B = \u22129.0eV \u00c5 2 and D = 1.5 eV \u00c5 2 . Subsequently, the Fermi velocity of the TSSs is given by v = A \/ \u210f \u2243 0.5\u00d710 5 m s \u22121 , which is consistent with the ab initio calculations. We can confirm that the topological insulator phase is stable against lattice distortions. The monoclinic phase of BBO, as shown in Fig. 1d , is related to the O-breathing and -tilting distortions. In the monoclinic Brillouin zone, the original R point of a cubic lattice is projected to the \u0393 point owing to band folding. One can see that the s \u2013 p band inversion at this \u0393 point is still present and the indirect gap is unchanged (0.7 eV) in the bulk band structure. In addition, traditional ab initio DFT calculations may overestimate the band inversion owing to the underestimation of the bandgap. Therefore, we performed band structure calculations using the hybrid functional method 21 , which is known to treat the dynamical correlation effect well for BBO (ref. 22 ). Here, we further validated the existence of band inversion for pristine cubic, electron-doped cubic and monoclinic distorted structures. (Details are described in the Supplementary Information .) Experimentally, electron-doped BBO may be achieved in BaBi(O 0.67 F 0.33 ) 3 by substituting F for O atoms. The O and F atom have comparable atomic radii and electronegativities, which can keep the octahedral BiO 6 stable. For example, F substitution for O was applied for the iron-based superconductor LaOFeAs to realize electron doping 23 . It is also possible to employ a state-of-the-art electrolyte gating technique to BBO to induce heavy electron doping, which has been realized for several mixed-valent compounds such as ZrNCl (ref. 24 ) and VO 2 (ref. 25 ). In particular, the electrolyte gating of VO 2 leads to the creation of oxygen vacancies, which induce a stable metallic phase even when removing the electrolyte 25 . As for the VO 2 case, we expect that electrolyte gating can also reach large electron doping by generating considerable oxygen vacancies, which were commonly observed as electron donors for BBO in previous experiments 26 . On the other hand, although the TSSs are unoccupied in pristine BBO compounds, it may be possible to monitor these states directly through monochromatic two-photon photoemission, as was recently employed to monitor the empty TSSs of Bi chalcogenides 27 . Thus far we can state that BBO becomes a superconductor with hole doping and a potential topological insulator with electron doping. If pn-junction-type devices are fabricated with BBO, an interface between the TSSs and the superconductor may be realized, which is necessary for the realization of the Majorana fermion proposal 9 for quantum computation. Here, we outline a double-gated thin-film configuration, as illustrated in Fig. 2 . If the bottom and top regions of the film are predoped as p and n type, respectively, the double-gated structure may feasibly induce a hole-rich bottom surface and an electron-rich top surface, resulting in TSSs and superconductivity states on the top and bottom surfaces, respectively. In the middle region of the slab, the TSSs overlap with the bulk bands and penetrate the bulk. These TSSs can then become superconducting as a result of the proximity effect with the bottom superconducting regime. Such a structure is likely to be attainable as high-quality BBO thin films, which have been successfully grown on SrTiO 3 (refs 28 , 29 , 30 ) and MgO (ref. 30 ) substrates. Moreover, the O-tilting lattice distortion was recently found to be suppressed in a BBO(001) thin film on MgO (ref. 30 ), which is very close to our required cubic structure. Figure 2: Schematic of the interface between the topological insulator and superconducting state in a double-gated thin-film device. The top and bottom surfaces are the topological insulator and superconducting regions, respectively. The position of the Fermi energy (dashed lines) shifts down from the top to bottom surfaces in the band structure. In the middle region, TSSs are interfaced with superconducting states and become superconducting owing to the proximity effect. Full size image The band structure of BaBiO 3 can act as a prototype for designing new perovskite topological insulators. Sc, Y or La can be substituted for Ba to obtain new compounds as analogues of an electron-doped BBO. We found in calculations that a similar band inversion exists in this case. However, these compounds are semimetals (the Sc\/Y\/La- d orbitals are lower in energy than the Bi- p states) and induce topological semimetals (see Supplementary Information ). In contrast, CsTlCl 3 -type halide perovskites, which are predicted to be superconductor candidates 14 , 15 , 16 , have band structures that are similar to BBO. However, we did not observe s \u2013 p inversion for ATlX 3 (A = Cs, Rb, X = F, Cl, Br, I), because the SOC of Tl is not strong enough. When we can substitute Sn or Pb for Tl, we find that heavier members of this family, such as CsPbI 3 , are near the boundary of a topological trivial\u2013non-trivial phase transition. Compressive pressure is necessary to drive these boundary materials into the topological insulator region, which is consistent with recent theoretical calculations of these halides 31 . Methods In band structure calculations, we employed ab initio DFT with the generalized gradient approximation. We employed the Vienna ab initio simulation package with a plane wave basis 32 . The core electrons were represented by the projector-augmented-wave potential. For hybrid-functional calculations, we adopted the HSE06 (ref. 21 ) type of functionals and interpolated the band structures using Wannier functions 33 , where the DFT wavefunctions were projected to Bi- s p , Ba- d and O- p orbitals. We adopted the lattice constants from their experimental values for both the cubic 34 ( a = 4.35 \u00c5) and monoclinic structures 10 . ","News_Body":"Most materials show one function, for example, a material can be a metal, a semiconductor, or an insulator. Metals such as copper are used as conducting wires with only low resistance and energy loss. Superconductors are metals which can conduct current even without any resistance, although only far below room temperature. Semiconductors, the foundation of current computer technology, show only low conduction of current, while insulators show no conductivity at all. Physicists have recently been excited about a new exotic type of materials, so-called topological insulators. A topological insulator is insulating inside the bulk like a normal insulator, while on the surface it shows conductivity like a metal. When a topological insulator is interfaced with a superconductor, a mysterious particle called Majorana fermion emerges, which can be used to fabricate a quantum computer that can run much more quickly than any current computer. Searching for Majorana fermions based on a topological insulator\u2013superconductor interface has thus become a hot race just very recently. Computer-based materials design has demonstrated its power in scientific research, saving resources and also accelerating the search for new materials for specific purposes. By employing state-of-art materials design methods, Dr. Binghai Yan and his collaborators from the Max Planck Institute for Chemical Physics of Solids and Johannes Gutenberg University Mainz (JGU) have recently predicted that the oxide compound BaBiO3 combines two required properties, i.e., topological insulator and superconductivity. This material has been known for about thirty years as a high-temperature superconductor of Tc of nearly 30 Kelvin with p-type doping. Now it has been discovered to be also a topological insulator with n-type doping. A p-n junction type of simple device assisted by gating or electrolyte gating is proposed to realize Majorana fermions for quantum computation, which does not require a complex interface between two materials. In addition to their options for use in quantum computers, topological insulators hold great potential applications in the emerging technology of spintronics and thermoelectrics for energy harvesting. One major obstacle for widespread application is the relatively small size of the bulk band gap, which is typically around 0.3 electron-volts (eV) for previously known topological insulator materials. Currently identified material exhibits a much larger energy-gap of 0.7 eV. Inside the energy-gap, metallic topological surface states exist with a Dirac-cone type of band structures. The research leading to the recent publication in Nature Physics was performed by a team of researchers from Dresden and Mainz around the theoretical physicist Dr. Binghai Yan and the experimental chemists Professor Martin Jansen and Professor Claudia Felser. \"Now we are trying to synthesize n-type doped BaBiO3,\" said Jansen. \"And we hope to be soon able to realize our idea.\" ","News_Title":"Researchers present new multifunctional topological insulator material with combined superconductivity","Topic":"Physics"}
{"Paper_Body":"Abstract We explored human induced pluripotent stem cells (hiPSCs) derived from different tissues to gain insights into genomic integrity at single-nucleotide resolution. We used genome sequencing data from two large hiPSC repositories involving 696 hiPSCs and daughter subclones. We find ultraviolet light (UV)-related damage in ~72% of skin fibroblast-derived hiPSCs (F-hiPSCs), occasionally resulting in substantial mutagenesis (up to 15 mutations per megabase). We demonstrate remarkable genomic heterogeneity between independent F-hiPSC clones derived during the same round of reprogramming due to oligoclonal fibroblast populations. In contrast, blood-derived hiPSCs (B-hiPSCs) had fewer mutations and no UV damage but a high prevalence of acquired BCOR mutations (26.9% of lines). We reveal strong selection pressure for BCOR mutations in F-hiPSCs and B-hiPSCs and provide evidence that they arise in vitro. Directed differentiation of hiPSCs and RNA sequencing showed that BCOR mutations have functional consequences. Our work strongly suggests that detailed nucleotide-resolution characterization is essential before using hiPSCs. Main In regenerative medicine, human induced pluripotent stem cells (hiPSCs) and latterly organoids have become attractive model systems because they can be propagated and differentiated into many cell types. Specifically, hiPSCs have been adopted as a cellular model of choice for in vitro disease modeling as well as being considered for cell-based therapies 1 , 2 , 3 . The genomic integrity and tumorigenic potential of human pluripotent stem cells have been explored previously, but systematic large-scale, whole-genome assessments of mutagenesis at single-nucleotide resolution have been limited 4 , 5 , 6 , 7 , 8 . Human embryonic stem cells (hESCs) cultured in vitro have been reported to harbor TP53 mutations and recurrent chromosomal-scale genomic abnormalities ascribed to selection pressure 9 , 10 , 11 , 12 , 13 , 14 . However, in contrast, a recent study showed a low mutation burden in clinical-grade hESCs, and no cancer driver mutations were detected 15 . The mutational burden in any given hiPSC comprises mutations that were preexisting in the parental somatic cells from which it was derived and mutations that have accumulated over the course of reprogramming, cell culture and passaging 7 , 16 , 17 , 18 , 19 , 20 , 21 , 22 . Several small-scale genomic studies have shown that in some cell lines, preexisting somatic mutations make up a substantial proportion of the total burden 22 , 23 , 24 , 25 , 26 , 27 , 28 . With the advent of clinical trials using hiPSCs (e.g., NCT04339764 ) comes the need to gain in-depth understanding of the mutational landscape and potential risks of using these cells 29 , 30 . Here we contrast skin-derived (F-hiPSCs) and blood-derived (B-hiPSCs) from one individual. We then comprehensively assess hiPSCs from one of the world\u2019s largest stem cell banks, HipSci, and an alternative cohort called Insignia. All lines had been karyotypically prescreened and deemed as chromosomally stable. We utilized combinations of whole-genome sequencing (WGS) and whole-exome sequencing (WES) of 555 hiPSC samples and 141 B-hiPSC-derived subclones (Supplementary Table 1 ) to understand the extent and origin of genomic damage and the possible implications. Results Genomic variations in skin and blood derived hiPSCs To first understand the extent to which the source of somatic cells used to make hiPSCs impacted on mutational load, we compared genomic variation in two independent F-hiPSCs and two independent B-hiPSCs from a 22-year-old healthy adult male (S2) (Fig. 1a ). F-hiPSCs were derived from skin fibroblasts, and B-hiPSCs were derived from peripheral blood endothelial progenitor cells (EPCs). Additionally, we derived F-hiPSCs and B-hiPSCs from six healthy males (S7, oaqd, paab, yemz, qorq and quls) and four healthy females (iudw, laey, eipl and fawm) (Fig. 1a ). Fig. 1: Comparison of mutation burden in EPC-derived and F-hiPSCs. a , Source of hiPSCs. Multiple hiPSC lines created from patient S2 contrasted to fibroblast- and EPC-derived hiPSCs created from ten other individuals. b , Mutation burden of substitutions, double substitutions (first row), substitution types (second row), skin-derived signatures (third row), indel types (fourth row) and rearrangements (lowest row). Supplementary Table 2 provides source information. UV-specific features, such as elevated CC>TT double substitutions and UV mutational signatures, were enriched in F-hiPSCs. Full size image WGS analysis revealed a greater number of mutations in F-hiPSCs as compared to B-hiPSCs in the individual S2 (~4.4 increase), and in lines derived from the other ten donors (Fig. 1b and Supplementary Table 2 ). There were very few structural variants (SVs) observed; thus, chromosomal-scale aberrations were not distinguishing between F-hiPSCs and B-hiPSCs (Supplementary Table 2 ). We noted considerable heterogeneity in the total numbers of mutations between sister hiPSCs from the same donor, S2; one F-hiPSC line (S2_SF3_P2) had 8,171 single substitutions, 1,879 double substitutions and 226 indels, whereas the other F-hiPSC line (S2_SF2_P2) had 1,873 single substitutions, 17 double substitutions and 71 indels (Fig. 1b ). Mutational signature analysis demonstrated striking predominance of UV-associated substitutions (Reference Signature\/COSMIC) Signature 7 (ref. 31 ) in the F-hiPSCs, characterized by C>T transitions at T C A, T C C and T C T (Fig. 1b and Extended Data Fig. 1 ). This finding is consistent with previously published work that attributed UV signatures in hiPSCs to preexisting damage in parental skin fibroblasts 8 , 32 . In contrast, EPC-derived B-hiPSCs did not show any evidence of UV damage but showed patterns consistent with possible oxidative damage (signature 18, characterized by C>A mutations at T C T, G C A and A C A; Fig. 1 and Extended Data Fig. 1 ). Consistent with in vitro studies 33 , 34 , double substitutions were enriched in UV-damaged F-hiPSCs (Fig. 1b ). In all, we concluded that F-hiPSCs carry UV-related genomic damage as a result of sunlight exposure in vivo that does not manifest in EPC-derived B-hiPSCs. Importantly, screening for copy-number aberrations underestimated the substantial substitution\/indel-based variation that exists in hiPSCs. High prevalence of UV-associated DNA damage in F-hiPSCs We asked whether these findings were applicable across F-hiPSC lines generally. Therefore, we interrogated all lines in the HipSci stem cell bank, comprising 452 F-hiPSCs generated from 288 healthy individuals (Fig. 2a and Supplementary Table 3 ). These F-hiPSC lines were generated using Sendai virus, cultured on irradiated mouse embryonic fibroblast feeder cells, which have been reported to help reduce genetic instability 20 and were expanded before sequencing (range, 7\u201346 passages; median, 18 passages; Extended Data Fig. 2 ) 35 . WGS data were available for 324 F-hiPSC and matched fibroblast lines (Fig. 2a ). We sought somatic mutations and identified 1,365,372 substitutions, 135,299 CC>TT double substitutions and 54,390 indels. Large variations in mutation distribution were noted, including 692\u201337,120 substitutions, 0\u20137,864 CC>TT double substitutions and 17\u2013641 indels per sample, respectively (Fig. 2b and Supplementary Table 4 ). Fig. 2: Mutation burden and mutational signatures in F-hiPSCs. a , Summary of HipSci F-hiPSC dataset. A total of 452 F-hiPSCs were generated from 288 healthy donors. A total of 324 hiPSCs were whole-genome sequenced (coverage, 41\u00d7), 381 were whole-exome sequenced (coverage, 72\u00d7), and 106 had their matched fibroblasts whole-exome sequenced with high-coverage (hcWES) (coverage, 271\u00d7). Supplementary Table 3 provides source information. Numbers within black circles denote the number of F-hiPSCs that had data from multiple sequencing experiments. b , Mutation burden of substitutions (subs), CC>TT double substitutions and indels in F-hiPSCs from WGS. Data summary is provided in Supplementary Table 4 . Black dots and error represent mean \u00b1 standard deviation of hiPSC observations, n = 324 WGS of hiPSCs. c , Distribution of mutational signatures in 324 F-hiPSC lines. The inset figure shows the relative exposures of mutational signatures types. d , Relationships between mutation burdens of CC>TT double substitutions and UV-caused mutation burden of substitutions in F-hiPSCs, n = 324 WGS of hiPSCs. e , f , Histograms of aggregated mutation burden on transcribed (red) and nontranscribed (cyan) strands for C>T ( e ) and CC>TT ( f ), n = 324 WGS of hiPSCs. Transcriptional strand asymmetry across replication timing regions was observed. g , h , Distribution of substitution burden of F-hiPSCs with respect to donor\u2019s age ( g ) and gender ( h ). Full size image Mutational signature analysis 31 revealed that 72% of F-hiPSCs carried detectable substitution signatures of UV damage (Fig. 2c ). hiPSCs with greater burden of UV-associated substitution signatures showed strong positive correlations with UV-associated CC>TT double substitutions 36 , 37 (Fig. 2d and Extended Data Fig. 3 ) and demonstrated clear transcriptional strand bias with an excess of C>T and CC>TT on the nontranscribed strand, enriched more in early replication timing domains 38 , 39 , 40 , 41 than in late ones (Fig. 2e,f ). These findings are consistent with previous reports of UV-related mutagenesis observed in fibroblasts 36 , 37 , 38 , 39 , 40 , 41 . Of note, similar to findings of UV damage in skin 42 , there was no correlation between total mutation burden of F-hiPSCs and donor age or gender (Fig. 2g,h ). Substantial genomic heterogeneity between F-hiPSCs clones F-hiPSCs comprise the majority of hiPSCs in stem cell banks globally and are a prime candidate for use in disease modeling and cell-based therapies. Yet, we and others observed substantial heterogeneity between hiPSC sister lines generated from one reprogramming experiment (Fig. 1b , subject S2) 5 , 6 , 23 , 32 . It has been postulated that this heterogeneity may result from the presence of genetically diverse clones within the fibroblasts. To explore this further, we compared mutational profiles of 118 pairs of F-hiPSCs present in HipSci, each pair having resulted from the same reprogramming experiment. In all, 54 pairs (46%) of hiPSCs shared more than ten mutations and had similar substitution numbers and profiles (cosine similarity >0.9; Fig. 3a ). The remaining 64 hiPSC pairs (54%) shared ten or fewer substitutions and were dissimilar in burden and profile (cosine similarity <0.9; Fig. 3a ). We found some striking differences; for example, the F-hiPSC line HPSI0314i-bubh_1, derived from donor HPSI0314i-bubh, had 900 substitutions with no UV signature, whereas HPSI0314i-bubh_3 had 11,000 substitutions with representation mostly from UV-associated damage (>90%). Analysis of the parental fibroblast line HPSI0314i-bubh showed some, albeit reduced, evidence of UV-associated mutagenesis. Hence, we postulate that F-hiPSCs from the same reprogramming experiment could show considerable variation in mutation burden because of different levels of sunlight exposure to each parental skin fibroblast. Fig. 3: Genomic heterogeneity in F-hiPSCs. a , Comparison of substitutions carried by pairs of hiPSCs that were derived from fibroblasts of the same donor and in the same reprogramming experiment. Each circle represents a single donor. Hollow circles indicate that two hiPSCs from the same donor share fewer than ten mutations, whereas filled circles indicate that they share ten or more mutations. Colors denote cosine similarity values between mutation profiles of two hiPSCs. A very high score (purple hues) indicates a strong likeness. n = 164 fibroblasts with two derived hiPSCs. b , Correlations between number of mutations in hiPSCs and their matched fibroblasts, n = 324 WGS of hiPSCs. c , Summary of subclonal clusters in fibroblasts ( n = 204 WGS of fibroblasts) and hiPSCs ( n = 324 WGS of hiPSCs). Kernel density estimation was used to smooth the distribution. Local maximums and minimums were calculated to identify subclonal clusters. Each dot represents a cluster that has at least 10% of total mutations in the sample. Most fibroblasts are polyclonal with a cluster VAF near 0.25, whereas hiPSCs are mostly clonal, with VAF near 0.5. d , Schematic illustration of genomic heterogeneity in F-hiPSCs. Through bulk sequencing of fibroblasts, all of these cells will carry all the mutations that were present in the gray cell, their most recent common ancestor. The individual cells will also carry their unique mutations depending on the DNA damage received by each cell. Each hiPSC clone is derived from a single cell. Subclone 1 and subclone 2 cells are more closely related and could share a lot of mutations in common, because they share a more recent common ancestor. However, they are distinct cells and will create separate hiPSC clones. Subclone 6 (orange cells) and subclone 12 (red cells) are not closely related to the green cells and have received more DNA damage from UV, making them genomically divergent from the green cells. They could still share some mutations in common, because they shared a common ancestor at some early point, but will have many of their own unique mutations (largely due to UV damage). Full size image To investigate further, we analyzed bulk-sequenced skin fibroblasts, which revealed high burdens of substitutions, CC>TT double substitutions and indels (Extended Data Fig. 4a ) consistent with UV exposure in the majority of fibroblasts (166\/204) (Extended Data Fig. 4b ). Mutation burdens of F-hiPSCs were positively correlated with their matched fibroblasts (Fig. 3b ), directly implicating the fibroblast population as the root cause of F-hiPSC mutation burden and heterogeneity. Investigating variant allele frequency (VAF) distributions of somatic mutations in F-hiPSCs and fibroblasts demonstrated that most F-hiPSC populations were clonal (VAF = 0.5), whereas most fibroblast populations showed oligoclonality (VAF < 0.5) (Fig. 3c and Extended Data Fig. 5 ). At least two peaks were observed in VAF distributions of fibroblasts: a peak close to VAF = 0 (representing the neutral tail due to accumulation of mutations, which follows the power law distribution) and a peak close to VAF = 0.25 (representing the subclones in the fibroblast population). From the mean VAF of the cluster, we can estimate the relative size of the subclones (e.g., VAF = 0.25, indicating that the subclone occupies half of the fibroblast population 43 ). According to this principle, we found that ~68% of fibroblasts contain oligoclonal populations with a VAF < 0.25. The mutational burden of F-hiPSCs is thus dependent on which specific cells they were derived from in that oligoclonal fibroblast population. F-hiPSCs derived from the same subclone will be more similar to each other, whereas hiPSCs derived from different subclones within the fibroblast population could have hugely different mutation burdens (Fig. 3d ). There are important implications that arise from subclonal heterogeneity observed in fibroblasts. First, when detecting somatic mutations, it is preferable to compare the F-hiPSC genome to a matched germline sample if possible; otherwise, F-hiPSC mutations that are also present in a prominent fibroblast subclone will be dismissed as germline variants, giving a false sense of low DNA damage in F-hiPSCs (Supplementary Fig. 1 ). Indeed, ~95% of HipSci F-hiPSCs had some shared mutations with matched fibroblasts (Extended Data Fig. 6 ) that demonstrated a strong UV signature (Extended Data Fig. 7 ). Second, some F-hiPSC mutations may be present in the parental fibroblast population but not detected through lack of sequencing depth. In comparing WES data of the originating fibroblasts, at standard and at high coverage (hcWES), we found that an increased sequencing depth uncovered additional coding mutations that had been acquired in vivo; WES data showed 47% of coding mutations detected in hiPSCs were shared with matched fibroblasts, compared to 64% using hcWES (Extended Data Fig. 8 ). The additional 17% of mutations identified only in hcWES exhibited a strong UV substitution signature (Extended Data Fig. 8 ). suggesting that they may have been acquired in vivo and have been present within the parental fibroblast population but undetected at standard sequence coverage. Given recent sequencing studies that have demonstrated a high level of cancer-associated mutations in normal cells 37 , 44 , 45 , 46 , 47 , 48 , it is therefore probable that some mutations identified in hiPSCs but not detected in corresponding fibroblasts were still acquired in vivo and not during cell culture. Third, this work highlights the need for careful clone selection and comprehensive genomic characterization, as reprogramming can produce hiPSC clones with vastly different genetic landscapes. Strong selection for BCOR mutations in hiPSCs When we mapped hiPSC coding mutations to the COSMIC Cancer Gene Census (a proxy for genes that are selected for), we found a total of 272 mutations in 145 of these cancer genes, across 177 lines (177\/452, 39%) from 137 donors. However, it is possible that some of these mutations are passenger mutations, given the heavy mutagenesis from UV damage in some hiPSCs. Hence, to determine potential selective advantage of coding mutations to F-hiPSCs, we conducted an agnostic analysis of selective pressure based on the ratio of divergence of non-synonymous to synonymous substitutions or dN\/dS and hotspot analysis across the HipSci F-hiPSCs cohort by examining all genes in the genome 49 . Although several cancer genes were hit across multiple F-hiPSC lines (Supplementary Table 5 ), only BCOR was found to demonstrate statistically significant positive selection ( q = 3.64 \u00d7 10 \u22128 ; Fig. 4a and Supplementary Table 6 ) using dNdScv 49 . To increase the statistical power for detection of selection, a restricted hypothesis test of known cancer genes 49 still revealed only BCOR mutations as significant. Interestingly, all the BCOR mutations found in 11 F-hiPSCs were truncating mutations and predicted to be pathogenic (Fig. 4b ). No reads containing these BCOR mutations were seen in any founding fibroblast, even when some fibroblasts were sequenced to high coverage (>150\u00d7, Fig. 4b ), indicating that BCOR mutations observed in F-hiPSCs were more likely to have arisen in vitro. Fig. 4: Selection analysis revealed positive selective forces on BCOR mutations in F-hiPSCs and B-hiPSCs. a , Driver discovery workflow. Each BCOR mutation was curated using a genome browser. b , Number of reads reporting BCOR mutations in 11 F-hiPSCs and their founding fibroblasts. Colors are indicative of whether the sequencing read contains a mutation or otherwise (green, reads with reference (REF) allele; orange, reads with alternate (ALT) allele). c , BCOR mutation status of Insignia B-hiPSCs. Not shown are 50 of 78 B-hiPSCs that do not carry any BCOR variants in parental hiPSCs or their corresponding subclones. Rows indicate B-hiPSCs derived from various donors, including patients with genetic defects and healthy controls. Columns indicate parental hiPSC (iPSC) and daughter subclones (s1\u2013s4). BCOR mutation status is shown in brown if at least one read contains a BCOR mutation in the sample or blue if there are no reads containing BCOR mutations in that sample. Gray indicates that a sample was not available (N\/A). Total count of substitutions and indels of each iPSC is shown in histogram on the right. d , Schematic illustration of positions of mutations in BCOR protein in all F-hiPSCs (blue) and B-hiPSCs (purple). e , Exploring relationship between hiPSC mutation burden and BCOR status (two-sided Mann\u2013Whitney test). hiPSCs with BCOR hits are highlighted with a darker shade of purple (B-hiPSCs) and blue (F-hiPSCs). MT, mutant; WT, wild-type. Full size image We extended our analysis to B-hiPSCs in the HipSci cohort. These B-hiPSCs were generated from erythroblasts derived from peripheral blood. Three of 17 sequenced B-hiPSCs carried mutations in BCOR (Supplementary Table 7 ), a higher proportion than F-hiPSCs in HipSci ( P = 0.0076, binomial test). However, many B-hiPSCs in the HipSci cohort did not have matched germline samples to perform subtraction of germline variation, rendering it possible (even if unlikely) for the BCOR mutations to be germline in origin. Therefore, we sought alternative cohorts of hiPSCs. Blood derivation methods for generating hiPSCs have been gaining popularity due to the ease of sample collection but are much less common than F-hiPSCs, and large cohorts of genomically characterized B-hiPSCs do not exist. Nevertheless, we accessed erythroblast-derived B-hiPSCs created from 78 individuals who were part of the Insignia project 50 , comprising 53 patients with inherited DNA repair defects, 5 patients with exposure to environmental agents and 20 healthy controls (Supplementary Table 8 and Supplementary Note ); WGS was performed, and somatic mutations were identified. dN\/dS analysis on all coding variants of the 78 B-hiPSCs showed that only BCOR was under significant positive selection 49 ( q = 0; Fig. 4a and Supplementary Tables 9 and 10 ), consistent with F-hiPSCs, but present at a much higher prevalence in 21 (26.9%) B-hiPSC lines. Hotspot analysis did not find any recurrently mutated sites in BCOR in B-hiPSC lines. Source of recurrent BCOR mutations BCOR encodes for the BCL6-corepressor protein and is a member of the ankyrin repeat domain containing gene family. The corepressor expressed by BCOR binds to BCL6, a DNA-binding protein that acts as a transcription repressor for genes involved in regulation of B cells. Somatic mutations in BCOR have been reported in hematological malignancies, including acute myeloid leukemia and myelodysplastic syndromes 51 , 52 , 53 , and has also been reported at a low prevalence in other cancers, including lung, endometrial, breast and colon cancers 54 . The high prevalence of BCOR mutations in B-hiPSCs, but not F-hiPSCs, led us to ask whether these could have been derived from hematopoietic stem cell clones that were present in the donors. Clonal hematopoiesis (CH) has been reported in older individuals 55 , 56 . Some of the most common genes that are mutated in CH include DNMT3A, TET2, ASXL1, JAK2 and TP53 57 , 58 , 59 . BCOR is not one of the frequently mutated CH genes but has been reported in rare cases of aplastic anemia 53 . However, DNMT3A , reported once in our cohort, could represent a B-hiPSC derived from a CH clone, as the mutation p.G543V has also been reported as a CH variant 60 . Another possibility is that BCOR dysregulation is selected for in the culture process particularly in erythroblast-derived hiPSCs. To examine these possibilities, we asked whether the BCOR variants could be detected earlier in the B-hiPSC derivation process, either at the erythroblast population stage or in the germline DNA sample. We did not observe any of these BCOR variants in any of the sequencing reads in either erythroblast or germline DNA samples. It is nevertheless possible that CH clones escaped detection through lack of very deep sequencing depth. Notably, 6 of 21 BCOR mutations were present at lower VAFs (VAF < 0.3) in the B-hiPSCs and could represent subclonal BCOR variants within the parental B-hiPSC clone. This would suggest that BCOR mutations were arising because of ongoing selection pressure in culture following erythroblast derivation or after reprogramming. To investigate further, we cultured parental B-hiPSCs for 12\u201315 days. A minimum of two (and up to four) subclones were derived from the parental lines (Fig. 4c and Supplementary Table 11 ). A total of 141 subclones were assessed by WGS (Supplementary Table 12 ). In all parental B-hiPSC clones where BCOR mutations were identified and where daughter subclones were successfully generated, the BCOR variants were recapitulated in related daughter subclones, serving as an internal validation of those BCOR variants (Fig. 4c and Supplementary Table 10 ). Interestingly, 7 additional B-hiPSCs that did not have BCOR mutations in the parental B-hiPSC population developed new BCOR mutations in daughter subclones; MSH71 had p.P1229fs*5 BCOR mutations in both subclones but not the parent, MSH68 had p.D1118fs*22 BCOR mutations in one of two subclones, MSH13 had p.P1115fs*45 BCOR mutation in one of four subclones, MSH29 had a p.S1122fs*37 BCOR mutation in one of two subclones, MSH90 had a p.D1118fs*44 in BCOR in two subclones, MSH3 had a p.S158fs*28 BCOR mutation in all three subclones and MSH41 had p.R1398fs*4 in BCOR in two subclones (Fig. 4c and Supplementary Table 10 ). Given that these BCOR mutations are present in some, but not all, subclones and are not present at a detectable frequency in the parental population, this finding suggests that they have arisen late in culture of the parental B-hiPSC. All BCOR mutations in B-hiPSCs were predicted to be truncating variants distributed throughout the gene (Fig. 4d ). No correlation was found between mutation burden in parental line and BCOR status in either F-hiPSCs ( P = 0.77, Mann\u2013Whitney test) or B-hiPSCs ( P = 0.55, Mann\u2013Whitney test), indicating that BCOR mutations were not simply mutations in hypermutated samples (Fig. 4e ). Therefore, in this analysis, we found a high prevalence of recurrent BCOR mutations in two independent cohorts of erythroblast-derived B-hiPSCs (18% of HipSci and 27% of Insignia (~25% overall, across all B-hiPSCs)). We did not find BCOR mutations in originating erythroblasts or in germline DNA samples. Instead, single-cell-derived subclones variably carried new BCOR mutations, suggesting that there may be selection for BCOR dysfunction post-reprogramming in B-hiPSCs. We cannot definitively exclude CH as the source of BCOR mutations, as extremely high sequencing depths may be required to detect CH clones. However, BCOR is not frequently mutated in CH. The majority of donors were young (<45 yr), and some were healthy controls. All arguments against BCOR being due to CH, and more likely due to selection pressure in B-hiPSCs in culture. That BCOR mutations are seen at a lower frequency in F-hiPSCs and never in fibroblasts hints at a culture-related selection pressure (Fig. 4b ). Why BCOR mutations are more enriched in B-hiPSCs remains unclear but may be related to the process of transforming peripheral blood mononuclear cells (PMBCs) toward the myeloid lineage. Global transcriptional changes in BCOR -mutated B-hiPSCs To understand the functional impact of recurrent BCOR variants in B-hiPSCs, we performed RNA sequencing on B-hiPSCs from all 78 Insignia donors. Global transcriptomic analysis revealed two principal components (PCs) driving variance in the dataset (Fig. 5a ). The first PC distinguished two groups, with almost all the BCOR -mutated B-hiPSCs (VAF > 0, highlighted in yellow or orange in Fig. 5a ) restricted to one group, herewith termed BCOR -mut. The other group comprised B-hiPSCs with no BCOR mutations, BCOR -wt (VAF = 0, highlighted in gray or blue in Fig. 5a ). The second PC distinguished the donors by gender; BCOR is on the X chromosome. This result suggests that BCOR mutations are associated with important global transcriptional changes in B-hiPSCs. Differential gene expression analysis showed that 10,486 genes were differently regulated in the BCOR -mut lines (Fig. 5b and Supplementary Table 13 ). Furthermore, we found that UTF1 (implicated in maintenance of pluripotency through chromatin regulation), VENTX , IRX4 , PITX2 and MIXL1 (all homeobox-related proteins with various roles in embryonic patterning) and FOXC1 (important in the development of organs derived from the mesodermal-lineage) were strongly upregulated in BCOR-mut lines, whereas RAX (involved in development of the hypothalamus and retina) was strongly downregulated (Fig. 5b ). Fig. 5: Functional validation of the impact of BCOR mutations in hiPSCs and their differentiation potential. a , PC analysis showing hiPSCs distribution based on their transcriptome expression. Transcriptomics changes are correlated with BCOR mutation VAF values. 0* denotes that no BCOR mutations were found in hiPSC parental lines but were seen in their subclones. b , Volcano plot of differential gene expression analysis in hiPSCs with BCOR mutations ( BCOR -mut) and no BCOR mutations ( BCOR -wt) lines. FC, fold change. c , Heatmap of relative expression levels of members of noncanonical polycomb repressive complex 1.1 (PRC1.1), hiPSCs pluripotency and the three germ layers markers for two BCOR -wt (blue: MSH34i2, MSH30i3) and two BCOR -mut (orange: MSH40i2, MSH93i6) hiPSC lines. d , Immunofluorescence characterization of hiPSCs, neural stem cells (NSCs) and neurons at day 0, day 12 and day 27, respectively. Both BCOR -mut and BCOR -wt hiPSCs have similar undifferentiated morphology and express pluripotency markers OCT4\/POU5F1 (red) and SSEA4 (green). BCOR -mut lines undergo inefficient neural induction, as highlighted by a reduced number of NSCs expressing PAX6 (green) at day 12 and a reduced number of neurons marked by TUBB3 (green) at day 27. Full size image BCOR -mutant hiPSCs have impaired differentiation capacity BCOR is a member of the polycomb group of proteins (PRC1.1) that regulates self-renewal and differentiation of stem cells and is critical for maintaining pluripotency 61 . BCOR depletion has been associated with decreased polycomb repressive activity, initiating differentiation toward the endodermal and mesodermal lineages 61 . Thus, we next asked whether the differentiation capacity was compromised in BCOR -mut lines, particularly for ectodermal lineages. We performed directed differentiation toward a neuronal lineage, contrasting two independent BCOR -wt B-hiPSC lines, MSH34i2 and MSH30i3, and two independent BCOR -mut B-hiPSC lines, MSH40i2 and MSH93i6 (Fig. 5c,d ), each with three biological replicates. To capture dynamic shifts through directed differentiation, we took samples during a time course, characterizing these lines morphologically and transcriptionally on day 0 as hiPSCs, day 6 and day 12 representative of early and late neural stem cell (NSC) induction stages respectively and on day 27 as neurons. There were no overt morphological disparities between BCOR -mut and BCO R-wt colonies at the hiPSC stage, with near-identical brightfield images and immunofluorescence characterization of pluripotent markers SSEA4 and OCT4 (day 0; Fig. 5d , Extended Data Figs. 9 and 10a ). However, upon NSC induction (days 6 and 12), BCOR -mut replicates showed inefficiency in differentiation confirmed by patchy expression of NSC marker PAX6 (Fig. 5d and Extended Data Fig. 10b ). At day 27, neuronal generation in BCOR -mut lines was markedly affected, as seen in depletion of neuronal marker TUBB3 (Fig. 5d and Extended Data Fig. 10c ). In keeping with the morphological characterization, global transcriptomics of each hiPSC line at each stage of differentiation revealed differences between BCOR -mut and BCOR -wt B-hiPSCs (Supplementary Fig. 2 ). At the pluripotent stage, BCOR -mut hiPSCs exhibited a modest reduction in BCOR expression but elevated levels of NANOG , KLF4 and NODAL compared to BCOR -wt (Fig. 5c ), similar to reports in other pluripotent models with BCOR -PRC1.1 defects 61 . At the NSC induction stage (days 6 and 12), transcriptional dynamics evolved to show that mesodermal markers such as PAX7 , TBX1 and PAX3 were substantially elevated in the BCOR -mut compared to BCOR -wt B-hiPSCs, implicating a drive toward mesodermal lineages in BCOR compromised lines (Fig. 5c ). This difference was maintained at late neuronal differentiation (day 27), where neuronal markers such as TUBB3 , DCX and FOXG1 were upregulated in BCOR -wt but not BCOR -mut B-hiPSCs, underscoring the failure of neuronal differentiation in the latter (Fig. 5c ). In all, these results demonstrate that B-hiPSC lines that acquire BCOR mutations may have compromised differentiation potential. BCOR -mutated lines seem to be less efficient at differentiation toward neuronal lineages and transcriptionally appear primed to differentiate toward mesodermal lineages. Long-term culture increases 8-oxo-dG DNA damage Finally, we examined the genomic effects of in vitro hiPSC culture. Mutational signature analyses revealed that all hiPSCs, regardless of primary cell of origin, have imprints of substitution signature 18, previously hypothesized to be due to oxidative damage in culture 22 (Figs. 1b and 2c ). Recently, knockouts of OGG1 , a gene encoding a glycosylase in base excision repair that specifically removes 8-oxoguanine (8-oxo-dG), have been shown to result in mutational signatures characterized by C>A at A C A>A A A, G C A>G A A, G C T>G A T, identical to signature 18 (ref. 62 ). Of note, signature 18 has also been reported in other cell culture systems such as in ES cells 15 , near haploid cell lines 63 and human tissue organoids in which the contribution to the overall mutation burden was reported to increase with in vitro culture 64 , 65 . We examined mutations shared between hiPSCs and their matched fibroblasts, representing in vivo and\/or early in vitro mutations, and private mutations that are only present in hiPSCs, most (but not all) of which are likely representative of mutations acquired in vitro. We observed that signature 18 is enriched among private mutations of nearly all hiPSCs (313 or 97%). By contrast, the majority of hiPSCs (262 or 80%) showed no evidence of signature 18 among shared variants (Fig. 6a\u2013c and Supplementary Fig. 3 ). We then investigated the relationship between signature 18 and passage number in the HipSci cohort. We found that there was a positive correlation between signature 18 and passage number (correlation = 0.327; P = 5.013 \u00d7 10 \u22129 ; Fig. 6d ), reinforcing the notion that prolonged time in culture is likely to be associated with increased acquisition of somatic mutations through elevated levels of DNA damage from 8-oxo-dG (Fig. 6e ). Fig. 6: Culture-associated mutagenesis in hiPSCs. a , Mutations in F-hiPSCs were separated into two groups: (1) mutations shared between F-hiPSCs and their founding fibroblasts (left) and (2) mutations that are private to hiPSCs (right). Proportional graphs show substitution signatures of shared and private mutations. Culture-related signature (signature 18) is enriched in private mutations. b , Box plot of exposures of substitution signatures of shared (blue) and private (orange) mutations. Culture signatures (signature 18) account for ~50% of private substitutions, in contrast to nearly zero among shared mutations. Box plots denote median (horizontal line) and 25th to 75th percentiles (boxes). The lower (minima) and upper (maxima) whiskers extend to 1.5\u00d7 the interquartile range. n = 324 WGS of hiPSCs. c , Number of samples carrying each signature within shared or private mutations. Most samples acquired the culture signature late in the hiPSC cloning process. d , Relationship between exposure of culture-related signature (signature 18) and passage number (Pearson\u2019s correlation test, two sided). e , Schematic illustration of the mutational processes in fibroblasts and F-hiPSCs. Full size image Discussion hiPSCs are on the verge of entering clinical practice. Therefore, there is a need to better understand the breadth and source of mutations in hiPSCs to minimize the risk of harm. Crucially, our work shows that the choice of a starting material for hiPSC derivation is complex. We demonstrate copious UV-associated genomic damage and substantial variation in genomic integrity between different clones from the same reprogramming experiment. In all, 39% of F-hiPSCs carried at least one mutation in a cancer driver gene. However, only BCOR mutations showed evidence of statistically significant positive selection, both in F-hiPSCs and in B-hiPSCs (prevalence of 25%), despite lower rates of genome-wide mutagenesis in the latter. The lack of BCOR mutations in the founder somatic cell populations (fibroblasts and erythroblasts) and the finding of new BCOR mutations arising in subclones during propagation experiments provides support for strong selection for BCOR variants in hiPSC culture. Furthermore, our work demonstrates that these mutations are associated with transcriptomic changes and influence differentiation of hiPSCs. Finally, both F-hiPSCs and B-hiPSCs exhibit oxidative damage that is increased by long-term culture. Critically, all of these lines had been previously screened for large-scale aberrations, and therefore, our work shows the value of WGS to fully capture variation at the resolution of nucleotides in hiPSCs. Further work will be necessary to investigate the functional significance of these mutations; for example, do they predispose cells toward malignant transformation or alter the fate of differentiated progeny? Our work highlights best practice points to consider when establishing hiPSC cellular models: an originating somatic cell type with low levels of preexisting genomic damage and minimizing duration of cell culture. Ultimately, however, comprehensive genomic characterization is indispensable to fully understand the magnitude and significance of mutagenesis in all cellular models. Methods Samples The research project complied with all relevant ethical regulations, and the protocols were approved by research ethics committees (details below). All participants were recruited voluntarily; provided written, informed consent; and were not financially compensated. The procedures used to derive fibroblasts and EPCs from two donors (S2 and S7) were previously described 22 . Two F-hiPSCs were obtained from S2. Four and two EPC B-hiPSCs were obtained from S7 and S2, respectively. A total of 452 F-hiPSCs were obtained from 288 donors and 17 erythroblast B-hiPSCs were obtained from 9 donors from the HipSci project. A total of 78 erythroblast B-hiPSCs and 141 subclones were obtained from 78 donors from the Insignia project. The details of HipSci and Insignia iPSC lines are described below. HipSci hiPSC line generation and growth All lines were incubated at 37 \u00b0C and 5% CO 2 (ref. 35 ). Primary fibroblasts were derived from 2-mm punch biopsies collected from organ donors or healthy research volunteers recruited from the NIHR Cambridge BioResource under ethics for hiPSC derivation 35 (Cambridgeshire and East of England Research Ethics Committee REC 09\/H306\/73, REC 09\/H0304\/77-V2 04\/01\/2013, REC 09\/H0304\/77-V3 15\/03\/2013). Biopsy fragments were mechanically dissociated and cultured with fibroblast growth medium (knockout DMEM with 20% fetal bovine serum; 10829018, ThermoFisher Scientific) until outgrowths appeared (within 14 days, on average). Approximately 30 days following dissection, when fibroblasts cultures had reached confluence, the cells were washed with phosphate-buffered saline (PBS), passaged using trypsin into a 25-cm 2 tissue-culture flask and then again to a 75-cm 2 flask upon reaching confluence. These cultures were then split into vials for cryopreservation and those seeded for reprogramming, with one frozen vial later used for DNA extraction for WES or WGS. For erythroblast derivation, two blood samples were obtained for each donor. All samples were scanned and checked for ethical approval. Peripheral blood mononuclear cell (PBMC) isolation, erythroblast expansion and hiPSC derivation were done by the Cellular Generation and Phenotyping facility at the Wellcome Sanger Institute, Hinxton. Briefly, whole-blood samples collected from consented patients were diluted with PBS, and PBMCs were separated using standard Ficoll Paque density gradient centrifugation method. Following the PBMC separation, cells were cultured in expansion media containing StemSpan H3000, stem cell factor, interleukin-3, erythropoietin IGF-1 and dexamethasone for a total of 9 days 66 . The EPCs were isolated using Ficoll separation of 100 ml peripheral blood from organ donors (REC 09\/H306\/73) and the buffy coat transferred onto a 5 \u03bcg\/cm 2 collagen (BD Biosciences, 402326)-coated T-75 flask. The EPCs were grown using EPC media (EGM-2MV supplemented with growth factors, ascorbic acid plus 20% Hyclone serum; CC-3202, Lonza and HYC-001-331G; ThermoFisher Scientific Hyclone respectively) 67 . EPC colonies appeared after 10 days, and these were passaged using trypsin in a 1 in 3 ratio and eventually frozen down using 90% EPC media and 10% dimethyl sulfoxide. Fibroblasts and erythroblasts were transduced using nonintegrating Sendai viral vectors expressing human OCT3\/4, SOX2, KLF4 and MYC51 (CytoTune, Life Technologies, A1377801) according to the manufacturer\u2019s instructions and cultured on irradiated mouse embryonic fibroblasts (MEFs; CF1). The EPCs were transduced using four Moloney murine leukemia retroviruses containing the coding sequences of human OCT4, SOX2, KLF4 and C-MYC and also cultured on irradiated MEFs. Following all reprogramming experiments, the medium was changed to hiPSC culture medium 35 containing advanced DMEM (Life Technologies), 10% knockout serum replacement (Life Technologies), 2 mM l -glutamine (Life Technologies), 0.007% 2-mercaptoethanol (Sigma-Aldrich) and 4 ng ml \u22121 recombinant zebrafish fibroblast growth factor 2 (CSCR, University of Cambridge) and 1% penicillin\/streptomycin (Life Technologies). Cells with an iPSC morphology first appeared approximately 14 to 28 days after transduction, and undifferentiated colonies (six per donor) were picked between days 28 and 40, transferred onto 12-well MEF-CF1 feeder plates and cultured in hiPSC medium with daily medium changes until ready to passage. Successful reprogramming was confirmed via genotyping array and expression array 35 . Pluripotency quality control (QC) was performed based on the HipSci QC steps, including the PluriTest using expression microarray data from the Illumina HT12v4 platform and copy number variation and loss of heterozygosity (CNV\/LOH) detection using the HumanExome BeadChip Kit platform. Pluripotent hiPSC lines were transferred onto feeder-free culture conditions, using 10 \u00b5g ml \u22121 Vitronectin XF (Stemcell Technologies)-coated plates and Essential 8 (E8) medium (DMEM\/F12 (HAM), E8 supplement (50\u00d7) and 1% penicillin\/streptomycin; Life Technologies) 35 . The media was changed daily, and cells were passaged every 5\u20137 days, depending on the confluence and morphology of the cells, at a maximum 1:3 split ratio until established, usually at passage five or six. The passaging method involved washing the confluent plate with PBS and incubating with PBS-EDTA (0.5 mM) for 5\u20138 min. After removing the PBS-EDTA, cells were resuspended in E8 media and replated onto Vitronectin-coated plates 35 . Once the hiPSCs were established in culture, lines were selected based on morphological qualities (undifferentiated, roundness and compactness of colonies) and expanded for banking and characterization. DNA from fibroblasts and hiPSCs was extracted using Qiagen Chemistry on a QIAcube automated extraction platform. HipSci hiPSC line whole-exome genome library preparation and sequencing A 96-well plate containing 500 ng genomic DNA in 120 \u00b5l was cherry-picked and an Agilent Bravo robot used to transfer the gDNA into a Covaris plate with glass wells and adaptive focused acoustics (AFA) fibers. This plate was then loaded into the LE220 for the shearing process. The sheared DNA was then transferred out of this plate and into an Eppendorf TwinTec 96 plate using the Agilent Bravo robot. Samples were then purified ready for library prep. In this step, the Agilent NGS Workstation transferred AMPure-XP beads and the sheared DNA to a Nunc deep-well plate, then collected, and washed the bead-bound DNA. The DNA was eluted and transferred along with the AMPure-XP beads to a fresh Eppendorf TwinTec plate. Library construction comprised end repair, A-tailing and adapter ligation reactions, performed by a liquid handling robot. In this step, the Agilent NGS Workstation transferred PEG\/NaCl solution and the adapter ligated libraries containing AMPure-XP beads to a Nunc deep-well plate and size-selected the bead-bound DNA. The DNA was eluted and transferred to a fresh Eppendorf TwinTec plate. Agilent Bravo and Labtech Mosquito robotics were used to set up a 384-well quantitative polymerase chain reaction (qPCR) plate, which was then ready to be assayed on the Roche Lightcycler. The Bravo was used to create a qPCR assay plate. This 384-well qPCR plate was then placed in the Roche Lightcycler. A Beckman NX08-02 was used to create an equimolar pool of the indexed adapter ligated libraries. The final pool was then assayed against a known set of standards on the ABI StepOne Plus. The data from the qPCR assay was used to determine the concentration of the equimolar pool. The pool was normalized using Beckman NX08-02. All paired-end sequencing was performed using a range of Illumina HiSeq platforms as the lines were generated over many years (HiSeq 2000 onwards). The sequencing coverage of WGS, WES and hcWES in hiPSC lines are 41\u00d7, 72\u00d7 and 271\u00d7, respectively. HipSci hiPSC sequence alignment, QC and variant calling Reads were aligned to the human genome assembly GRCh37d5 using bwa version 0.5.10 (ref. 68 ) (\u2018bwa aln -q 15\u2019 and \u2018bwa sampe\u2019) followed by quality score recalibration and indel realignment using GATK version 1.5-9 (ref. 69 ) and duplicate marking using biobambam2 version 0.0.147. VerifyBamID version 1.1.3 was used to check for possible contamination of the cell lines, and all but one passed (Supplementary Fig. 4 ). Variable sites were called jointly in each fibroblast and hiPSC sample using BCFtools\/mpileup and BCFtools\/call version 1.4.25. The initial call set was then prefiltered to exclude germline variants that were above 0.1% minor allele frequency in 1000 Genomes phase 3 (ref. 70 ) or ExAC 0.3.1 (ref. 71 ). For efficiency we also excluded low coverage sites that cannot reach statistical significance and for subsequent analyses considered only sites that had a minimum sequencing depth of 20 or more reads in both the fibroblast and hiPSC and at least 3 reads with a nonreference allele in either the fibroblast or hiPSC sample. At each variable site a Fisher\u2019s exact test was performed on a two-by-two contingency table, with rows representing the number of reference and alternate reads and the columns the fibroblast or hiPSC sample. This approach for mutation calling is implemented through BCFtools\/ad-bias, and we have adopted it preferentially instead of existing tumor-normal somatic-variant calling tools because, by definition, tools developed for the analysis of tumor-normal data assume that mutations of interest are absent from the normal tissue. However, in our experiment, many mutations were present, albeit at low frequency, in the source tissue fibroblasts. More information on bcftools ad-bias can be found on the online bcftools-man page at  . The ad-bias protocol is distributed as a plugin in the main bcftools package, which can be downloaded from  . Bcftools ad-bias implements a Fisher test on a 2 \u00d7 2 contingency table that contains read counts of reference\/alternate alleles found in either the iPSC or fibroblast sample. We ran bcftools ad-bias with default settings as follows: $$\\begin{array}{l}{{{\\mathrm{bcftools}}}} + {{{\\mathrm{ad}}}} - {{{\\mathrm{bias}}}}\\,{{{\\mathrm{exome}}}}.{{{\\mathrm{bcf}}}} - - \\, - {{{\\mathrm{t}}}}1 - {{{\\mathrm{s}}}}\\,{{{\\mathrm{sample}}}}.{{{\\mathrm{pairs}}}}.{{{\\mathrm{txt}}}} - {{{\\mathrm{f}}}}\\\\ ^{\\prime} \\% {{{\\mathrm{REF}}}}\\backslash {{{\\mathrm{t}}}}\\% {{{\\mathrm{ALT}}}}\\backslash {{{\\mathrm{t}}}}\\% {{{\\mathrm{CSQ}}}}\\backslash {{{\\mathrm{t}}}}\\% {{{\\mathrm{INFO}}}}\/{{{\\mathrm{ExAC}}}}\\backslash {{{\\mathrm{t}}}}\\% {{{\\mathrm{INFO}}}}\/{{{\\mathrm{UK1KG}}}}^{\\prime} \\end{array}$$ where \u2018exome.bcf\u2019 was the BCF file created by our variant calling pipeline, described in Methods , and sample.pairs.txt was a file that contained matched pairs of the iPSC and corresponding fibroblast sample, one per line, as follows: $$\\begin{array}{l}{{{\\mathrm{HPSI}}}}0213{{{\\mathrm{i}}}} - {{{\\mathrm{koun}}}}\\_2\\quad {{{\\mathrm{HPSI}}}}0213{{{\\mathrm{pf}}}} - {{{\\mathrm{koun}}}}\\\\ {{{\\mathrm{HPSI}}}}0213{{{\\mathrm{i}}}} - {{{\\mathrm{nawk}}}}\\_55\\quad {{{\\mathrm{HPSI}}}}0213{{{\\mathrm{pf}}}} - {{{\\mathrm{nawk}}}}\\\\ {{{\\mathrm{HPSI}}}}0313{{{\\mathrm{i}}}} - {{{\\mathrm{airc}}}}\\_2\\quad {{{\\mathrm{HPSI}}}}0313{{{\\mathrm{pf}}}} - {{{\\mathrm{airc}}}}\\end{array}.$$ We corrected for the total number of tests (84.8 M) using the Benjamini\u2013Hochberg procedure at a false discovery rate of 5%, equivalent to a P value threshold of 9.9 \u00d7 10 \u22124 , to call a mutation as a significant change in allele frequency between the fibroblast and iPSC samples. Furthermore, we annotated sites from regions of low mappability and sites that overlapped with copy-number alterations previously called from array genotypes 35 and removed sites that had greater than 0.6 alternate allele frequency in either the fibroblast or hiPSC, as these sites are likely to be enriched for false positives. Dinucleotide mutations were called by sorting mutations occurring in the same iPSC line by genomic position and marking mutations that were immediately adjacent as dinucleotides. Mutation calling of hiPSCs derived from S2, S7 and ten HipSci lines by using fibroblasts as germline controls Single substitutions were called using CaVEMan (Cancer Variants Through Expectation Maximization;  ) algorithm 72 . To avoid mapping artefacts, we removed variants with a median alignment score <90 and those with a clipping index >0. Indels were called using cgpPindel (  ). We discarded indels that occurred in repeat regions with repeat count >10 and variant call format (VCF) quality <250. Double substitutions were identified as two adjacent single substitutions called by CaVEMan. The ten HipSci lines are HPSI0714i-iudw_4, HPSI0914i-laey_4, HPSI0114i-eipl_1, HPSI0414i-oaqd_2, HPSI0414i-oaqd_3, HPSI1014i-quls_2, HPSI1013i-yemz_3, HPSI0614i-paab_3, HPSI1113i-qorq_2 and HPSI0215i-fawm_4. Mutational signature analysis Mutational signature analysis was performed on S7 EPC-hiPSCs, S2 F-hiPSCs, S2 EPC-hiPSCs and the HipSci F-hiPSC WGS dataset. All dinucleotide mutations were excluded from this analysis. We generated 96-channel single substitution profiles for 324 hiPSCs and 204 fibroblasts. We fitted previously discovered skin-specific substitutions to each sample using an R package (signature.tools.lib) 31 . Function SignatureFit_withBootstrap() was used with default parameters. In downstream analysis, the exposure of two UV-caused signatures Skin_D and Skin_J were summed up to represent the total signature exposure caused by UV (signature 7). A de novo signature extraction was performed on 324 WGS HipSci F-hiPSCs to confirm that the UV-associated skin signatures (Skin_D and Skin_J, signature 7) and culture-associated one (Skin_A, signature 18) are also the most prominent signatures identified in de novo signature extraction (Supplementary Fig. 5 ). Analysis of C>T\/CC\u00bbTT transcriptional strand bias in replication timing regions Reference information of replication timing regions were obtained from Repli-seq data of the ENCODE project (  ) 73 . The transcriptional strand coordinates were inferred from the known footprints and transcriptional direction of protein coding genes. In our dataset, we first orientated all G>A and GG>AA to C>T and CC>TT (using pyrimidine as the mutated base). Then, we mapped C>T and CC>TT to the genomic coordinates of all gene footprints and replication timing regions. Lastly, we counted the number of C>T\/CC>TT mutations on transcribed and nontranscribed gene regions in different replication timing regions. Identification of fibroblast-shared mutations and private mutations in HipSci F-hiPSCs We classified mutations (substitutions and indels) in HipSci F-hiPSCs into fibroblast-shared mutations and private mutations. Fibroblast-shared mutations in hiPSCs are the ones that have at least one read from the mutant allele found in the corresponding fibroblast. Private mutations are the ones that have no reads from the mutant allele in the fibroblast. Mutational signature fitting was performed separately for fibroblast-shared substitutions and private substitutions in hiPSCs. For indels, only the percentage of different indel types was compared between fibroblast-shared indels and private indels. Clonality of samples We inspected the distribution of VAFs of substitutions in HipSci fibroblasts and HipSci F-hiPSCs. Almost all hiPSCs had VAFs distributed around 50%, indicating that they were clonal. In contrast, all fibroblasts had lower VAFs, which distributed around 25% or lower, indicating that they were oligoclonal. We computed kernel density estimates for VAF distributions of each sample. Based on the kernel density estimation, the number of clusters in a VAF distribution was determined by identification of the local maximum. Accordingly, the size of each cluster was estimated by summing up mutations having VAF between two local minimums. Variant consequence annotation Variant consequences were calculated using the Variant Effect Predictor 74 and BCFtools\/csq 75 . For dinucleotide mutations, we recorded only the most impactful consequence of either of the two members of the dinucleotide, where the scale from least to most impactful was intergenic, intronic, synonymous, 3\u2032 untranslated region, 5\u2032 untranslated region, splice region, missense, splice donor, splice acceptor, start lost, stop lost and stop gained. We identified overlaps with putative cancer driver mutations using the COSMIC \u2018All Mutations in Census Genes\u2019 mutation list (CosmicMutantExportCensus.tsv.gz) version 92, 27 August 2020. dNdScv analysis To detect genes under positive selection, we used dN\/dS ratios as implemented in the dNdScv R package (  ) 49 . dNdScv uses maximum likelihood models to calculate the ratio of nonsynonymous to synonymous mutations per gene, normalized by sequence composition, trinucleotide substitution rates and the local mutability of each gene based on epigenetic covariates. Three analyses were run for 452 F-hiPSCs and 78 B-hiPSCs sequencing data: (1) default dNdScv (exome-wide, looking at all genes in the genome or exome for selection); (2) restricted hypothesis testing of known cancer genes (to increase the statistical power on known drivers, using the gene list from Martincorena et al. 49 ); and (3) detection of mutational hotspots (using the sitednds function in dNdScv on hotspots detected in The Cancer Genome Atlas). Insignia B-hiPSC line generation, growth, QC and sequencing Erythroblasts were derived from PBMCs, following appropriate ethics committee approvals (REC 13\/EE\/0302), and reprogrammed using the nonintegrating CytoTune Sendai virus reprogramming kit (OCT3\/4, SOX2, KLF4 and C-MYC) by the Cellular Generation and Phenotyping facility at the Wellcome Sanger Institute in the same way as for the HipSci lines (described above). After establishment of B-hiPSCs lines that had passed all QC steps (described above) and at cell passage equivalent to about 30 doublings, expanded clones were single-cell subcloned to generate two to four daughter subclones for each B-hiPSC line. WGS was run on germline, erythroblasts, B-hiPSC parental clones and B-hiPSC subclones. The average sequencing coverage of WGS was 38\u00d7 (Supplementary Table 14 ). Single-nucleotide polymorphism genotyping was performed as a QC measure to ensure matches between all hiPSCs and respective original starting sample. RNA sequencing was run on 78 iPSC parental clones. Insignia B-hiPSC mutation calling by using blood as germline controls Single substitutions were called using CaVEMan (Cancer Variants Through Expectation Maximization;  ) algorithm 72 . To avoid mapping artefacts, we removed variants with a median alignment score <140 and those with a clipping index >0. Indels were called using cgpPindel (  ). We discarded indels that occurred in repeat regions with repeat count >10 and VCF quality <250. Double substitutions were identified as two adjacent single substitutions called by CaVEMan. Mutation calls were obtained for erythroblasts, iPSC parental clones and subclones. Differentiation of Insignia B-hiPSCs ( BCOR mutant and BCOR wild-type) and RNA sequencing The BCOR -mutant B-hiPSCs (MSH40i2, MSH93i6) and the BCOR -wild-type B-hiPSCs (MSH34i2, MSH30i3) were maintained in feeder-free conditions cultured in Essential E8 medium (ThermoFisher Scientific, A1517001) on Vitronectin FX (Stemcell Technologies, 07180)-coated plates. hiPSC medium was changed daily, and the cells were monitored to ensure there were no signs of spontaneous differentiation. hiPSCs were expanded every 3 or 4 days as small clumps using 0.5 mM UltraPure EDTA (ThermoFisher Scientific, 1557020) diluted in Dulbecco\u2019s phosphate buffered saline (DPBS) (ThermoFisher Scientific, 14190342). Before neural induction, three independent replicates of hiPSC from each donor line were generated and cultured for 1 week as described above. Healthy, nondifferentiating hiPSCs colonies were dissociated into single-cell suspension using TrypLE Express Enzyme (ThermoFisher Scientific, 12605010) and plated on Vitronectin FX-coated plates at 50,000 cells\/cm 2 density in the presence of RevitalCell Supplement (ThermoFisher Scientific, A26445-01, lot 2170092). The cells were cultured for another 2 days until they reached 60\u201375% confluence. At day 0, the culture medium was switched to neural induction medium (NIM) containing V\/V DMEMF12 HEPES (ThermoFisher Scientific, 11330032, lot 2186798) and neurobasal medium (ThermoFisher Scientific, 21103-049, lot 2161553), 1\u00d7 B-27 Supplement (ThermoFisher Scientific, 17504-044, lot 2188886), 1\u00d7 N2 Supplement (ThermoFisher Scientific, 17502-048; Lot: 2193551), MEM NEAA (ThermoFisher Scientific, 11140-035, lot 2202923), 1\u00d7 Glutamax-I (ThermoFisher Scientific, 35050-061, lot 2085268), 1\u00d7 penicillin\/streptomycin in the presence of 10 \u00b5M SB431542 (Tocris, 1414\/10) and 200 nM LDN193189 (Tocris, 6053\/10) with an addition of 1\u00d7 RevitaCell. Starting from day 1, NIM without RevitaCell was changed every day until day 12. At the end of the neural induction process (day 12), the cells were dissociated into single-cell suspension using TrypLE Express Enzyme and plated at high cell density (200,000 cells\/cm 2 ) in double-coated plates of PDL (ThermoFisher Scientific, A3890401, lot 881772E) and 15 \u00b5g ml \u22121 Cultrex mLaminin I Pathclear (Biotechne, 3400-010-02; Lot: 1594368). The NIM was switched to neuron differentiation medium (NDM) containing BrainPhys Neural Medium (Stemcell Technologies, 05790, batch 1000031535), 1\u00d7 B-27 Supplement (ThermoFisher Scientific, 17504-044, lot 2188886), 1\u00d7 N2 Supplement (ThermoFisher Scientific, 17502-048, lot 2193551), 50 \u00b5M dibutyryl-cAMP, sodium salt (Tocris, 1141\/50), 200 nM l -ascorbic acid (Tocris, 4055), 20 ng ml \u22121 BDNF (Cambridge Bioscience, GFH1-100), 20 ng ml \u22121 GDNF (Cambridge Bioscience, GFM37-100) in presence of 10 \u00b5M Y-27632 (Tocris, 1254\/10). On day 13, the medium was changed to NDM without Y-27632, and the cells were allowed to differentiate for another 14 days. Two thirds of the medium was changed three times a week. During the cell differentiation process, cell pellets from all culture replicates were harvested at days 0, 6, 12 and 27 (endpoint) for an RNA-sequencing serial time study. Immunostaining characterization was performed at days 0, 12 and 27 of differentiation to assess the differentiation efficiency. Total RNA was extracted using PureLink RNA Mini Kit (ThermoFisher Scientific, 12183018 A) following the manufacturer\u2019s recommendations. The RNA was quality controlled; the cDNA libraries were prepared and sequenced using Illumina NovaSeq 6000 technology. Each sequenced sample had \u226520 million read pairs of 150-bp paired-end reads. Processing RNA-sequencing data Splice-aware STAR v2.5.0a 76 was used to map RNA-sequencing data to the reference genome. For the human decoy reference genome hs37d5.fa.gz, a genome index was first generated. Then, using the splice junction information from Gencode GTF annotation file v19, fastq files were mapped. The fragments of reads linked with the gene features were then counted using featureCounts v2.0.1 (ref. 77 ). The samples\u2019 raw counts matrices were then analyzed in R version 4.0.4. Differential gene expression was performed using the DESeq2 R package. Immunofluorescence staining The expression of pluripotency markers at day 0 (hiPSC) of differentiation was assessed using a commercially available PSC (OCT4, SSEA4) Immunocytochemistry Kit (ThermoFisher Scientific, A25526 , lot 2194558). NSCs at day 12 and neurons at day 27 of differentiation were stained as already described before with minor modifications 78 . Briefly, the medium was discarded from the plates, and the cells were rinsed gently with DPBS. A 4% solution of paraformaldehyde was used to fix the cells for 20 min at room temperature. The cells were rinsed twice with DPBS and permeabilized for 20 min with 0.1% Triton X-100 (Sigma-Aldrich, T8787-50ML). Nonspecific epitopes were blocked with 0.5% BSA solution for 1 h at room temperature. Cells were incubated overnight at 4 \u00b0C with the primary antibodies as follows: on day 12, cells were incubated with an anti-PAX6 antibody (ThermoFisher Scientific, 14-9914-82, dilution 1:100); on day 27, cells were incubated with an anti-Tubulin beta III (TUBB3) (Millipore, MAB1637, dilution 1:400). The cells were then rinsed three times with 1\u00d7 DPBS and incubated with the secondary antibody Alexa Fluor 488 donkey anti-mouse (ThermoFisher Scientific, A21202 , dilution 1:500). The cells were rinsed three times with DPBS and the nuclei counterstained with NucBlue Fixed Cell Stain ReadyProbes (ThermoFisher Scientific, R37606 ). The images were acquired within 48 h using EVOS FL Auto 2 microscope (ThermoFisher Scientific, AMAFD2000), and the figures were made using the FigureJ plugin in ImageJ software. Statistics and reproducibility All statistical analyses were performed in R 79 . The effects of age and sex on mutation burden of F-hiPSCs were estimated using Mann\u2013Whitney test, \u2018wilcox.test()\u2019 in R. Tests for correlation in the study were performed using \u2018cor.test()\u2019 in R. For cancer driver mutations identified in HipSci F-hiPSCs, a two-sided Fisher test was used to call a mutation as a significant change in allele frequency between the fibroblast and iPSC samples (Supplementary Table 5 ). A Benjamini\u2013Hochberg procedure for multiple hypothesis testing was used. For the differentiation and immunostaining experiments, each BCOR -mut and BCOR -wt cell line had three independent biological replicates differentiated, and for each of these replicates, three wells were stained and imaged using immunofluorescence. At every stage of the neural differentiation (day 0, day 12 and day 27), a total of 36 images were analyzed for both BCOR -mut and BCOR -wt cell lines. Differential gene expression analysis of Insignia B-hiPSCs was performed using DESeq2, which fits each gene\u2019s negative binomial generalized linear model. The default DESeq2 Wald test was used for significance testing, and a threshold of <0.05 of the adjusted P value was applied (Supplementary Table 13 ). Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability All datasets generated as part of this study are included in this data availability statement with no omissions. Links to raw sequencing data produced in this study are available from the HipSci project website (WGS data:  ; WES data  ). Raw data have been deposited in the European Nucleotide Archive under accession numbers ERP006946 (WES, open access samples) and ERP017015 (WGS, open access samples) and the European Genotype-phenotype Archive under accession number EGAS00001000592 (WES, managed access samples involving healthy donors, following completion and approval of a data Access Agreement via eDAM at the Welcome Sanger Institute;  ). The raw sequence files of Insignia samples are deposited at the European Genome-phenome Archive with accession number EGAD00001007029 (WGS, open access samples,  ). The open access data samples are freely available to download, whereas the managed access data are available following a request and a data access agreement (via Wellcome Sanger Institute electronic Data Access Mechanism). The variant call sets are deposited at Mendeley (  ) 80 . Code availability All code used for analysis is detailed in Methods and the Reporting summary. The code of bespoke software pertaining to data processing and analysis is on GitHub (  The code of statistical analysis and figures is on GitHub (  Differential gene expression analysis was performed using the DESeq2 R package (  ) 81 . Signature fitting was conducted using the signature.tools.lib R package (  ) 31 . dN\/dS ratios were calculated using the dNdScv R package (  ) 49 . ","News_Body":"DNA damage caused by factors such as ultraviolet radiation affect nearly three-quarters of all stem cell lines derived from human skin cells, say Cambridge researchers, who argue that whole genome sequencing is essential for confirming if cell lines are usable. Stem cells are a special type of cell that can be programmed to become almost any type of cell within the body. They are currently used for studies on the development of organs and even the early stages of the embryo. Increasingly, researchers are turning to stem cells as ways of developing new treatments, known as cell-based therapies. Other potential applications include programming stem cells to grow into nerve cells to replace those lost to neurodegeneration in diseases such as Parkinson's. Originally, stem cells were derived from embryos, but it is now possible to derive stem cells from adult skin cells. These so-called induced pluripotent stem cells (iPSCs) have now been generated from a range of tissues, including blood, which is increasing in popularity due to its ease of derivation. However, researchers at the University of Cambridge and Wellcome Sanger Institute have discovered a problem with stem cell lines derived from both skin cells and blood. When they examined the genomes of the stem cell lines in detail, they found that nearly three quarters carried substantial damage to their DNA that could compromise their use both in research and, crucially, in cell-based therapies. Their findings represent the largest genetic study to date of iPSCs and are published today in Nature Genetics. DNA is made up of three billions pairs of nucleotides, molecules represented by the letters A, C, G and T. Over time, damage to our DNA, for example from ultraviolet radiation, can lead to mutations\u2014a letter C might change to a letter T, for example. \"Fingerprints\" left on our DNA can reveal what is responsible for this damage. As these mutations accumulate, they can have a profound effect on the function of cells and in some cases lead to tumors. Dr. Foad Rouhani, who carried out the work while at the University of Cambridge and the Wellcome Sanger Institute, said: \"We noticed that some of the iPS cells that we were generating looked really different from each other, even when they were derived from the same patient and derived in the same experiment. The most striking thing was that pairs of iPS cells would have a vastly different genetic landscape\u2014one line would have minimal damage and the other would have a level of mutations more commonly seen in tumors. One possible reason for this could be that a cell on the surface of the skin is likely to have greater exposure to sunlight than a cell below the surface and therefore eventually may lead to iPS cells with greater levels of genomic damage.\" The researchers used a common technique known as whole genome sequencing to inspect the entire DNA of stem cell lines in different cohorts, including the HipSci cohort at the Wellcome Sanger Institute and discovered that as many as 72% of the lines showed signs of major UV damage. Professor Serena Nik-Zainal from the Department of Medical Genetics at the University of Cambridge said: \"Almost three-quarters of the cell lines had UV damage. Some samples had an enormous amount of mutations\u2014sometimes more than we find in tumors. We were all hugely surprised to learn this, given that most of these lines were derived from skin biopsies of healthy people.\" They decided to turn their attention to cell lines not derived from skin and focused on blood derived iPSCs as these are becoming increasingly popular due to the ease of obtaining blood samples. They found that while these blood-derived iPSCs, too, carried mutations, they had lower levels of mutations than skin-derived iPS cells and no UV damage. However, around a quarter carried mutations in a gene called BCOR, an important gene in blood cancers. To investigate whether these BCOR mutations had any functional impact, they differentiated the iPSCs and turned them into neurons, tracking their progress along the way. Dr. Rouhani said: \"What we saw was that there were problems in generating neurons from iPSCs that have BCOR mutations\u2014they had a tendency to favor other cell types instead. This is a significant finding, particularly if one is intending to use those lines for neurological research.\" When they examined the blood samples, they discovered that the BCOR mutations were not present within the patient: instead, the process of culturing cells appears to increase the frequency of these mutations, which may have implications for other researchers working with cells in culture. Scientists typically screen their cell lines for problems at the chromosomal level\u2014for example by checking to see that the requisite 23 pairs of chromosomes are present. However, this would not be sufficiently detailed to pick up the potentially major problems that this new study has identified. Importantly, without looking in detail at the genomes of these stem cells, researchers and clinicians would be unaware of the underlying damage that is present with the cell lines they are working with. \"The DNA damage that we saw was at a nucleotide level,\" says Professor Nik-Zainal. \"If you think of the human genome as like a book, most researchers would check the number of chapters and be satisfied that there were none missing. But what we saw was that even with the correct number of chapters in place, lots of the words were garbled.\" Fortunately, says Professor Nik-Zainal, there is a way round the problem: using whole genome sequencing to look in detail for the errors at the outset. \"The cost of whole genome sequencing has dropped dramatically in recent years to around \u00a3500 per sample, though it's the analysis and interpretation that's the hardest bit. If a research question involves cell lines and cellular models, and particularly if we're going to introduce these lines back into patients, we may have to consider sequencing the genomes of these lines to understand what we are dealing with and get a sense of whether they are suitable for use.\" Dr. Rouhani adds: \"In recent years we have been finding out more and more about how even our healthy cells carry many mutations and therefore it is not a realistic aim to produce stem cell lines with zero mutations. The goal should be to know as much as possible about the nature and extent of the DNA damage to make informed choices about the ultimate use of these stem cell lines. \"If a line is to be used for cell based therapies in patients for example, then we need to understand more about the implications of these mutations so that both clinicians and patients are better informed of the risks involved in the treatment.\" ","News_Title":"Large number of stem cell lines carry significant DNA damage, say researchers","Topic":"Biology"}
{"Paper_Body":"Abstract Atomic beams are a longstanding technology for atom-based sensors and clocks with widespread use in commercial frequency standards. Here, we report the demonstration of a chip-scale microwave atomic beam clock using coherent population trapping (CPT) interrogation in a passively pumped atomic beam device. The beam device consists of a hermetically sealed vacuum cell fabricated from an anodically bonded stack of glass and Si wafers in which lithographically defined capillaries produce Rb atomic beams and passive pumps maintain the vacuum environment. A prototype chip-scale clock is realized using Ramsey CPT spectroscopy of the atomic beam over a 10 mm distance and demonstrates a fractional frequency stability of \u22481.2 \u00d7 10 \u22129 \/ \\(\\sqrt{\\tau }\\) for integration times, \u03c4, from 1 s to 250 s, limited by detection noise. Optimized atomic beam clocks based on this approach may exceed the long-term stability of existing chip-scale clocks, and leading long-term systematics are predicted to limit the ultimate fractional frequency stability below 10 \u221212 . Introduction The development of low-power, chip-scale atomic devices including clocks and magnetometers has been enabled by advances in the optical interrogation of atoms confined in microfabricated vapor cells 1 . These miniaturized devices commonly use coherent population trapping (CPT) resonances in alkali atoms, which generate a coherent dark state between hyperfine atomic ground states using two optical fields in a \u039b-scheme 2 . Optical probing of the microwave transition avoids the need for bulky microwave cavities, providing a compact and low-power method for probing the atoms and enabling battery-powered operation 3 , 4 . Buffer gases are commonly used to reduce the decoherence rate from wall collisions and narrow the atomic line. As a result, devices such as the chip-scale atomic clock (CSAC) can realize \u224810 \u221211 fractional frequency stability at 1000 s of averaging while consuming only 120 mW of power 5 . Thermal drifts and aging of the buffer gas environment, along with light shifts and other systematics, contribute to the long-term instability of buffer gas systems and degrades clock performance in existing CSACs beyond 1000 s of averaging with a drift rate of ~10 \u22129 per month. Clocks based on atomic beams and laser-cooled gases operate in ultra-high vacuum (UHV) environments and avoid shifts from buffer gases, allowing for higher frequency stability and continuous averaging over periods of days or weeks. Laser-cooling technology underpins the most advanced atomic clocks 6 , and while recent efforts in photonic integration 7 , 8 and vacuum technology 9 , 10 , 11 have advanced the state of the art, significant hurdles to miniaturization and low-power operation remain 12 . Atomic beams have played a significant role throughout the history of frequency metrology, serving as commercial frequency standards since the 1960s and as national frequency standards for realization of the SI second 13 , 14 . Miniaturized atomic beams 15 , 16 , 17 , 18 , 19 offer a path for exceeding the long-term stability of existing chip-scale devices while circumventing the complexity and power needs of more advanced laser-cooled schemes. In this work, we demonstrate a chip-scale atomic beam clock built using a passively pumped Rb atomic beam device as shown in Fig. 1 . The beam device contains a Rb reservoir that feeds a microcapillary array and generates Rb atomic beams in an internal, evacuated cavity. Fabrication of the device is realized using a stack of lithographically defined planar structures which are anodically bonded to form a hermetic package. Spectroscopic measurements of the atomic flux and beam collimation are presented to demonstrate the successful realization of the atomic beam device. The atom beam device presents a pathway for realizing low-power, low-drift atomic sensors using microfabricated components and supports further integration with advanced thermal and photonic packaging to realize highly manufacturable quantum sensors. Fig. 1: Overview of chip-scale atomic beam device. a Image of atomic beam device with labeled components (peanut for scale). Rb vapor in the source cavity feeds a buried microcapillary array and forms an atomic beam (indicated by a red-to-blue arrow) in the drift cavity. Non-evaporable getters (NEGs) and graphite maintain the vacuum environment in the device. b Expanded view of the beam device showing component layers as well as Rb pill dispensers, graphite rods, and NEG pumps. c Schematic of the microcapillary array etched in a Si wafer. Each capillary has a 100 \u00b5m \u00d7 100 \u00b5m square cross-section. The array collimates the atomic beam and provides differential pumping between the source and drift regions. d The final anodic bond which hermetically seals the device occurs in an ultra-high vacuum (UHV) chamber. Full size image The microwave atomic beam clock is demonstrated using Ramsey CPT interrogation in the atomic beam device. Ramsey spectroscopy of the 87 Rb ground-state hyperfine transitions is measured across a 10 mm distance and demonstrates quantum coherence across the device. The magnetically insensitive m F = 0 hyperfine transition is used to realize the atomic beam clock signal, and a clock short-term fractional frequency stability of \u22481.2 \u00d7 10 \u22129 is achieved at 1 s of integration in this prototype device. The performance of this beam clock is limited by technical noise, and an optimized cm-scale device is expected to achieve stability better than 10 \u221210 at 1 s of integration with a stability floor below 10 \u221212 , supported by a detailed analysis of the sources of drift in atomic beam clocks. Results The passively pumped atom beam device is fabricated from a multi-layer stack of Si and glass wafers as shown in Fig. 1 . The layers are anodically bonded to form a hermetically sealed vacuum cell with dimensions of 25 mm \u00d7 23 mm \u00d7 5 mm and \u22480.4 cm 3 of internal volume. Internal components include Rb molybdate Zr\/Al pill-type dispensers for generating Rb vapor in an internal source cavity 20 as well as graphite and Zr\/V\/Fe non-evaporable getters (NEGs) in a separate drift cavity which maintain the vacuum environment. A series of microcapillaries connect the two internal cavities and produce atomic beams which freely propagate for 15 mm in the drift cavity. The device is heated to generate Rb vapor in the source cavity, and the atomic beam flux and divergence are defined by the capillary geometry 21 . Microfabrication allows for arbitrary modification of the shape, continuity, and divergence of the capillaries to control the atomic beam properties 18 , 19 . The arrangement of alternating glass and Si layers and internal components which comprise the beam device are shown in Fig. 1b . The features etched in Si are created using deep reactive ion etching (DRIE), and the cavity in the central glass layer is conventionally machined. The two transparent encapsulating glass layers are low-He-permeation aluminosilicate glass 22 with 700 \u00b5m thickness. The microcapillary array is etched into a 2 mm-thick Si layer which houses the internal components used for passive pumping and sourcing Rb. Two additional layers, a 600 \u00b5m-thick Si layer, and a 1 mm-thick borosilicate glass layer, act as a spacer to position the microcapillary array near the center of the device thickness and provide volume into which the atomic beam can expand in the drift cavity. The device is assembled by first anodically bonding the four upper layers under ambient conditions (see Methods) to create a preform structure. The four-layer preform is populated with the getters and Rb pill dispensers and topped with the final glass wafer. The stack is placed in an ultra-high vacuum (UHV) chamber and baked at 520 K for 20 h to degas the components, and the NEGs are thermally activated using laser heating to remove their passivation layer before sealing the device. The final interface is then anodically bonded to hermetically seal the vacuum device (see Fig. 1d ). Rb atomic beams are generated in the drift cavity as vapor from the source cavity flows through the microcapillary array 18 . The atomic flux is determined by the source region Rb density and the geometry of the capillary array, which consists of 10 straight capillaries with 100 \u00b5m \u00d7 100 \u00b5m square cross section, 50 \u00b5m spacing, and 3 mm length. The flux through the capillaries and angular profile of the atomic beam are well described by analytic molecular flow models based on the capillary\u2019s aspect ratio L\/a , where L is the capillary length and a is its width 21 . The near-axis flux is similar to that of a \u201ccosine\u201d emitter for angles \u03b8 less than a\/L from normal. The total flux through the capillary array \\({F}_{n}=\\frac{1}{4}{{w\\; n}}_{{{{{{\\rm{Rb}}}}}},1}\\bar{v}{A}_{c}\\) , where \\({w}=\\,1\/(1\\,+\\,3L\/4a)\\) is the transmission probability of the channel, \\({n}_{{{{{{\\rm{Rb}}}}}},1}\\) is the Rb density in the source region, \\(\\bar{v}\\) is the mean thermal speed of the atoms, and \\({A}_{c}\\) is the cross-sectional area of the capillary array (here \\(10{a}^{2}\\) ). More complex capillary geometries such as the non-parallel or cascaded collimators can be used to further engineer the beam profile or reduce off-axis flux 18 , 19 . The performance of the atomic beam device is measured using optical spectroscopy on the Rb D 2 line at ~780 nm. The atoms are probed using a 5 \u00b5W elliptical laser beam with w y \u2248 2100 \u00b5m, w z \u2248 350 \u00b5m (1\/e 2 radius) normally incident on the device surface (propagating along the x axis). The total density n Rb,1 of 85 Rb and 87 Rb (including all spin states) in the source cavity is measured using absorption spectroscopy with device temperature varying between 330 K and 385 K. A representative spectrum measured in the source cavity at 363 K (Fig. 2a ) shows a Doppler broadened spectrum consistent with thermal Rb vapor ( \\(\\bar{v}\\) \u2248 300 m\/s) and a density of n Rb,1 \u2248 2.4 \u00d7 10 18 m \u22123 . The measured \\({n}_{{{{{{\\rm{Rb}}}}}},1}\\) is consistent within experimental uncertainty with published values for the vapor pressure of liquid Rb metal across the temperature range probed. Fig. 2: Spectroscopic beam cell characterization. a Source cavity absorption (gray) and drift cavity fluorescence at z = 1 mm (red) and z = 11 mm (blue) measures the Rb number density, flux, and the velocity distribution normal to the device surface at 363 K. b Fluorescence at z = 11 mm includes narrow peaks from the atomic beam signal as well as a broad signal corresponding to background Rb vapor (light blue curve). Passive and differential pumping generates a large (\u22486500\u00d7) Rb partial pressure differential between the source and drift cavities. c The measured atomic beam flux F meas and spectral FWHM are plotted versus distance from the capillary array at 363 K. Flux prediction based on n Rb,1 (black dashed lines) and the geometrical FWHM limit set by the fluorescence imaging (red dotted line) are shown. Inset shows the estimated total capillary flux F tot versus device temperature and comparison to the total expected capillary array flux \\({F}_{n}\\) (dashed line). Error bars represent 68% confidence intervals. Full size image The flux and angular divergence of the atomic beams are measured using fluorescence spectroscopy in the drift cavity. Fluorescence is collected using a 1:1 imaging system with \u22481.9% collection efficiency mounted at 45\u00b0 from the beam axis in the x-z plane. The imaged area corresponds to a 1 mm \u00d7 1.4 mm region in the x-y plane. Fluorescence spectra scanning around the 85 Rb F = 3 \u2192 F\u2019 = 4 transition (labeled as zero optical detuning) are measured at varying distances along z from the capillary array. Example spectra at z = 1 mm and z = 11 mm at 363 K (shown in Fig. 2a, b ) demonstrate narrow spectral features corresponding to the atomic beam signal and broader features arising from thermal background Rb vapor. The measured atomic beam flux is calculated from the number of detected atoms in the imaged volume N det (see methods) as \\({F}_{{{{{{\\rm{meas}}}}}}}={N}_{{{{{{\\rm{det }}}}}}}{v}_{{{{{{\\rm{beam}}}}}}}\/L,\\) where \\({v}_{{{{{{\\rm{beam}}}}}}}\\) is the most probable longitudinal velocity of the atomic beam and L is the length over which the atoms interact with the probe beam. At 363 K and z = 1 mm, \\({F}_{{{{{{\\rm{meas}}}}}}}\\) = 5 \u00d7 10 11 s \u22121 and the FWHM of the fluorescence lines is \u2248150 MHz, corresponding to a transverse velocity FWHM of \u2248120 m\/s. At this distance, \u224865% of the total capillary array is probed and the total atomic beam flux is estimated to be \\({F}_{{{{{{\\rm{tot}}}}}}}\\) = 7.7 \u00d7 10 11 s \u22121 , consistent with the measured density in the source cavity and molecular flow predictions through the capillaries (see Fig. 2c inset). Near the end of the drift cavity (z = 10 mm) F meas = 3.0 \u00d7 10 10 s \u22121 or \u22483.9% of the total flux due to the divergence of the atomic beam. This value matches the theoretical expectation (Fig. 2c dashed black line) of 3.2 \u00d7 10 10 s \u22121 based on \\({n}_{{{{{{\\rm{Rb}}}}}},1}\\) , the detected area of \u22481.4 mm 2 , and the angular distribution function of our capillaries under molecular flow 21 . This agreement indicates that atomic beam loss due to collisions is consistent with zero within our level of systematic uncertainty. We note that the relatively strong divergence of this beam is typical of microcapillary collimation due to the presence of two atomic flux components, one that is direct (line-of-sight to the source) and the other indirect (diffuse scatter from capillary walls). For measurements of direct atoms ( \\(\\theta < a\/L\\) ), the atomic flux within this range is \\(\\approx {{{{{{\\rm{sin }}}}}}}^{2}\\left(\\theta \\right){F}_{{{{{{\\rm{tot}}}}}}}\/w\\) or \u2248 2.6% of \\({F}_{{{{{{\\rm{tot}}}}}}}\\) for the presented capillary geometry. The beam fluorescence FWHM is \u224840 MHz at this distance, set primarily by the range of x -velocities collected in the imaging system. At \\({F}_{{{{{{\\rm{tot}}}}}}}\\) = 7.7 \u00d7 10 11 s \u22121 , 10 years of sustained operation would require 20 mm 3 of metallic Rb. Reported flux and density values have an estimated statistical uncertainty of 15% and systematic uncertainty of 30%. From the lack of measured collisional loss over a 10 mm distance in the drift cavity, we estimate an upper bound on the background pressure of ~1 Pa. For collisional loss to be significant given our systematic uncertainty in the absolute value of the atomic flux, the transport mean free path of Rb would need to be <1 cm, and this would require partial pressures of >12 Pa of H 2 or >3 Pa of N 2 , which are common vacuum contaminants 23 . The true background pressure may be significantly lower due to the high gettering efficiency of the NEGs for most common background gases including H 2 , N 2 , O 2 , CO, and CO 2 . The pumping speeds are estimated to be \u22481.4 L\/s for H 2 and \u22480.14 L\/s for CO at room temperature 24 . He is not pumped by the passive getters and the steady state He partial pressure will approach the ambient value of \u22480.5 Pa. However, this equilibration may be slowed by our use of low-He-permeation aluminosilicate glass 22 . Operation of the atomic beam device over many months indicates the rate of oxidation of deposited Rb is negligible. Fluorescence from thermal background Rb vapor in the drift cavity is evident in all measured spectra, and the measured background Rb density is estimated as \\({n}_{{{{{{\\rm{Rb}}}}}},2}\\) = 3.7 \u00d7 10 14 m \u22123 at z \u2248 11 mm (see Fig. 2b ), equivalent to a partial pressure of \u2248 2 \u00d7 10 \u22126 Pa. This density is \u2248 6500\u00d7 lower than the Rb density in the source cavity due to differential pumping from the microcapillary array and passive pumping primarily from the graphite getters. Graphite getters are commonly used in atomic clocks due to graphite\u2019s affinity for intercalating alkali vapor, and the pressure differential implies a Rb pumping speed of \u22482 L\/s for the \u22480.9 cm 2 of graphite surface area used. The graphite getters used (Entegris CZR-2) have high porosity and low strength relative to other commonly available graphites 25 , 26 . Recent work has demonstrated that graphite can also act as a solid-state reservoir for alkali-metal 27 , 28 and highly oriented pyrolytic graphite (HOPG) can serve both as an alkali getter and source depending on the operating temperature 29 . The beam device has been operated intermittently (\u22481000 operation hours) over a period of 15 months without observed degradation of the vacuum environment. The deposited Rb metal in the source cavity is slowly consumed during normal device operation, and partial laser-thermal activation of the pill dispensers has been performed 9 times to deposit additional Rb metal in this cavity. Normal operation of the beam device is observed within minutes after activation and thermal equilibration of the device, and no period of excessive background pressure is observed. Complete activation of the pill dispensers in a single process is likely achievable but has not been attempted in this device. Saturation of the NEGs or graphite has not been observed, indicating that any real or virtual leaks are small, although absolute measurements of the pressure in the presented device have not been made. The potential utility of the chip-scale atomic beam device is demonstrated using CPT Ramsey spectroscopy of the 87 Rb ground state hyperfine splitting ( \\({\\nu }_{{HF}}\\) \u2248 6.835 GHz) over a 10 mm distance, similar to previous laboratory CPT atomic beam clocks 15 , 30 , 31 . We address the D 1 F = 1,2 \u2192 F\u2019 = 2 \u039b-system (Fig. 3a ) using two circularly polarized, \u2248250 \u00b5W laser beams propagating along the x- direction ( w y \u2248 550 \u00b5m, w z \u2248 150 \u00b5m). The laser light is phase modulated at \\({\\nu }_{{{{{{\\rm{mod}}}}}}}\\) using a fiber-based electro-optical modulator, and the carrier frequency and a 1st order sideband address the CPT \u039b-system with approximately equal optical powers. The modulated light is split into two equal-length, parallel paths which intersect the atomic beam at z = 1 mm and z = 11 mm to perform two-zone Ramsey spectroscopy. Fluorescence from atoms in the 2nd zone (1.4 mm 2 imaged area) is collected on a Si photodiode with \u22481.9% efficiency. A magnetic field of \u22482.8 \u00d7 10 \u22124 T is applied along the x -direction and separates the Zeeman-state dependent transitions. Fig. 3: Ramsey CPT spectroscopy of the atomic beam. a Schematic of two-zone Ramsey interrogation of the atomic beam. b Level diagram illustrating the CPT \u039b-systems. c Spectroscopic signal observed in 2nd Ramsey zone shows three, MHz-width CPT features. d The central CPT feature contains the clock signal Ramsey fringe with \u224815 kHz width and \u224815% fluorescence contrast. Full size image CPT spectra measured in the second Ramsey zone at 363 K (Fig. 3c ) demonstrate three, MHz-wide CPT resonances corresponding to m F = \u22121, 0, and 1 \u039b-systems from \u22481 \u03bcs long interaction with the optical fields in the 2nd Ramsey zone. At the center of each of these resonances (Rabi pedestals) are narrower Ramsey fringes arising from interaction with light in both Ramsey zones. The central Ramsey fringe (Fig. 3d ) serves as our clock signal and has a fringe width of \u224815 kHz arising from the 30 \u03bcs transit time between the two zones. The signal height is \u22483.6 pW and the contrast relative to the one-photon fluorescence is \u224815%. Contrast is limited in our probing scheme by the spread of atoms among m F levels and optical pumping out of the \u039b-system. Other probing schemes involving pumping with both circular polarizations can reduce loss outside of the desired m F level and increase fringe contrast 32 , 33 , 34 . The clock fringe is offset by \u22484.5 kHz from the vacuum value of \\({\\nu }_{{HF}}\\) due to the second-order Zeeman effect. Optical path length uncertainty of 0.5 mm between the two Ramsey zones limits comparison to the vacuum value of \\({\\nu }_{{HF}}\\) at the \u2248400 Hz level. An atomic beam clock is realized using the central Ramsey fringe to stabilize the CPT microwave modulation frequency. For this measurement, the beam device is heated to 392 K and the observed peak-to-valley height of the clock Ramsey fringe signal is \u224816 pW using 200 \u03bcW of optical power in each Ramsey zone. An error signal is formed using 150 Hz square wave modulation of the clock frequency at an amplitude of 11 kHz, and feedback is used to steer the microwave synthesizer\u2019s center frequency \\({\\nu }_{{{{{{\\rm{clock}}}}}}}\\) with a bandwidth of \u22481 Hz. The synthesizer is referenced to a hydrogen maser, and a time series of \\({\\nu }_{{{{{{\\rm{clock}}}}}}}\\) is recorded. The measured overlapping Allan deviation (ADEV) of the fractional frequency stability of \\({\\nu }_{{{{{{\\rm{clock}}}}}}}\\) (Fig. 4 ) demonstrates a short-term stability of \u2248 1.2 \u00d7 10 \u22129 \/ \\(\\sqrt{\\tau }\\) from 1 s to 250 s, limited by the signal height and the \u224813.5 fW\/ \\(\\sqrt{{{{{{\\rm{Hz}}}}}}}\\) noise equivalent power of the amplified Si detector used. Straightforward improvement in fluorescence collection efficiency and fringe contrast could improve the short-term stability below 1 \u00d7 10 \u221210 \/ \\(\\sqrt{\\tau }\\) , similar to the performance of existing chip-scale atomic clocks 1 . Quantum projection noise limits the potential stability of the presented measurement to \u22489 \u00d7 10 \u221212 \/ \\(\\sqrt{\\tau }\\) , assuming a thermal 87 Rb beam at 392 K, a detectable flux of 1.8 \u00d7 10 10 s \u22121 , and a fringe contrast of 25%. Fig. 4: Beam clock stability measurement. The Allan deviation (ADEV) of the chip-scale atomic beam clock frequency (black points) is measured for integration times \u03c4 < 250 s. The short-term fractional frequency stability \u03c3 y ( \u03c4 ) is \u2248 1.2 \u00d7 10 \u22129 \/ \\(\\sqrt{\\tau }\\) (red line) over this range. Error bars represent 68% confidence intervals. Full size image Discussion We have demonstrated a chip-scale atomic clock based on miniaturized atomic beams. The key components of the passively pumped atomic beam device are planar, lithographically defined structures etched in Si and glass wafers, compatible with volume microfabrication. The 10-channel microcapillary array etched into one of the Si device layers provides a total atomic flux of \u22487.7 \u00d7 10 11 s \u22121 at 363 K, and \u22483.9% of the atoms pass through a 1.4 mm 2 detection area 10 mm downstream. The measured performance of the atomic beam matches expectations based on molecular flow through the collimator array with no free parameters, indicating that collisions with background gases are minimal and the background pressure is ~1 Pa or lower. Passive and differential pumping of the Rb vapor supports the \u2248 6500 \u00d7 Rb partial pressure differential between the source and drift cavities and enables high beam flux while minimizing the background Rb pressure in the drift cavity. The presented beam system has been operated intermittently for 15 months without degradation of the vacuum environment or saturation of the passive pumps. The realization of a microwave Ramsey CPT beam clock demonstrates the potential utility of the atomic beam device. CPT Ramsey fringes are measured using the atomic beam over a 10 mm distance, demonstrating atomic coherence across the drift cavity. A clock signal is formed using the magnetically insensitive, m F = 0 transitions between the 87 Rb hyperfine ground states, and Ramsey fringes at an operating temperature of 392 K are measured to be 15 kHz-wide with 16 pW of CPT signal. This clock signal is used to stabilize the microwave oscillator driving the CPT transitions, and the clock demonstrates a short-term fractional frequency stability of \u2248 1.2 \u00d7 10 \u22129 \/ \\(\\sqrt{\\tau }\\) from 1 s to 250 s. The presented short-term stability is limited by the available signal-to-noise ratio (SNR) of \u2248 1200 at 1 s, and improvement of the short-term stability below 1 \u00d7 10 \u221210 \/ \\(\\sqrt{\\tau }\\) appears feasible, competitive with existing buffer gas-based miniature atomic clocks, by straightforward improvement to the collection optics and use of a higher contrast pumping scheme 32 , 33 , 35 . The presented beam clock approach has the potential to exceed existing chip-scale atomic clocks in both long-term stability and accuracy 36 . Commercial atomic beam clocks based on microwave excitation of the clock transition using a Ramsey length of \u224815 cm achieve a stability at 5 days of 10 \u221214 and an accuracy of 5 \u00d7 10 \u221213 . Many of the key systematics in beam clocks scale with the clock transition linewidth, and hence inversely with the Ramsey distance, implying that a 15 mm beam clock could achieve stability at the 10 \u221213 level. Work on CPT atomic beam clocks using Na and a 15 cm Ramsey length achieved a stability of 1.5 \u00d7 10 \u221211 at 1000 s without evidence of a flicker floor 31 , 37 . Projected to Cs with a Ramsey length of 15 mm implies an achievable stability at the level of 1.0 \u00d7 10 \u221211 at 1000 s, equivalent to less than 100 ns timing error at 1 day of integration. Realization of this stability will depend on managing drift in the optical, vacuum, and atomic environments in a fully miniaturized beam clock system. The leading systematic shifts that will impact a compact beam clock include Doppler shifts, Zeeman shifts, end-to-end cavity phase shifts, collisional shifts, and light shifts. Each of these shifts has been studied extensively in conventional microwave atomic beam frequency standards 13 , 38 and in CPT beam clocks 31 . We evaluate these requirements assuming a stability goal of 10 \u221212 , equivalent to \u22486.8 mHz stability of \\({\\nu }_{{{{{{\\rm{clock}}}}}}}\\) , for a 1.5 cm Ramsey length. Optical path length instability of the CPT laser beam can lead to both Doppler shifts and end-to-end cavity phase shifts. Doppler shifts arise from CPT laser beam pointing drift (thermal or aging) along the atomic beam axis and shifts the clock frequency at \u22487.5 kHz rad \u22121 , requiring \u00b5rad beam pointing stability to reach 10 \u221212 frequency stability. Optical path length stability at the 10 nm level is needed to minimize end-to-end cavity phase shifts. This shift can be minimized using symmetrical Ramsey beam paths, which make thermal expansion common mode along the two Ramsey arms and largely eliminates the bias. Asymmetrical path length variation can arise from thermal gradients along the beam paths and will induce clock shifts. For 15 mm beam paths fabricated using glass or Si substrates, 100 mK temperature uniformity is sufficient to achieve the desired stability. Collisional shifts place limits on the vacuum stability required in the atomic beam device. Common background gases such as H 2 and He induce collisional shifts of \\({\\nu }_{{{{{{\\rm{HF}}}}}}}\\) at the level of 5 Hz Pa \u22121 , and 1 mPa pressure stability is needed to achieve 10 \u221212 fractional frequency stability 23 , 39 . Given the inferred pressure of ~1 Pa in our device at 363 K, 100 mK temperature stability is sufficient assuming zero background pressure variation. Stabilizing the He partial pressure in passively pumped devices is challenging due to the high He diffusivity in many materials and insufficient getter material for He. We use low-He-permeation aluminosilicate glass to reduce the rate of He ingress, stabilizing He partial pressure variations 22 , 40 , 41 , 42 . Collisions with background Rb atoms along the drift cavity generate spin-exchange shifts of \\({\\nu }_{{{{{{\\rm{HF}}}}}}}\\) , and the magnitude of the Rb-Rb collisional shift depends on the occupancy ratio between ground state hyperfine levels before CPT interrogation. Assuming optical pumping into the F = 2 ground state, \\({\\nu }_{{{{{{\\rm{HF}}}}}}}\\) shifts at \u22483400 Hz Pa \u22121 . The total clock shift is estimated to be \u22486.8 mHz for our demonstrated \u22482 \u00d7 10 \u22126 Pa Rb background partial pressure 43 and places lax requirements on the background pressure stability. Intra-beam collisional shifts also exist at approximately the same level and can be reduced using cascaded collimators to reduce the atomic beam density 18 . Light shifts are another significant source of clock instability and can arise from both the ac-Stark effect 44 as well as incomplete optical pumping into the CPT dark state 45 , 46 . The magnitude and sign of the light shifts can depend strongly on the intensity ratio of the CPT driving fields and the pumping scheme used 47 , and the shift scales inversely with the Ramsey time. We estimate the light shift sensitivity in the proposed geometry at the level of 1 \u00d7 10 \u221212 for a 0.1% change in the CPT field intensity ratio based on measured light shifts in a cold-atom clock using \\({\\sigma }_{+}\\) \/ \\({\\sigma }_{-}\\) optical pumping, nominally equal CPT field intensities, and a 4 ms Ramsey time 47 . This level of stability may require use of active monitoring of the optical modulation used to generate the CPT fields 42 , 48 , 49 . Several methods have been developed to manage light shifts in atomic clocks using multiple measurements of the clock frequency, such as auto-balanced Ramsey spectroscopy 50 , 51 or power-modulation spectroscopy 52 , 53 . Zeeman shifts arise from variations in the quantization magnetic field, and at a field of \u224810 \u22124 T (sufficient to separate the magnetically sensitive m F \u2260 0 transitions from the clock transition), the Zeeman effect shifts \\({\\nu }_{{HF}}\\) by \u2248575 Hz. This dictates a field stability at six parts-per-million (ppm), which can be achieved using intermittent interrogation of a m F \u2260 0 transition to correct the field strength. Considering each of the common sources of drift summarized in Table 1 , an ultimate fractional frequency stability at or below the level of 10 \u221212 appears feasible in a chip-scale atomic beam clock. The presented beam clock presents a path for realizing low size, weight, and power (low-SWaP) atomic clocks. Future efforts will focus on realizing this long-term clock stability goal using integration with micro-optical and thermal packaging to produce a fully integrated device at the size- and power-scale of existing CSACs. Such a device should achieve sub-\u00b5s timing error at a week of integration and would contribute to low-SWaP timing holdover applications. The chip-scale beam device presented here is a general platform for quantum sensing, and future work using this system could explore applications including inertial sensing using atom interferometry, electrometry using Rydberg spectroscopy, and higher-performance compact clocks using optical transitions. Table 1 Expected chip-scale clock systematics Full size table Methods Fabrication of atom beam device Features including two internal cavities and the microcapillary array are etched into Si wafers using DRIE. The beam cell is assembled by first anodically bonding the Si layers, intermediate glass layer, and one encapsulating glass layer to create a preform structure. The preform bonds are performed in air at 623\u2013673 K using a bonding voltage of 800\u2212900 V for several hours. The Rb pills, graphite rods, and NEGs are loosely held in cavities etched in the 2-mm-thick Si wafer, leaving room for expansion during operation. The final bond, which hermetically seals the package, occurs in a UHV chamber using an electrode to press the final encapsulating glass layer onto the preform. The device is vacuum baked at \u22481 \u00d7 10 \u22125 Pa and 520 K for 20 h to reduce volatile contaminants, and the NEGs are thermally activated using a few W of 975 nm laser light focused onto each getter for \u2248600 s. The NEGs are observed to glow a red color during activation, consistent with a temperature ~1170 K. After NEG activation, the final anodic bond hermetically seals the device. Pressure in the UHV chamber is maintained at \u22483.5 \u00d7 10 \u22125 Pa for the bond with the cell at \u2248640 K and bonding voltage of \u22121800 V for 21 h. The sealed device is removed from the UHV bonding chamber and mounted on a temperature-controlled plate for operation in air. Pill dispensers (1 mm diameter, 0.6 mm thickness, 0.4 mg total Rb content) are activated using laser heating with a few W of 975 nm light to a temperature of \u2248950 K until a stable Rb density is observed in the source cavity. Atom beam flux characterization The atomic beam flux is measured using fluorescence spectroscopy on the Rb D 2 transitions as shown in Fig. 2 19 , 54 . Spectroscopy is performed using a 5 \u00b5W elliptical laser beam traveling normal to the beam device surface with 1\/e 2 radii of w y \u2248 2100 \u00b5m and w z \u2248 350 \u00b5m. The peak intensity is \u22480.1 mW\/cm 2 , and the low intensity limits optical pumping during transit through the probing beam. Atomic fluorescence is imaged using a 1:1 imaging system mounted at 45 degrees in the x-z plane to collect \u22481.9% of the atomic fluorescence onto a 1 mm \u00d7 1 mm Si photodiode. The effective probed volume at constant interrogating intensity is \u22481 mm \u00d7 1.4 mm \u00d7 \\({w}_{z}\\sqrt{\\pi \/2}\\) or \u22480.6 mm 3 , and this volume can be translated across the drift region. The factor \\({{L}_{z}=w}_{z}\\sqrt{\\pi \/2}\\) accounts for the Gaussian intensity variation along the z-direction. The flux through this volume \\({F}_{{{{{{\\rm{meas}}}}}}}={N}_{{{{{{\\rm{meas}}}}}}}{v}_{{{{{{\\rm{beam}}}}}}}\/{L}_{z}\\) where \\({N}_{{{{{{\\rm{meas}}}}}}}\\) is the number of atoms measured in the probed volume, \\({v}_{{{{{{\\rm{beam}}}}}}}=\\sqrt{3{k}_{B}T\/m}\\) is the most probable longitudinal velocity, k B is Boltzmann\u2019s constant, and m is the atomic mass. We measure N det using the integrated spectrum of radiated power \\({\\Phi }_{{{{{{\\rm{Total}}}}}}}=\\int P\\left(\\omega \\right)d\\omega\\) across the 85 Rb F = 3\u2212> F \u2019 = 4 transition where P (\u03c9) is the measured optical power at angular detuning \u03c9. At low saturation intensity the integrated spectral power per atom \\({\\Phi }_{0}=\\frac{{hc}\\pi }{4\\lambda }s{\\Gamma }^{2}\\) and \\({N}_{{{{{{\\rm{meas}}}}}}}=\\frac{{\\Phi }_{{Total}}}{{\\Phi }_{0}}\\) , where h is Plank\u2019s constant, c is the speed of light, \u03bb is the D 2 transition wavelength, s is the transition saturation parameter, and \\(\\Gamma\\) \u2248 2\u03c0 \u00d7 6.067 MHz is the natural linewidth of the D 2 transition. Data availability Data underlying the results of this study are available from the authors upon request. Code availability The codes that support the findings of this study are available from the authors upon request. ","News_Body":"A new type of miniature atomic clock could provide better timing over the span of weeks and months compared with current systems. Researchers at the National Institute of Standards and Technology (NIST), in collaboration with researchers from Georgia Tech, have made the first-of-its-kind chip-scale beam clock. Their work has been published in Nature Communications. Atomic clocks take many forms, but the oldest and one of the most prominent designs is built using atomic beams. These clocks send a beam of atoms through a vacuum chamber. At one end of the chamber, the atoms are set in a specific quantum state, and they start \"ticking.\" At the other end their ticking rate is measured or \"read out.\" Using the atoms' precise ticking rate, other clocks can be compared to atomic beam clocks, and adjusted to match their timing. NIST has been using atomic beams for timekeeping since the 1950s. For decades, beam clocks were used to keep the primary standard for the second, and they are still part of NIST's national timekeeping ensemble. Beam clocks are precise, stable and accurate, but they're currently not the most portable. The vacuum chambers where the atoms travel are key to the success of these clocks, but they're bulky in part due to the size of the microwave cavity used to probe the atomic \"ticking.\" The vacuum chamber for NIST-7, the last beam clock used for the primary frequency standard in the U.S., was more than 2.5 meters or 8 feet long. Smaller commercial clocks about the size of a briefcase are common, but they still require a significant amount of power (about 50 watts) to run. For comparison, smartphones require about a third of a watt for typical operation. Chip-scale atomic clocks (CSACs) were developed by NIST in 2001. Advances in microfabrication techniques let NIST make vapor cells, tiny chambers where the clock's atoms are held and measured, the size of a grain of rice; the entire clock is about the size of a piece of sushi. These clocks consume very little power and can run on batteries to provide timing in critical situations where GPS can't reach. CSACs have found numerous applications in underwater oil and gas exploration, military navigation, and even telecommunications. However, the clocks' timekeeping tends to drift when temperatures shift and the gas surrounding the atoms degrades. \"The CSAC is low-power and has high performance given its size. It's a wonderful device, but it does drift after running for a few thousand seconds,\" said William McGehee, a physicist at NIST. \"Beam clocks have been around since the 1950s and are stable, but still need a lot of power. What if we could combine the best aspects of these two systems?\" Using microfabrication techniques learned from the CSAC, the group fabricated a chip-scale atomic beam device using a stack of etched silicon and glass layers. This device is a highly miniaturized version of the chambers that have been used in atomic beam clocks like NIST-7 and is about the size of a postage stamp. Atomic vapor cell construction techniques developed at NIST and etched microcapillary arrays developed at Georgia Tech were key to shrinking the vacuum chambers of larger beam clocks. In the device, one chamber contains a small pill of rubidium. That chamber heats up, releasing a stream of rubidium atoms through microcapillaries, channels only 100 micrometers wide. Those tiny channels connect to another chamber with materials that can absorb\u2014or collect\u2014individual gas molecules, called non-evaporable getters, or NEGs, which pull the rubidium atoms along and collect them, keeping the vacuum in the microcapillaries clean. Tiny rods of graphite also help collect stray atoms through the process. Right now, this chip-scale beam device is a prototype for a miniature atomic beam clock. Initial tests of the chip-scale beam clock showed performance at a level slightly worse than existing CSACs, but the team sees a path toward improved stability. The researchers hope to push their precision by another factor of 10, and to exceed the stability of existing CSACs by 100 times over week time scales. ","News_Title":"New tiny atomic beam clock could bring stable timing to places GPS can't reach","Topic":"Physics"}
{"Paper_Body":"Abstract In polycystic kidney disease (PKD), fluid-filled cysts arise from tubules in kidneys and other organs. Human kidney organoids can reconstitute PKD cystogenesis in a genetically specific way, but the mechanisms underlying cystogenesis remain elusive. Here we show that subjecting organoids to fluid shear stress in a PKD-on-a-chip microphysiological system promotes cyst expansion via an absorptive rather than a secretory pathway. A diffusive static condition partially substitutes for fluid flow, implicating volume and solute concentration as key mediators of this effect. Surprisingly, cyst-lining epithelia in organoids polarize outwards towards the media, arguing against a secretory mechanism. Rather, cyst formation is driven by glucose transport into lumens of outwards-facing epithelia, which can be blocked pharmacologically. In PKD mice, glucose is imported through cysts into the renal interstitium, which detaches from tubules to license expansion. Thus, absorption can mediate PKD cyst growth in human organoids, with implications for disease mechanism and potential for therapy development. Introduction Autosomal dominant polycystic kidney disease (PKD) is commonly inherited as a heterozygous, loss-of-function mutation in either PKD1 or PKD2 , which encode the proteins polycystin-1 (PC1) or polycystin-2 (PC2), respectively 1 , 2 . PKD is characterized by the growth of large, fluid-filled cysts from ductal structures in kidneys and other organs, and is among the most common life-threatening monogenic diseases and kidney disorders 3 . Tolvaptan (Jynarque), a vasopressin receptor antagonist that decreases water absorption into the collecting ducts, was recently approved for treatment of PKD in the United States, but only modestly delays cyst growth and has side effects that limit its use 4 , 5 . At the molecular level, PC1 and PC2 form a receptor-channel complex at the primary cilium that is poorly understood but possibly acts as a flow-sensitive mechanosensor 6 , 7 , 8 , 9 , 10 , 11 . Loss of this complex results in the gradual expansion and dedifferentiation of the tubular epithelium, including increased proliferation and altered transporter expression and localization 12 , 13 , 14 . As mechanisms of PKD are difficult to decipher in vivo, and murine models do not fully phenocopy or genocopy the human disease, we have developed a human model of PKD in vitro 15 , 16 , 17 . We, together with other groups around the world, have invented methods to derive kidney organoids from human pluripotent stem cells (hPSC), which contain podocyte, proximal tubule, and distal tubule segments in contiguous, nephron-like arrangements 17 , 18 , 19 , 20 . Differentiation of these organoids is highly sensitive to the physical properties of the extracellular microenvironment 21 . Organoids derived from gene-edited hPSC with biallelic, truncating mutations in PKD1 or PKD2 develop cysts from kidney tubules, reconstituting the pathognomonic hallmark of the disease 15 , 16 , 17 . Interestingly, culture of organoids under suspension conditions dramatically increases the expressivity of the PKD phenotype, revealing a critical role for microenvironment in cystogenesis 16 . Fluid flow is a major feature of the nephron microenvironment, which is believed to play an important role in PKD 4 , 5 , 7 , 8 , 22 . However, physiological rates of flow have not yet been achieved in kidney organoid cultures or PKD models. \u2018Kidney on a chip\u2019 microphysiological systems provide fit-for-purpose platforms integrating flow with kidney cells to model physiology and disease in a setting that more closely simulates the in vivo condition than monolayer cultures 23 , 24 , 25 , 26 , 27 . There is currently intense interest in integrating organ on chip systems with organoids, which can be derived from hPSC as a renewable and gene-editable cell source 28 , 29 , 30 , 31 , 32 . We therefore investigated the effect of flow on PKD in a human organoid on a chip microphysiological system. Results Flow induces cyst swelling in PKD organoids Prior to introducing flow, we first confirmed the specificity and timing of the PKD phenotype in static cultures. PKD1 \u2212\/\u2212 or PKD2 \u2212\/\u2212 hPSC were differentiated side-by-side with isogenic controls under static, adherent culture conditions to form kidney organoids. On day 18 of differentiation, prior to cyst formation, organoids were carefully detached from the underlying substratum and transferred to suspension cultures in low-attachment plates. Under these conditions, the majority of PKD1 \u2212\/\u2212 or PKD2 \u2212\/\u2212 organoids formed cysts within 1\u20132 weeks, whereas isogenic control organoids rarely formed cysts (Fig. 1a ). In repeated trials, the difference between PKD organoids and isogenic controls was quantifiable and highly significant (Fig. 1a ). Thus, PKD organoid formed cysts in a genotype-specific manner, strongly suggesting that this phenotype was specific to the disease state. This differs from other types of three-dimensional cultures of epithelial cells, in which hollow \u2018cysts\u2019 (spheroids) arise irrespective of PKD genotype and represent a default configuration of the epithelium rather than a disease-specific phenotype 17 , 33 , 34 , 35 . Fig. 1: Organoid PKD cysts expand under flow. a Representative images of organoids on days following transfer to suspension culture (upper), with quantification (lower) of cyst incidence as a fraction of the total number of organoids (mean \u00b1 s.e.m. from n \u2265 4 independent experiments per condition; **** p < 0.0001). b Schematic of workflow for fluidic condition. c Time-lapse phase contrast images of PKD organoids under flow (0.2 dynes\/cm 2 ), representative of four independent experiments. d Average growth rates of control organoids (Ctrl org.), non-cystic compartments of PKD organoids (PKD org.), and cystic compartments of PKD organoids (PKD cysts) under flow (0.2 dynes\/cm 2 ). Each experiment was performed for 6 h. Cyst growth rate was calculated on an individual basis as the maximal size of the cyst during the time course, divided by the time point at which the cyst reached this size (mean \u00b1 s.e.m. from n \u2265 four independent experiments; each dot represents the average growth rate of organoids in a single experiment. **** p < 0.0001). Full size image To understand how flow affects PKD in organoids, we designed a microfluidic system that allows for live imaging of kidney organoids during the early stages of cyst formation (Fig. 1b ). hPSC were first differentiated into organoids under static, adherent culture conditions for 26 days, at which time point tubular structures had formed with small cysts in the PKD cultures. Organoids were then purified by microdissection using a syringe needle 16 , and transferred into gas-permeable, tissue culture-treated polymer flow chambers (0.4 mm height \u00d7 3.8 mm width), which were optically clear and large enough to comfortably accommodate organoids and cysts. The channels were pre-coated with a thin layer of Matrigel, and organoids were allowed to attach overnight. PKD and isogenic control organoids were subjected to fluid flow with a wall shear stress of 0.2 dynes\/cm 2 , which approximates physiological shear stress within kidney tubules 27 , 36 , 37 , 38 . In these devices, we observed that cysts in PKD organoids increased in size rapidly under flow (change in area of ~20,000 \u03bcm 2 \/hr, or ~160 \u03bcm\/hr in diameter), compared to non-cystic compartments within these organoids, or isogenic control organoids lacking PKD mutations, which did not swell appreciably (Fig. 1c, d and Supplementary Movie 1 ). Diffusion can partially substitute for flow Having observed that cysts expand under microfluidic conditions, it was important to establish a corresponding static condition lacking flow as a negative control. Initially we utilized the same chambers and syringe pump in the absence of pump activation, which is a commonly used control format for microfluidic experiments. However, we observed that food dye contained within the syringe failed to enter the microfluidic chamber under these conditions (Supplementary Fig. 1a ). This indicated a lack of diffusion, which meant that organoids would be exposed only to the volume of media present within the channel of the microfluidic device (~200 \u00b5L), which was much lower than the volume they would encounter under fluidic conditions (~60 mL\/6 h). Such a static condition could not be readily compared to fluidic conditions to determine the effects of flow, since other parameters such as volume and total solute mass would also be very different. To more accurately control for the effects of flow, we designed a diffusive static condition that exposed organoids to an equivalent volume of culture media as in the flow condition. This consisted of a reservoir of media (maximum volume of 25 mL) connected to the microfluidic chip by wider tubing to allow for efficient and uninhibited diffusion of small molecules into the microfluidic channel. In this static format, food dye diffused from the media reservoir into the channel after 2\u20133 h (Supplementary Fig. 1b ). Similarly, rhodamine-labeled dextran (10 kDa) diffused from the media reservoir into the channel and equilibrated with fluidic epifluorescence within 48 h (Fig. 2a ). Fig. 2: Volume can partially substitute for flow in cyst expansion. a Rhodamine dextran (10 kDa) epifluorescence in static (non-diffusive), diffusive static, and fluidic conditions. \u2018Lane\u2019 indicates channel interior, and b time lapse phase contrast images of cysts in these conditions. Images are representative of n \u2265 4 independent experiments. c Average growth rates (\u03bcm 2 \/hr) of cysts in diffusive static condition with different volumes, compared to fluidic or non-diffusive static. Each experiment was performed for 6 h. Cyst growth rate was calculated on an individual basis as the maximal size of the cyst during the time course, divided by the time point at which the cyst reached this size. ( n \u2265 8 cysts (dots) pooled from two or more independent experiments; *** p < 0.05). d Schematic of experiment testing effect of volume vs. pressure on cyst growth. Elements of the image were illustrated using Biorender software under license. e Representative phase contrast images and ( f ) quantification of growth rate of cysts suspended in either 0.5 or 10 mL of media under equivalent hydrostatic pressures (mean \u00b1 s.e.m. of n \u2265 14 cysts per condition pooled from three independent experiments; ***, p < 0.05). g Growth profiles of individual cysts (lines) over time in microfluidic devices from 0\u20135 h. Measurements made every 5 min using ImageJ software. Cyst Area was normalized by dividing by the starting area. Data points are from three or more independent experiments. h Sum of Squares values from linear regression models were run on each individual cyst ( n \u2265 7 organoids per condition, pooled from four or more independent experiments; p = 0.0342 versus diffusive static and 0.0411 versus static). Error bars, standard error. Full size image To further validate this \u2018diffusive static\u2019 condition, we varied the volume of media in the reservoir and analyzed cyst growth over a period of 12 h. Cysts exposed to a reservoir containing 1 mL of media expanded at a rate of ~3,000 \u03bcm 2 \/hr, whereas a reservoir containing 25 mL increased expansion to ~10,000 \u03bcm 2 \/hr, approximately half the rate observed in the fluidic condition (Fig. 2b, c , Supplementary Movies 2 \u2013 4 ). Using the equation Pressure = \u03c1gh , the hydrostatic pressure on organoids with 1 mL and 25 mL media reservoirs was calculated to be 1174 Pa and 1956 Pa, respectively. As this represented a substantial pressure difference of 5.9 mmHg, we conducted experiments to distinguish between the effects of pressure versus volume on cyst growth. Cystic organoids were suspended in either 500 \u00b5l or 10 ml, with a constant fluid column height of 1 cm (Fig. 2d ). Cysts exposed to 10 mL of media grew significantly more than those exposed to 500 \u00b5L of media (Fig. 2e, f ). Thus, media volume was identified as a major determinant of expansion that could partially substitute for flow in this system. Not all aspects of the fluidic condition were replicated by the diffusive static condition. Time-lapse microscopy under continuous flow revealed that PKD cysts exhibited fluctuating growth profiles, expanding and constricting (deflating) in cyclical, \u201cbreath-like\u201d movements. Constrictions occurred rapidly when the cysts appeared to be fully inflated, suggesting that they resulted from rupture of the epithelium, for instance in response to expansive fluid force (Fig. 2g ). Growth and constriction events occurred within hours after the initiation of flow, indicating a rapid physical mechanism rather than a slower one based on cell proliferation. This oscillatory behavior was unique to the fluidic condition, and was not observed in either the diffusive static or non-diffusive static conditions, nor in non-cystic controls (Fig. 2g and Supplementary Movie 1 ). Using the sum of squares method, we found that cyst dynamics (variance in size within an individual structure over time) were much greater in the fluidic condition, compared to either of the static conditions (Fig. 2h ). As solute exposure was likely to occur much more rapidly in the fluidic condition, we proceeded to examine solute uptake under these conditions. Cysts absorb glucose during flow-mediated expansion Kidneys are highly reabsorptive organs, retrieving ~180 L of fluid and solutes per day through the tubular epithelium back into the blood. Glucose is an abundant renal solute and transport cargo, which might explain the effects of media exposure on cyst expansion, but whether kidney organoids absorb glucose is unknown. We therefore studied glucose transport in cysts and organoids using a fluorescent glucose analog, NBD glucose (2-( N -(7-Nitrobenz-2-oxa-1,3-diazol-4-yl)Amino)\u22122-Deoxyglucose). The low height of the channels in our flow devices enabled continuous time lapse imaging of fluorescent molecules without high background fluorescence. Glucose was observed to infiltrate into the devices under both diffusive static as well as fluidic conditions. Epifluorescence of NBD glucose gradually increased and plateaued at similar levels after 12 h in both the diffusive static condition and the fluidic condition, but did not accumulate detectably within the channels in the non-diffusive static condition (Fig. 3a ). Fig. 3: PKD organoids absorb glucose under fluidic and static conditions. a NBD Glucose background levels in non-diffusive static, diffusive static, and fluidic conditions after 12 h (representative of three independent experiments). b Phase contrast and wide field fluorescence images of organoids in diffusive static and fluidic conditions, 5 h after introduction of NBD glucose (representative of three independent experiments). Arrows are drawn to indicate representative line scans. c Line scan analysis of glucose absorption in PKD cysts under static and fluidic conditions after 5 h (mean \u00b1 s.e.m. from n \u2265 7 cysts per condition pooled from three independent experiments; each n indicates the average of four line scans taken from a single cyst). Background fluorescence levels were calculated at each timepoint by measuring the fluorescence intensity of a square region placed in the non-organoid region of the image. d NBD Glucose absorption in the non-cystic compartment of PKD organoid, for Diffusive static 20 mL vs. 1 mL (110 \u00b5M NBD Glucose, mean \u00b1 s.e.m., n \u2265 4 independent experiments), and ( e ) diffusive static 25 mL vs. Fluidic (36.5 \u00b5M NBD Glucose, n \u2265 5 independent experiments). f Confocal fluorescence images of SGLT2 and ZO1 in PKD1 tubules (representative of three independent experiments). g Confocal fluorescent images of NBD Glucose in organoid tubules, fixed and stained with fluorescent cell surface markers (representative of three independent experiments). h Time-lapse images of NBD Glucose accumulation in a PKD organoid cyst, followed by washout into media containing unlabeled glucose after 24 h, all performed under continuous flow (representative of three independent experiments). Full size image When this assay was performed in channels seeded with organoids, PKD cysts absorbed glucose under fluidic and diffusive static conditions (Fig. 3b and Supplementary Movies 5 \u2013 6 ). Line scan analysis of these images showed that there was no significant difference in absorption between the fluidic and diffusive static conditions (Fig. 3c ). Analysis of glucose absorption in organoid tubules over time confirmed that the volume of media in the static condition was a crucial factor in nutrient absorption (Fig. 3d ). Glucose absorption in organoids over time under the diffusive static condition followed an S-shaped absorption curve, whereas glucose levels in the fluidic condition increased rapidly and then plateaued, approximating an exponential curve, but both conditions plateaued at approximately the same maximal level of glucose absorption (Fig. 3e ). These studies suggested that flow has no additional effect on glucose absorption in organoids when compared to a static control presenting equivalent total glucose exposure. Glucose absorption was a general property of kidney organoids. In non-cystic structures, sodium-glucose transporter-2 (SGLT2) was expressed in organoid tubules and enriched at the apical surface, delineated by the tight junction marker ZO-1 (Fig. 3f ). Immunofluorescence confirmed that NBD glucose was absorbed into and accumulated inside organoid proximal and distal tubules (Fig. 3g ). Immunoblot analysis indicated similar levels of SGLT2 in control and PKD organoid cultures (Supplementary Fig. 2a, b ). Cyst-lining epithelia expressed SGLT2, and accumulated glucose both intracellularly as well as inside their lumens (Supplementary Fig. 2c ). Intracellular glucose levels were generally higher than extracellular levels, consistent with the tendency of NBD glucose to accumulate inside cells (Supplementary Fig. 3a\u2013c ). Although cysts were much less cell-dense than attached non-cystic compartments, cystic and non-cystic compartments accumulated similar total levels of glucose, owing to the larger size of the cysts (Supplementary Fig. 3d ). When PKD organoids loaded with NBD glucose were switched into media containing only unlabeled glucose (washout), NBD glucose disappeared rapidly from these structures (Fig. 3h and Supplementary Movie 7 \u2013 8 ). Thus, organoids continuously accumulated and released glucose in a dynamic fashion. Inhibition of glucose transport blocks cyst growth In animal models, inhibitors of glucose transport are suggested to have both positive and negative effects in PKD 39 , 40 . To test functionally whether cyst growth is linked to glucose transport in human organoids, cyst expansion was quantified in increasing concentrations of D-glucose under static conditions (96-well plate). Growth was maximal at 15\u201330 mM glucose, causing ~50% increase in cyst expansion, relative to lower or higher concentrations (Fig. 4a, b and Supplementary Fig. 4a ). Live\/dead analysis of cysts treated with 60 mM glucose detected cytotoxicity, explaining the reduction in cyst growth at this higher concentration (Supplementary Fig. 4b\u2013d ). Fig. 4: PKD cysts expand in response to glucose stimulation. a Representative time lapse brightfield images and ( b ) quantification of change in cyst size in PKD organoids in static suspension cultures containing with D-Glucose concentrations (mean \u00b1 s.e.m., n \u2265 6 pooled from four independent experiments, each dot indicates a single cyst). c Representative time lapse images and ( d ) quantification of PKD organoids in 15 mM D-Glucose treated with phloretin (mean \u00b1 s.e.m., n \u2265 10 cysts pooled from four independent experiments, p = 0.0231). e Quantification of maximum intensity projections of live\/dead staining in organoids treated with phloretin (mean \u00b1 s.e.m., n \u2265 11, pooled from two independent experiments, each dot indicates a cystic organoid). f Images of live staining with Calcein AM (representative of three independent experiments). g Brightfield images and ( h ) quantification of size changes in cystic PKD organoids in 15 mM D-Glucose treated with probenecid (mean \u00b1 s.e.m., n \u2265 9 pooled from two independent experiments). Full size image The preceding findings, together with the rapid turnover of glucose in organoids described above, suggested that inhibition of glucose import might enable export mechanisms to dominate, resulting in blockade or even reversal of cyst growth due to osmotic effects. To test this hypothesis, we examined the effects of pharmacological transport inhibitors on cysts in static conditions. Phloretin, a broad spectrum inhibitor of glucose uptake, was tested in 15 mM glucose, and found to decrease cyst size by 77% at a concentration of 800 \u03bcM (Fig. 4c, d and Supplementary Fig. 5a ). Live-dead staining at 24 and 48 h of phloretin treatment revealed no significant toxicity (Fig. 4d, e and Supplementary Fig. 5b, c ). Treatment with either phloridzin, a non-selective inhibitor of both SGLT1 and SGLT2, or with dapagliflozin, a specific inhibitor of SGLT2, reduced cyst growth to baseline at non-toxic doses, further supporting the hypothesis (Supplementary Fig. 5d, e ). Net shrinkage of cysts was not observed with phloridzin or dapagliflozin, suggesting either decreased potency of these compounds relative to phloretin, or an off-target effect of phloretin beyond glucose transport that further reduces cyst size. In contrast to SGLT inhibitors, probenecid, an inhibitor of the OAT1 transporter on the basolateral membrane, had no effect on cyst growth compared to controls at non-toxic doses (Fig. 4f\u2013h and Supplementary Fig. 5f ). Overall, these findings supported the hypothesis that pharmacological inhibitors of glucose uptake block cyst expansion in the PKD organoid model. Organoid cysts polarize outwards Some previous studies have suggested that cyst expansion may be due to increased secretory (basolateral-to-apical) solute transport 41 , 42 , 43 , 44 . However, glucose transport in the proximal tubule is predominantly reabsorptive (apical-to-basolateral) rather than secretory. To better understand the directionality of transport within organoids, we determined the apicobasal polarity of tubules and cysts using antibodies against tight junctions and cilia. In both PKD and control organoids, the ciliated surface of these tubules faced inwards (Fig. 5a ). Surprisingly, however, PKD cysts were polarized with the apical ciliated surface facing outwards towards the media and exposed to flow (Fig. 5a ). Thus, the external cyst surface resembled the apical surface of a tubule in this system. Line scan analysis confirmed this inverted polarization, with primary cilia and tight junction intensity profiles reversed in organoids vs. cysts (Fig. 5b ). Fig. 5: PKD cysts form via expansion of outwards-facing epithelium. a Confocal immunofluorescence images of cilia (acetylated \u03b1-tubulin, abbreviated AcT) and tight junctions (ZO-1) in proximal tubules (LTL) of PKD and non-PKD organoids, as well as in PKD cyst lining epithelial cells. Dashed arrow indicates how line scans were drawn. Images are representative of three independent experiments. b ZO1 and AcT intensity profiles in cysts vs. organoids. Line scans were drawn through cilia from lumen to exterior of structures. (mean \u00b1 s.e.m. from n = 5 line scans pooled from three organoids or cysts per condition from three independent experiments). c Fluorescent images of stromal markers in PKD organoids compared to human kidney tissue from a female patient 50 years of age with autosomal dominant PKD. Scale bars 20 \u00b5m. d Fluorescent images of cysts after having been overlaid with collagen. Images are representative of three independent experiments. e Z-stack confocal images of early (day 30) PKD organoid cyst in adherent culture. Zoom shows boxed region. White arrow indicates a podocyte cluster continuous with the peripheral epithelium. Images are representative of three independent experiments. f Close-up image showing peripheral epithelium of control (non-PKD) organoid in adherent culture. Yellow arrowhead indicates region of epithelial invagination. Images are representative of three independent experiments. g Phase contrast time-lapse images showing formation of PKD cysts from non-cystic structures in adherent cultures. Red arrows indicates tubular structures internal to the peripheral cyst. Images are representative of three independent experiments. h Schematic model of absorptive cyst expansion in organoids. Fluid flow (blue arrows) is absorbed into outwards-facing proximal tubular epithelium, which generates internal pressure that drives expansion and stretching of the epithelium (red arrows). A simplified organoid lacking podocytes or multiple nephron branches is shown for clarity. Full size image Close examination of PKD organoid cysts revealed that a subpopulation of these contained a layer of cells expressing alpha smooth muscle actin immediately beneath the cyst-lining epithelium, which formed a laminin-rich basement membrane (Fig. 5c ). In contrast, in human kidney tissue the basement membrane and myofibroblast-like cells surrounded cysts externally (Fig. 5c ). Thus, apical cell polarity aligned opposite the basement membrane in both systems. Simple spheroids of Madine-Darby Canine Kidney cells in suspension culture polarize outwards, but can reverse apicobasal polarity from outwards to inwards when embedded in collagen 34 . When PKD cysts in organoids were overlaid with collagen, however, cyst polarity remained inverted and did not repolarize with the ciliated surface facing away from the extracellular matrix, indicating that organoid cyst polarity was deeply entrenched and governed by more dominant, internal cues (Fig. 5d ). The observation that cysts polarized outwards seemed counter-intuitive, as tubule structures in human kidney organoids typically polarize inwards, with tight junctions and apical markers abutting one another from diametrically opposed epithelia (as shown in Fig. 5a ) 17 . To resolve this conundrum, we closely examined PKD organoids in three-dimensional confocal image z-stacks. Lotus tetragonolobus lectin (LTL), which is expressed more strongly in tubules than in cysts, was used to label the epithelium, while primary cilia and ZO-1 were used to indicate cell polarity. These experiments revealed that young cysts comprised epithelial spheroid structures (predominantly LTL + ) with underlying tubular infolds, which faced inwards (Fig. 5e ). We further examined organoids without cysts (controls) in confocal microscopy z-stacks. We noted that epithelium lining the periphery of these organoids faced outwards, whereas \u2018tubules\u2019 internal to organoids were invaginations of this peripheral epithelium (Fig. 5f and Supplementary Fig. 6a, b ). The innermost regions of these invaginated tubules were enriched for ECAD, a marker of distal tubule, whereas the external peripheral epithelia were enriched for LTL, a marker of proximal tubule (Fig. 5f and Supplementary Fig. 6a ). Thus, organoids constituted a continuous, proximal-to-distal epithelium, with the apical surface polarized outwards on the peripheral (more proximal) epithelium and inwards in the internal (more distal) epithelium of the structure. To observe the process of cyst formation in real time, we collected time-lapse images of young PKD organoids undergoing cystogenesis over eight days in culture. Consistently, cysts formed at the periphery of the organoids (Fig. 5g , Supplementary Fig. 7a, b , and Supplementary Movie 9 ). During the early stages of cystogenesis, tubular structures remained visible inside the cysts as they expanded (Fig. 5g , Supplementary Fig. 7a, b , and Supplementary Movie 9 ). Thus, time-lapse imaging supported the idea that cysts formed from the peripheral epithelium of the organoids that faced outwards towards the media, rather than from the internal tubular invaginations, which tended to stay anchored (Fig. 5h ). This was consistent with an absorptive mechanism mediated by the peripheral epithelium. Absorptive cysts form in vivo It is important to understand how these findings in organoids might relate to PKD cyst formation in vivo, where cyst-lining epithelia face inwards rather than outwards. Microcysts smaller than 1 mm diameter and undetectable by magnetic resonance imaging are numerous in kidney sections from patients with early stages of PKD, and are proposed to form as focal outpouchings of tubular epithelium 45 , 46 . If such an outpouching remained connected to a small segment of the original tubule via apical junctions, it could accumulate fluid through tubular reabsorption. The preceding suggested a possible model for cyst formation in vivo (Fig. 6a ). Absorption of glucose through the apical surface of the tubular epithelium is followed by water along the osmotic gradient via paracellular or transcellular routes to maintain balanced concentrations on either side of the epithelium. There is a lack of appropriate outlet for this absorptive activity, creating a pressure within the interstitium and leading to its detachment from neighboring tubules, which undergo deformation and expansion to fill the resultant interstitial space. This process continues as the cyst grows, and may be exacerbated by the gradual loss or detachment of associated peritubular capillaries (which reduces the absorptive sink), and by growth of interstitial mesenchymal stromal cells, which provide a scaffold and synthesize extracellular matrix to accommodate the expanding epithelium. Fig. 6: PKD cysts in vivo absorb glucose into the surrounding interstitium. a Hypothetical schematic of absorptive cyst formation in kidney tissue. Fluid (blue arrows) is absorbed through proximal tubules into the underlying interstitium, which partially detach from the epithelium. The tubules then expand and deform to fill the interstitial space, reaching a low-energy conformation in which the withheld volume is ultimately transferred back into the luminal space of the nascent microcyst. A simplified model is shown and represents one possible explanation of the findings. b PAS stains of 2-month-old and 6-month-old Pkd1 RC\/RC mice (C57BL\/6 J background). Scale bars 50 \u00b5m. Images are representative of 4 animals per condition (two male and two female). c Confocal images of stromal basement membrane (LAMA1) with cilia (AcT) or ( d ) endothelial cells (CD31) in Pkd1 RC\/RC versus control ( Pkd1 +\/+ ) 2-month-old mice. All mice were C57BL\/6 J background. Yellow arrowheads indicate areas of detached or expanded interstitium surrounding the cyst. Images are representative of four animals per condition (two male and two female). e Schematic of glucose uptake assay, illustrated using Biorender software under license. f Representative images and ( g ) line scan analysis of PKD cysts after perfusion with fluorescent NBD glucose or unlabeled PBS control (mean \u00b1 s.e.m., n \u2265 17 cysts per condition pooled from a total of three female and two male Pkd1 RC\/RC mice of C57BL\/6J background). Dashed magenta arrows indicate how line scans were drawn. Full size image To investigate the plausibility of such a mechanism in vivo, we analyzed microcysts in the Pkd1 RC\/RC mouse strain, which has a hypomorphic Pkd1 gene mutation orthologous to patient disease variant PKD1 p. R3277C, and manifest a slowly progressive PKD during adulthood over a period of several months 47 , 48 . Histology sections and confocal images of 2-month-old mouse tissue revealed continuous basement membranes between tubules and microcysts, consistent with the possibility that microcysts form from tubular outpouchings that remain capable of absorption through the wall of the neighboring tubule (Fig. 6b, c ). While much of these microcysts remained tightly associated with peritubular capillaries, suggesting that they continue to reabsorb, portions of the epithelium appeared to have detached from the endothelium, resulting in areas of fluid accumulation or interstitial expansion (Fig. 6b\u2013d ). To determine whether PKD cysts absorbed glucose in vivo, we devised a methodology to inject mice with NBD glucose and immediately retrieve their kidneys (Fig. 6e ). Fluorescence microscopy analysis of kidney tissue sections revealed that cyst-lining epithelia and the surrounding interstitium readily took up NBD glucose (Fig. 6f\u2013g ). Thus, cysts remained absorptive in vivo and PKD kidneys as a whole readily accumulated glucose. Discussion Coupling the structural and functional characteristics of organoids with the controlled, microfluidic microenvironments of organ-on-a-chip devices is a promising approach to in vitro disease modeling 28 . Our study combines CRISPR-Cas9 gene editing to reconstitute disease phenotype with organoid-on-a-chip technology to understand the effect of flow, which is difficult to assess in vivo (where it is constant) and has hitherto been absent from kidney organoid models at physiological strength. The \u2018human kidney organoid on a chip\u2019 microphysiological system described here incorporates organoids with PKD mutations in a wide-channel format, which allows liquid to flow over the organoids, similar in geometry to other recently described organoid flow systems 29 , 30 . At the core of this system are human organoids that strikingly recapitulate the genotype-phenotype correlation in PKD. This is fundamentally different from other other types of generic spheroids that form in vitro as a default configuration of the epithelium. While certain aspects of the organoid system differ from in vivo, we do not see a plausible explanation wherein the genotype-phenotype correlation is preserved, but the entire system is somehow irrelevant or opposite to the fundamental mechanism of PKD. Rather, the system is teaching us which aspects of PKD are most important for the phenotype. The system can be readily assembled from commercially available components, and produces a shear stress associated with the physiological range found in human kidney kidney tubules 27 , 36 , 37 , 38 . This is ~6-fold greater than the maximum rate of 0.035 dyn\/cm 2 used in a previous kidney organoid-on-a-chip device, a shear stress that was nevertheless sufficient to stimulate expansion of vasculature within the device when compared to static conditions 29 , and to induce dilation of tubular structures derived from hPSC with mutations associated with autosomal recessive PKD (ARPKD) 49 . The physiological relevance of such low flow rates is not clear, and the cohort of ARPKD cell lines that was studied includes hPSC previously generated by our laboratory that we found to lack definitive ARPKD mutations 50 . It is nevertheless interesting and encouraging that flow over the organoids was capable of inducing swelling in both systems. Importantly, we have also developed a static module using the same basic chip that is capable of natural diffusion from a syringe reservoir. This enables us to distinguish the effects of flow from those of exposure to fluid volume and mass of reabsorbable solute, which is difficult to achieve in conventional systems with limited diffusion such as tightly connecting a reservoir to a Luer lock syringe. Our discovery that volume can partially substitute for flow is reminiscent of a recent study in which immersion in >100-fold volumes induced three-dimensional morphogenesis of intestinal epithelial cells similar to flow 51 . In contrast, increased volume was unable to substitute for flow in the aforementioned study of endothelial expansion in kidney organoid cultures. This may reflect a sensitivity of vascular cells to fluid shear stress, or alternatively the limited volumes possible in closed loop systems 29 . In addition to volume, hydrostatic pressure is increased in our diffusive static condition, which may play a role in PKD phenotype 52 . Of note, cysts in our diffusive static condition did not exhibit the dramatic oscillations in size observed under flow, indicating roles for flow-induced mechanoregulation that cannot be readily replicated by diffusion effects, for instance involving stretch-activated ion channels. Our findings indicate that flow, volume, and solute concentrations are positive regulators of cyst expansion. Cystogenesis can be enhanced through mechanisms of tubular absorption and glucose transport. A limitation of these systems is that the perfusion passes over the organoids, rather than through them as it does through tubules in vivo. However, as peripheral epithelia in our organoids face outwards towards the media, the net result is for the apical surface to be in contact with the directional flow, similar to the epithelium of a tubule in vivo. This fortuitously enables us to assess reabsorptive function, the primary characteristic of kidney tubular network, which fluxes ~180 L through its apical surface every day. In this regard, the arrangement in the organoid system may have greater functional relevance than spheroid systems in which cyst polarity faces inwards but the liquid is trapped inside with no possibility of perfusion (unlike the arrangement in the kidneys). The observation that PKD cysts can form inside-out, such that the secretion (basolateral-to-apical transport) would occur in the opposite direction from cysts in vivo, argues against secretion as the critical driver of cystogenesis in this system 43 . Our experiments in animals also demonstrate that kidney cysts remain reabsorptive even in advanced PKD. In our studies in vivo, we also made the interesting discovery that the tubular epithelium detaches focally from the underlying interstitium during pre-cystic stages of disease, which may reflect the consequences of a possible absorptive phenotype. Studies of PKD in living animals, however, carry significant constraints for studying mechanism. Kidneys are concealed within the body, preventing detailed time-lapse microscopy, and perturbing renal absorption is experimentally challenging and causes complex side effects. Demonstrating glucose absorption in cystic kidneys in vivo, and showing interstitial detachment, as we have done, required significant methods development and careful analysis. Further methods development and more detailed studies are required to causally link absorption, interstitial detachment, and cyst formation in vivo. Nevertheless, it is clear that renal cysts can continue to absorb glucose, even in vivo, and in organoids, glucose absorption is linked to the PKD phenotype, which is demonstrably specific to the genotype and thus mechanistically relevant. These findings are consistent with macropuncture studies showing that wall pressures inside PKD cysts in vivo resemble their originating nephron segments, and studies of excised cysts in vitro, which demonstrate that the epithelium is slowly expanding and absorptive under steady-state conditions 44 , 45 . In a more recent clinical analysis, patients with ADPKD demonstrated lower excretion of renally secreted solutes, rather than higher levels of secretion 53 . Drugs that activate CFTR, which is hypothesized to drive a secretory phenotype in PKD, have shown promise in treating PKD in mice, rather than exacerbating the disease, which is also inconsistent with a secretory hypothesis 54 . Indeed, a phenotype related to absorption is a much more natural fit for the specialized properties of kidney epithelia (which are predominantly absorptive) than secretion. This is not to say that secretion cannot be a causative mechanism in PKD cystogenesis, but rather that absorption can also play a critical role. In our model, absorption of fluid into the interstitium creates space for epithelia to expand and fill. During this process of expansion and space filling by the epithelium, which is triggered by changes within the microenvironment surrounding the tubules, it is conceivable that secretory processes play a role. Previously, we observed that transfer of PKD organoids from adherent cultures into suspension cultures was associated with dramatically increased rates of cystogenesis 16 . Our current findings add greatly to our understanding of this phenomenon. Upon release from the underlying substratum, the peripheral organoid epithelium grows out and envelops the rest of the organoid 16 . This forms an enclosed, outwards-facing structure in an ideal conformation to absorb fluid from the surrounding media and expand into a cyst. Although we did not detect differences in the levels of SGLT2, differences may exist in SGLT2 activity, or in the levels or activity of other transporters involved in absorption, resulting in increased absorptive flux in PKD epithelia, compared to non-PKD. Alternatively, there might exist a difference in the pliability of PKD epithelia versus non-PKD epithelia undergoing equivalent levels of absorptive flux. We note that polycystin-2 is a non-selective cation channel expressed at the apical plasma membrane 9 , 10 , which could conceivably play a role in transporter function and reabsorption. The polycystin complex may also possess force- or pressure-sensitive mechanoreceptor properties, which could regulate the epithelial response to fluid influx 4 , 5 , 7 , 8 , 22 , 52 . Although we favor a direct role for glucose absorption in driving cyst expansion, glucose transport could also function separately of water transport to impact cyst formation, for instance by altering mitochondrial metabolism or signaling changes to the actin cytoskeleton, which could promote cystogenesis regardless of which direction the cells face 55 , 56 , 57 , 58 . Of note, cysts form not only in the proximal tubules that are primarily responsible for glucose reabsorption, but also in the collecting ducts, where they can reach very large sizes. As cysts can and must originate from these very different epithelial cell types, the process of cystogenesis is not likely to be explained by a simple absorption\/secretion ratio for any one solute. One goal for future development of our PKD organoid system is to incorporate collecting ducts, as this lineage is important to PKD cystogenesis but does not mature in human kidney organoid cultures 17 , 59 , 60 . A limitation of the current system is that the organoid phenotype is limited to biallelic mutants, in which disease processes are greatly accelerated 61 , 62 , 63 . In contrast, germline mutations in PKD patients are monoallelic, and phenotypes take decades to develop, likely due to the necessity of developing \u2018second hit\u2019 somatic mutations in the second allele 64 , 65 . The current system involving biallelic mutants may more closely phenocopy early-onset autosomal recessive PKD than late-onset autosomal dominant PKD, which should be considered when extrapolating these findings into a clinical context 16 . Generation of well-controlled allelic series of PKD organoids, together with methodologies to model the acquisition of somatic mutations, may ultimately produce human organoid models with greater fidelity to autosomal dominant PKD. Canagliflozin (Invokana), an inhibitor of SGLT2, has recently been approved for the treatment of type 2 diabetes, and appears to have a protective effect in the kidneys 66 , 67 . SGLT inhibitors have not yet been tried in patients with PKD. Our findings suggest that blocking SGLT activity could reduce proximal tubule cysts by preventing glucose reabsorption. However, this would also expose the collecting ducts downstream to higher glucose concentrations. Indeed, it was previously suggested that inhibition of glucose transport reduces PKD in the Han:SPRD rat because its cysts originate from proximal tubules, whereas the same treatments in the PCK rat worsen PKD because its cysts originate in more distal nephron segments 39 , 40 . Caution must therefore be exercised when considering how to conduct human clinical trials for PKD with SGLT inhibitors. In summary, we have developed a microfluidic kidney organoid module that enables detailed studies of renal tubular absorption and PKD cyst growth. The cyst-lining epithelium in this system is exposed to flow in a mirror image of the nephron structure in vivo. Using this system, we have identified glucose levels and its transport into cyst structures as a driver of cystic expansion in proximal nephron-like structures. Therapeutics that modulate reabsorption may therefore be beneficial in reducing cyst growth in specific nephron segments, with relevance for future PKD clinical trials 4 , 66 . Methods Ethics Research complied with all relevant ethical regulations. Human PKD kidney tissue (nephrectomy) was obtained with informed consent under a human subjects protocol approved by the University of Washington Institutional Review Board. No compensation was provided to study participants. Kidney organoid differentiation Work with hPSC was performed under the approval and auspices of the University of Washington Embryonic Stem Cell Research Oversight Committee. Specific cell lines used in this study are described below and are sourced from commercially available hPSC obtained with informed consent. hPSC stocks were maintained in mTeSR1 media with daily media changes and weekly passaging using Accutase or ReLeSR (STEMCELL Technologies, Vancouver). 5,000\u201320,000 hPSCs per well were placed in each 24-well plate pre-coated with 300 \u00b5L of DMEM-F12 containing 0.2 mg\/mL Matrigel and sandwiched the following day with 0.2 mg\/mL Matrigel in mTeSR1 (STEMCELL Technologies, Vancouver) to produce scattered, isolated spheroid colonies. 48 hrs after sandwiching, hPSC spheroids were treated with 12\u03bcM CHIR99021 (Tocris Bioscience) for 36 h, then changed to RB (Advanced RPMI + 1X Glutamax + 1X B27 Supplement, all from Thermo Fisher Scientific) after 48 h, and replaced with fresh RB every 3 days thereafter. Organoid perfusion in microfluidic chip Ibidi \u03bc-Slide VI 0.4 were coated with 3.0% Reduced Growth Factor Geltrex (Life Technologies) and left at 37 \u00b0C overnight to solidify. Kidney organoids (21\u201340d) were picked from adhered culture plates, pipetted into the slide channels (2\u20133 per channel) with RB, and left for 24 hrs at 37 \u00b0C to attach. Organoids were distributed randomly within the channel. For the fluidic condition, 60 mL syringes filled with RB were attached to channels using clear tubing (Cole-Parmer, 0.02'' ID, 0.083'' OD). A clamp was used to close off the tubing, and the media in the syringe was changed to 25 mL RB + 36.5 \u03bcM 2-NBD-Glucose fluorescent glucose (Abcam ab146200). A Harvard Apparatus syringe infusion pump was used to direct media flow into microfluidic chip at 160 \u03bcL\/min (0.2 dynes\/cm 2 ). Media was collected at the outlet and filtered for repeated use. For the static condition, a 25 mL syringe containing RB was attached to the channel using wide clear tubing (Cole-Parmer, 0.125'' ID, 0.188'' OD). The syringe was detached momentarily, the plunger removed, and the open syringe reattached and filled slowly with 25 mL RB + 36.5 \u03bcM 2-NBD-Glucose. From this point on, diffusion of the fluorescent glucose began from the open syringe into the channel via the tubing. Alternatively, NDB-glucose was substituted with food dye (invert sugar, 360 g\/mol), or alternatively the organoids were perfused with media in the absence of any additives. Image\/video collection Image collection was performed on a Nikon Ti Live-Cell Inverted Widefield microscope inside of an incubated live imaging chamber supplemented with 5% carbon dioxide. Experiments in microfluidic devices were recorded for 6 h. During this time, cysts changed in volume (grew and shrank) and in some cases were destroyed due to bubbles arising in the tubing. Cyst growth rate in microfluidic devices was therefore calculated on an individual basis, when each cyst reached its maximal volume, which varied for each sample from 1 h to 5 h after the start of the experiment. For longer-term experiments conducted in static 96-well cultures, organoids were imaged at regular intervals (typically 24 h) and analyzed at the endpoint indicated in the figure graphs. Phase contrast and GFP (200 ms exposure) images were taken every 5 min for a maximum of 12 h. Images of fixed samples were collected on a Nikon A1R point scanning confocal microscope. Animal studies Kidney tissue from Pkd1 RC\/RC mice maintained in C57BL\/6 J background (gift of Mayo Clinic Translational PKD Center) and C57BL\/6 J controls were utilized. In order to investigate the process of cystogenesis, younger Pkd1 RC\/RC mice 6\u20137 weeks of age, along with wild-type C57BL\/6 J mice of the same age were used. Kidneys were harvested after systemic perfusion with ice-cold PBS, followed by fixation with paraformaldehyde fixative and immersion in 18\u201330% sucrose at 4 \u00b0C overnight. Tissues were embedded and frozen in optimal cutting temperature compound (OCT, Sakura Finetek, Torrance, CA). Cryostat-cut mouse kidney sections (5\u201310 \u03bcm) were stained for acetylated \u03b1-tubulin, laminin-1, and CD31 (see \u201cImmunostaining\u201d for primary antibodies and dilutions). For perfusion experiments, NBD Glucose was freshly dissolved in PBS to a concentration of 1 mM. Freshly sacrificed Pkd1 RC\/RC mice (>8 months old) were incised through the chest and nicked at the vena cava with a 27-gauge needle. Keeping pressure on the vena cava, mice were perfused systemically through the heart with a syringe containing 10 ml of PBS, followed by a second syringe containing 5 ml of either PBS alone (control) or PBS + 1 mM NBD-Glucose. Kidneys were harvested immediately and embedded fresh without fixation or sucrose equilibration in OCT. Cryostat-cut mouse kidney sections (20 \u03bcm) were mounted in OCT and imaged on a confocal microscope with 10X objective. All animal studies were conducted in accordance with all relevant ethical regulations under protocols approved by the Institutional Animal Care and Use Committee at the University of Washington in Seattle. Mice were maintained on a standard diet under standard pathogen-free housing conditions, with food and water freely available. Immunostaining Immunostaining followed by confocal microscopy was used to localize various proteins and transporters in the cysts and organoids. Prior to staining, an equal volume of 8% paraformaldehyde was added to the culture media (4% final concentration) for 15 mins at room temperature. After fixing, samples were washed in PBS, blocked in 5% donkey serum (Millipore)\/0.3% Triton-X-100\/PBS, incubated overnight in 1% bovine serum albumin\/0.3% Triton-X-100\/10\u03bcM CaCl 2 \/PBS with primary antibodies, washed, incubated with Alexa-Fluor secondary antibodies (Invitrogen), washed and imaged. Primary antibodies or labels include acetylated \u03b1-tubulin (Sigma T7451, 1:5000), ZO-1 (Invitrogen 61-7300, 1:200), Biotinylated LTL (Vector Labs B-1325, 1:500), E-Cadherin (Abcam ab11512, 1:500), SGLT2 (Abcam ab37296, 1:100), laminin-1 (Sigma L9393, 1:50), alpha smooth muscle actin (Sigma A2547, 1:500), CD31 (BD Biosciences 557355, 1:300). Fluorescence images were captured using a Nikon A1R inverted confocal microscope with objectives ranging from 10X to 60X. Statistical analysis Experiments were performed using a cohort of PKD hPSC, previously generated and characterized, including three PKD2 \u2212\/\u2212 hPSC lines and three isogenic control lines that were subjected to CRISPR mutagenesis (gRNA CGTGGAGCCGCGATAACCC) but were found to be unmodified at the targeted locus by Sanger sequencing of each allele and immunoblot 16 , 17 . Altogether these represented two distinct genetic backgrounds, genders, and cell types: (i) male WTC11 iPS cells (Coriell Institute Biobank, GM25256, two isogenic pairs) and (ii) female H9 ES cells (WiCell, Madison Wisconsin, WA09, one isogenic pair). Quantification was performed on data obtained from experiments performed on controls and treatment conditions side by side on at least three different occasions or cell lines (biological replicates). Error bars are mean \u00b1 standard error (s.e.m.). Statistical analyses were performed using GraphPad Prism Software. To test significance, p -values were calculated using two-tailed, unpaired or paired t -test (as appropriate to the experiment) with Welch\u2019s correction (unequal variances). For multiple comparisons, standard ANOVA was used. Statistical significance was defined as p < 0.05. Exact or approximate p- values are provided in the figure legends in experiments that showed statistical significance. For traces of cysts over time, the least squares progression model was applied to fit the data to lines in GraphPad Prism. Line scans of equal length were averaged from multiple images and structures based on raw data intensity values in the GFP channel. Lines were drawn transecting representative regions of each structure (e.g. avoiding heterogeneities, brightness artifacts, or areas where cysts and organoids overlapped), placed such that the first half of each line represented the background in the image. The intensity of each point (pixel) along the line was then averaged for all of the lines, producing an averaged line scan with error measurements. Arrows are provided in representative images showing the direction and length of the line scans used to quantify the data. Unless otherwise noted, raw intensity values (bytes per pixel) were were used without background subtraction. Hydrostatic pressure calculation The following calculation was performed: $${{{{{\\rm{Pressure}}}}}}=\\rho {{{{{\\rm{gh}}}}}}=\\left(997\\frac{{kg}}{{m}^{3}}\\right)\\left(9.81\\frac{m}{{s}^{2}}\\right)\\left({{{{\\boldsymbol{{{{{\\mathscr{x}}}}}}}}}}\\,m\\right)={Pressure}\\left(\\frac{{kg}}{m\\cdot {s}^{2}}\\right)$$ The height from channel to top of media in reservoir was measured to be: Static 1 mL: ~12 cm Static 25 mL: ~20 cm Therefore, the calculation for each of these conditions was: $$Pressur{e}_{1mL} =\\rho {{{{{\\rm{gh}}}}}}=\\left(997\\frac{kg}{{m}^{3}}\\right)\\left(9.81\\frac{m}{{s}^{2}}\\right)(0.12\\;m) \\\\ =1173.7\\left(\\frac{kg}{m\\cdot {s}^{2}}\\right)\\left(\\frac{mmHg}{133.32\\,Pa}\\right)=8.8\\,mmHg$$ $$Pressur{e}_{25mL} =\\rho {{{{{\\rm{gh}}}}}}=\\left(997\\frac{kg}{{m}^{3}}\\right)\\left(9.81\\frac{m}{{s}^{2}}\\right)(0.20\\,m) \\\\ =1956.1\\left(\\frac{kg}{m\\cdot {s}^{2}}\\right)\\left(\\frac{mmHg}{133.32\\,Pa}\\right)=14.7\\,mmHg$$ This amounted to a total difference in pressure of (14.7\u20138.8 = 5.9) mmHg. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The main data supporting the results in this study are available within the paper and its supplementary information. The raw and analysed datasets generated during the study are too large and complex to be publicly shared (numerous cell lines, replicates, images, blots, and experiments, maintained and analysed in specialized file formats and with unique identifiers). Datapoints are shown as dots in the plots provided in this paper and the Supplement. All datasets, including raw data and statistical analysis, are available upon reasonable request from the corresponding author. PKD mutant cell lines used in this study may be obtained from the corresponding author upon request and in accordance with material transfer agreements from the University of Washington and any third-party originating sources. Source data are provided with this paper. ","News_Body":"A study of kidney organoids in a novel lab environment might have downstream implications for the treatment of polycystic kidney disease (PKD), an incurable condition that affects more than 12 million people worldwide. One key discovery of the study: Sugar appears to play a role in the formation of fluid-filled cysts that are PKD's hallmark. In people, these cysts grow big enough to impair kidney function and ultimately cause the organs to fail, necessitating dialysis therapy or transplant. The findings were published in Nature Communications. The co-lead authors are Sienna Li and Ramila Gulieva, research scientists in the lab of Benjamin Freedman, a nephrology investigator at the University of Washington School of Medicine. \"Sugar uptake is something that kidneys do all the time,\" said Freedman, a co-senior author. \"We found that increasing the levels of sugar in the dish cultures caused cysts to swell. And when we employed drugs known to block sugar absorption in the kidneys, it blocked this swelling. But I think it relates less to blood sugar level and more to how kidney cells take in sugar\u2014which in this process seemed to go rogue and give rise to cysts.\" For years Freedman has studied PKD in organoids grown from pluripotent stem cells. Organoids resemble miniature kidneys: They contain filtering cells connected to tubes and can respond to infection and therapeutics in ways that parallel the responses of kidneys in people. Mini-kidney tube structures have sugar receptors (red, upper left) and form outward-facing PKD cysts (center), which swell by taking in sugar (green, lower right). Credit: Benjamin Freedman Lab \/ University of Washington School of Medicine Although his team can grow organoids that give rise to PKD cysts, the mechanisms of those cysts' formation are not yet understood. In this investigation, the researchers focused on how the flow of fluid within the kidney contributes to PKD. To do so, they invented a new tool that merged a kidney organoid with a microfluidic chip. This allowed a combination of water, sugar, amino acids and other nutrients to flow over organoids that had been gene-edited to mimic PKD. \"We were expecting the PKD cysts in the organoids to get worse under flow because the disease is associated with the physiological flow rates that we were exploring,\" Freedman explained. \"The surprising part was that the process of cyst-swelling involved absorption: the intake of fluid inward through cells from outside the cyst. That's the opposite of what is commonly thought, which is that cysts form by pushing fluid outward through cells. It's a whole new way of thinking about cyst formation.\" In the chips, the researchers observed that the cells lining the walls of the PKD cysts faced outward as they stretched and swelled, such that the tops of the cells were on the outside of the cysts. This inverted arrangement\u2014these cells would be facing inward in living kidneys\u2014suggests that cysts grow by pulling in sugar-rich fluid, not by secreting the liquid. The observation gives researchers more information about how cysts form in organoids, a finding that will have to be tested further in vivo. As well, the fact that sugar levels drive cyst development points to new potential therapeutic options. \"The results of the experiment are significant because there is a whole class of molecules that block sugar uptake in the kidneys and are attractive therapeutics for a number of conditions,\" Freedman said. \"They haven't been tested yet, but we view this as a proof-of-concept that these drugs could potentially help PKD patients.\" ","News_Title":"Study illuminates sugar's role in common kidney disease","Topic":"Medicine"}
{"Paper_Body":"Abstract Groundwater discharge generates streamflow and influences stream thermal regimes. However, the water quality and thermal buffering capacity of groundwater depends on the aquifer source-depth. Here, we pair multi-year air and stream temperature signals to categorize 1729 sites across the continental United States as having major dam influence, shallow or deep groundwater signatures, or lack of pronounced groundwater (atmospheric) signatures. Approximately 40% of non-dam stream sites have substantial groundwater contributions as indicated by characteristic paired air and stream temperature signal metrics. Streams with shallow groundwater signatures account for half of all groundwater signature sites and show reduced baseflow and a higher proportion of warming trends compared to sites with deep groundwater signatures. These findings align with theory that shallow groundwater is more vulnerable to temperature increase and depletion. Streams with atmospheric signatures tend to drain watersheds with low slope and greater human disturbance, indicating reduced stream-groundwater connectivity in populated valley settings. Introduction Groundwater discharge zones establish active stream\u2013groundwater hydrologic connectivity through the advective exchange of water. As a critical contributor to streamflow generation, groundwater discharge influences water quantity and quality throughout stream networks, especially during seasonal low flows and dry conditions 1 . Many streams host ecologically important \u2018groundwater-dependent ecosystems\u2019 2 , yet these habitats face growing threats from climate change and groundwater contamination 1 , 3 , 4 . Aquatic organisms are particularly susceptible to shifts in thermal regimes because they have life cycles that rely on annual thermal cues 5 and metabolic rates influenced by stream temperature 6 . The relatively stable thermal regimes of some groundwater discharge zones can buffer stream temperatures against long-term air temperature trends and short-term hot and cold extremes 2 ; therefore, groundwater discharges can provide important stream channel thermal refuges and refugia for sensitive aquatic organisms such as salmonid fishes 7 , 8 . However, in response to climate change and land development, streams and rivers have recently shown widespread warming 9 , 10 . Observed stream warming trends are spatially heterogeneous due in part to spatially variable groundwater contributions to streamflow 11 . Thus, effective watershed management will require a process-based characterization of groundwater contribution to streamflow 12 at ecologically relevant scales to predict future stream thermal regimes. The magnitude, spatial distribution, and source-flow path characteristics of groundwater discharge can control the physical characteristics of individual streams 8 , 13 , 14 and whole stream networks 15 . Characterizing the depth of contributing groundwater is particularly important for understanding broad-scale responses of stream ecosystems to land development and climate change 16 for three main reasons: first, groundwater depth is associated with annual thermal stability as natural surface temperature fluctuations are prominent within the shallow aquifer but quickly attenuate with depth 13 . Deeper groundwater (defined here as greater than approximately 6 m from the land surface) shows little annual thermal variability relative to shallow groundwater 17 that flows through the near-surface portion of the \u2018critical zone\u2019 18 .Therefore, groundwater discharge can either impart stability (deep groundwater) or variability (shallow groundwater) on atmospheric-driven stream thermal regimes. Hydrogeologic climate simulations support this definition, as water tables below 5 m have shown decoupling from surface energy balances 19 . Second, shallow groundwater is inherently more sensitive to land-use changes 20 and surface contamination 21 , 22 , 23 . Thus, effective watershed management may have a different urgency depending on the depth of contributing groundwater. Also, naturally, deep and shallow groundwater tend to have different chemical profiles 24 , 25 , 26 , which has important implications for surface water quality and stream ecosystem function including delivery of legacy contaminants 15 . Third, shallow groundwater can be directly depleted via transpiration 27 , irrigation withdrawals 28 , and is more vulnerable to seasonal water table drawdown during dry periods while discharge from deeper groundwater sources is more seasonably stable 29 . This depth-dependent effect can affect stream water transit times and catchment water balance, emphasizing the importance of parsing shallow versus deep contributing groundwater flow paths 24 . Though understanding the implications of climate change and land development for stream ecosystems requires quantifying the magnitude and source-depth of groundwater discharge, we lack efficient and broadly applicable methods to characterize source groundwater depth. Most hydrologic techniques for evaluating the physical properties of groundwater discharge are labor-intensive and not spatially and temporally scalable 30 . More efficient methods, such as stream water temperature sensitivity linear regression analyses 31 or physically based hydrograph separation techniques 32 do not directly differentiate groundwater source-depth. Inference of groundwater source-depth is possible using water chemistry end-member mixing 33 or water isotopic data 34 , but these analyses cannot inherently specify shallow groundwater flow paths without additional hydrologic characterization, and are time and resource-intensive. In the absence of groundwater discharge, annual stream water temperature signals are often well coupled to seasonal variation of local air temperature 35 . A departure from this coupling in terms of seasonal magnitude and timing is characteristic of influence from varied depth groundwater discharge 8 or dam operation 36 . Discharge of shallow groundwater to streams has physical properties closely tied to seasonally dynamic air temperature and precipitation, quickly responding to short-term perturbations such as hot, dry summers 37 . Discharge from deep groundwater sources does not tend to respond to anomalous weather years but is sensitive to long-term climate trends at extended time scales ranging from decadal to centennial 16 , 38 , 39 . In this work, we used a newly refined methodology to classify 1729 stream sites across the continental United States as having shallow or deep groundwater signatures, lacking a pronounced groundwater signature, or having major dam influence, based on publicly available multi-year air and stream water temperature records and metadata. Our analysis harnesses the relatively high annual variability in shallow groundwater temperatures and the stability of deep groundwater temperatures to identify characteristic paired air and stream water annual temperature signal relations. We used our classification to (1) compare our annual temperature signal-based categorization to baseflow indices, (2) explore continental spatial patterns and landscape drivers of groundwater discharge characteristics, and (3) evaluate how stream temperature is changing over time (14\u201330 years) among streams with varied source-depth of groundwater discharge. We present an unprecedented broad-scale inference of groundwater discharge contribution to streams that will inform more accurate predictions of stream responses to changing climate and land use conditions. Results and Discussion Continental classification We used paired air and stream water annual temperature signal relations to broadly classify stream and river sites with atmospheric (i.e., lacking a pronounced groundwater signature), deep groundwater, shallow groundwater, or major dam signatures across the continental U.S. Our sites represent a broad range of stream sizes encompassing 1st to 9th order (median: 3rd order) across 21 of the 25 U.S. physiographic provinces (categorized based on large-scale geomorphology; Supplemental Table 1 ). We used multi-year annual temperature signals as a diagnostic tool because they are less susceptible to variable flow and weather than other stream temperature-based groundwater discharge metrics that rely on short-term thermal variance 40 . Streams below major dams have complex, management-influenced annual thermal regimes 36 and are not explored in detail here. For streams with substantial groundwater discharge, the amplitude and phase of paired annual air and stream water temperature signals decouple in distinctive ways. At sites with a deep groundwater signature, the annual stream temperature signal is highly damped compared to air\u2014quantified by the stream water\/air amplitude ratio\u2014but the signals are approximately in-phase. Groundwater discharge from shallow flow paths causes variable stream temperature signal damping, but uniquely shifts the timing of the annual stream water temperature signal later relative to the annual air temperature signal\u2014quantified by the time-forward phase lag. This characteristic phase lag propagates into stream water from adjacent shallow aquifers, whereas deeper groundwater flow paths have a highly attenuated annual temperature signal and thus do not influence the stream water signal phase 8 . For our broad-scale analysis, we assigned categories of shallow and deep groundwater signatures according to paired air and stream water annual signal metrics of amplitude ratios and phase lags based on previous analyses 8 , 40 , 41 . We assigned sites that either had phase lags of greater than 40 days, which is not an expected outcome of even extreme shallow groundwater discharge mixing with stream water 8 , or are within 25 km downstream of major dams, as sites with major dam signatures. Of the 1729 sites we categorized, 305 sites met this dam criterion and are removed from the groundwater signature analysis. Sites classified as having pronounced groundwater signatures are common in this national dataset. We found that of the 1424 sites analyzed for groundwater signatures, groundwater substantially influences the annual thermal regimes of 39% ( n = 556). We classified 47% ( n = 264) of these sites as having deep groundwater signatures, and 53% ( n = 292) as having shallow groundwater signatures (Fig. 1 ). The average amplitude ratio is 0.54 ( \u03c3 = 0.10) for sites with deep groundwater signatures and 0.59 ( \u03c3 = 0.18) for sites with shallow groundwater signatures. The air to stream water annual signal phase lag averaged 16.6 days ( \u03c3 = 6.6 days) for sites with shallow groundwater signatures and 3.8 days ( \u03c3 = 3.4) for sites with deep groundwater signatures. In contrast, the average amplitude ratio for sites with atmospheric signatures is better coupled to annual air temperature at 0.85 ( \u03c3 = 0.12) with a negligible average phase lag of 2.3 days ( \u03c3 = 2.7 days) that is not significantly different than zero phase lag. Fig. 1: Spatial distribution of stream sites by categorical groundwater signature. Categorical groundwater (GW) signatures derived from annual paired air\u2013stream water temperature signals a across the continental United States and b within a single watershed, the North Fork of the Clearwater River-Lake Creek watershed, Idaho\u2014Montana, USA (Hydrologic Unit Code HUC10 \u2013 1706030701). Lake Creek stream is highlighted. Across the United States, counts of each category are atmospheric signature (pink) n = 868; shallow groundwater GW signature (yellow) n = 292; deep groundwater GW signature (blue) n = 264. Legend descriptions are maintained between a and b . Base map a was generated from R package \u2018maps\u2019 version 3.3.0 and the Nation Hydrography Dataset 70 b was created from 7.5-minute ground surface elevation data courtesy of the U.S. Geological Survey. Full size image Deep and shallow groundwater contributions to streamflow are not mutually exclusive, often a spectrum of flow path depths contributes to streamflow 42 , but our analysis derives which signature is dominant. The distribution of annual signal metrics within our groundwater contribution categories indicate that our thresholds that define the groundwater signature categories occur near natural breaks (Supplementary Fig. 1 ), indicating alignment with potential groundwater-driven separations of underlying populations in the data. We compared our temperature-based approach for classifying groundwater contribution to streamflow data by using multi-year baseflow regression analysis for the subset of sites that had concurrent streamflow records ( n = 554) (Fig. 2 ). Specifically, we calculated the baseflow index (BFI), an estimate of the ratio of baseflow to total streamflow based on the annual stream hydrograph, as it is one of the few current methods for quantifying relative groundwater contributions to streamflow efficiently at broad scales 32 . As may be expected, sites with atmospheric thermal signatures had significantly lower BFIs (median\u20140.69) than sites with either shallow groundwater (median BFI \u2013 0.79) or deep groundwater (median BFI\u20140.86) signatures (Fig. 2 ). This result aligns with theory that the primary driver of baseflow throughout river networks is groundwater discharge. Fig. 2: Categorical groundwater (GW) signatures compared to baseflow index (BFI). Letters indicate significance at p < 0.05 reported alongside median BFI. Counts of each category are atmospheric signature (pink) n = 401; shallow groundwater GW signature (yellow) n = 71; deep groundwater GW signature (blue) n = 82. Boxplots center line is the median and box limits are the upper and lower quartiles. Full size image BFI varies among groundwater contribution categories; streams with shallow groundwater signatures have significantly lower BFIs than those with deep groundwater signatures. This observation supports site-specific research that found shallow groundwater sources are less reliable for generating baseflow at seasonal timescales 29 , 37 . Shallow (less than 6 m depth) aquifer flow paths drain a relatively small groundwater reservoir that is highly sensitive to seasonally dynamic recharge rates and transpiration 27 , and are therefore less-reliable generators of stream baseflow. In contrast, deep groundwater flow from larger reservoirs is generally sustained throughout the year 42 , 43 at a more constant rate 44 , increasing the average baseflow index in streams dominated by deeper groundwater discharge. This result highlights that effective water resource and aquatic habitat management in a changing world should consider both groundwater connectivity and the source-depth of groundwater discharge. Spatial patterns and physical drivers Our results demonstrate that the spatial distribution of groundwater contributions to streamflow is complex across the continental United States, but large-scale spatial patterns emerge (Fig. 1a ). Physiographic provinces with the highest percentage of deep groundwater signatures are often associated with those expected to have productive aquifers, such as glaciated terrains (e.g., 31% of sites in New England have a deep groundwater signature) or sedimentary bedrock (e.g., 27% of sites in the Colorado Plateau have a deep groundwater signature) (Supplementary Table 1 ). Physiographic provinces that have a high proportion of streams draining steep mountainous terrain with thin soil coverage generally have a higher percentage of shallow groundwater signatures (e.g., Northern Rocky Mountains\u201474% of sites have shallow groundwater signatures) (Supplementary Table 1 ). Thus, landforms and geologic structures are likely, in part, controlling the spatial patterning of groundwater contribution to streams across the United States. Yet, within regions, there is substantial heterogeneity in groundwater signatures. For example, in the Cascades-Sierra Mountains, 38% of sites have shallow groundwater signatures, and 32% of sites have deep groundwater signatures. This observation is likely in part because of the geologic variation between the High Cascades (younger, highly fractured volcanic bedrock) and Western Cascades (shallow soils, and abundance of clay) 37 . Also, within the Coastal Plain province (eastern coastline of the United States from Massachusetts to Mexico), while 91% of sites have an atmospheric signature, sites with shallow and deep groundwater signatures do occur in isolated areas such as the Floridian Section that is dominated by karst aquifers (Fig. 1 , Supplementary Table 1 ). Indeed, atmospheric, shallow, and deep groundwater signatures co-occur within all eight physiographic regions and within 18 out of 21 physiographic provinces considered in our study. Previous research has shown broad-scale mapping of expected stream water\u2013 groundwater connectivity characteristics which can be inferred with a combination of physiography and climate, a concept supported with relatively sparse BFI analysis 43 . Because low-cost stream temperature measurements are currently being performed at thousands of publicly available sites nationally, paired air and stream water temperature signal-based analysis offers a highly scalable approach to provide additional specificity regarding groundwater discharge dynamics, refining broad-scale zonation of stream water\u2013groundwater connectivity. Among physiographic regions, local watershed characteristics likely also play an important role influencing groundwater discharge to streams 45 . Overall, sites with shallow groundwater signatures tend to have higher watershed slopes than sites with atmospheric or deep groundwater signatures (Fig. 3a ). We hypothesize that watersheds with higher slopes are more likely to have a shallow depth to bedrock, which is a known driver of near-surface hillslope groundwater flow to streams 46 . Yet, our results show that strong connectivity of streams and shallow groundwater occurs in environments beyond smaller, steep headwater streams, such as areas with shallow confining layers 47 . Sites with shallow groundwater signatures drain larger watersheds (median 153 km 2 ; Q1\u2013Q3: [17 km 2 , 2131 km 2 ]), have higher streamflow (median 13 m 3 s \u22121 ), and have a greater range of streamflow (Q1\u2013Q3: [2 m 3 s \u22121 , 98 m 3 s \u22121 ]) than sites with deep groundwater signatures (watershed size: 65 km 2 ; Q1\u2013Q3: [18 km 2 , 616 km 2 ]; streamflow 2 m 3 s \u22121 ; [0.4 m 3 s \u22121 , 10 m 3 s \u22121 ]) suggesting shallow groundwater signatures occur across a wide spectrum of hydrogeologic settings that may not be predicted by current conceptual models of baseflow generation. Fig. 3: Watershed properties for groundwater (GW) signature categories. a Mean slope of the watershed draining to each site. b Percent impervious surface from the year 2011 of the local catchment draining to each site. Y -axis is truncated at 40% impervious surface, which removed 44 outliers from atmospheric signature and 5 outliers from shallow groundwater GW signature categories. c The Hydrologic Disturbance Index for each site based on the GAGES-II dataset 52 , 63 . Higher values indicate more disturbance. For a and c site counts of each category are atmospheric signature (pink) n = 277; shallow groundwater GW signature (yellow) n = 40; deep groundwater GW signature (blue) n = 51. Boxplots center line is the median and box limits are the upper and lower quartiles. For b site counts of each category are atmospheric signature (pink) n = 831; shallow groundwater GW signature (yellow) n = 275; deep GW groundwater signature (blue) n = 246. Full size image Heterogeneity in groundwater signatures exists even at the sub-watershed scale. For example, at the North Fork Clear Water\u2014Lake Creek watershed in Idaho, USA (Fig. 1b ), sites within the steep headwaters are dominated by shallow groundwater signatures while sites along the mainstem river valley are largely characterized by deep groundwater signatures, with the outlet of the watershed shifting to an atmospheric signature. This watershed represents an important habitat for a range of cold-water salmonid species 48 . Interestingly, a major tributary (Lake Creek, highlighted in Fig. 1b ) was moved to the list of impaired waters in 2010 by the Idaho Department of Environmental Quality for elevated temperature criteria violations 48 . Without explicit consideration of groundwater dynamics, this impairment was attributed to a slight reduction in canopy shading (4%) compared to the local shade optimal target. However, of the four sites we investigated in upper Lake Creek watershed, one main stem stream and three tributaries, all are classified as having shallow groundwater signatures of greater than 15-day phase lags. These large phase lags suggest dominance of the annual thermal regime by shallow groundwater, and we speculate that the previously observed warm stream impairment is due in part due to warming of shallow groundwater. Consideration of local to regional groundwater responses to climatic and watershed modifications is crucial yet often overlooked in stream temperature predictions, which can mislead future projections and produce less effective mitigation strategies when ignored. The multi-scale heterogeneity of groundwater contribution to streamflow within and among physiographic regions and individual watersheds provides the impetus for higher spatial resolution regional characterization for targeted cold-water species management. Human drivers of stream\/groundwater disconnection Human alterations can also influence the spatial patterns of groundwater connectivity and discharge to streams 49 . Our results demonstrate that streams with atmospheric signatures tend to occur in local catchments (area directly draining to a river segment, excluding any upstream contribution 50 ) with a higher percentage of impervious surface area (Fig. 3b ). Sites with atmospheric signatures also tend to have a higher \u201cHydrologic Disturbance Index\u201d (HDI), which is a more holistic metric of human influence derived from seven anthropogenic watershed modifications, not including percent impervious cover 51 , 52 (Fig. 3c ). The median HDI score for atmospheric signature sites is 16 and a maximum of 31. Sites with pronounced deep groundwater signatures have a median watershed HDI of 9 and shallow groundwater signature sites have a median HDI score of 5.5 (Fig. 3c ). This discrepancy in HDI scores between groundwater categories may result in part from the fact that human disturbance is more immediately influential to shallow groundwater dynamics, and therefore fewer streams in such disturbed basins show shallow groundwater discharge signatures, compared to more resilient deeper groundwater. One of these seven HDI parameters is groundwater withdrawal, which has been shown to have immediate effects on streamflow generation, especially within areas reliant on irrigation, and is generally projected to increase in the future to offset droughts 53 . We hypothesize that in addition to pumping, the relative lack of sites with groundwater signatures observed in this study in more disturbed landscapes is a result of the many human landscape modifications that reduce groundwater discharge to streams and rivers. These impacts occur either directly through groundwater withdrawal or indirectly through impervious cover and stormwater infrastructure that saps shallow groundwater and diverts precipitation to streams, reducing infiltration and aquifer recharge. Therefore, streams within watersheds with high human modification, predominantly in lowlands, are likely to have lower groundwater connectivity and be more susceptible to warming, though recent research suggests that extreme low flows may be buffered along urban corridors due to infrastructure-based recharge 54 . Understanding how human modifications alter groundwater discharge dynamics across the U.S. will therefore involve disentangling how urban development interacts with geology and landscape features. Stream temperature temporal trends Quantifying the thermal stability of streams influenced by groundwater discharge is essential in predicting the effects of climate change on stream networks. The capacity of stream water temperature to be buffered against a warming world depends in part on the source depth of groundwater discharge 55 , and high groundwater connectivity is often invoked as a primary driver of persistent cold-water habitat 8 . Indeed, of the 184 sites that had long-term contiguous temperature records (ranging 14 to 30 years), we found that sites with deep groundwater signatures had a substantially smaller proportion of significant positive temperature trends than sites with shallow groundwater or atmospheric signatures (Fig. 4 ). More than half of the long-term sites with atmospheric signatures ( n = 132) have stream water temperatures that are increasing over the last 14 to 30 years ( n = 70), ranging from 0.01 to 0.09 \u00b0C yr \u22121 (\u03bc: 0.04 \u00b0C yr \u22121 ). Similarly, for long-term sites with shallow groundwater signatures ( n = 29), we found that 45% have stream water temperatures that are increasing with rates of warming ranging from 0.01 to 0.1 \u00b0C yr \u22121 (\u03bc: 0.04 \u00b0C yr \u22121 ). The rates of warming for sites with shallow groundwater signatures and atmospheric signatures are consistent with previously reported stream water warming trends 9 , 10 . Fig. 4: Stream water temperature trends based on average monthly values for 14\u201330 years of data post 1990. a Spatial distribution of stream water temperature annual trends across the United States by groundwater (GW) signature category. Base map was generated from R package \u2018maps\u2019 version 3.3.0. b The proportion of sites with long-term annual temperature increasing (warming (red), p < 0.05), decreasing (cooling (blue), p < 0.05) monotonic trends, or stable condition (gray) ( p > 0.05) by GW signature category. c Similarly, the long-term temperature trends based on summer temperatures (June \u2013 August) by GW signature category. Site counts of each category in a \u2013 c are atmospheric signature (triangle) n = 132; shallow GW signature (circle) n = 29; and deep GW signature (square) n = 23. Full size image In contrast to sites with shallow groundwater signatures, 52% of sites with deep groundwater signatures had stable stream water temperature regimes (Fig. 4a,b ). This finding underscores the strong thermal buffering capacity of deep groundwater discharge and the likely greater resistance to climate warming of groundwater-dependent and cold-water habitat sourced by deep compared to shallow groundwater. The six deep groundwater signature sites with significant warming trends had rates ranging from 0.01 to 0.05 \u00b0C yr \u22121 (\u03bc: 0.01 \u00b0C yr \u22121 ). Sites with deep groundwater signatures also showed the greatest proportion (22% of sites) of significant cooling trends. Although stream cooling trends appear counterintuitive under climate change, they have also been identified in previous work 56 , and may be due to localized changes in winter precipitation patterns 57 . The difference in thermal buffering capacity of streams dominated by shallow versus deep groundwater discharge has been predicted by modeling efforts for individual watersheds 29 , 37 , 55 . Our empirical results confirm these predictions and expand evidence to sites across the United States. We recognize that there are confounding factors that influence long-term stream temperature, notably discharge variability. Therefore, streams fed by shallow groundwater could warm at a faster rate in part because of drought conditions or groundwater withdrawal (e.g., for irrigation) lowering groundwater levels, which disproportionately affects shallow groundwater 28 . The disparity between long-term stream temperature trends of sites with shallow versus deep groundwater signatures also occurs during the summer season, when cold water fishes are most often thermally stressed. Over 70% of sites with shallow groundwater signatures show significant summer season warming trends compared to 43% of sites with deep groundwater and 61% of sites with atmospheric signatures (Fig. 4c ). These seasonal warming trends follow the fundamental nature of the classification method, which relies on the pronounced annual temperature signals of shallow groundwater to be transferred to stream water via groundwater discharge zones. Sites with shallow groundwater signatures will be immediately sensitive to hotter summers, exacerbating thermal stress on sensitive aquatic organisms 41 . Thus, vulnerable biota within streams dominated by shallow groundwater may not only have to adapt to a warming baseline condition, but also be particularly vulnerable to the impacts of single season heatwaves. Deep groundwater is more resistant to land surface temperature changes, but still sensitive to longer-term thermal shifts at timescales tied to source flow path depth 38 . This re-emphasizes the importance of distinguishing shallow versus deep groundwater source-depth, rather than assuming streams with strong baseflow components imply thermal stability. Groundwater discharge to streams and rivers occurs via a spectrum of source groundwater flow paths, which exerts high-level controls on streamflow, channel thermal stability, and stream water quality characteristics that are tightly linked to the source aquifer. The relative flow path depth of contributing groundwater is particularly important for stream ecosystems; yet, until recently we lacked efficient process-based methodology to parse the relative dominance of shallow or deeper groundwater discharge to streams at broad spatial scales. Our continental-scale characterization demonstrates a framework for harnessing burgeoning publicly available air and stream temperature datasets to categorize the relative flow path depth of groundwater contribution to streams and rivers, which can inform how both hydrologic models and stream ecosystem management approaches incorporate groundwater dynamics. Implications of groundwater discharge source-depth Groundwater-dependent ecosystems have become an important consideration for watershed management decisions 1 , and streams with substantial groundwater contributions are generally considered most resilient to change. Our work underscores the need for expanding the direct incorporation of groundwater discharge dynamics, especially source-flow path depth, into decision-making processes and predictive frameworks. Streams with shallow or deep groundwater signatures were ubiquitous nationally (nearly 40% of sites) and distributed across stream sizes, U.S. physiographic provinces, and within regional subwatersheds. Yet, regional generalizations remain uncertain at scales relevant for managing stream habitat. Although the more thermally stable streams with deep groundwater signatures tended to occur more frequently in regions with productive aquifers and in watersheds with lower slopes, they also occurred across nearly all physiographic provinces, and a range of watershed slopes and drainage areas. Human land development may explain some of the heterogeneity in groundwater connection, as we found that sites with groundwater signatures were less likely to be associated within catchments with high impervious cover or other types of human disturbance, including groundwater pumping and channelization. Our characterization of groundwater contribution to streamflow has important implications for understanding and predicting how streamflow and water quality respond to climate change, groundwater extraction, and watershed development. By definition, shallow aquifer flow paths with pronounced annual temperature signals are tightly coupled to seasonal temperature (and precipitation) dynamics, and our analysis shows that streams influenced by shallow groundwater are more likely to be warming over time than sites with deep groundwater signatures. Shallow groundwater discharge will then have reduced stream cooling potential in summer, particularly during anomalous seasons, when thermal refuges in marginal cold-water habitat are most needed. Our analysis also shows that streams influenced by shallow groundwater tend to have a reduced fraction of total streamflow composed of baseflow compared to deep groundwater. Thus, streams with substantial shallow groundwater contribution are more vulnerable to extreme low flows or drying from climate change-related increases in drought or evapotranspiration, or from increased groundwater extraction. The high responsiveness of shallow groundwater to land surface disturbances also suggests streams with substantial shallow stream water contributions are likely more susceptible to diffuse nutrient and other pollution additions, while deeper groundwater can perpetuate legacy watershed land uses 3 and emerging contaminants such as per- and polyfluoroalkyl substances from outside the river corridor 4 . Still, shallow groundwater dominated streams may be more responsive to short-term management actions that reduce groundwater extraction and limit land application of fertilizers and other chemicals. Thus, our analysis provides foundational knowledge to the importance of source groundwater discharge flow path depth on stream temperature, flow, and water quality. We consider this additional dimension of groundwater discharge essential to informing current stream process models and necessary to building robust predictions in a time of change. Methods We classified streams by their groundwater signature based on the observed decoupling of annual air and stream water temperature signals, both in terms of amplitude and timing (phase), which is driven by the magnitude and relative source-depth (shallow versus deep) of groundwater discharge to streams 8 . Shallow groundwater is defined here as groundwater within the near-surface critical zone where annual aquifer temperature is highly variable (within approximately 6 m from land surface), and this variability is transferred to streams through groundwater discharge zones causing annual temperature signal mixing with characteristic outcomes. Thermally stable, deeper groundwater discharge serves to attenuate annual stream temperature signals but does not cause notable phase shifts, as deeper groundwater temperature signals are highly attenuated. We used this newly expanded signal processing-based methodology (explained below, see refs. 8 , 40 ) to infer the source-flow path depth of groundwater discharge to streams based on these first principles. We acquired publicly available data from ~4000 discrete stream water temperature stations, of which 1811 met our required data criteria of being located within 25 km of a National Oceanic and Atmospheric Administration (NOAA) air temperature station, and having at least 2 consecutive years of temperature data collected in 2010 or after without gaps of 30 continuous days or more. This data gap criteria is supported by parallel paired air and water temperature signal analysis research 58 . Stream temperature datasets were used from three repositories: the USGS National Water Information System database (NWIS) 59 , the NorWeST Stream Temperature dataset 60 , and the Spatial Hydro-Ecological Decision System (SHEDS); all repositories are assumed to have internal quality assurance and quality control (QA\/QC) protocol. 1729 sites met our data quality review, which are discussed in the Temperature Signal Processing Approach section below. We acquired the paired daily air temperature record for each stream station from the Global Historical Climatology Network-Daily (GHCN-daily) Database 61 using the R package \u2018rnoaa\u2019 62 . We extracted data from the two nearest NOAA stations. The nearest air station data were used first; however, if the data did not meet our criteria (75% of annual data available and 75% data overlap with paired stream temperature), then a second NOAA station, if available, was evaluated and used if the criteria were met ( n = 191). We linked coordinates of each stream site to the nearest National Hydrography Dataset Plus flowline common identifier (COMID) (within 250 m) and paired with the U.S. Environmental Protection Agency (EPA) Stream-Catchment (StreamCat) dataset 50 to obtain watershed land cover. We also paired NWIS sites 59 with the USGS Geospatial Attributes of Gages for Evaluating Streamflow, version II (GAGES II) dataset 63 by station identifier (ID) value to obtain distance from nearest major dam, watershed slope, and the Hydrologic Disturbance Index. The Hydrologic Disturbance Index is derived from anthropogenic disturbances within the site\u2019s watershed including the presence of major dams, change in reservoir storage from 1950 to 2009, percentage of canals, road density, distance to nearest major pollutant discharge site, estimate of fresh-water withdrawal, and calculated fragmentation of undeveloped land 51 . To categorize sites into shallow groundwater, deep groundwater, atmospheric, or major dam signatures, we designed an automated signal processing software tool in Python that fits a static sine curve to the stream water and local air temperature data and derives the paired air and stream water signal metrics of amplitude ratio and phase lag. Although some datasets were collected at sub-daily frequency, average daily values were used for both air temperature and stream water temperature input data. Based on principles described in previous studies, we excluded average daily temperature readings below 1\u02daC from the analysis, because the paired air\u2013stream temperature relationships decouple due to the freeze\u2013thaw dynamics of water 35 . Also, stream values greater than 60 \u02daC were removed during analysis. For each discrete temperature record, we fit the annual temperature cycle using a linearized static sinusoidal function (equation 1) by minimizing the root mean square error (RMSE) of the average daily temperature residuals (\u00b0C) with the Python scipy optimize curve fit module 64 . This function was chosen to most simply extract the \u2018average\u2019 fundamental (annual) signal from the time series and is consistent with the analysis conducted by previous studies 8 , 40 . The average daily root-mean-square errot for both air and stream water signals at each site are provided in the Fig. 1 Source files. $$\\alpha \\sin (t) + \\beta \\cos (t) + C$$ (1) Using the calculated regression coefficients \u03b1 and \u03b2 , we calculated the amplitude ( A ; equation 2) and the phase ( \u03d5 in radians; equation 3) of each signal. January 1 was defined as 1\/365. $$A = \\sqrt {\\alpha ^2 + \\beta ^2}$$ (2) $$\\phi = {\\mathrm{arctan}}\\left( {\\frac{\\beta }{\\alpha }} \\right)$$ (3) We defined the groundwater signature categories by the paired air and stream water signal metrics, which are amplitude ratio ( A r ) and phase lag (\u0394 \u03d5) . We calculated A r by dividing the annual stream water signal amplitude by the annual air temperature signal amplitude; \u0394 \u03d5 is calculated as the difference between the phase of the annual stream water temperature signal and that of the air temperature signal and converted from radians to days (d) using 365 divided by 2\u03c0. A positive phase lag indicates the number of days the fitted stream water signal is delayed with respect to the fitted air temperature signal. Negative phase lags imply that stream water temperature responds to atmospheric thermal input faster than air, which is not logical for natural stream systems (except those influenced by geothermal heating). As a result, within the dataset we explored negative phase lags ( n = 454, mean of \u22124). Negative phase lags greater than 10 days ( n = 25) were dropped from the analysis as these data were associated with heavily managed stream flows as indicated by visual inspection of the stream temperature patterns or highly variable winter air temperature data that were not well captured by the fitted sine curve. Negative phase lags between 0 and \u221210 days ( n = 429) are still included within the dataset but set to 0 for calculations. These data and multi-day atmospheric signature phase lags were attributed to inherent imprecision of signal fitting to natural data, as other studies that use this same method did not show any negative phase lags when using streamside air signals 40 , 41 . Because the classification analysis only utilized parameters \u03b1 and \u03b2 , and not C , we assumed altitude differences between air temperature and stream water temperature sampling location did not have substantial influence on the amplitude ratio or phase lag. We categorized sites as having an atmospheric, shallow groundwater, or deep groundwater signature by identifying \u2018conservative\u2019 threshold values of A r (0.65) and \u0394\u03d5 (10 days) that parsed only sites with pronounced groundwater signatures (Supplementary Fig. 1 ). These threshold values were chosen based on previously presented stream and groundwater annual signal-mixing theory, process-based modeling, and field data 8 , 40 . Specifically, we developed A r and \u0394\u03d5 thresholds using evidence from three well-studied systems, the Quashnet River, Cape Cod, Massachusetts 8 , Shenandoah National Park, Virgina 41 , and the Olympic Experimental State Forest, Washington 40 . The hydrogeology of the Quashnet River has been extensively characterized 65 , 66 , indicating streamflow is dominated by deep groundwater discharge that at times makes up close to 100% of total streamflow. Using a dynamic sinusoidal regression technique, Briggs et al. 8 found that A r ranged from approximately 0.49 to 0.63 over a 3-year period with varied climatic conditions. Thus, we chose a threshold of 0.65 to indicate a deep groundwater signature for our study. It is likely that A r values up to approximately 0.75 also indicate substantial deep groundwater influence, but with less certainty. Other physical factors such as channel confinement, aspect, and shading could affect A r , but to date no published work that we are aware of indicates these factors could explain A r < 0.65 without the influence of groundwater. However, we hypothesize that these factors are likely to change the distance downstream these annual signals can be detected. All A r values less than 0.4 were manually checked for a major dam within 30 km upstream of the site by visual inspection. Extensive field data collected at Shenandoah National Park, a region known to be dominated by shallow bedrock conditions, indicates an average \u0394\u03d5 of 11 days, and conceptual mixing models of stream and groundwater annual temperature signals from Shenandoah headwater streams indicate a \u0394\u03d5 of about 10 days or greater when shallow groundwater discharge contributes at least 25% of total streamflow 8 . Therefore, for our analysis we used the threshold phase lag of 10 days to identify sites with a shallow groundwater signature. A r and \u0394\u03d5 thresholds may vary among watersheds and regions and thus can and should be modified based on additional information about individual watersheds for more precise, localized analyses. However, for the purposes of our analysis these thresholds represent conservative estimates applied across broad spatial scales. Sites with atmospheric signatures in our dataset had an A r between 0.65 and 1.1. Sites with deep groundwater signatures had an A r of 0.05 to 0.65. Sites with amplitude ratio values greater than 1.1 were removed as these extremes likely reflected poor pairings between the air and stream water station data, or measurement error. Because there are different numbers of sites within each groundwater signature category, we used a modified comparison of means for unbalanced designs for all statistical comparisons 67 . For sites within the USGS NWIS dataset 59 , stream discharge data for 554 stream water sites were available for the same time record as the analyzed temperature dataset. We calculated baseflow index (BFI) for the 554 stream discharge stations to provide a direct comparison between typically used hydrograph separation methods and our temperature-based methods. We used the \u2018bfi\u2019 function within USGS-R \u2018DVstats\u2019 package version 0.3.4 to calculate percent baseflow for each site by averaging the percent daily baseflow (daily baseflow discharge divided by total daily flow) over the time period of the temperature record. We analyzed a subset of our stream water temperature records for monotonic 14-year to 30-year trends (January 1990\u2014December 2019). This record length was chosen to account for the El Ni\u00f1o-Southern Oscillation (ENSO) period, which is three to seven years, thus the minimum length of record (14 years) would encapsulate at least two full cycles. We recognize that these time series are short when accounting for Pacific Decadal Oscillations; however, our results indicate there is not a distinction between sites located in the western United States and the rest of the sites. Of the 1424 stream sites without major dam signatures, 197 sites had stream water temperature records with greater than 14 years of complete year records (i.e., greater than 75% of daily average temperature data) within a 30-year time span (1990\u20132019). Of the remaining sites, we removed a total of 13 sites manually due to data inconsistencies, such as anomalous value sets and managed patterns determined by visual inspection; therefore, 184 sites were analyzed for long-term stream temperature trends. We determined non-parametric Theil\u2013Sen regression slopes for both annual and summer (June\u2013August) time periods using the TheilSen function from the R package \u2018openair\u2019 68 , which allows for the seasonality of average monthly data to be detrended and is robust against outliers. Previous studies have stated the Theil Sen approach is comparable to a simple linear regression method when analyzing long-term stream temperatures 9 . We used the monthly averages to reduce autocorrelation and the \u2018deseason\u2019 option of the function to account for potentially important seasonal temperature influences such as changes to snowmelt. Data availability The datasets generated during and\/or analyzed during the current study are available in the USGS National Water Information System (NWIS) repository (  ); the NorWest Stream Temperature repository (  ); the Spatial Hydro-Ecological Decision System (SHEDS) repository (  ); and the NOAA Daily Global Historical Climatology Network (GHCN-Daily) repository (  ). Watershed parameters are from two publicly available datasets: the USGS data release for GAGES-II (  ) and EPA StreamCat dataset (  ). Source data are provided with this paper. Code availability Mathematical algorithms used for the analysis are presented within the text and provide sufficient information for data replication. Signal processing automation code is available on GitHub 69 . ","News_Body":"A UConn Ph.D. candidate and a faculty member have developed a novel way of gathering data about streams fed by groundwater that provide important insights about the possible effects of climate change. Water is constantly on the move: through the air, through waterways, and underground. Life depends on a consistent supply of water and details about its journey are necessary for understanding and managing this dynamic resource. However, those details are often difficult to measure. UConn Ph.D. candidate Danielle Hare, in the lab of associate professor of Natural Resources and the Environment Ashley Helton, has expanded on a novel method to easily access vital details about groundwater, and in doing so, they have discovered that many streams are more vulnerable to stressors like climate change than previously thought. The team has published their findings in the latest issue of Nature Communications. Precipitation enters streams and rivers by flowing over land surfaces, or it percolates through soil into the groundwater. Groundwater then flows back into waterways, but understanding the details, such as the depth of groundwater entering streams, is more challenging. \"Normally, you'd have to go to a site and spend a lot of time and money just to figure out the source of groundwater discharging to the stream,\" she says. These details are important for watershed managers, who take into account numerous variables to keep water clean and safe, both for drinking water and for wildlife habitats. Details like depth are crucial because, for example, shallower groundwater reserves are more prone to disturbances than deeper sources. Hare says one of the threats to the streams supplied by shallower groundwater is climate change, as shallow groundwater is more susceptible to warming and has grave impacts on water resources down the line. Helton explains some of the roles groundwater plays for streams and groundwater-dependent ecosystems. \"You can think about the three services that groundwater provides to streams as it discharges back to the streams at the surface,\" she says. \"First is flow; groundwater provides water and deeper groundwater provides more consistent flow. Second, groundwater provides a temperature buffer and what is called thermal refuge for organisms, and deeper groundwater provides more stable temperatures. Third, groundwater provides nutrients and carbon for ecosystems and deeper groundwater often has a different chemical profile.\" In the case of streams with significant groundwater inputs, Helton says management often defaults to assuming that groundwater-dependent streams are managed similarly. Hare, with a strong interest in stream temperatures and groundwater dynamics, sought to explore if this was truly the case as part of a class project. \"This project was open-ended and it was a great opportunity to combine my interests. We were not sure if it would work, but even if it didn't, I knew I would learn along the way,\" says Hare. Hare used data that is frequently gathered and often publicly accessible: stream and air temperature measurement. These data are paired at over 1,700 streams nationwide, and the researchers were able to deduce which streams had substantial groundwater inputs and, of those, which were deep or shallow groundwater-fed. The findings were eye-opening. \"Something that surprised me was just how prominent shallow groundwater sites are across the US. We saw about 40% of the sites had substantial groundwater component, and how many of those were shallow were about 50%. I would not have guessed that; I would have guessed that there were more deep groundwater,\" says Hare. The researchers were excited that what started as a course project for Hare has turned into such a powerful tool. \"This method is straightforward and accessible to watershed managers and stakeholders. There is a lot of power to that. There is no need to spend a lot money to define different geology, we can simply use a temperature logger or thermometer to monitor the temperatures. They are widely available and straightforward,\" says Hare. Hare and Helton are hopeful this information will be considered in making watershed management decisions going forward. \"The sites that are dominated by groundwater are really wide spread and about half were shallow,\" says Helton. However, this could be problematic when sites are managed as if they are deep groundwater-fed sites. Hare cautions that managers could be missing out on important conservation opportunities in the face of challenges that can impact groundwater replenishment. \"The streams that are shallow are not going to be buffered as well as we previously thought,\" says Hare. \"Especially when considering the groundwater dependent ecosystems, when we're thinking about fishes that we really do need to consider or else we may have a missed opportunity as far as mitigating, supporting, observing that important ecosystem resource.\" For those tasked with managing these important watersheds, this new method ensures vital information is no longer out of reach, says Hare. \"Where the power is in this study and what makes it distinct is we separate the shallow versus deep components of groundwater. Not only are we able to find streams that are more groundwater-dominated, we can parse that information into whether it is groundwater shallow or deep. The shallow are going to be more susceptible to both climate warming and development changes.\" ","News_Title":"Groundwater information is no longer out of depth","Topic":"Earth"}
{"Paper_Body":"Abstract Achieving national targets for net-zero carbon emissions will require atmospheric carbon dioxide removal strategies compatible with rising agricultural production. One possible method for delivering on these goals is enhanced rock weathering, which involves modifying soils with crushed silicate rocks, such as basalt. Here we use dynamic carbon budget modelling to assess the carbon dioxide removal potential and agricultural benefits of implementing enhanced rock weathering strategies across UK arable croplands. We find that enhanced rock weathering could deliver net carbon dioxide removal of 6\u201330 MtCO 2 yr \u2212 1 for the United Kingdom by 2050, representing up to 45% of the atmospheric carbon removal required nationally to meet net-zero emissions. This suggests that enhanced rock weathering could play a crucial role in national climate mitigation strategies if it were to gain acceptance across national political, local community and farm scales. We show that it is feasible to eliminate the energy-demanding requirement for milling rocks to fine particle sizes. Co-benefits of enhanced rock weathering include substantial mitigation of nitrous oxide, the third most important greenhouse gas, widespread reversal of soil acidification and considerable cost savings from reduced fertilizer usage. Our analyses provide a guide for other nations to pursue their carbon dioxide removal ambitions and decarbonize agriculture\u2014a key source of greenhouse gases.     Main Governments worldwide are increasingly translating the Paris Agreement under the United Nations Framework Convention on Climate Change into national strategies for achieving net-zero carbon emissions by 2050. More than 120 nations have set full decarbonization goals that account for 51% of global CO 2 emissions, with the United Kingdom among several of these nations legislating for net-zero emissions 1 . The United Kingdom, where the industrial revolution driven by burning fossil fuels originated, is responsible for ~5% of the cumulative CO 2 emissions over the period 1751\u20132018 that drive climate change 2 . Carbon emissions in the United Kingdom have declined by 43% between 1990 and 2018 owing to the rise of renewables, and the transition from coal to natural gas, while growing the economy by 75% (ref. 3 ). Continued phase-out of emissions is, however, required to meet the United Kingdom\u2019s net-zero commitment, together with the capture and storage of residual emissions using carbon dioxide removal (CDR) technologies and a strengthening of nature-based carbon sinks 4 . Enhanced rock weathering (ERW), a CDR strategy based on amending soils with crushed calcium- and magnesium-rich silicate rocks, aims to accelerate natural CO 2 sequestration processes 5 , 6 , 7 , 8 . The estimated net global potential for ERW deployed on croplands to draw down CO 2 is substantial, up to 2 GtCO 2 yr \u2212 1 (ref. 6 ), with co-benefits for production 9 , 10 , 11 , soil restoration and ocean acidification 7 , 8 , 12 . Agricultural co-benefits can create demand for ERW deployment that is unaffected by diminishing income from carbon-tax receipts generated by other CDR technologies as the transition to clean energy advances and emissions approach net zero 13 . Global action on CDR, and hence progress towards net zero, requires leadership from early-adopting countries through their development of flexible action plans to support policymakers of other nations. Assessment of the contribution of ERW to the United Kingdom\u2019s net-zero commitment is therefore required, given that it is a CDR strategy for assisting with decarbonization while improving food production and rebuilding soils degraded by intensified land management 9 . Here we examine in detail the technical potential of ERW implementation on UK arable croplands in a national net-zero context and provide a blueprint by which other nations may proceed with this CDR technology as part of their legislated plans for decarbonization. Using coupled climate\u2013carbon\u2013nitrogen (climate\u2013C\u2013N) cycle modelling of ERW (Methods and Extended Data Fig. 1 ), we constructed dynamic UK net 2020\u20132070 C removal budgets and CDR costs after accounting for secondary CO 2 emissions from the ERW supply chain (Methods and Extended Data Fig. 2 ). Coupled C\u2013N cycle ERW modelling provides the fundamental advance in assessing the effects of cropland N fertilizers on the soil alkalinity balance and mineral weathering kinetics (Methods and Extended Data Fig. 3 ; Supplementary Information ) and ERW-related mitigation of nitrous oxide (N 2 O) emissions from agricultural soils 14 . Nitrous oxide is a key long-lived greenhouse gas and important stratospheric-ozone-depleting substance 15 ; UK agriculture accounts for 75% of N 2 O emissions nationally with high external costs (~\u00a31 billion yr \u22121 ) 16 . Our analysis, constrained by future energy policies 17 , utilizes basalt as an abundant natural silicate rock suitable for ERW with croplands 9 , 10 , 11 , with low- (S1), medium- (S2) and high- (S3) extraction scenarios between 2035 and 2050 (Methods and Extended Data Fig. 4 ; Supplementary Information ). Patterns of cropland CDR Across basalt supply scenarios S1 to S3, ERW implementation on arable lands was simulated to remove 6\u201330 MtCO 2 yr \u22121 by 2050 (Fig. 1a\u2013c ); that is, up to 45% of the CO 2 emissions removal required for UK net-zero emissions (balanced net-zero pathway engineered carbon removal requirement ~58 MtCO 2 yr \u22121 ; range 45\u2013112 MtCO 2 yr \u22121 ) 4 . Modelled maximum CDR rates were predominantly governed by the geographical extent of ERW application, which increased as resource provision allowed (Fig. 1a\u2013c ). Year-on-year legacy effects are also important. CDR rates per unit area increased over time with successive annual applications of rock dust, even if the land area of deployment remained constant. These effects are evident in all scenarios when basalt extraction levelled off, and result from slower-weathering silicate minerals continuing to capture CO 2 in years post-application before they are fully dissolved 6 . By quantifying the geochemical dissolution rates governing ERW and legacy effects, our simulations indicated the CDR potential of ERW rise over time to exceed that suggested by previous mass balance estimates 18 , 19 , 20 . Fig. 1: Net CDR by ERW deployed on UK arable croplands. a \u2013 c , Simulated net CDR (left y axis) and annual basalt extraction (right y axis) for S1 ( a ), S2 ( b ) and S3 ( c ) resource extraction scenarios. Results are shown for two particle size distributions (p80 = 10 \u00b5m diameter and p80 = 100 \u00b5m diameter). The shaded envelopes denote 95% confidence limits. d \u2013 f , Isolines of UK decadal running-average net CDR (MtCO 2 yr \u22121 ) for S1 ( d ), S2 ( e ) and S3 ( f ) over time (2020\u20132070). a \u2013 f show mean results for three UK-specific basalts. g \u2013 i , Cumulative net CDR over time for S1 ( g ), S2 ( h ) and S3 ( i ) resource extraction scenarios by UK region, showing the mean of simulations with p80 = 10 \u00b5m and p80 = 100 \u00b5m and three UK-specific basalts. Insets in g \u2013 i show the cumulative CDR time series for 2020 to 2050. Source data Full size image Net-zero pathways for greenhouse gas removal internationally 21 , and in the United Kingdom 4 , have tended to focus narrowly on bioenergy with carbon capture and storage (BECCS), and direct air carbon capture and storage (DACCS). However, our new results indicate that ERW could be an important overlooked component of national CDR technology net-zero portfolios, working synergistically with croplands, rather than competing with them, as large-scale deployment of BECCS might. In S1, for example, ERW reaches net CDR of 5 MtCO 2 yr \u22121 by 2050, equalling the DACCS estimate 5 , and closer to 10 MtCO 2 yr \u22121 by 2060 (Fig. 1a ). In the highest resource scenario, S3, ERW delivers approximately half of the net CDR forecast for UK BECCS facilities 5 by 2050 (Fig. 1c ). Milling rocks to fine particle sizes is the most energy-demanding step in the ERW supply chain 18 , 22 . We therefore assessed a range of options for milled rock particle sizes, as defined by p80 (that is, 80% of the particles have a diameter of less than or equal to the specified value), and the associated energy demands across scenarios S1 to S3 (Fig. 1d\u2013f ). For all scenarios, we show that particle size typically has a small effect on net CDR for the first 10\u201320 years of implementation, as indicated by flat CDR isolines. In the model, ERW deployment locations were prioritized over time, starting from high weathering potential and progressing to low weathering potential. The prioritization of sites with high weathering potential in the first couple of decades means that basalt particles are weathered rapidly regardless of size\u2014a result verified with soil column experiments 23 . In S2, for example, a drawdown of 3 MtCO 2 yr \u22121 in 2035 with a p80 of 500 \u00b5m was achieved only 5 years earlier by milling to a p80 of 10 \u00b5m. Our dynamic simulations of temporal ERW carbon budgets, together with recent experimental findings 23 , challenge the assumption that rocks must be ground finely to accelerate dissolution for effective CDR 7 , 8 , 18 , 22 . Coarser particles minimize health and safety risks when handling rock dust, in addition to reducing energy demand. However, as S2 and S3 encompass rock dust application on more agricultural land post-2040, with a greater proportion of sub-optimal weathering locations, the dissolution of small particles becomes relevant and the effect of p80 on net CDR increasingly apparent. Energy requirements for delivering ERW are generally low. Before 2035, the energy demand for rock grinding was minimal across all three scenarios at ~1 TWh yr \u22121 , which is less than 0.2% of the United Kingdom\u2019s power production (Extended Data Fig. 5 ). After 2040, the energy demand for grinding an increased rock mass to be distributed across an expanding area of arable land increased. However, limiting grinding to achieve rock dust with a p80 of 100 \u00b5m or more keeps energy demand to less than or equal to 4 TWh yr \u22121 , or 0.6% of UK production for all scenarios. These results mitigate previous concerns that undertaking extensive deployment of ERW in the United Kingdom may compromise energy security 13 . Reducing cumulative CO 2 emissions on the pathway to net zero helps minimize the United Kingdom\u2019s contribution to the remaining future carbon budget consistent with keeping warming below a given level 24 . Assuming that ERW practices are maintained between 2020 and 2070, the resulting cumulative net CO 2 drawdown was simulated to be 200, 410, and 800 MtCO 2 by 2070 (Fig. 1g\u2013i ). Longer-term compensatory ocean outgassing and sediment CaCO 3 uptake could reduce net CDR effectiveness by 10\u201315% by 2070 (Extended Data Fig. 6 ). Attained over 50 years with ERW, these cumulative CDR ranges compare with an estimated ~696 MtCO 2 sequestration over 100 years for afforestation in organic soils of the Scottish uplands 24 and avoids possible soil carbon loss from tree planting 25 and sustained long-term management requirements. More broadly, cumulative ERW-based CDR ranges are comparable to CO 2 removal estimates for UK woodland creation schemes aligned to a balanced net-zero framework (112 MtCO 2 by 2050 and ~300 MtCO 2 by 2070) 26 . A breakdown of cumulative CDR by region revealed marked shifts in regional contributions from S1 to S3, with increasing contributions over time from croplands in Scotland, northeastern and southwest England, and the Midlands. These regions have acidic soils, where early deployment offers increasing CDR over time from legacy weathering effects. The more aggressive CDR strategy of S3 requires less optimal regions for ERW with the lowest rainfall (southeast and eastern England). Mapped UK-wide CDR rates per unit area provide fine-scale estimates of modelled carbon removal potential across space and time provide an important tool for precisely targeting ERW interventions (Fig. 2a\u2013c ). Results highlight the limited cropland area required for CDR by ERW in the first couple of decades in S1 and S2, and the rise in CDR per unit area over time. Across all decades and scenarios, our geospatial net CDR estimates typically exceed those for the low-carbon farming practices forming part of net-zero pathways for agriculture 4 , including switching to less intensive tillage (typically ~1 tCO 2 ha \u22121 yr \u22121 ) 27 , conversion of arable land to ley pasture (~1\u20135 tCO 2 ha \u22121 yr \u22121 ) 28 and inclusion of cover crops in cropping systems (1.1 \u00b1 0.3 tCO 2 ha \u22121 yr \u22121 ) 29 . Fig. 2: Mapped fine-scale decadal average UK net CDR. a \u2013 c , Mapped net CDR from ERW deployed on arable croplands for S1 ( a ), S2 ( b ) and S3 ( c ) resource extraction scenarios is shown for the decades indicated. The mean of simulations with p80 = 10 \u00b5m and p80 = 100 \u00b5m and three UK-specific basalts is shown. Source data Full size image Underlying the geospatial maps of net CDR are strong cycles in alkalinity generation and soil pH, and intra-annual dissolution\/precipitation of soil carbonates, driven by seasonal climate and crop production effects (Extended Data Fig. 7 ). These results show a decline in the periodic dissolution of soil (pedogenic) carbonates over decades as the cumulative effect of alkalinity systematically raises the seasonal minimum in soil pH and drives a steady increase in the net CDR per unit area each year. Rising alkalinity over time increases the soil buffer capacity, which reduces the risk of pH reversal, thereby improving security of CO 2 storage. These results for the UK maritime climate are consistent with soil carbonate accumulation and persistent in arid systems 30 , and highlight the challenge of monitoring, reporting and verifying CDR via seasonal dynamics of soil carbonates, and soil fluid alkalinity discharge, over multiple field seasons. Costs of cropland CDR The costs of CDR must be known to evaluate commercial feasibility, permit comparison with other CDR technologies and allow governments to understand the carbon price required to pay for it. Between 2020 and 2070, CDR costs fall from \u00a3200\u2013250 tCO 2 \u22121 yr \u22121 in 2020 to \u00a380\u2013110 tCO 2 \u22121 yr \u22121 by 2070 (Fig. 2a\u2013c ). Modelled longer-term cost trends are driven by rising CDR with successive rock dust applications (Fig. 1a\u2013c ) and declining renewable energy prices (Methods and Extended Data Fig. 8 ). Grinding rocks to smaller particle sizes carries a minor financial penalty. As the geographical deployment of ERW increases in S3, the price of CDR rises from 2030 to 2050 due to higher total energy costs associated with grinding more rock and the requirement for more extensive logistical operations, particularly spreading of the rock dust over farms. However, it subsequently falls as CDR rates increase with repeated rock dust applications (Fig. 1 ). The dominant cost elements are electricity for rock grinding and fuel for spreading the milled rock on farmland (Fig. 3d\u2013f ). Mineral P and K nutrient fertilizers are expensive (\u00a3300\u2013400 t \u22121 and \u00a3250\u2013300 t \u22121 for P and K fertilizers, respectively) 31 . Given fertilizer application rates per unit of land area typical for arable crops (Extended Data Fig. 9 ), using basalt could provide savings sufficient to cover transport costs (Fig. 3d\u2013f ). Fig. 3: Costs of CDR by ERW deployed on UK arable croplands. a \u2013 c , Costs of net CDR for S1 ( a ), S2 ( b ) and S3 ( c ) resource extraction scenarios over time (2020\u20132070). Results are shown for two particle size distributions (p80 = 10 \u00b5m and p80 = 100 \u00b5m). The shaded envelopes denote 95% confidence limits. d \u2013 f , Breakdown of ERW processes contributing to CDR costs, including savings resulting from basalt substituting for P and K fertilizers averaged for 2060\u20132070 under S1 ( d ), S2 ( e ) and S3 ( f ). Error bars indicate 95% confidence limits. All panels display average results for three UK-specific basalts. Source data Full size image Modelled average CDR costs for ERW practices are towards the lower end of the range for BECCS, which varies widely across sectors 4 (\u00a370\u2013275 tCO 2 \u22121 ), and half of that estimated for early-stage DACCS plants. DACCS CDR has an indicative price of \u00a3400 tCO 2 \u22121 during the 2020s and \u00a3180 tCO 2 \u22121 by 2050 as the technology develops and scales up globally 4 , 21 . ERW is thus competitive relative to industrial CDR technologies such as these that will also be required to help achieve net-zero emissions. Fine-scale spatial and temporal assessment of CDR costs (Fig. 4a\u2013c ), combined with analysis of regional CO 2 drawdown (Fig. 2a\u2013c ), informs geographical prioritization of near-term opportunities for rapid ERW deployment and public consultations on these activities. Costs in all scenarios decrease through time as CDR rises, with geographical variations in CDR costs approximately twofold by 2050\u20132060. These patterns reflect differences in CDR and, to a lesser extent, transport distances between source rocks and croplands. By 2060\u20132070, the lowest costs (\u00a375\u2013100 tCO 2 \u22121 ) occur in the northeast of England, the Midlands and Scotland, where CDR rates are highest because of favourable soil weathering environments and regional climate effects on site water balance (precipitation minus evapotranspiration). Fig. 4: Mapped fine-scale decadal average UK net CDR costs. a \u2013 c , Mapped net CDR costs of ERW deployed on arable croplands for SI ( a ), S2 ( b ) and S3 ( c ) resource extraction scenarios for the decades indicated. The mean of simulations with p80 = 10 \u00b5m and p80 = 100 \u00b5m and three UK-specific basalts is shown. Source data Full size image Nations committing to net-zero targets require carefully designed economic and policy frameworks to incentivize uptake and cover the costs of CDR technologies 13 , 21 , as well as the modification of existing emissions trading schemes. Costs might be met in the near term through farming subsidies; agriculture is heavily supported in most countries worldwide 13 . Actions to enhance soil carbon storage are already subsidized in the United States, and European proposals to incentivize CDR by farmers are underway 32 . Redesigned agricultural policies in the United Kingdom post-Brexit aim to provide public funding to support farmers in delivering environmental public goods and contributing to net-zero emissions 33 by 2050. Identifying strategic options, such as ERW, with multiple co-benefits for agricultural productivity and the environment is key to enhancing uptake. Co-benefits of ERW for agriculture Arable soils are a critical resource supporting multiple ecosystem services, and the adoption of ERW into current agricultural practices could enhance soil functions. We quantified three major soil-based co-benefits with the potential to increase the demand for early deployment of the technology: reducing excess soil acidity, increasing the primary supply of fertilizer-based mineral nutrients (P and K) 5 , 9 , 10 and mitigating soil N 2 O fluxes 14 . Soil acidity (that is, pH below 6.5) 34 limits yields and correction is essential for good soil management, crop growth, nutrient use efficiency and environmental protection 35 . Following initialization with topsoil (0\u201315 cm) pH values based on high-resolution field datasets (Methods), the implementation of ERW reduces the fraction of arable soils with pH less than 6.5 in England to 13% by 2035 (S1), and completely by 2045 and 2055 in S2 and S3, respectively (Fig. 5a ). In Scotland, where agricultural soils are more acidic than in England, the co-benefit of ERW in raising soil pH could be considerable, with reductions to 10% by 2050 in S1 and eliminating acidic soils by 2045\u20132050 in S2 and S3 (Fig. 5b ). Reversing soil acidification across England and Scotland could increase nutrient uptake to boost yields on underperforming croplands 34 , 35 , lower the potential for metal toxicity 10 at low pH and enhance N fixation by legumes 36 . Calcium released by ERW can also stimulate root growth and water uptake 37 and multi-element basalt can fortify staple crops such as cereals with important micronutrients, including iron and zinc 9 . Raising soil pH with widespread ERW practices in the United Kingdom, and elsewhere, to improve agricultural productivity 38 releases land for additional CDR opportunities, including afforestation and bioenergy cropping 4 , 21 . Fig. 5: Agricultural ecosystem co-benefits of ERW. a , b , Reduction in the fraction of acidic land in England ( a ) and Scotland ( b ) following deployment of ERW. c , d , CO 2 emissions avoided ( c ) and cost savings ( d ) resulting from using basalt to substitute for P and K fertilizers. e , f , Soil N 2 O emissions reductions from croplands ( e ) and percentage change from 2010 ( f ) following ERW deployment. N 2 O results are shown as 10 yr annual running averages. The black line in f and g denotes results of the control 'no basalt' simulations. Results are shown for S1, S2 and S3 resource extraction scenarios in all panels, with the line style indicating the particle size distribution (p80 = 10 \u00b5m and p80 = 100 \u00b5m); the legend in b applies to all panels. The shaded envelopes denote 95% confidence limits. Source data Full size image Calculated rates of inorganic P and K nutrient supply for crops via ERW of basalt are comparable to typical P and K fertilizer application rates for major tillage crops (Extended Data Fig. 9 ). ERW with basalt could therefore substantially reduce the reliance of agriculture on the expensive and finite rock-derived sources of P and K fertilizers required to support increased agricultural production over the next 50 years in the United Kingdom, and globally, to meet the demands of a growing human population 39 . Reductions in P and K fertilizer usage lower unintended environmental impacts, supply chain CO 2 emissions and costs. For the United Kingdom, assuming that annual fertilizer application on ERW cropland areas in S1\u2013S3 to replenish pools of P and K is reduced, the avoided carbon emissions are estimated to be 0.1\u20131 MtCO 2 yr \u22121 , with maximum cost savings of \u00a3100\u2013700 million yr \u22121 by 2070 (Fig. 5c\u2013f ). However, we note that not all crops require annual fertilization. These savings could contribute to offsetting the cost of undertaking ERW practices, but may be reduced by precision farming techniques, including applying variable levels of fertilizers within fields, and controlled-release fertilizers. Practices that optimize the efficient use of N on croplands to reduce N 2 O emissions from soils are important for ambitious net-zero agriculture pathways in the United Kingdom 4 . Our process-based model simulations, calibrated with field data 14 , indicate that ERW deployment on UK croplands could reduce soil N 2 O emissions by ~0.1 Mt of CO 2 equivalent (CO 2 e) per year, ~1 MtCO 2 e yr \u22121 and ~1.5 MtCO 2 e yr \u22121 by 2070 in S1, S2 and S3, respectively (Fig. 5e ); this equates to a reduction of up to 20% relative to croplands in 2010 (Fig. 5f ). This contrasts with large-scale land-based CDR strategies for increasing soil organic carbon stocks, which can increase soil N 2 O emissions 40 . ERW may therefore offer a new management option for mitigating soil N 2 O fluxes that is comparable in magnitude to other proposed abatement measures 41 with the additional win of CDR. Societal and community acceptability Societal acceptance of ERW practices is needed on all scales, from the national-political to local community and individual farm scales. \u2018Acceptance\u2019 in this context should be regarded not as an absolute mandate to proceed, but instead as recognition of the need to work with stakeholders and affected publics to identify the conditions under which this technology might proceed 42 . Additional mining operations with unintended environmental impacts raise particular sensitivities 42 and two of our scenarios (S2 and S3) require new mines to be established between 2035 and 2050 to provide basalt; increases post-2035 account for delays due to complex licensing procedures (Extended Data Fig. 4 ). Concentrating resource production at larger sites (~1 Mt basalt yr \u22121 ) requires annual increases in mine numbers of 6% (S2) and 13% (S3); smaller mines (~250 kt yr \u22121 ) necessitate larger annual increases ( Supplementary Information ). However, the scale-up rate is less than the historical 10-year maximum (1960\u20131970) and limited to 15 years. Recycling the United Kingdom\u2019s annually produced calcium silicate construction and demolition waste (~80 Mt yr \u22121 ) 43 , which has potential to substitute for basalt 6 , could substantially reduce mined resource demand by between 80% (S2) and 45% (S3). Traditional mining operations provide local employment opportunities but have encountered controversy nonetheless because of concerns about sustainability, community impacts and local health and environmental risks 44 . Mining operations to enhance national carbon sequestration may raise different ethical and risk\u2013benefit narratives 45 . Procedural and distributional fairness in siting mines, alongside long-term proactive engagement with the communities likely to be affected by any new mining operations, will be critical for acceptance 44 , together with sustainable management plans for quarry restoration post-extraction 46 , 47 . The issue of mining new materials for CDR is part of a wider debate regarding the sustainability of increasing resource extraction for green technologies, such as electric vehicles or photovoltaic cells. Achieving this at scale requires the development of innovative solutions that combine improved resource efficiency and use of waste mining products, circular economy production systems and extraction efforts focused primarily in the regions or countries where materials are to be used 48 . Although nature-based techniques for CDR (for example forestry, carbon sequestration in soils) are likely to be preferred by public groups over engineered technologies 42 , 49 , they are unlikely to be sufficient to deliver net-zero emissions nationally or globally. Above all, broad societal support is unlikely to be forthcoming unless ERW is developed alongside an ambitious portfolio of conventional climate mitigation policies 49 . Implications for ERW deployment Our analysis with dynamic ERW carbon budget modelling suggests that this technically straightforward-to-implement CDR technology could prove transformative for utilizing agriculture to mitigate climate change 6 , 9 , 10 and play a larger role in national CDR portfolio programmes than previously realized. Unlike industrial CDR processes, including BECCS or DACCS, ERW could be rolled out without major new industrial infrastructure, and incentivized through amended agricultural subsidy frameworks. We show that eliminating the energy-demanding requirement for milling rocks to fine particle sizes requires early and sustained implementation of ERW practices, subject to public acceptance. This has the additional important advantages of maximizing CDR and lowering costs to a highly competitive price of \u00a380\u2013110 tCO 2 \u22121 yr \u22121 by 2070. Our findings underscore the urgent need for long-term field trials across a range of agricultural systems to evaluate this technology with empirical evidence, alongside monitoring of potential unintended negative consequences 9 , 50 . High-resolution geospatial ERW assessments provide a detailed basis for mapping out routes to technological development and afford opportunities to minimize social and economic barriers by identifying priority regions for public engagement. Scaling up ERW in the United Kingdom and other G20 nations will require funding, public support, regulation and governance to ensure sustainability, and a stable policy framework 4 , 13 to accelerate global CDR goals with agriculture 6 , 9 , 10 as the world transitions to net-zero emissions. Methods Resource extraction scenarios Under S1, per-capita production of aggregates continues to fall from 1.9 to 1.5 t yr \u22121 by 2032 and remains constant thereafter, with the spare capacity used and ramped up for ERW. Under S2, rock extraction is scaled up by 7% (half the historical maximum rate of increase) until the total additional capacity is equal to the maximum historical value in 1990 (100 Mt yr \u22121 ). Under S3, rock extraction is scaled up by 15% (that is, historical annual 10-yr rolling average) until the additional capacity is 160 Mt yr \u22121 ; that is, equivalent to the total increase in the UK crushed rock supply post-1945 ( Supplementary Information ). Extraction of resources scales at rates compatible with historical patterns (Extended Data Fig. 4 ) and those advanced for delivering CDR by BECCS (and its supply chains) and DACCS 4 . Soil profile ERW modelling Our analysis used a one-dimensional vertical reactive transport model for rock weathering with steady-state flow and transport through a series of soil layers. The transport equation included a source term that represents rock grain dissolution within the soil profile 4 with advancements to incorporate the effects of the biogeochemical transformations of N fertilizers ( Supplementary Information ). The core model accounted for changing dissolution rates with soil depth and time as grains dissolve, chemical inhibition of dissolution as pore fluids approach equilibrium with respect to the reacting basaltic mineral phases, and the formation and dissolution of pedogenic calcium carbonate mineral in equilibrium with pore fluids 4 . Simulations considered UK basalts with specified mineralogies from three commercial quarries ( Supplementary Information ). We modelled the ERW of a defined particle size distribution (psd) with theory developed previously 4 . As the existing psds at each soil layer are at different stages of weathering, the combined psd at each level, and for each mineral, was calculated and tracked over time 4 . We accounted for repeated basalt applications by combining the existing psd with the psd of the new application. Simulated mineral dissolution fluxes from the model output were used to calculate the release of P and K over time. Mass transfer of P within the relatively more rapidly dissolving 51 accessory mineral apatite was calculated on the basis of the P content of the rock and the volume of bulk minerals dissolved during each time step. The mathematical model combined a multi-species geochemical transport model with a mineral mass balance and rate equations for the chemical dissolution of basaltic mineral phases. The model included an alkalinity mass balance that incorporated the effect of fertilizer applications and soil N cycling and dynamic calculations of pH in soil pore waters. The main governing equations are detailed below. Transport equation The calculated state variable in the transport equation is the dissolved molar equivalents of elements released by stoichiometric dissolution of mineral i , in units of mol l \u22121 ; \u03d5 is the volumetric water content, C i is the dissolved concentration (in mol l \u22121 ) of mineral i transferred to solution, t is time (months), q is vertical water flux (m y \u22121 ), z is the distance along the vertical flow path (m), R i is the weathering rate of basalt mineral i (mole per litre of bulk soil month \u22121 ) and C eqi is the solution concentration of weathering product at equilibrium with the mineral phase i (equation (1) ). Values for C eqi for each of the mineral phases in the basalt grains were obtained by calibrating the results of the performance model against those of a 1D reactive transport model, as described previously 4 . Rates of basalt grain weathering defined the source term for weathering products and were calculated as a function of soil pH, soil temperature, soil hydrology, soil respiration and crop net primary productivity. The vertical water flux was zero when pore water content was below a critical threshold for vertical flow. Weathering occurred under no-flow conditions and the accumulated solutes in pore water were then advected when water flow was initiated under sufficient wetting, tracked using a single bucket model. $$\\phi \\frac{{\\partial C_i}}{{\\partial t}} = - q\\frac{{\\partial C_i}}{{\\partial z}} + R_i\\left( {1 - \\frac{{C_i}}{{C_{\\mathrm{eqi}}}}} \\right)$$ (1) Mineral mass balance The change in mass of basalt mineral i , B i , is defined by the rate of stoichiometric mass transfer of mineral i elements to solution. Equation (2) is required because we considered a finite mass of weathering rock, which over time could react to completion, either when solubility equilibrium between minerals and pore water composition was reached, or when applied basalt was fully depleted. $$\\frac{{\\partial B_i}}{{\\partial t}} = - R_i\\left( {1 - \\frac{{C_i}}{{C_{\\mathrm{eqi}}}}} \\right)$$ (2) Removal of weathering products The total mass balance over time (equation (3) ) for basalt mineral weathering allows calculation of the products transported from the soil profile. The total mass of weathering basalt is defined as follows where m is the total number of weathering minerals in the rock, T is the duration of weathering and L is the total depth of the soil profile (in m). We define q , the vertical water flux, as the net monthly sum of water from precipitation and irrigation, minus evapotranspiration, as calculated by the Community Land Model v.5 (CLM5). $${{{\\mathrm{Total}}}}\\;{{{\\mathrm{weathered}}}}\\;{{{\\mathrm{basalt}}}} = \\mathop {\\sum }\\limits_{i = 1}^m \\phi \\mathop {\\smallint }\\limits_{z = 0}^L C_i\\left( {t,z} \\right){{{\\mathrm{d}}}}z + q\\mathop {\\smallint }\\limits_{t = 0}^{T} C_i\\left( {t,L} \\right){{{\\mathrm{d}}}}t$$ (3) Coupled climate\u2013C\u2013N cycle ERW simulations Our model simulation framework (Extended Data Fig. 1 ) started with future UK climates (2020\u20132070) from the medium-mitigation future pathway climate (Shared Socioeconomic Pathway (SSP) 3-7.0) ensemble of Coupled Model Intercomparison Project Phase 6 (CMIP6) runs with the Community Earth System Model v.2. Future climates were used to drive CLM5 to simulate at high spatial resolution (23 km \u00d7 31 km) and high temporal resolution (30 min) terrestrial C and N cycling with prognostic crop growth and other ecosystem processes, including heterotrophic respiration 52 , 53 ( Supplementary Information ). CLM5 simulates monthly crop productivity, soil hydrology (precipitation minus evapotranspiration), soil respiration and N cycling. CLM5 includes representation of eight crop functional types, each with specific ecophysiological, phenological and biogeochemical parameters 52 , 53 . CLM5 includes CO 2 fertilization effects on agricultural systems benchmarked against experiments and observations 54 , 55 . An atmospheric CO 2 increase of ~200 ppm from 2015 to 2070 is defined by SSP3-7.0. In our CLM5 simulations with rising CO 2 and climate change, wheat net primary productivity increased by 8%, evapotranspiration decreased by 21% and water-use efficiency increased by 25% ( Supplementary Information ). Both increasing net primary productivity and decreasing evapotranspiration can facilitate weathering in our soil profile ERW model ( Supplementary Information ). We initialized CLM5 simulations for 2010 using fully spun-up conditions from global runs at ~100 km \u00d7 100 km resolution, adding an extra 60 yr spin-up in the regional set-up to stabilize the C and N pools to the higher-resolution setting. CLM5 includes an interactive N fertilization scheme that simulates fertilization by adding N directly to the soil mineral N pool to meet crop N demands using both synthetic fertilizer and manure application 52 , 53 . Synthetic fertilizer application was prescribed by crop type and varied spatially for each year based on the Land Use Model Intercomparison Project and land-cover change time series (Land-Use Harmonization 2 for historical rates and SSP3 for future rates) 55 , 56 . N fertilizer rates increased by 18% per decade from 2020 to 2050 in agreement with the United Kingdom\u2019s Committee on Climate Change forecasts of future N fertilizer usage 57 , and then stabilized from 2050 to 2070. Average UK CLM5 fertilizer application rates (148 kg N ha \u22121 yr \u22121 ) are consistent with current practices 58 . Organic fertilizer was applied at a fixed rate (20 kg N ha \u22121 yr \u22121 ) throughout the simulations. CLM5 tracks N content in soil, plant and organic matter as an array of separate N pools and biogeochemical transformations, with exchange fluxes of N between these pools 52 , 53 . The model represents inorganic N transformations based on the DayCent model, which includes separate dissolved NH 4 + and NO 3 \u2212 pools, as well as environmentally controlled nitrification, denitrification and volatilization rates 59 . To model the effect of basalt addition on fluxes of N 2 O from soil, we included the updated denitrification DayCent module 14 , modified to capture the soil pH ranges in UK croplands. The possible effect of increased soil pH from basalt application increasing NH 3 volatilization and, indirectly, N 2 O emissions, was not explicitly modelled. However, the error term is likely to be small, given that it accounts for less than 5% of total agricultural N 2 O emissions 60 , 61 . Cropland CLM5 soil N emissions are within the range of estimates in UK croplands based on bottom-up inventories and other land surface models, with N 2 O fluxes showing broad similarities in terms of regional patterns and magnitude with the UK National Atmospheric Emission Inventory ( Supplementary Information ). Modelling soil N effects on ERW The inclusion of mechanistic simulation of N cycling processes coupled to ERW via 16 stoichiometric N transformations that influence the soil weathering environment represents a theoretical advance over previous modelling ( Supplementary Information ). The modelling accounts for 20 depths (20 soil layers) in the soil profile at each location with a monthly time step; the variables passed from CLM5 by time and depth to the 1D ERW model are given in the Supplementary Information . At each depth, we computed N transformation effects on soil water alkalinity with reaction stoichiometries that added or removed alkalinity. Together with soil CO 2 levels, this affected pore water pH and the aqueous speciation that determined mineral weathering rates. This modelling advance allowed us to mechanistically account for the impact of N fertilization (which is recognized to potentially lead to nitric acid-dominated weathering 62 , 63 at low pH with no C capture) of cropland on basalt weathering rates. Dynamic modelling at monthly time steps resolved seasonal cycles of CDR via alkalinity fluxes and soil carbonate formation\/dissolution in response to future changes in atmospheric CO 2 , climate, land surface hydrology, and crop and soil processes. The effect of the N cycle on the soil acidity balance (Extended Data Fig. 3 ) was derived from N transformations associated with the production or consumption of hydrogen ions ( Supplementary Information ). We assigned a stoichiometric acidity flux \u2206 H i ,N (mol H + mol \u22121 N) to each N flux F i ,N (g N m \u22123 soil s \u22121 ) calculated by the CLM5 code ( Supplementary Information ). The product ( F i ,N \u2206 H i ,N ), with appropriate unit conversions, gives the acidity flux during the time step \u2206 t (s month \u22121 ) for the i th reaction of the CLM5 N cycle. Their sum (equation (4) ) is, therefore, the total change in acidity \u2206Acidity N due to the CLM5 N cycle: $$\\Delta {{{\\mathrm{Acidity}}}}_{{{\\mathrm{N}}}} = \\sum \\left( {F_{i,{{{\\mathrm{N}}}}}\\Delta H_{i,{{{\\mathrm{N}}}}}} \\right)\/14.0067\\Delta {{{t}}}$$ (4) where 14.0067 gN mol \u22121 N is the atomic weight of N and the time step is one month. Along with the Ca, Mg, K and Na ions released from weathering the applied minerals, \u2206Acidity N contributes a negative term to the soil water alkalinity balance used to calculate the soil pH 4 . $${{{\\mathrm{Alk}}}}_{{t}}{{{\\mathrm{ = Alk}}}}_{{{t - 1}}}{{{\\mathrm{+2}}}}\\times\\left( {{{{\\mathrm{Ca}}}}_{{{{\\mathrm{weath}}}}}{{{\\mathrm{ + Mg}}}}_{{{{\\mathrm{weath}}}}}} \\right){{{\\mathrm{ + K}}}}_{{{{\\mathrm{weath}}}}}{{{\\mathrm{ + Na}}}}_{{{{\\mathrm{weath}}}}}{{{\\mathrm{-}}}}\\Delta {{{\\mathrm{Acidity}}}}_{{{\\mathrm{N}}}}$$ (5) This pH value is one component that is accounted for in the rate laws for mineral dissolution and therefore influences the net alkalinity produced at each depth within the soil profile, which contributes to CDR 4 . The initial alkalinity profile in each grid cell was determined from the starting soil pH and the partial pressure of CO 2 ( \\(p_{\\mathrm{CO}_2}\\) ) profile at steady state based on spin-up of the model with average long-term biomass production and soil organic matter decomposition that reflected the long-term land use history of a particular location. The alkalinity mass and flux balance for an adaptive time step accounted for alkalinity and acidity inputs from (1) mineral dissolution rates and secondary mineral precipitation (pedogenic carbonate), (2) biomass production and decomposition 64 and (3) biogeochemical N transformations. The soil pH profile was determined from an empirical soil pH buffering capacity 65 relating soil pH to the alkalinity at each depth. The soil \\(p_{\\mathrm{CO}_2}\\) depth profile of a grid cell was generated with the standard gas diffusion equation 66 , scaled by monthly soil respiration from CLM5. At any particular location, the soil solution was in dynamic equilibrium with dissolved inorganic C species and the values of gas phase soil and atmospheric \\(p_{\\mathrm{CO}_2}\\) . The relative change induced by weathering will be the consumption of H + and the production of HCO 3 \u2212 . Using this modelling framework (Extended Data Fig. 1 ), we analysed a baseline application rate of 40 t ha \u22121 yr \u22121 (equivalent to a <2-mm-thick layer of rock powder distributed on croplands) to UK croplands. Similar road transport of mass occurred in reverse during grain transport from field to market during UK harvest 67 , indicating the appropriate capacity of rural transport networks to move basalt to the fields for ERW. Gross CDR calculations The gross CDR by ERW of crushed basalt applied to soils was calculated as the sum of two pathways: (1) the transfer of weathered base cations (Ca 2+ , Mg 2+ , Na + and K + ) from soil drainage waters to surface waters that are charge balanced by the formation of HCO 3 \u2212 ions and transported to the ocean (equation (6) ), and (2) the formation of pedogenic carbonates (equation (7) ). Pathway 1 for calcium ions: $${{{\\mathrm{CaSiO}}}}_{{{\\mathrm{3}}}}{{{\\mathrm{ + 2CO}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{ + 3H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}} \\to {{{\\mathrm{Ca}}}}^{{{{\\mathrm{2 + }}}}}{{{\\mathrm{ + 2HCO}}}}_{3}^ - {{{\\mathrm{ + H}}}}_{{{\\mathrm{4}}}}{{{\\mathrm{SiO}}}}_{{{\\mathrm{4}}}}$$ (6) Pathway 2 for calcium carbonate formation: $${{{\\mathrm{Ca}}}}^{{{{\\mathrm{2 + }}}}}{{{{\\mathrm{ + 2HCO}}}}_{{{{\\mathrm{3}}}}}^{-}} \\to {{{\\mathrm{CaCO}}}}_{{{\\mathrm{3}}}}{{{\\mathrm{ + CO}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{ + H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}$$ (7) CDR, via pathway 1, potentially sequesters two moles of CO 2 from the atmosphere per mole of divalent cation. However, ocean carbonate chemistry reduces the efficiency of CO 2 removal to an extent, depending on ocean temperature, salinity and the dissolved CO 2 concentration of the surface ocean. We used annual ERW alkalinity flux time series (2020\u20132070) calculated with our 1D ERW model for S1 to S3 as inputs to GENIE (version 2.7.7) 68 , 69 . GENIE is an intermediate-complexity Earth system model with ocean biogeochemistry that allows computation of oceanic CDR via pathway 1. We used the same methodology as described previously 12 to simulate atmospheric CDR via the release of enhanced weathering alkalinity products into the ocean. The uncertainty for each scenario was determined by ensemble GENIE simulations with 84 different parameter sets that varied 28 parameters, each calibrated to simulate a reasonable pre-industrial and historical transient climate and carbon cycle 68 , 69 , 70 . CDR via pathway 2 occurred if dissolved inorganic carbon derived from atmospheric CO 2 precipitates as pedogenic carbonate, and sequestered 1 mol CO 2 per mole of Ca 2+ . Costs and carbon emissions of logistical operations Mining A breakdown of mining costs (in \u00a3 t \u22121 ) of rock for the year 2010 and a representative granite mine with a daily 1,500 output and annual 375,000 output were obtained from a comprehensive analysis of UK aggregate mining 71 . Capital expenditure costs amounted to \u00a324,395,636 over a 50-yr life cycle (\u00a31.30 t \u22121 rock) whereas operating expenses (OPEX) amounted to \u00a31,150,072 yr \u22121 (\u00a33.07 t \u22121 rock) for a total \u00a34.37 t \u22121 rock for the year 2010. To obtain cost projections over 2020\u20132070, the contributions of wages, diesel fuel and electricity consumption in OPEX (35.9%, 2.5% and 20.0% respectively) were normalized and projected for 2020\u20132070 using E3ME outputs of median wage, diesel prices and industrial electricity tariffs, respectively ( Supplementary Information ). Capital expenditure costs and the remaining OPEX (plant, buildings, equipment, tyres; in \u00a3 t \u22121 rock) remained constant over the period. Emissions of CO 2 e t \u22121 rock extracted using diesel fuel and explosives were set at 4.29 kgCO 2 e t \u22121 rock (ref. 71 ). Emissions of CO 2 e per unit of electricity consumed were obtained by combining electricity requirements per tonne of rock (1.48 kWh t \u22121 rock) and projected life cycle emissions (in kgCO 2 e kWh \u22121 ) from 2020 to 2070. Grinding Grinding breakdown costs were obtained from ref. 18 ). Capital expenditure costs were set at \u00a31.59 t \u22121 rock, while OPEX for plant, buildings and equipment were set at \u00a30.97 t \u22121 rock. Diesel fuel and personnel costs (\u00a30.08 t \u22121 rock and \u00a30.85 t \u22121 rock for 2010) were projected to 2020\u20132070 using the methodology described above. We expressed electricity consumption per tonne of rock milled as a function of particle size, defined as p80 6 . To obtain electricity costs, we multiplied electricity consumption (in kWh t \u22121 rock milled) by E3ME projections of the unit cost of electricity (\u00a3 kWh \u22121 ) and grinding emissions by multiplying electricity consumption by E3ME projections of electricity-production life cycle emissions (gCO 2 e kWh \u22121 ). Spreading Spreading costs were set at \u00a38.3 t \u22121 rock for the year 2020 by averaging costs in the United Kingdom and United States 6 . Spreading costs were assigned equally to equipment, fuel\/electricity and wages, with E3ME data used to provide cost projections to 2070 for the last two. A sigmoid function showing the transition to electric cars was obtained from E3ME, to which a 10 yr lag was added to signify delayed uptake by heavy agriculture vehicles ( Supplementary Information ). Spreading emissions were set at 0.003 kgCO 2 t \u22121 rock (ref. 18 ). Our cost assessments assume that ERW practices are undertaken on farms as part of business-as-usual land-management practices. The pricing of external contracting of land management for rock dust application to soils is uncertain but could increase CDR prices per tCO 2 on the order of 10\u201315%. Fertilizers Projections of P fertilizer prices (2020\u20132070) for a global medium-resource scenario were obtained from ref. 72 , showing an increase in global prices due to the depletion of phosphate reserves 72 , 73 , 74 . Even though K resources are also depleting, we kept K prices constant as alternative technologies and the opening of new mines in the Global South might alleviate the problem 75 . UK fertilizer prices for the year 2020 were used 31 as a baseline for our projections. Fertilizer savings were obtained as the product of release (in kg) of P and K by their unit price (\u00a3 kg \u22121 ) over the time period 2020\u20132070. Life-cycle CO 2 emissions for P and K fertilizers were calculated as average values for different time horizons from the methodologies included in the Ecoinvent database 76 ( Supplementary Information ). Global markets for these products were selected for this analysis to include all that those fertilizers coming to the United Kingdom from any region of the world. Energy requirements Electricity supply characteristics for the United Kingdom were obtained from E3ME simulations (see the Transportation section). The annual electricity supply increases from 320 GWh yr \u22121 in 2020 to 637 GWh yr \u22121 in 2070, with life-cycle emissions dropping from 177.4 gCO 2 kWh \u22121 to \u221264.5 gCO 2 kWh \u22121 . The electricity mix profile shows an initial transition to onshore wind energy, followed by a marked uptake of solar and various CCS technologies. The annual costs of ERW (in \u00a3 tCO 2 \u22121 ) CDR was obtained from equation (8) by summing the logistical costs for all locations (loc) (in \u00a3) that rock was applied according to each scenario for the particular year ( y ) and dividing by their total net CDR (in tCO 2 ) (equation (8) ). Mining (Min) and spreading (Spread) costs are functions of the year, as the application rate was the same for all locations. Grinding (Grind) costs are a function of the year and p80. Transport (Transp) costs are function of the year and location, and consider the distance from the rock source. P and K release is a function of the year, p80 and location, as both particle size and location (climate) affect weathering rates and, subsequently, elemental release. All process costs are functions of the year due to time-varying wage, fuel, electricity and fertilizer costs. $$\\begin{array}{ll}{\\mathrm{Costs}}\\left( {y,{\\mathrm{p}}80} \\right) \\\\ = \\mathop {\\sum }\\limits_{\\mathrm{Locations}} \\frac{{\\mathrm{Min}\\left( y \\right) + {\\mathrm{Grind}}\\left( {y,{\\mathrm{p}}80} \\right) + {\\mathrm{Transp}}\\left( {y,{\\mathrm{loc}}} \\right) + {\\mathrm{Spread}}\\left( y \\right) - {\\mathrm{P}}\\left( {y,{\\mathrm{p}}80,{\\mathrm{loc}}} \\right) - {\\mathrm{K}}(y,{\\mathrm{p}}80,{\\mathrm{loc}})}}{{{\\mathrm{CO}}_2{\\mathrm{Gross}}\\;{\\mathrm{Seq}}\\left( {y,{\\mathrm{p}}80,{\\mathrm{loc}}} \\right) - {\\mathrm{CO}}_2{\\mathrm{Secondary}}\\;{\\mathrm{Emissions}}(y,{\\mathrm{p}}80,{\\mathrm{loc}})}}\\end{array}$$ (8) Secondary emissions (in tCO 2 ) for each location were obtained by summing the emissions of each process (tCO 2 t \u22121 rock) in that year and multiplying by rock application (Rock) (in t rock) (equation (9) ) $$\\begin{array}{ll}{\\mathrm{CO}}_2{\\mathrm{Secondary}}\\;{\\mathrm{Emissions}}\\left( {y,{\\mathrm{p}}80,{\\mathrm{loc}}} \\right)\\\\ = \\left[ {{\\mathrm{Min}}\\left( y \\right) + {\\mathrm{Grind}}\\left( {y,{\\mathrm{p}}80} \\right) + {\\mathrm{Transp}}\\left( {y,{\\mathrm{loc}}} \\right)}\\right. \\\\ \\left.{ + {\\mathrm{Spread}}\\left( y \\right) - {\\mathrm{P}}\\left( {y,{\\mathrm{p}}80,{\\mathrm{loc}}} \\right) - {\\mathrm{K}}\\left( {y,{\\mathrm{p}}80,{\\mathrm{loc}}} \\right)} \\right]\\\\ \\times {\\mathrm{Rock}}(y,{\\mathrm{loc}})\\end{array}$$ (9) An initial run determined the order of the grid cells on the basis of their weathering potential. Rock was then applied, prioritizing grid cells with the highest potential, while the addition of rock in new areas each year was constrained by the annual rock availability of each scenario. Transportation Detailed transport analyses (based on UK road and rail networks) were undertaken to calculate distance costs and CO 2 emissions for the distribution of rock dust from source areas to croplands. We used the GLiM database for the UK distribution of basalt deposits 77 and the 2019 land cover map ( Supplementary Information ) to calculate transportation distances, cost (\u00a3 t \u22121 rock dust km \u22121 ) and emissions (tCO 2 km \u2212 1 ) from potential local rock sources to cropland areas, together with UK road and rail transport networks 78 . Spatial analysis was undertaken with least-cost path algorithms from the ArcGIS software 79 . Wages and electricity\/fuel prices and CO 2 emission factors were derived from E3ME\u2019s 1.5 \u00b0C energy scenario 2 . We started using typical fuel\/electricity consumption values for both freight road (2.82 km l \u22121 and 3.07 kWh km \u22121 ) 71 and rail (98 km l \u22121 ) 76 to estimate the projected transport efficiency expressed in cost\/emissions of a tonne of rock dust per kilometre (t km \u22121 ) 80 , 81 , 82 . Transport cost distribution per tonne-kilometre was derived using generic road and rail cost models that include wages, fuel, maintenance and depreciation 83 , 84 . The UK rail freight diesel-to-electricity decarbonization transition is already underway 85 , 86 , and we used the continued projection for this transport mode. For road freight, the transport technology transition from the E3ME for electric vehicles was adopted, modified under the assumption that diesel ban policies and the availability of electric heavy goods vehicles for basalt transportation take place after 2030 87 . Energy and economic forecasts UK energy\u2013economic modelling (2020\u20132070) 88 , 89 , 90 was based on an updated version of the scenario described in ref. 17 that includes carbon pricing and has responses for the power sector (output and efficiency) consistent with government policy 91 ( Supplementary Information ). Total renewable energy sources over time were similar but with solar instead of 40 GW of offshore wind power. The simulations considered the phase-out of conventional vehicles by 2030, in line with government policy, and a consistent shift in aviation and freight towards biofuels, and electrified rail, as well as increased efficiency in buildings and the use of heat pumps. These simulations provide outputs for the United Kingdom for 2020 to 2070 of CO 2 emissions per unit energy, total energy mix and output, labour costs, electricity costs, fuel costs, and road and rail transport costs, which were inputs for calculating the costs of ERW CDR and secondary emissions during the grinding of rocks (Extended Data Fig. 2 ). Data availability Soil pH data were obtained from  and  . The high-resolution monthly fields of soil temperature and precipitation data were obtained from  . Additional environmental and climate drivers were acquired through simulations of CLM5 available at  . The UK crop cover map was obtained from  , annual time series of crop yields from  and UK fertilizer usage data from  . UK national border data were obtained from  . The GLiM v1.0 dataset used to identify rock sources is available at  . Datasets with 5 min resolution on global crop production and yield area to identify cropland are available at  . Datasets on road and rail vector data used for transport network analysis are available at  . Datasets on LCA impact factors used for K and P fertilizers are available within Ecoinvent 3.6 at  . Source data are provided with this paper. Code availability The weathering model was developed in MATLAB v.R2019a, and data processing was conducted in both MATLAB v.R2019a and Python v.3.7. MATLAB and Python codes developed for this study belong to the Leverhulme Centre for Climate Change Mitigation. These codes, and the modified codes in CLM5 developed in this study, are available from the corresponding author upon reasonable request. ","News_Body":"Adding rock dust to UK agricultural soils could absorb up to 45% of the atmospheric carbon dioxide needed to reach net zero, according to a major new study led by scientists at the University of Sheffield. The study, led by the Leverhulme Centre for Climate Change Mitigation at the University, provides the first detailed analysis of the potential and costs of greenhouse gas removal by enhanced weathering in the UK over the next 50 years. The authors show this technique could make a major overlooked contribution to the UK's requirement for greenhouse gas removal in the coming decades with a removal potential of 6\u201330 million tons of carbon dioxide annually by 2050. This represents up to 45% of the atmospheric carbon removal required nationally to meet net-zero greenhouse gas emissions alongside emissions reductions. Deployment could be straightforward because the approach uses existing infrastructure and has costs of carbon removal lower than other Carbon Dioxide Removal (CDR) strategies, such as direct air capture with carbon capture storage, and bioenergy crops with carbon capture and storage. A clear advantage of this approach to CDR is the potential to deliver major wins for agriculture in terms of lowering emissions of nitrous oxide, reversing soil acidification that limits yields and reducing demands for imported fertilizers. The advantages of reducing reliance on imported food and fertilizers have been highlighted by the war in Ukraine that has caused the price of food and fertilizers to spike worldwide as exports of both are interrupted. The authors of the study highlight that societal acceptance is required from national politics through to local community and farm scales. While mining operations for producing the basalt rock dust will generate additional employment and could contribute to the UK government's leveling up agenda; however this will need to be done in ways which are both fair and respectful of local community concerns. This new study provides much needed detail of what enhanced rock weathering as a carbon dioxide removal strategy could deliver for the UK's net-zero commitment by 2050. The Committee on Climate Change, which provides independent advice to the government on climate change and carbon budgets, overlooked enhanced weathering in their recent net-zero report because it required further research. The new study now indicates enhanced weathering is comparable to other options on the table and has considerable co-benefits to UK food production and soil health. Professor David Beerling, Director of the Leverhulme Centre for Climate Change Mitigation at the University of Sheffield and senior author of the study, says that their \"analysis highlights the potential of UK agriculture to deliver substantial carbon drawdown by transitioning to managing arable farms with rock dust, with added benefits for soil health and food security.\" Dr. Euripides Kantzas of the Leverhulme Centre for Climate Change Mitigation at the University of Sheffield and lead author, says that \"by quantifying the carbon removal potential and co-benefits of amending crops with crushed rock in the UK, we provide a blueprint for deploying enhanced rock weathering on a national level, adding to the toolbox of solutions for carbon-neutral economies.\" Professor Nick Pidgeon, a partner in the study and Director of the Understanding Risk Group at Cardiff University, says that \"meeting our net zero targets will need widespread changes to the way UK agriculture and land is managed. For this transformation to succeed we will need to fully engage rural communities and farmers in this important journey.\" The research was published in Nature Geoscience. ","News_Title":"Managing UK agriculture with rock dust could absorb up to 45% the atmospheric carbon dioxide needed for net-zero","Topic":"Earth"}
{"Paper_Body":"Abstract Reef ecosystems are highly diverse habitats that harbor many ecologically and economically significant species. Yet, globally they are under threat from multiple stressors including overexploitation of predatory fishes and habitat degradation. While these two human-driven activities often occur concomitantly, they are typically studied independently. Using a factorial design, we examined effects of predator presence, habitat complexity, and their interaction on patch reef fish communities in a nearshore ecosystem on Great Abaco Island, The Bahamas. We manipulated the presence of Nassau groupers ( Epinephelus striatus ), a reef predator that is critically endangered largely due to overharvest, and varied patch reef structure (cinder blocks with and without PVC) to reflect high or low complexity-four treatments in total. To assess changes in fish community composition we measured fish abundances, species richness, and evenness. We found that predators present and high reef complexity had an additive, positive effect on total fish abundance: fish abundance increased by ~ 250% and 300%, compared to predators absent and low complexity reef treatments, respectively. Species richness increased with reef complexity. Variation in community structure was explained by the interaction between factors, largely driven by juvenile Tomtate grunt ( Haemulon aurolineatum ) abundances. Specifically, Tomtate grunt abundance was significantly higher on high complexity reefs with predators present, but on low complexity reefs predators present had no effect on Tomtate grunt abundance. Our data suggest that both fisheries management of large-bodied piscivores and reef habitat restoration are critical to the management and conservation of reef ecosystem functions and services. Access provided by Universit\u00e4t des es,  -und  Working on a manuscript? Avoid the common mistakes Introduction Coral dominated reefs are highly productive ecosystems that can harbor large and diverse fish communities, but are threatened worldwide by myriad stressors (Hughes and Connell 1999 ; Hoegh-Guldberg et al. 2007 ). Two primary stressors are overfishing at higher trophic levels and habitat degradation (Dulvy et al. 2004 ; Lee 2006 ). These stressors can fundamentally change processes that act from both the top-down (e.g., trophic cascades) (Baum and Worm 2009 ; Allgeier et al. 2016 ; Valdivia et al. 2017 ) and bottom-up (e.g., nutrient cycling regimes and provision of refugia) of interaction networks (Beukers and Jones 1998 ; Lee 2006 ; Smith et al. 2006 ; Graham and Nash 2013 ). Overexploitation of predators and the loss of habitat complexity are typically concomitant. Yet because these factors are most often studied independently, our understanding of how their simultaneous effects combine or interact to alter reef communities remains limited (but see Wilson et al. 2008 ). Predators and habitat structural complexity play a central role in determining post-settlement coral reef fish communities (Hixon and Carr 1997 ; Steele 1999 ). Direct effects of predators can alter fish communities including species richness (Freestone et al. 2011 ) and overall community composition (Almany 2003 ). Predators also have indirect effects on their surrounding communities, however the outcome is often context-dependent for both predator and prey identity making the effect difficult to predict or generalize (Almany 2004a ; Stallings 2008 ; Chamberlain et al. 2014 ). Additionally, predator impacts on reef communities can be mediated by the structural complexity of reefs (Wilson et al. 2008 ). Specifically, habitat complexity can decrease predators\u2019 direct effects by reducing prey encounter rates (Swisher et al. 1998 ; Almany 2004b ; Warfe and Barmuta 2004 ). In contrast, structural complexity can increase predators\u2019 indirect effects due to predators and their prey residing in close proximity to one another (Grabowski et al. 2008 ). To date, most studies examining predator effects in marine systems have tended to use simplified interaction webs within mesocosms, focused on small-bodied predator species, or a combination of both (Steele 1999 ; Johnson 2006 ; Grabowski et al. 2008 ). However, fishing typically targets large-bodied predators (e.g., sharks, tunas, and groupers) that are more difficult to study. Further complicating this scenario is the challenge of understanding how losses of predators may interact with concomitant changes in habitat complexity that is a result of major shifts in foundation species (e.g., coral to sponge, or macroalgae) that can render reefs flat, reducing available refugia for fauna. This potential interaction has important implications for management and conservation efforts seeking to mitigate human impacts on coral reefs. Here we ask: how do predators present and reef complexity affect reef fish communities in terms of total abundance, species richness, evenness, and overall community composition? We conducted an in situ experiment designed to examine effects of large predator presence, habitat complexity, and their interaction on artificial patch reef communities in a nearshore ecosystem on Great Abaco Island, The Bahamas. Nearshore, patch reef habitats are ideal for this experiment because they are isolated, complex, vertical habitats surrounded by hard or soft low-relief substrate and, being located in nearshore habitats, they are strongly influenced by local stressors (Stallings 2009 ). Artificial patch reefs are particularly ideal because they are relatively easy to manipulate and subsequently study whole community effects (Carr and Hixon 1997 ). In a 2 \u00d7 2 factorial design, we manipulated the presence of a locally abundant reef predator, Nassau grouper ( Epinephelus striatus ), on patch reefs of high and low complexity. We used Nassau grouper as our predator species because they are an important fishery species that has experienced drastic population declines throughout the Caribbean from overexploitation (Dahlgren et al. 2016 ; Sherman et al. 2018 ). We hypothesized that reefs with Nassau groupers present would have more fishes compared to those without Nassau groupers due to strong indirect predator effects on smaller-bodied, mesopredator species. Additionally Nassau groupers present would alter fish community composition, i.e., relative abundance of the constituent species, due to a combination of their direct and indirect predator effects. For habitat complexity, we hypothesized that complex reefs would have more fishes as well as more species present than non-complex reefs to do an increase in refugia hole availability and diversity in refugia morphology (shape). Methods Our study occurred in a back-reef system in the Sea of Abaco along the eastern shoreline of Marsh Harbour on Great Abaco Island, The Bahamas, from May to August 2014 (Fig. 1 ). Water visibility is typically \u2265 5 m and low tide depth ranges from 2 to 5 m. The nearshore seascape is a mosaic of hard-bottom, sand, algal beds, seagrass meadows dominated by turtle grass ( Thalassia testudinum), and scattered coral and artificial patch reefs. Coral patch reefs are characterized by low relief (\u2264 2 m height) and typically covered with some variation of encrusting or soft coral (e.g. Orbicella spp. or Gorgonian spp.), exposed limestone, sponges, and macroalgae (e.g. Halimeda spp. and Sargassum spp.). Artificial patch reefs are broadly defined as any human-introduced structure that is submerged on the benthos, usually introduced to mimic the function of patch reefs by providing structural complexity for biota to use as refugia and foraging grounds (Seaman 2000 ). Our experimental units were cinder block artificial reefs, which have been used widely over the past four decades to study reef fish assemblages (e.g. Hixon and Beets 1989 , Carr and Hixon 1997 ). In April 2014, we constructed 16 artificial reefs (~ 1.4 m 3 ), each using 35 cinder blocks (15 \u00d7 20 \u00d7 40 cm) on mixed sand\/seagrass substrate. For all 16 artificial reefs (hereafter reefs), low tide depth was ~ 3.0 m, distance to shoreline ranged between 250 and 500 m, and ambient water temperatures and currents were similar amongst all reefs. Each reef was located > 200 m from any other artificial or natural patch reef; location of experimental reefs followed Yeager et al. ( 2014 ) where reef isolation was set at > 200 m to minimize among-reef movements of more transient fish species (Carr and Hixon 1997 ; Allgeier et al. 2018 ). We measured seagrass density (within a 2 m radius of the reef) at the beginning of the experiment as a potential covariate because seagrass density adjacent to reefs has been shown to alter certain grunt species\u2019 reef densities (Haemulids; Yeager et al. 2011 ). Fig. 1 Sixteen artificial reefs constructed in the Sea of Abaco, The Bahamas, in April 2014. They were constructed on mixed sand and seagrass benthic habitat > 80 m away from hard bottom substrate and > 200 m from natural or artificial patch reefs. At the bottom are representative images of the four treatments from left to right: predator presence \u00d7 high reef complexity (PH), predator absent \u00d7 high reef complexity (AH), predator presence \u00d7 low reef complexity (PL), and low predator absent \u00d7 low reef complexity (AL) Full size image To test how predator presence and structural complexity affect reef fish communities, we randomly assigned each of the 16 reefs to one of four treatments: predator present \u00d7 high complexity (PH), predator absent \u00d7 high complexity (AH), predator present \u00d7 low complexity (PL), or predator absent \u00d7 low complexity (AL; Fig. 1 ). For predator treatments we used Nassau groupers which have a complex life cycle undergoing a series of ontogenetic shifts in both habitat and diet (Eggleston et al. 1998 ; Dahlgren et al. 2006 ). When they are ~ 3 months old, individuals begin to migrate from nearshore macroalgae beds to hard bottom or patch reef habitat. At this stage, individuals show strong site fidelity to their home activity area, i.e., often returning and reusing the same patch reef habitat as well as a limited home range of ~ 50 m from their home reef (Eggleston et al. 1998 and Dahlgren unpublished data). Due to their high site fidelity, grouper additions are difficult, and thus removals are the optimal method to manipulate their presence and absence (Stallings personal communication). Prior to the start of the experiment in May 2014, all 16 experimental reefs had at least two Nassau groupers present, ranging from 16 to 33 cm total length (TL). For predator absent treatments, we removed Nassau groupers with either trap or hand nets and relocated each individual to a reef habitat > 3 km from our study site to reduce the chance of their return (Stallings personal communication). We tagged each individual before release, and we did not observe the return of any individuals during the duration of our experiment. We also removed any non-native, invasive lionfish ( Pterois volitans ) throughout the study period. To establish high complexity reef treatments, we installed a PVC tree structure mimicking historically important reef-building corals within our study system, i.e., Acropora cervicornis , A. palmata , and A. prolifera . Low complexity reefs had no PVC structure, and the cinder block holes were filled with cement, leaving only three large gaps for potential refugia (Fig. 1 ). This low complexity reef architecture mimics a shift from branching, reef building corals to boulder-like reef heads dominated by encrusting corals and macroalgae. To determine if fish community structure differed among treatments, we monitored all reefs weekly with Underwater Visual Census (UVC) surveys. We also deployed GoPro \u00ae cameras three times at each reef during the study to monitor and verify predator treatments. Video footage was not used to record fish species present or total fish abundance due to several logistical issues such as limited visibility for cryptic species that reside within the reefs and the number of cameras to record all 16 reefs within 24\u201348 h. The UVC surveys entailed monitoring each reef using a mask and snorkel for 10 min and recording all fishes within 1 m of the reef at the species-level. Once all the active swimming fish were recorded, a flashlight was used to search every reef hole twice for less active or cryptic species. All 16 reefs were surveyed within each survey date. During the last 2 weeks of the experiment, visual estimates for the TL of each fish were recorded to the nearest centimeter. We used UVC surveys to quantify species richness and evenness, as measured by the reciprocal Simpson\u2019s \\(D = 1 - \\sum \\left( {n - N} \\right)^{2}\\) , where n is the abundance of that species per survey and N is the number of total species per survey (Simpson 1949 ). For all analyses, we used UVC surveys from 60 days after reef construction in order to capture reef fish assemblage at the reefs\u2019 oldest age possible; we were not able to monitor predator presence after 60 days to validate predator treatment. We did not include Nassau groupers in fish community response variables. We used two-way analysis of covariance (ANCOVA) to test for independent and interactive treatment effects on total fish abundance, species richness, and the reciprocal Simpson\u2019s Index of Diversity (Simpson 1949 ) at the species-level. We log-transformed fish abundance prior to analysis to meet homoscedasticity assumptions, and included seagrass density in each model as a potential covariate (but removed this variable when not significant for reasons of parsimony). To analyze fish community structure across treatments, i.e., relative abundance of the constituent species, we used non-metric multidimensional scaling (nMDS), and permutational multivariate analysis of variance (PERMANOVA) on square-root transformed fish abundances at the species-level. We then conducted similarity percentage analysis (SIMPER) computations using the Bray\u2013Curtis similarity coefficient, limited to the species contributing to the top 70% of dissimilarity between treatment\u2019s reef fish assemblage (Bray and Curtis 1957 ). Herein, we refer to the results of this nMDS analysis of reef fish assemblages as community structure. We conducted all analyses using program R (Team RC 2017 ). Results For UVC surveys at 60 days after reef construction, we documented a total of 2461 fish from 40 species and 19 families. Wrasses (Labridae), parrotfish (Scaridae), damselfish (Pomacentridae), and grunts (Haemulidae) were present on all 16 reefs (see supplemental material for complete species list). Community abundance and biodiversity Predator present and reef complexity had significant positive effects on total fish abundance, largely due to changes in Tomtate grunt ( Haemulon aurolineatum ) abundance (see supplemental material), but their interaction was not significant (Table 1 ). The highest fish abundance was found in predator present \u00d7 high complexity (PH) treatments, with an average of six times more fish than predator absent \u00d7 low complexity (AL) treatments (Figs. 2 , 3 ). We also tested for the same effects on total fish biomass\u2014trends followed those of total fish abundances (see supplemental material). Species richness was only significantly affected by reef complexity, with high complexity reefs averaging ~ 4 more fish species compared to low complexity reefs (Fig. 2 ; Table 1 ). Inverse Simpson\u2019s index of diversity at the species-level was not affected by either factor or their interaction [ANOVA global model (3, 12) = 1.27, P = 0.33; Table 1 ]. Table 1 Results of two-way ANOVA and PERMANOVA for reef fish communities based on fish species\u2019 abundances at 60 days after reef construction Full size table Fig. 2 Effects of Nassau grouper presence (present, P or absent, A) and reef structural complexity (high, H or low, L) for end-of-experiment (60 days) reef fish total abundance and species richness ( n = 16) Full size image Fig. 3 Photographs demonstrate an example of the difference in total fish abundance between the two extreme reef treatments at 60 days after reef construction: predators present \u00d7 high complexity (left) and predators absent \u00d7 low complexity (right) Full size image Community composition Fish community structure, i.e., relative abundance of the constituent species, of high and low complexity reefs differed (PERMANOVA R = 0.28, P < 0.01; Table 1 ; Fig. 4 ). For high complexity treatments, reefs with and without predators differed from each other (PERMANOVA permutations = 999, R = 0.13, P = 0.03; Table 1 ; Fig. 4 ). Based on Bray\u2013Curtis similarity indices, differences in fish communities among reef treatments were largely due to changes in Tomtate grunt abundances. Tomtate abundances differed by 26.9\u201372.9% between treatments (SIMPER). PH reefs versus AL reefs had the largest disparity in Tomtate grunt abundances, with predator present \u00d7 high complexity reefs having an average of 15 \u00d7 more Tomtate grunts than predator absent \u00d7 low complexity reefs (Fig. 5 ). To help explain this pattern we also conducted a one-way ANOVA to test the effect of predators on the second most abundant species, White grunts ( Haemulon plumieri ), a potential competitor with Tomtate grunts (one-way ANOVA P = 0.18; Fig. 6 ). Fig. 4 Non-metric multidimensional scaling (nMDS) plot for fish community structure for four reef treatments ( n = 16). Reef fish communities were analyzed using reef fish abundances at day 60 of experimental treatments. \u201cP\u201d and \u201cA\u201d represent Nassau groupers present or absent, and \u201cH\u201d and \u201cL\u201d represent high and low reef complexity treatments. Each colored point represents one reef Full size image Fig. 5 Reef treatment effects on end-of-experiment (60 days) Tomtate grunt abundance ( n = 16, two-way ANOVA P = 0.03). Letters above each point indicate statistical difference between treatments; treatments that do not share the same letter are significantly different from one another. Reef treatments included predator present \u00d7 high reef complexity (PH), predator absent \u00d7 high reef complexity (AH), predator present \u00d7 low reef complexity (PL), and low predator absent \u00d7 low reef complexity (AL) Full size image Fig. 6 Average White grunt abundances for reef treatments at end-of-experiment (60 days). White grunt abundance was comprised from White grunt individuals that were estimated \u2264 5 cm in total length during UVC surveys to compare with similarly-sized Tomtate grunts. One-way ANOVA was conducted to compare white grunt abundances on predator present \u00d7 high complexity reefs (PH) versus predator absent \u00d7 high complexity reefs (AH; n = 8, P = 0.18) Full size image Discussion Overharvest of fishes and habitat degradation are two main anthropogenic threats to coastal and nearshore ecosystems (Lotze et al. 2006 ). We manipulated Nassau grouper presence and artificial patch reef complexity to simulate how simultaneous overexploitation of large-bodied predator species and reef degradation alter coral reef fish communities. We found predator present and high habitat complexity to have a positive, additive effect on total reef fish abundance (Figs. 2 , 3 ). Comparing our two most extreme treatments that simulated healthy versus degraded reefs, total reef fish abundance differed by 300% (Figs. 2 , 3 ). High reef complexity was positively associated with species richness as well as fish abundance for almost all 40 fish species recorded, except three species in which complexity had no effect (Fig. 2 ). The effect of predator present on fish abundance was dependent on fish species identity and reef type (Fig. 5 ). Overall, our study suggests that both top-down and bottom-up up changes (i.e., predators and habitat architecture) to interaction networks can have far-reaching effects on entire fish communities and, by extension, coral ecosystems. Further, rather than isolating top-down or bottom-up impacts, it is valuable to consider these effects in tandem. Reefs on which predators, in this case Nassau groupers, were present tended to have a higher total abundance of fish (Fig. 2 ). This result may be counterintuitive because Nassau groupers at this life stage are known to consume fishes (Eggleston et al. 1998 ). Our result is also in conflict with studies focused on groupers in fringing or barrier reef habitat that have typically found groupers to decrease prey abundance (Hixon and Carr 1997 ; Almany 2004a ). There are a few possible explanations for this observation. First, Nassau groupers can initiate trophic cascades in which their presence has an indirect, positive effect on smaller organisms. For example, Stallings ( 2008 ) showed that Nassau groupers present on reefs reduced the movement of two smaller-bodied grouper species, which indirectly increased reef fish recruitment. Such a behaviorally-mediated trophic cascade could be occurring in our study system because, even though we did not observe any small-bodied grouper species, we did observe other residential piscivores such as moray eels (Moridae) and transient predators such as jacks (Carangidae), albeit at low abundances. Another explanation might be due to unique life history characteristics of Nassau groupers. For instance, Nassau groupers are known to make nocturnal hunting migrations to other habitats directly adjacent to their home reefs. Therefore, the diurnal fish assemblages found on patch reefs may not be especially susceptible to predation from Nassau groupers residing on the same reef (Eggleston 1996 ; Sadovy and Eklund 1999 ). A third possibility is that Nassau groupers may select particular prey species and, in doing so, release those species\u2019 competitors; we expand on this idea below. Our findings with respect to habitat complexity reinforce previous studies that reef structural complexity affects fish communities (Table 1 and Fig. 2 ; Hixon and Beets 1993 ; Almany 2004a ; Graham and Nash 2013 ). Reef complexity and heterogeneity in the morphology of a reef\u2019s structure have been shown to be positively associated with fish abundance and species diversity (Hackradt et al. 2011 ; Kerry and Bellwood 2011 ), likely due to a combination of reducing competition for refugia and providing varied shapes and sizes of refugia to match fishes\u2019 morphologies. Throughout the study, we observed different fishes consistently using different parts of the reefs, which may to some degree reflect habitat niche-partitioning. For example, grunt species were frequently observed aggregating within the high complex reefs\u2019 PVC structures that mimicked an architecture of Acropora spp. (Lirman 1999 ), while squirrelfish and soldierfish (Holocentridae) were often found within cinder block holes. Therefore, high complexity reef treatments likely reduced competition for refugia by providing space for many individuals and species to cohabitate. Our results also show that even with predators present, high structural complexity seems to be an important factor mediating overall fish community composition (Fig. 4 ). A plausible explanation could be that our high complexity treatments decreased the consumption rate of Nassau groupers or other piscivores through the provision of smaller-sized refugia where individual piscivores could not enter (Hixon and Menge 1991 ; Almany 2004b ). We did not observe an interaction between predator presence and reef complexity for total fish abundance, but did observe a clear interaction when comparing fish community structure, i.e., relative abundance of the constituent species (Table 1 ). Our nMDS plot shows a separation between fish communities on high complexity reefs with and without predators present (Fig. 4 ). Results from SIMPER analyses show that differences in Tomtate grunt abundances, the most abundant species on all experimental reefs, largely explain the treatment effect on reef fish community composition (Table 1 and supplemental material). For high complexity reefs, there were more Tomtate grunts when Nassau groupers were present, while on low complexity reefs, Nassau groupers presence had a no effect on Tomtate grunt abundances (Fig. 5 ). A plausible explanation for this trend is that on complex reefs Nassau groupers present may have altered the outcome of interspecific competition between Tomtate grunts and other fish species that utilize similar resources but that are inferior at finding refugia from groupers (Persson 1991 ). Examining high complexity reefs only, we compared average abundances of similarly-sized (\u2264 5 cm in TL) Tomtate and White grunts ( Haemulon plumieri ), the latter being the second most abundant species on all experimental reefs. Although statistically insignificant, when Nassau groupers were present on reefs, there were less White grunts than when groupers were absent (one-way ANOVA P = 0.18; Fig. 6 ). Thus, Tomtate grunts may have been superior than White grunts accessing refugia when Nassau groupers were present. Lastly, we did not study the direct mechanism for predator effects on Tomtate grunt abundances and another potential explanation could be density-independent factors. For example, pelagic fish larvae settlement is known to be influenced by a reef\u2019s soundscape and both Nassau grouper and grunt species are well known to be vocal animals, and therefore, could have affected reef fish settlement patterns (Hazlett and Winn 1962 ; Freeman and Freeman 2016 ). The high abundance of Tomtate grunts on complex reefs with groupers present could have effects on other facilitative species interactions (Meyer and Schultz 1985 ). For example, Huntington et al. ( 2017 ) suggested that large fish aggregations on high complexity reefs provide sufficient consumer-mediated nutrients to facilitate coral growth and survivorship, which in turn can increase reef complexity over time and feedback to support larger fish aggregations. In general, grunt species form dense, diurnal aggregations on reefs, and make nocturnal migrations to nearby seagrass and mangrove habitats to hunt (Meyer et al. 1983 ). Because they often makeup a large percentage of the biomass found on reefs and make nightly migrations, they are thought to be important transporters of critical nutrients for primary producers including the symbiotic algae (zooxanthellae) that resides in reef-building corals (Allgeier et al. 2017 ). In light of our results that show grunt abundances are sensitive to predator presence, we argue that current impact assessments could be underestimating the ecological impact of intense fishing pressure on large-bodied reef predators like Nassau grouper. Our study is one of few to examine how fisheries-targeted predator removal, declining reef habitat complexity, and their interaction affect fish community assemblage using in situ manipulations. Currently, most coral reef ecosystems throughout the world are faced with multiple stressors and, consequently, it is important to understand not only how species interactions may change, but also how these changes may scale up to alter community composition and ecosystem function. In the context of fisheries management and coral reef restoration and conservation, we have shown how the removal of large-bodied piscivores and the decrease in reef complexity can alter reef fish densities and species richness. We suggest that in order to maintain biodiverse coral reef communities and preserve ecosystem processes, management should focus on both the conservation of large-bodied piscivores and the restoration of reef habitat complexity, through either reintroducing live coral or introducing artificial structure mimicking lost coral morphology. ","News_Body":"New research highlights two factors that play a critical role in supporting reef fish populations and\u2014ultimately\u2014creating conditions that are more favorable for the growth of both coral reefs and seagrass. \"Previous work has shown mixed results on whether the presence of large predator species benefits reef fish populations, but we found that the presence of Nassau grouper (Epinephelus striatus) had an overall positive effect on fish abundance,\" says Enie Hensel, a former Ph.D. student at North Carolina State University and lead author of a paper on the work. \"We also found that habitat complexity benefits both fish abundance and species richness, likely because it gives fish a larger variety of places to shelter.\" This is consistent with previous work. \"One of the surprises here was that the effect of predator presence on fish abundance was comparable to the effect of habitat complexity,\" Hensel says. To better understand the effect of these variables, researchers constructed 16 artificial \"patch\" reefs in shallow waters off the coast of Great Abaco Island in The Bahamas. Eight of the reefs consisted of cement-filled cinder blocks, mimicking degraded reefs with limited habitat complexity. The remaining eight reefs consisted of unfilled cinder blocks and branching pipe structures, mimicking the more complex physical environment of healthier reefs. Once in place, the researchers waited for groupers to move in and claim the new reef territory. The groupers were large juveniles, ranging in size from 16-33 centimeters. The researchers then removed the groupers from four of the degraded reef sites and from four of the complex reef sites. Groupers that were removed were relocated to distant reefs. Researchers monitored the sites for 60 days to ensure that the grouper-free reefs remained free of groupers. At the end of the 60 days, the researchers assessed the total number of fish at each reef site, as well as the total number of fish species at each site. The differences were significant. Simple artificial reef structures, like that on the left, did little to support fish populations. Complex structures, like the one on the right, helped to support larger communities of fish. Credit: Enie Hensel Fish abundance, or the total number of fish, was highest at sites that had both a resident grouper and complex habitat. Abundance at these sites ranged from 275 fish to more than 500\u2014which is remarkable given that each reef was less than a meter long in any direction. By comparison, sites that had simple structures and no grouper had fewer than 50 fish on average. Simple structures with predators had around 75 fish, while complex sites without grouper had around 100. \"We think the presence of the grouper drives away other predators, which benefits overall fish abundance,\" Hensel says. \"And a complex habitat offers niches of various sizes and shapes, which can shelter more and different kinds of fish than a degraded, simple habitat.\" The presence of grouper had little or no effect on species richness, or the number of different species present at each site. However, habitat complexity made a significant difference. Complex sites had 11-13 species, while degraded sites had around seven. \"We found that the sites with complex habitats and the presence of predators had fish populations that were actually larger than what we see at surrounding, similar-sized natural reefs,\" Hensel says. \"That's because the natural reefs in the area are all degraded due to a variety of stressors. \"We also found that the presence of grouper on complex reefs led to a significant jump in the population of Tomtate grunts (Haemulon aurolineatum),\" Hensel says. \"That's good news, because Tomtates are a species that provides a lot of ecosystem services, which would be good for creating conditions that are more amenable to both coral reef growth and seagrass growth. \"Currently, my colleagues and I are building from these findings in two directions. We're measuring long-term community and ecosystem level responses to coral restoration or the reintroduction of structurally complex habitat; and we are also measuring long-term biological and physiological responses of fishes residing on restored reefs. For the latter, Haley Gambill, an undergraduate at NC State, is measuring changes in the age and growth of grunts. \"It's also worth noting that this is an area that was hit hard by Hurricane Dorian. Because we've done so much reef population work in that area, I'm hoping to return to do some work that can help us understand how extreme weather events can affect these ecosystems.\" ","News_Title":"Predators and hidey-holes are good for reef fish populations","Topic":"Biology"}
{"Paper_Body":"Abstract Observation of a quantum spin liquid (QSL) state is one of the most important goals in condensed-matter physics, as well as the development of new spintronic devices that support next-generation industries. The QSL in two dimensional quantum spin systems is expected to be due to geometrical magnetic frustration, and thus a kagome-based lattice is the most probable playground for QSL. Here, we report the first experimental results of the QSL state on a square-kagome quantum antiferromagnet, KCu 6 AlBiO 4 (SO 4 ) 5 Cl. Comprehensive experimental studies via magnetic susceptibility, magnetisation, heat capacity, muon spin relaxation ( \u03bc SR), and inelastic neutron scattering (INS) measurements reveal the formation of a gapless QSL at very low temperatures close to the ground state. The QSL behavior cannot be explained fully by a frustrated Heisenberg model with nearest-neighbor exchange interactions, providing a theoretical challenge to unveil the nature of the QSL state. Introduction Magnetic phases of low-dimensional magnets have been studied both theoretically and experimentally in the last half century. Intensive studies of one-dimensional (1D) spin systems have successfully captured the exotic quantum states, such as the Tomonaga\u2013Luttinger spin-liquid state 1 and the Haldane state 2 . Recent progress in synthesising ideal 1D magnets has evolved this research field 3 . On the other hand, in 2D spin systems, the spin-1\/2 kagome antiferromagnet is an excellent choice for searching for the QSL state induced by geometrical frustration 4 . A possible compound for QSL in the kagome antiferromagnets was herbertsmithite, which has the Cu 2+ layers with ideal kagome geometry sandwiched by nonmagnetic Zn 2+ layers 5 . To date, no long-range order has been found at any temperature, and a continuum of spin excitations was observed by INS experiments. However, the low-energy magnetic excitation is still unclear as seen in a controversy on gapless 6 or gapped 7 excitation. This is related to the fact that herbertsmithite is obtained by selectively replacing magnetic Cu 2+ ions with nonmagnetic Zn 2+ ions on the triangular-lattice planes of its parent compound clinoatacamite 8 , Cu 2 (OH) 3 Cl. This replacement inevitably causes site mixing 9 . Other materials with the kagome lattice exhibit long-range magnetic or valence-bond crystal (VBC) orders caused by lattice distortions, the DM interaction and further neighbour interactions 10 , 11 , 12 , 13 , 14 . The lack of a suitable model material exhibiting the QSL hinders observations of the QSL state in the 2D spin-1\/2 systems. Another highly frustrated 2D quantum spin system expected to be a QSL state is a compound with the square-kagome lattice (SKL). The SKL was introduced by Siddharthan et al. 15 . It has the same coordination number as the kagome lattice ( z = 4), but with a composition of two inequivalent sublattices in contrast to the kagome lattice. Richter et al. reported that the ground state of the spin-1\/2 SKL with three equivalent exchange interactions (the case of J 1 = J 2 = J 3 and J X = 0 in Fig. 1 c) is similar to that of the kagome lattice 16 . The ground state of the spin-1\/2 J 1 \u2013 J 2 SKL antiferromagnet (the case of J 2 = J 3 and J X = 0 in Fig. 1 c) was calculated by Rousochatzakis et al. 17 . It has been predicted to be a crossed-dimer VBC state and a square pinwheels VBC state, depending on J 2 \/ J 1 . Moreover, there is a possibility that the QSL ground states are realised in the SKL with three nonequivalent exchange interactions (the case of J X = 0 in Fig. 1 c), which lead to the melting of these VBC states 18 . Very recently, it has also predicted to be a topological nematic spin-liquid state 19 . In the magnetic field, the existence of the magnetisation plateaus of M \/ M sat = 1\/3 and 2\/3 has theoretically clarified 16 , 17 , 18 , 20 , where M sat is the saturation magnetisation. These plateau phases exhibit VBC, up\u2013up\u2013down structure, and alternate trimerized states. In the high magnetic field and low-temperature regime, a magnetic-field-driven Berezinskii\u2013Kosterlitz\u2013Thouless phase transition exists 21 . However, the lack of a model compound for the SKL system has obstructed a deeper understanding of its spin state. Motivated by the present status on the study of the SKL system, we searched for compounds with the SKL containing Cu 2+ spins, and synthesised the first compound of a SKL antiferromagnet, KCu 6 AlBiO 4 (SO 4 ) 5 Cl, successfully. Here, we use thermodynamic, muon spin relaxation and neutron-scattering experiments on powder samples of KCu 6 AlBiO 4 (SO 4 ) 5 Cl, to demonstrate the absence of magnetic ordering and the presence of gapless continuum of spin excitations. Fig. 1: Spin-1\/2 J 1 \u2013 J 2 \u2013 J 3 square-kagome lattice in KCu 6 AlBiO 4 (SO 4 ) 5 Cl. a Crystal structure of KCu 6 AlBiO 4 (SO 4 ) 5 Cl featuring a large interlayer spacing. b Arrangement of the Cu 2+ orbitals in SKL. The \\({d}_{{x}^{2}-{y}^{2}}\\) orbitals carrying spin-1\/2 are depicted on the Cu sites. c Square-kagome lattice of KCu 6 AlBiO 4 (SO 4 ) 5 Cl consisting of Cu 2+ ions with nearest-neighbour exchange couplings J 1 , J 2 , J 3 and next-nearest-neighbour exchange coupling J X . Full size image Results Crystal structure The synthesis of KCu 6 AlBiO 4 (SO 4 ) 5 Cl was conceived following the identification of the naturally occurring mineral atlasovite, KCu 6 FeBiO 4 (SO 4 ) 5 Cl 22 . The space group and structural parameters of KCu 6 AlBiO 4 (SO 4 ) 5 Cl are determined as P 4\/ n c c , (the same space group as atlasovite) and a = 9.8248(9) \u00c5, c = 20.5715(24) \u00c5, respectively (see Supplementary Note 1 ). As shown Fig. 1a and b, the SKL in the crystal structure of KCu 6 AlBiO 4 (SO 4 ) 5 Cl comprises the six-coordinated Cu 2+ ions. In each SK unit, the square is enclosed by four scalene triangles. From this crystal structure, it is recognised that KCu 6 AlBiO 4 (SO 4 ) 5 Cl has three types of first neighbour interactions, J 1 , J 2 and J 3 , as shown in Fig. 1 c. The orbital arrangements can be reasonably deduced from the oxygen and chloride positions around the Cu 2+ ions. Judging from the \\({d}_{{x}^{2}-{y}^{2}}\\) orbitals arranged on the SKL, the nearest-neighbour (NN) magnetic couplings J i ( i = 1\u20133) are superexchange interactions occurring through Cu\u2013O\u2013Cu bonds: J 1 through the Cu1\u2013O\u2013Cu1 bond with a bond of angle 112.62\u00b0, and J 2 and J 3 through Cu1\u2013O\u2013Cu2 with bond angles of 120.12\u00b0 and 108.61\u00b0, respectively. Since the Cu\u2013O\u2013Cu angle significantly influences on the value of the exchange interactions, the variation of the angles can give strong bond-dependent exchange interactions 23 . Therefore, J 2 with the largest angle is expected to be the largest antiferromagnetic interaction, while J 3 with the smallest angle is considered to be the smallest antiferromagnetic interaction among the three interactions. One prominent and important feature of the present structure is the occupancy of nonmagnetic atoms in the interlayer space of the unit cell (Fig. 1 b), which elongate the interlayer spacing. Furthermore, the Cu 2+ ions and nonmagnetic ions have different valence numbers in KCu 6 AlBiO 4 (SO 4 ) 5 Cl, avoiding site mixing, unlike the Cu 2+ and Zn 2+ site mixing observed in herbertsmithite (for more details, see Supplementary Notes 1 and 2 ). Therefore, the crystal perfectness and high two-dimensionality of KCu 6 AlBiO 4 (SO 4 ) 5 Cl are ideal for studying the intrinsic magnetism on frustrated 2D magnets. However, the obtained INS experimental results are inconsistent with the calculated results for the J 1 - J 2 - J 3 SKL model (discussed below). Magnetic and thermodynamic properties Figure 2 a presents the temperature dependence of the magnetic susceptibility \u03c7 ( T ) and the inverse magnetic susceptibility 1\/ \u03c7 ( T ) of KCu 6 AlBiO 4 (SO 4 ) 5 Cl in the temperature range 1.8\u2013300 K. With decreasing temperature, the magnetic susceptibility gradually increases. This feature suggests the absence of any long-range order down to 1.8 K. From 1\/ \u03c7 ( T ) with the Curie\u2013Weiss law C \/( T \u2013 \u03b8 CW ), between 200 K and 300 K, we estimated the Curie constant and Weiss temperature to be C = 2.86(1) and \u03b8 CW = \u2212237(2) K, respectively. The C corresponds to an effective moment of 1.96 \u03bc B , consistent with the spin S = 1\/2 of Cu 2+ . The large negative Weiss temperature and the absence of long-range orders suggest an antiferromagnetic frustrated system. Fig. 2: Magnetic and thermodynamic properties of KCu 6 AlBiO 4 (SO 4 ) 5 Cl. a Temperature dependence of the magnetic susceptibility \u03c7 (open red circles) and the inverse susceptibilities 1\/ \u03c7 (open blue circles) of KCu 6 AlBiO 4 (SO 4 ) 5 Cl measured at 1 T. The \u03c7 is obtained by subtracting the Pascal's diamagnetic contribution from the experimental data. The solid grey lines denote the fitting curves by the Curie\u2013Weiss law. b High-field magnetisation measured up to 60 T at 1.8 K. The observed data M obs. (filled red circles) are broken down into two components: M bulk (black solid line) and M free (open green circles). Inset shows the magnetisation measured using MPMS at 1.8 K. The grey line is the Brillouin function for g = 2 and 2.4% of free S = 1\/2 spins. c Temperature dependence of the total specific heat measured at zero field (filled red circles). The grey line is the assumed lattice contribution C lattice. = 0.000555 T 3 . The green dashed line is the estimated magnetic entropy. Inset shows a log-log plot of the same data. Full size image The magnetisation curve measured at 1.8 K, as shown in the inset of Fig. 2 b, has two components: an intrinsic component M bulk and free spin component M free . Following the analysis for herbertsmithite 24 , a saturated magnetisation of M free can be estimated by subtraction of the linear M bulk from the measured total magnetisation M obs. . The M free can be fitted a Brillouin function for a spin-1\/2, suggesting the component is attributed to the paramagnetic impurity or the unpaired spins on surface of powder particles. The saturated value of M free indicates the presence of free spins with about 2.4% in total Cu 2+ ions in our sample. The M bulk at high magnetic field is only ~0.15 \u03bc B \/Cu 2+ at 60 T, indicating that strong antiferromagnetic exchange interaction dominates in this system (see Fig. 2 b). A Schottky-like peak in the heat capacity is observed at around T * \u2248 2 K, as shown in Fig. 2 c. As the released magnetic entropy at 15 K is only 16% of the expected total entropy, which is similar to that of herbertsmithite. In herbertsmithite, this behaviour was attributed to weakly coupled spins residing on the interlayer sites 9 . However, in KCu 6 AlBiO 4 (SO 4 ) 5 Cl, it is difficult to assign the 16% entropy to the site mixing because of the valence of nonmagnetic ions different from Cu 2+ . Rather the observed peak can be attributed easily to the development of short-range spin correlations. Similar characteristics are observed in the calculated specific heat and entropy of the the spin-1\/2 kagome antiferromagnet 25 . Small broad peak appear at around T \u2248 J \/100, and the released entropy at around this temperature is about 20%. As discussed below, the magnitude of the exchange interaction J a v \u2261 ( J 1 + J 2 + J 3 )\/3 = 137 K for KCu 6 AlBiO 4 (SO 4 ) 5 Cl, namely, J a v \/100 \u2248 T * . However, careful consideration is necessary about what origin of this peak is. We therefore conclude that the long-range magnetic and VBC-ordering behaviours are not observed in magnetic susceptibility, magnetisation and specific heat. Quantum spin fluctuations in KCu 6 AlBiO 4 (SO 4 ) 5 Cl To confirm the absence of spin ordering caused by quantum fluctuations, we performed \u03bc SR measurements. Figure 3 a shows the weak longitudinal-field (LF) (=50 G) \u03bc SR spectra at various temperatures. The weak LF was applied to quench the depolarisation due to random local fields from nuclear magnetic moments. The spectra are well fitted by the exponential function $$a(t)={a}_{1}\\exp (-\\lambda t)+{a}_{{\\rm{B}}G},$$ (1) where a 1 is an intrinsic initial asymmetry a 1 = 0.133, a B G is a constant background a B G = 0.047 (see Supplementary Note 2 ), \u03bb is the muon spin relaxation rate. Hartree potential calculation predicted a local potential minimum in the lattice (see Fig. 3 d, e) 26 , 27 , 28 . A muon site corresponding to a local potential minimum is located at the 16g site. Quantum fluctuations of the Cu 2+ spins down to 58 mK without spin ordering\/freezing are evidenced by the long-time \u03bc SR spectra. The weak LF signals at the lowest temperature (58 mK) decrease continuously without oscillations up to 15 \u03bcs, as shown in Fig. 3 b. If this spectrum is due to static magnetism, the internal field (estimate as \u03bb ZF \/ \u03b3 \u03bc , where \u03b3 \u03bc is the muon gyromagnetic ratio) should be less than 20 G. (see Supplementary Note 3 ). However, relaxation is clearly observed in the LF spectrum even at 0.395 T, which is evidence for the fluctuation of Cu 2+ electron spins without spin ordering\/freezing (see Fig. 3 c). As shown in Fig. 3 f, the increase of \u03bb at around T * renders evidence for a slowing down of the spin fluctuation resulting from the development of short-range correlations. In addition, they exhibit a plateau with weak temperature dependence at low temperature, which has been found in other QSL candidates 29 . The LF spectra measured at 58 mK under several magnetic fields are also fitted by Eq. (1). Using the power law represented by 1\/( a + b H \u03b1 ) with \u03b1 = 0.46, where a and b depend on the fluctuation rate and fluctuating field, we obtain a good fitting to the LF dependence of the muon spin relaxation rate \u03bb , as shown in Fig. 3 g. Incidentally, the 1\/( a + b H 2 ) is a standard case that the \u03bb obeys the Redfield equation. In ordinary disordered spin systems, the muon spin relaxation rate exhibits a field- inverse square dependence. Such a spectral-weight function is commonly used to describe classical fluctuations in the paramagnetic regime. The observed values, \u03b1 = 0.46, are inconsistent with the existence of a single timescale and suggest a more exotic spectral density, such as the one at play in a QSL. All of these \u03bc SR results strongly support the formation of a QSL at very low temperature close to the ground state in KCu 6 AlBiO 4 (SO 4 ) 5 Cl 30 , 31 . Fig. 3: Muon spin relaxation data of KCu 6 AlBiO 4 (SO 4 ) 5 Cl. a LF- \u03bc SR spectra (obtained in a dilution refrigerator) at representative temperatures (see Supplementary Note 3 for the spectra obtained using the 4 He cryostat). The thick lines behind the data points are the fitted curves (see text for details). b The LF- \u03bc SR spectrum measured at 58 mK. The spectrum decreases continuously without oscillations up to 15 \u03bcs. c \u03bc SR spectra measured at 58 mK under several longitudinal magnetic fields. d Projection along the c axis. e Projection along the a axis. The muon site was obtained by a Hartree potential calculation. f Temperature dependence of the muon spin relaxation rate \u03bb . The grey solid lines are guides for the eyes. g Magnetic-field dependences of the muon spin relaxation rate \u03bb . The solid curves are fitted to a power law of the form 1\/( a + b H \u03b1 ). The error bars in a , b and c represent 1 s.d. and in f and g the maximum possible variation due to correlation of parameters. Full size image Gapless continuum of spin excitations The quantum statistics of quasiparticle excitations depend on the type of QSL, in particular, the nature of their excitation. To grasp the whole picture of the spin excitation, first we performed the INS experiment in a wide energy range. As shown in Fig. 4 a, streak-like excitation at Q = 0.8 \u00c5 \u22121 and flat signals at around E = 7 and 10 meV are observed at 5 K. The E -dependence of the INS intensity can be fitted well by two or three Gaussian functions and linear baseline, and the corresponding integrated intensities are obtained (for more details, see Supplementary Note 4 ). As shown in Fig. 4 b, the peak positions of excitations are estimated to be 10.1(1) meV, 9.4(3) meV, and 7.3(1) meV, respectively. The signal due to magnetic excitation is generally enhanced at low- Q values, whereas phonon excitation is dominant at high- Q . As shown in Fig. 4 c, the baseline increase with increasing with Q . Therefore, the baseline may well comes from a number of phonon excitations in a multi-element material KCu 6 AlBiO 4 (SO 4 ) 5 Cl. The peak at 9.4 meV also increases with increasing with Q , indicating that it comes from phonon excitation. On the other hand, the flat signals have a characteristic feature of magnetic excitation. In order to investigate whether the spin excitation is gapless or gapped, we performed the INS experiments in the low-energy region. These signals are also observed at 0.3 K, as shown in Fig. 4 d, there are the streak-like excitation and flat signals are also observed. As shown in Fig. 4 e and g, the INS spectra exhibit the feature of a gapless continuum of spin excitations. Streak-like excitation at Q = 0.8 \u00c5 \u22121 is clearly visible down to the elastic line, and its intensity increases continuously without signature of energy gap at least within the instrumental resolution (FWHM = 0.05 meV for E i = 1.69 meV). The excitation persist up to at least T = 30 K (see Supplementary Fig. 5 ), which is consistent with the exchange constants estimated later. The Q -dependence of the INS intensity after integration over a finite energy interval is shown in Fig. 4 h. There are three peaks at Q = 0.8, 1.25, and 1.58 \u00c5 \u22121 at 0.3 K, and the peaks are observed even at low temperatures close to the ground state (48 mK). As discussed below, this result is inconsistent with the calculated dynamical spin structure factor S ( q , \u03c9 ) in the J 1 \u2013 J 2 \u2013 J 3 SKL antiferromagnet with parameters, which reproduce magnetic susceptibility and magnetisation process. These INS data are consistent with a gapless continuum of spinon excitations. From the above, the flat signals at approximately 10 and 7 meV probably indicate a van Hove singularity of spinon continuum edges at this energy. Fig. 4: Inelastic neutron-scattering data of KCu 6 AlBiO 4 (SO 4 ) 5 Cl. a INS spectra at 5 K observed using HRC with an incident neutron energy of 45.95 meV. b Energy dependence of the scattering integrated over Q in the range 1.9 \u00c5 \u22121 < Q < 2.1 \u00c5 \u22121 and 3.9 \u00c5 \u22121 < Q < 4.1 \u00c5 \u22121 measured at 5 K (HRC). The solid lines are the fitted curves (see text for details), the thin lines are its components. c Q -dependence of the integrated intensity for the different Gaussian components ( E = 10.1(1) meV, 9.4(3) meV and 7.3(1) meV). The solid thick lines are guides for the eyes. d INS spectra at 0.3 K observed using AMATERAS with an incident neutron energy of 15.16 meV. e INS spectra at 0.3 K observed using AMATERAS with incident neutron energy of 1.69 meV. f Energy dependence of the scattering integrated over Q in the range 0.6 \u00c5 \u22121 < Q < 1.0 \u00c5 \u22121 measured at 0.3 K. The grey solid line is guides for the eyes. g INS spectra at 0.3 K observed using AMATERAS with incident neutron energy of 3.14 meV. h Q -dependence of the scattering integrated over energy transfers 0.5 meV < E < 1.5 meV measured at 0.3 K (AMATERAS) and 50 mK (PELICAN). The error bars represent standard deviation. Full size image Comparison with theory To determine the magnetic parameters and to clarify the magnetic properties of KCu 6 AlBiO 4 (SO 4 ) 5 Cl, we calculated the magnetic susceptibility, the magnetisation curve at zero temperature, and the magnetic excitation at zero temperature by mean of the exact diagonalization (ED), finite temperature Lanczos (FTL) 32 and density-matrix renormalization group (DMRG) method. We succeeded in reproducing the magnetic susceptibility and magnetisation curve of KCu 6 AlBiO 4 (SO 4 ) 5 Cl with the J 1 \u2013 J 2 \u2013 J 3 SKL model with J 1 = 135 K, J 2 = 162 K, J 3 = 115 K and g = 2.11, as shown in Fig. 5 a and b, where g is the gyromagnetic ratio. In the magnetisation process, the magnetisation plateaus of M \/ M sat = 1\/3 and 2\/3 were confirmed at around 150 T and 270 T, respectively. This indicates the possibility to observe magnetisation plateaus experimentally if the measurement of the magnetisation process in a further strong magnetic field is performed. However, the result of inelastic neutron scattering that is the most important evidence of QSL cannot be reproduced in the J 1 \u2013 J 2 \u2013 J 3 SKL with these parameters. In the inelastic neutron-scattering experiment, in the low-energy region, the strongest intensity become around Q = 0.8 \u00c5 \u22121 as shown Fig. 4 f, g, while in the dynamical DMRG method, it is around Q = 1.3 \u00c5 \u22121 as shown in Fig. 5 c. To eliminate this discrepancy, we also calculated the SKL model with next-nearest-neighbour (NNN) interaction J X in the diagonal direction of the Cu 2+ square. We calculated this SKL model with various values of the parameters, but we could not reproduce the experimental results. Therefore, in order to understand the experiment correctly, we need to calculate the model with further interactions. Fig. 5: Experimental results of KCu 6 AlBiO 4 (SO 4 ) 5 Cl compared to theory. a Temperature dependence of the magnetic susceptibility \u03c7 (open red circles) of KCu 6 AlBiO 4 (SO 4 ) 5 Cl and the fitted calculation data obtained by the FTL method for a 36-site cluster (blue line) and ED method for a 18-site cluster (green dashed line). Note that the statistical error of the FTL is within the grey area (for more details, see Supplementary Note 5). b High-field magnetisation measured up to 60 T at 1.8 K (open red circles) and the fitted calculation data at T = 0 K obtained by the Lanczos-type ED method for a 36-site cluster (blue dashed line) and DMRG method for a 60-site cluster (black solid line). c Q -dependence of powder-averaged dynamical spin structure factor S ( Q , E ) integrated over 0.5 meV < E < 1.5 meV at T = 0 K obtained by dynamical DMRG for a 48-site PBC cluster of the SKL. Full size image Discussion We have synthesised a SKL spin-1\/2 antiferromagnet KCu 6 AlBiO 4 (SO 4 ) 5 Cl without site disorder, thus providing a first candidate to investigate the SKL magnetism. The \u03bc SR measurement shows no long-range ordering down to 58 mK, roughly three orders of magnitude lower than the NN interactions. The INS spectrum exhibits a streak-like gapless excitation and flat dispersionless excitation, consistent with powder-averaged spinon excitations. Our experimental results strongly suggest the formation of a gapless QSL in KCu 6 AlBiO 4 (SO 4 ) 5 Cl at very low temperature close to the ground state; however, they are inconsistent with the theoretical studies based on the J 1 \u2013 J 2 \u2013 J 3 SKL Heisenberg model. In the J 1 \u2013 J 2 \u2013 J 3 SKL Heisenberg model, the VBC and N\u00e9el order stats are expected with high probability. In fact, the VBC state is the ground state of the J 1 \u2013 J 2 SKL antiferromagnet regardless of the magnitude relation of J 1 \/ J 2 17 . Thus, to realise the QSL state in the SKL, we must impose an additional condition such as longer-range exchange interactions. Further theoretical study would reveal the conditions inducing the QSL state in SKL antiferromagnets. Methods Sample synthesis Single phase polycrystalline KCu 6 AlBiO 4 (SO 4 ) 5 Cl was synthesised by the solid-state reaction in which high-purity KAl(SO 4 ) 2 , CuCl 2 , CuSO 4 , CuO and Bi 2 O 3 powders were mixed in a molar ratio of 2:1:6:5:1, followed by heating at 600 \u00b0C for 3 days and slow cooling in air. X-ray diffraction Synchrotron powder XRD data were collected using an imaging plate diffractometer installed at the BL-8B of the Photon Factory. The incident synchrotron X-ray energy of 18.0 keV (0.68892 \u00c5) was selected. Magnetic susceptibility and low-field magnetisation Magnetic susceptibility and low-field magnetisation measurements were performed using a commercial superconducting quantum interference device magnetometer (MPMS-XL7AC: Quantum Design). High-field magnetisation High-field magnetisation measurements up to 60 T were conducted using an induction method in a pulsed magnetic field at the International MegaGauss Science Laboratory, The University of Tokyo. Heat capacity The specific heat was measured between 0.2 and 20 K using a PPMS (physical property measurement system; Quantum Design). Muon spin relaxation ( \u03bc SR) The \u03bc SR experiments were performed using the spin-polarised pulsed surface-muon ( \u03bc + ) beam at the D1 beamline of the Materials and Life Science Experimental Facility (MLF) of the Japan Proton Accelerator Research Complex (J-PARC). The spectra were collected in the temperature range from 58 mK to 300 K using a dilution refrigerator and 4 He cryostat. Inelastic neutron scattering (INS) The high-energy INS experiment was performed on the HRC 33 , installed at BL12 beamline at MLF of J-PARC. At the HRC, white neutrons are monochromatised by a Fermi chopper synchronised with the production timing of the pulsed neutrons. The energy transfer was determined from the time-of-flight of the scattered neutrons detected at position sensitive detectors. A 200-Hz Fermi chopper was used to obtain a high neutron flux. A GM-type closed cycle cryostat was used to achieve 5 K. The energy of incident neutrons were E i = 45.95 meV. The data collected by HRC were analysed using the software suite HANA 34 . The low-energy INS experiments were performed using the cold-neutron time-of-flight spectrometer PELICAN at the OPAL reactor at ANSTO 35 . The instrument was aligned for an incident energy E i = 2.1 meV. The sample was held in an oxygen-free copper can and cooled using a dilution insert installed in a top-loading cryostat and data collected at 25 K, 15 K and 48 mK. The sample was corrected for background scattering from an empty can and normalised to the scattering from a vanadium standard. The PELICAN data corrections were performed using the freely available LAMP software. The INS spectra in a wide momentum-energy range were measured using the cold-neutron disk chopper spectrometer AMATERAS installed in the MLF at J-PARC 36 . The sample was cooled to 0.3 K using a 3 He refrigerator. The scattering data were collected with a set of incident neutron energies, E i = 1.69, 3.14 and 15.16 meV. The data collected by AMATERAS were analysed using the software suite UTSUSEMI 37 . Calculations Magnetic susceptibility of the SKL is calculated by the full ED method for 18-site and FTL method for 36-site under the periodic boundary condition (PBC). The result of the FTL method is deduced by the statistical average of 40 sampling. The magnetisation curve at T = 0 K is calculated by the Lanczos-type ED calculations for a 36-site PBC cluster and the DMRG method for a 60-site PBC cluster. The truncation number in the DMRG calculation is 6000 and resulting truncation errors are less than 2 \u00d7 10 \u22125 . The dynamical spin structure factor S ( q , \u03c9 ) is calculated using the dynamical DMRG 38 method for a 48-site PBC cluster. The truncation number m = 6000 and the truncation error are less than 5 \u00d7 10 \u22123 . S ( q , \u03c9 ) is defined as follows: $$S({\\bf{q}},\\omega )=-\\frac{1}{\\pi N}{\\rm{Im}}\\left\\langle 0\\right|{S}_{-{\\bf{q}}}^{z}\\frac{1}{\\omega -{\\mathscr{H}}+{E}_{0}+i\\eta }{S}_{{\\bf{q}}}^{z}\\left|0\\right\\rangle ,$$ (2) where q is the momentum, \\(\\left|0\\right\\rangle\\) is the ground state with energy E 0 , \u03b7 is a broadening factor and \\({S}_{{\\bf{q}}}^{z}={N}^{-1\/2}{\\sum }_{i}{e}^{i{{\\bf{q}}r}_{i}}{S}_{i}^{z}\\) with r i being the position of spin i and \\({S}_{i}^{z}\\) being the z component of S i . The value of \u03b7 is taken to be 1.16 meV. Data availability The data that support the findings of this study are available from the corresponding author upon reasonable request. ","News_Body":"Aside from the deep understanding of the natural world that quantum physics theory offers, scientists worldwide are striving to bring forth a technological revolution by leveraging this newfound knowledge in engineering applications. Spintronics is an emerging field that aims to surpass the limits of traditional electronics by using the spin of electrons, which can be roughly seen as their angular rotation, as a means to transmit information. But the design of devices that can operate using spin is extremely challenging and requires the use of new materials in exotic states\u2014even some that scientists do not fully understand and have not experimentally observed yet. In a recent study published in Nature Communications, scientists from the Department of Applied Physics at Tokyo University of Science, Japan, describe a newly synthesized compound with the formula KCu6AlBiO4(SO4)5Cl that may be key in understanding the elusive \"quantum spin liquid (QSL)\" state. Lead scientist Dr. Masayoshi Fujihala explains his motivation: \"Observation of a QSL state is one of the most important goals in condensed-matter physics as well as the development of new spintronic devices. However, the QSL state in two-dimensional (2-D) systems has not been clearly observed in real materials owing to the presence of disorder or deviations from ideal models.\" What is the quantum spin liquid state? In antiferromagnetic materials below specific temperatures, the spins of electrons naturally align into large-scale patterns. In materials in a QSL state, however, the spins are disordered in a way similar to how molecules in liquid water are disordered in comparison to crystalline ice. This disorder arises from a structural phenomenon called frustration, in which there is no possible configuration of spins that is symmetrical and energetically favorable for all electrons. KCu6AlBiO4(SO4)5Cl is a newly synthesized compound whose copper atoms are arranged in a particular 2-D pattern known as the \"square kagome lattice (SKL),\" an arrangement that is expected to produce a QSL state through frustration. Professor Setsuo Mitsuda, co-author of the study, states: \"The lack of a model compound for the SKL system has obstructed a deeper understanding of its spin state. Motivated by this, we synthesized KCu6AlBiO4(SO4)5Cl, the first SKL antiferromagnet, and demonstrated the absence of magnetic ordering at extremely low temperatures\u2014a QSL state.\" However, the experimental results obtained could not be replicated through theoretical calculations using a standard \"J1-J2-J3 SKL Heisenberg\" model. This approach considers the interactions between each copper ion in the crystal network and its nearest neighbors. Co-author Dr. Katsuhiro Morita explains: \"To try to eliminate the discrepancy, we calculated an SKL model considering next-nearest-neighbor interactions using various sets of parameters. Still, we could not reproduce the experimental results. Therefore, to understand the experiment correctly, we need to calculate the model with further interactions.\" This disagreement between experiment and calculations highlights the need for refining existing theoretical approaches, as co-author Prof Takami Tohyama concludes: \"While the SKL antiferromagnet we synthesized is a first candidate to investigate SKL magnetism, we may have to consider longer-range interactions to obtain a quantum spin liquid in our models. This represents a theoretical challenge to unveil the nature of the QSL state.\" Let us hope physicists manage to tackle this challenge to bring us yet another step closer to the wonderful promise of spintronics. ","News_Title":"The spin state story: Observation of the quantum spin liquid state in novel material","Topic":"Physics"}
{"Paper_Body":"Abstract The human glycine transporter 1 (GlyT1) regulates glycine-mediated neuronal excitation and inhibition through the sodium- and chloride-dependent reuptake of glycine 1 , 2 , 3 . Inhibition of GlyT1 prolongs neurotransmitter signalling, and has long been a key strategy in the development of therapies for a broad range of disorders of the central nervous system, including schizophrenia and cognitive impairments 4 . Here, using a synthetic single-domain antibody (sybody) and serial synchrotron crystallography, we have determined the structure of GlyT1 in complex with a benzoylpiperazine chemotype inhibitor at 3.4 \u00c5 resolution. We find that the inhibitor locks GlyT1 in an inward-open conformation and binds at the intracellular gate of the release pathway, overlapping with the glycine-release site. The inhibitor is likely to reach GlyT1 from the cytoplasmic leaflet of the plasma membrane. Our results define the mechanism of inhibition and enable the rational design of new, clinically efficacious GlyT1 inhibitors.     Main Glycine is a conditionally essential amino acid with a dual role in the central nervous system (CNS). It acts as a classical neurotransmitter at inhibitory glycinergic synapses, where it induces hyperpolarizing chloride influx at postsynaptic terminals through ionotropic glycine receptors 1 , 2 . Yet, as the obligatory co-agonist of the N -methyl- d -aspartate (NMDA) receptor, glycine also positively modulates calcium-dependent neuronal excitation and plasticity at glutamatergic synapses 1 , 3 . Glycine homeostasis is tightly regulated by reuptake transporters\u2014including the glycine-specific GlyT1 and GlyT2\u2014that belong to the secondary active neurotransmitter\/sodium symporters (NSSs) of the solute carrier 6 (SLC6) transport family 5 . GlyT1 (encoded by the SLC6A9 gene), GlyT2 (encoded by SLC6A5 ) and the other members of the NSS family, such as the serotonin transporter (SERT), dopamine transporter (DAT) and \u03b3-aminobutyric acid (GABA) transporter (GAT), share a sequence identity of approximately 50%. GlyT1 is located on presynaptic neurons and astrocytes surrounding both inhibitory glycinergic and excitatory glutamatergic synapses, and is considered the main regulator of extracellular levels of glycine in the brain 1 , 6 . At glutamatergic synapses, GlyT1 has a key role in maintaining subsaturating concentrations of regulatory glycine for the NMDA receptor 7 , 8 . Hypofunction of the NMDA receptor has been implicated in the pathophysiology of schizophrenia 9 , but pharmacological interventions to directly enhance neurotransmission via this receptor in patients with the condition have been unsuccessful 10 , 11 . Selective inhibition of glycine reuptake by GlyT1 is an alternative approach to increase endogenous extracellular levels of glycine and potentiate NMDA transmission 1 , 4 . Several chemotypes of potent and selective GlyT1 inhibitors, such as bitopertin, have been developed to achieve antipsychotic and procognitive activity for the treatment of schizophrenia 4 , 12 . Bitopertin has shown clear signs of enhancing neuroplasticity 13 , 14 via the glycine-binding site of the NMDA receptor; however, it failed to show efficacy in phase III clinical trials (at a reduced dose), and a drug candidate that targets GlyT1 has yet to emerge. Studies of NSS and homologues have revealed an alternating-access mechanism 15 , which involves a binding and occlusion of the extracellular substrate, dependent on a Na + (and Cl \u2212 in eukaryotic NSS) gradient. Binding is followed by a rearrangement to an inward-facing state and subsequent intracellular opening and release of bound ions and substrate. Conformational rearrangements of transmembrane helices during the transport cycle expose the substrate-binding site to either side of the membrane 16 , 17 , 18 , 19 , 20 , 21 , 22 , 23 . Bitopertin behaves functionally as a non-competitive inhibitor of glycine reuptake 24 ; nevertheless, detailed structural information on the inhibitor\u2019s binding site, selectivity and underlying molecular mechanism of glycine reuptake inhibition have yet to be obtained. Here we present the structure of human GlyT1 in complex with a highly selective bitopertin analogue 25 , 26 , Cmpd1, and an inhibition-state-selective synthetic nanobody (sybody). Cmpd1 has been patented as a more potent inhibitor targeting GlyT1 that contains a benzoylisoindoline scaffold originating from the bitopertin chemical series 26 . The structure of GlyT1 reveals the molecular determinants and mechanism of action underlying the inhibition of glycine reuptake. Stabilization and crystal structure of GlyT1 Wild-type human GlyT1 (encoded by SLC6A9 ) is unstable when extracted from the membrane, and contains unstructured termini and a large, flexible extracellular loop 2 (EL2). To enable structure determination, we screened for point mutations that increase thermal stability while preserving ligand-binding activity. For the final crystallization construct, we combined the point mutations L153A, S297A, I368A and C633A with a shortened EL2 (\u0394240\u2013256) and truncated amino and carboxyl termini (\u03941\u201390 and \u0394685\u2013706) (see Methods ), and were able to measure persistent transport activity, albeit 42-fold decreased compared with that of wild-type GlyT1 (Extended Data Fig. 1 ). Adding the selective GlyT1 inhibitor Cmpd1 increases the thermal stability of the transporter further by 30.5 \u00b0C (Fig. 1a ). Indicative of high-affinity binding with a stabilizing effect, we measured a half-maximal inhibitory concentration (IC 50 ) for Cmpd1 of 12.9 \u00b1 0.9 nM and 7.2 \u00b1 0.4 nM on human and mouse GlyT1, respectively (Fig. 1b ), in a membrane-based competition assay with the [ 3 H]Org24598 compound 27 (a non-competitive GlyT1 inhibitor). We therefore purified GlyT1 in the presence of Cmpd1 and generated sybodies to further stabilize the transporter in the inhibition-state conformation, identifying sybody Sb_GlyT1#7 to bind GlyT1 with an affinity of 9 nM (ref. 28 ). We then obtained microcrystals of GlyT1 in complex with Sb_GlyT1#7 and Cpmd1 in lipidic cubic phase. Merging the oscillation patterns collected from 409 mounted loops containing microcrystals by a serial synchrotron crystallography approach yielded a complete dataset at 3.4 \u00c5 resolution. The structure was determined by molecular replacement using structures of the inward-occluded bacterial multiple hydrophobic amino acid transporter (MhsT; Protein Data Bank identification code (PDB ID) 4US3) and the inward-open human SERT (PDB ID 6DZZ) 17 , 19 . The high quality of the resulting electron density maps enabled us to unambiguously model human GlyT1 in complex with the sybody and bound ligand (Fig. 1c and Extended Data Fig. 2 ). Fig. 1: Stabilization, binding and recognition of inhibitor Cmpd1 by human GlyT1. a , Increasing concentrations of Cmpd1 show a strong dose-dependent stabilization of GlyT1, raising the melting point from 48.8 \u00b1 0.4 \u00b0C to 79.3 \u00b1 0.3 \u00b0C (mean \u00b1 s.e.m.). Data for GlyT1 minimal (containing deletions of the N and C termini) with and without addition of the inhibitor are depicted in green and black, respectively. Individual data points from n = 4 technical replicates are shown. AU, arbitrary units. b , Cmpd1 inhibits mouse and human GlyT1 with an IC 50 of 7.2 \u00b1 0.4 nM and 12.9 \u00b1 0.9 nM (mean \u00b1 s.e.m.), respectively, in membrane-based competition assays with [ 3 H]Org24598. Curves were calculated from n = 4 technical replicates (individual data points are shown; whiskers extend from minimum to maximum). c , Overall structure of human GlyT1 bound to the selective inhibitor Cmpd1 and an inhibition-state-selective sybody. A magnified view of the inhibitor-binding pocket in a 2 F o \u2013 F c electron density map (blue mesh) countered at 1.0 r.m.s.d. is depicted. TM8 is not shown for clarity. d , Topology diagram of the GlyT1 crystallization construct. EL2 carries a strictly conserved disulfide bridge (C220\u2013C229) and four N -linked glycosylation sites, N237, N240, N250 and N256. Three glycosylation sites were removed by the EL2 truncation (240\u2013256), but N237 was essential for membrane-based ligand binding, probably enabling correct trafficking of the transporter to the plasma membrane 40 . The one remaining glycosylation site at N237 is shown as a sphere on EL2. The locations of the single point mutations L153A, S297A, I368A and C633A on transmembrane helices are shown. Full size image Architecture and conformation of GlyT1 GlyT1 adopts the general architecture of SLC6 transporters, with 12 \u03b1-helical transmembrane segments (TMs 1\u201312) and an inverted pseudo-twofold symmetry that relates two transmembrane domains, TMs 1\u20135 and 6\u201310, denoted as the LeuT fold 17 , 18 , 21 , 22 , 29 (Fig. 1c, d ). The transporter structure exhibits an inward-open conformation, and superposition of this structure to the inward-open structures of SERT and leucine transporter (LeuT) and inward-oriented occluded MhsT yields C\u03b1 root mean square deviations of 1.8 \u00c5, 2.3 \u00c5 and 3.2 \u00c5, respectively (see Methods ). TM1 and TM6 possess nonhelical segments in the middle of the lipid bilayer; these segments coordinate Na + and Cl \u2212 ions 18 , 20 , 21 , accommodate substrates and inhibitors of various sizes 18 , 19 , 22 , and stabilize the ligand-free return state 17 . The intracellular part of TM5 is unwound at the conserved helix-breaking Gly(X 9 )Pro motif 17 (G313(X 9 )P323 in GlyT1), and the N-terminal segment of TM1 (TM1a) is bent away from the core of GlyT1, opening the intracellular pathway to the centre of the transporter (Fig. 2a ). The splayed motion of TM1 disrupts the interaction between the conserved residues W103 of TM1a and Y385 at the cytoplasmic part of TM6 that is otherwise present in outward-open and occluded conformations 17 , 18 , 20 , 22 (Extended Data Fig. 3 and Supplementary Fig. 1 ). Fig. 2: Inhibition of glycine uptake and binding mode of Cmpd1 at inward-open GlyT1. a , Surface representation of the inward-open structure of GlyT1, viewed parallel to the membrane. The closed extracellular vestibule around W124 (yellow) and the open intracellular pathway are displayed. Residues R125 (TM1), P437 (EL4), L524 and D528 (TM10) are shown as sticks. b , c , Comparison of the binding modes of Cmpd1 (green) in GlyT1 with the inhibitor-binding sites in other NSS transporters. Paroxetine (orange) and ibogaine (yellow) bound to SERT (PDB IDs 5I6X and 6DZY, respectively) and cocaine (purple) bound to Drosophila melanogaster DAT (dDAT, PDB ID 4XP4) are shown as examples. The differences in the locations of the bound ligands in the transporters are marked with dotted lines in b . Compared with paroxetine, ibogaine and cocaine, Cmpd1 is located 5.6 \u00b1 0.1 \u00c5 further away from the centre of the transporter (shown in c ). This distance is measured between the centre of the phenyl ring of Cmpd1 and the centre of mass of the other NSS inhibitors shown. d , Cmpd1 inhibits the uptake of glycine by human GlyT1 with an IC 50 of 26.4 \u00b1 0.6 nM (mean \u00b1 s.e.m.). The curve was calculated from n = 4 technical replicates (individual data points are shown; whiskers extend from minimum to maximum). Full size image Comparison of GlyT1 with inward-open SERT shows structural differences mainly at the intracellular halves of the helices (Extended Data Fig. 4a\u2013e ), and in particular at the intracellular gate of GlyT1 defined by TM1a and TM5. The intracellular half of TM5 has splayed away from the transporter core by 17\u00b0, whereas TM1a is by 29\u00b0 closer, compared with corresponding segments of SERT. As a result, the intracellular gate\u2014measured as the C\u03b1\u2013C\u03b1 distance between the conserved W103 on TM1a and V315 on TM5\u2014is by 4 \u00c5 more closed than that of the inward-open structure of ibogaine-bound SERT (Extended Data Fig. 4b, e ). On the extracellular side, a C\u03b1\u2013C\u03b1 distance of 8.9 \u00c5 between R125 of TM1a and D528 of TM10, and a close packing of the extracellular vestibule around W124 in the NSS-conserved NVWRFPY motif of TM1, indicates a closed extracellular gate (Fig. 2a , Extended Data Fig. 3 and Supplementary Fig. 1 ). The conformation-specific sybody binds through several interactions to the extracellular segment of GlyT1, involving EL2, EL4, TM5 and TM7 (Fig. 1c and Extended Data Fig. 5a ). Sb_GlyT1#7 is selective for the inward-open conformation of GlyT1 and has a conformation-stabilizing effect, as shown by an increase of 10 \u00b0C in thermal stability and an apparent affinity increase for [ 3 H]Org24598 of almost twofold in a scintillation proximity assay 28 . In addition to stabilizing the inhibition state, the sybody takes a central role in forming the lattice contacts, packing against the neighbouring sybody in the crystal (Extended Data Fig. 5d ). Unique binding mode among NSS transporters An unambiguous electron density for the inhibitor Cmpd1 was observed in proximity to the central binding pocket of GlyT1, between transmembrane helices 1, 3, 6 and 8 (Fig. 1c and Extended Data Fig. 5b ). Comparison of the inhibitor-binding site in GlyT1 with the equivalent site of other NSS structures shows that Cmpd1 is within 6.0 \u00b1 0.5 \u00c5 of the core, with its centre of mass located 14 \u00c5 from the cytosolic face of the transporter, while inhibitors of SERT and DAT bind at the central binding site within 21\u201322 \u00c5 of the cytosolic face (Fig. 2b, c ). Furthermore, the inhibitor binds GlyT1 in a unique binding mode, lodged in proximity to the centre of the transporter and extending into the intracellular release pathway for substrate and ions between TM6b and TM1a, accessible to solvent. This mode of inhibition is not observed in other NSS\u2013inhibitor complexes (Fig. 2b, c ). Cmpd1 is from the benzoylisoindoline class of selective GlyT1 inhibitors 25 , and inhibits the uptake of glycine in mammalian cells (Flp-in-CHO cells) expressing mouse 26 or human GlyT1 with an IC 50 of 7.0 \u00b1 0.4 nM and 26.4 \u00b1 0.6 nM, respectively (Fig. 2d ). The isoindoline scaffold of Cmpd1 forms a \u03c0-stacking interaction with Y116 of TM1. The phenyl ring is engaged in an edge-to-face stacking interaction with the aromatic ring of W376 located on the unwound region of TM6. The inhibitor is further stabilized by hydrogen-bond and van der Waals interactions with residues from TM1, TM3, TM6 and TM8 (Fig. 3a and Extended Data Figs. 5 c, 6a, b ). Fig. 3: Binding pocket. a , Close-up view of the Cmpd1-binding pocket in GlyT1. The two ends of the inhibitor are stabilized by hydrogen-bond interactions with residues from TM1 and TM6; the backbone amine groups of G121 and L120 form hydrogen bonds with sulfonyl oxygen atoms, and N386 from TM6 forms a hydrogen bond with the oxygen atom of the tetrahydropyran moiety of the inhibitor. From TM8, the hydroxyl group of T472 participates in a hydrogen-bonding interaction with the carbonyl oxygen of the scaffold. The aromatic ring of Y116, localized 4.2 \u00c5 from the isoindoline scaffold of the compound (a \u03c0-stacking interaction), is shown. The hydroxyl group of Y196 from TM3 probably forms a weaker hydrogen-bond interaction with the methyl sulfone moiety of the inhibitor. Inhibitor binding is also supported by an edge-to-face stacking interaction between the phenyl ring of the ligand and the aromatic sidechain of W376. The residues that form the binding pocket, G373, L379 and M382 (TM6) and I399 (TM7), are also depicted. b , Docking of bitopertin (orange) into the inhibitor-binding pocket of GlyT1, comparing the binding modes of bitopertin and Cmpd1 (green). c , Comparison of Cmpd1 (benzoylisoindoline series, top) and bitopertin (benzoylpiperazine series, bottom). The scaffolds of the compounds are marked with grey dashed lines, and the three R groups are marked with orange dashed lines. Full size image We generated a stable construct with a single point mutation, I192A, that was not able to bind the inhibitor. Notably, I192 is within van der Waals distance of the W376 side chain, which is stabilized in a rotamer perpendicular to the phenyl ring of the inhibitor (Extended Data Fig. 6c\u2013e ). W376 is the bulky hydrophobic residue of a conserved (G\/A\/C)\u03a6G motif in the unwound segment of TM6 that determines the substrate selectivity of SLC6 transporters 30 , 31 , 32 , and the AWG sequence observed in GlyT1 is indeed fitting for a small glycine substrate. I192, although not in direct interaction with the inhibitor, plays an important part in the binding of Cmpd1 by reducing the rotational freedom of the W376 side chain, which may also further restrict the binding pocket for glycine. Adding a lichenase fusion protein construct 33 (PDB ID 2CIT) to the N terminus of the GlyT1 construct, we generated and crystallized a GlyT1\u2013Lic fusion protein in complex with Sb_GlyT1#7, and obtained a dataset at 3.9 \u00c5 resolution collected from 1,222 mounted loops containing microcrystals (Extended Data Fig. 5d ). The electrogenic reuptake of glycine via GlyT1 is coupled to the transport of two Na + and one Cl \u2212 ions. Both the GlyT1 and the GlyT1\u2013Lic constructs were purified and crystallized in the presence of 150 mM NaCl and adopt the same inward-open, inhibitor-bound conformation. However, we observe electron density for Na + and Cl \u2212 ions only in the lower-resolution map of the GlyT1\u2013Lic crystal structure, which may have captured a preceding state in transitions associated with ion release to the intracellular environment (Extended Data Fig. 7a, b ). Plasticity of the binding pocket Similar to reported benzoylisoindolines 25 , Cmpd1 is more than 1,000-fold selective for GlyT1 against GlyT2 (Extended Data Fig. 6f ). Comparing the binding-pocket residues of GlyT1 with corresponding residues in GlyT2 points to direct clues. G373 in GlyT1 corresponds to S497 in GlyT2. Notably, N -methyl glycine (sarcosine) and N -ethyl glycine are substrates of GlyT1 and the S497G mutant of GlyT2, but not of wild-type GlyT2 31 , 34 , 35 , which can be explained readily by a steric clash with S497. Furthermore, GlyT1 residues M382 and I399 correspond to leucine and valine, respectively, in GlyT2; the latter two diminish the van der Waals interactions between the inhibitor and the transporter. Molecular docking places bitopertin in the binding pocket of GlyT1, with its benzoylpiperazine scaffold matching the benzoylisoindoline scaffold of Cmpd1 (Fig. 3b ). The binding mode and scaffold substituent interactions (R 1 \u2013R 3 ) are supported by the previously reported structure\u2013activity relationships of the benzoylpiperazine and benzoylisoindoline series 12 , 25 . The R 1 pocket (hosting a methyl sulfone moiety) is spatially constrained and prefers small, polar substituents with a hydrogen-bond-acceptor group. The pocket harbouring R 2 substituents (O\u2013C 3 F 5 ) is mainly hydrophobic and accommodates linear and cyclic substituents up to a ring size of six. The R 3 (tetrahydropyran) pocket is fairly large and exposed to solvent, and can accommodate diverse groups with different functionalities (Fig. 3c ). We observed a higher flexibility for the tetrahydropyran moiety, as the corresponding portion of the electron density was not well resolved. Considering the size and solvent exposure of this pocket, the R 3 position is the favourable handle to fine-tune the physicochemical properties of the inhibitor. Superposition of glycine-bound LeuT and tryptophan-bound MhsT on inhibitor-bound GlyT1 shows that the sulfonyl moiety of the inhibitor probably mimics the carboxylate group of the glycine substrate, interacting with TM1 and TM3 (Extended Data Fig. 7c, d ). We observe that at a glycine concentration of more than 0.1 mM, selective inhibitors of GlyT1 are outcompeted, further supporting the existence of overlapping binding sites (Extended Data Fig. 7e ). Mechanism of inhibition Although GlyT1\u2019s binding site for bitopertin and Cmpd1 appears to overlap with its glycine-binding site, these are not competitive glycine-reuptake inhibitors 4 , 24 (Extended Data Figs. 7 c\u2013e, 8 ). It is likely that, owing to their hydrophobic nature 12 , 25 , Cmpd1, bitopertin and related chemotypes diffuse across the cell membrane and bind from the cytoplasmic side to an inward-open structure, involving unwinding of the TM5 segment and a hinge-like motion of TM1a to fit the bulky inhibitor (Fig. 4 ). Glycine, on the other hand, binds to the outward-open conformation, which is exposed to high concentrations of the driving Na + and Cl \u2212 ions at the synaptic environment. Following binding of glycine and ions, the transporter transforms to an inward-open conformation with low affinity for glycine, and this is where direct binding competition can occur, with the inhibitor having a high affinity for the site. Fig. 4: Mechanism of inhibition of GlyT1. Left, glycine (purple) binds with high affinity to the outward-open conformation of GlyT1 (homology model based on dDAT, PDB ID 4M48), which is exposed to high concentrations of the driving Na + and Cl \u2212 ions (orange and green spheres, respectively) in the synaptic environment. Right, the inhibitor Cmpd1 (green) can diffuse across the synaptic cell membrane and reach the intracellular side of GlyT1. Cmpd1 locks the transporter in an inward-open conformation, with the characteristic hinge-like motion of TM1a and unwinding of TM5. Cmpd1 inhibits GlyT1 by shifting the conformational equilibrium to the inward-open state. Full size image Release of ions and glycine from the inward-open state enables bitopertin, Cmpd1 and similar transport inhibitors to bind and shift the conformational equilibrium towards an inward-open conformation. As with the inhibition of inward-open SERT by ibogaine 36 , the binding sites of glycine and non-competitive inhibitors of GlyT1 explore two distinct conformational states, outward and inward oriented (Fig. 4 ). Considering the high membrane permeability measured for Cmpd1 and bitopertin 12 , 25 , it is likely that the inhibitor dissipates into locations other than the synapse. In fact, GlyT1 is also expressed in peripheral tissues, including erythrocytes where glycine plays a key part in the biosynthesis of haem. Inhibition of GlyT1 by bitopertin in these cells results in a tolerable decrease in the level of haemoglobin. However, the possible risks associated with such an effect were a prohibitory factor in phase III clinical trials of bitopertin, which was therefore administered at a lower dose than in the proof-of-concept phase II clinical studies. It also remains unclear whether administration of bitopertin reached optimal GlyT1 occupancy in trial subjects, or whether a higher placebo response in clinical trials resulted in an indistinguishable efficacy of bitopertin 10 , 37 , 38 . The sybody Sb_GlyT1#7 is also highly selective for the inhibited, inward-open conformation of GlyT1. Recent efforts to engineer antibodies that achieve effective targeting and efficient crossing of the blood\u2013brain barrier 39 to deliver an inhibition-state-specific sybody represent an alternative approach to small-molecule inhibitors of GlyT1. The structure of human GlyT1 presented here provides a platform for the rational design of new small-molecule inhibitors and antibodies that target the glycine-reuptake transporter. Methods No statistical methods were used to predetermine sample size. The experiments were not randomized, and investigators were not blinded to allocation during experiments and outcome assessment. GlyT1 constructs The human GlyT1 complementary DNA sequence was codon optimized and synthesized by Genewiz for expression in mammalian cells, and the GlyT1\u2013Lic sequence for insect cell expression. Both constructs contain N- and C-terminal deletions of residues 1\u201390 and 685\u2013706, respectively (minimal construct GlyT1 minimal ) as well as a deletion in the extracellular loop 2 (EL2) between residues 240 and 256. To improve the thermal stability of the constructs, we introduced single point mutations to the transmembrane helices of GlyT1 minimal , and screened the constructs on the basis of their expression level, thermal stability and ability to bind inhibitor. In total, we introduced 329 single mutations into the minimal construct, of which we combined the point mutations L153A, S297A, I368A and C633A in the final construct for crystallization. In addition, we omitted the N-terminal residue 91 from the GlyT1\u2013Lic sequence, and residues 9\u2013281 of lichenase (PDB ID 2CIT) have been fused at the N terminus in order to increase the hydrophilic surface area of the transporter and to facilitate crystallization. The sequences of GlyT1 and GlyT1\u2013Lic followed by a C-terminal enhanced green fluorescent protein (eGFP) and a decahistidine tag were cloned into a pCDNA3.1 vector for transient transfection in human embryonic kidney (HEK293) cells (Invitrogen; not authenticated and not tested for mycoplasma contamination), and a pFastBac vector for baculovirus expression in Spodoptera frugiperda ( Sf9 ) insect cells (American Type Culture Collection (ATCC), catalogue number CRL-1711; authenticated and free of mycoplasma contamination), respectively. Transporter expression and purification GlyT1 was expressed in FreeStyle 293 expression medium (Thermo Fisher Scientific) in 1-litre scale in 600 ml TubeSpin bioreactors, incubating in an orbital shaker at 37 \u00b0C, 8% CO 2 and 220 rpm in a humidified atmosphere. The cells were transfected at a density of 1 \u00d7 10 6 cells per ml and a viability of above 95%. A 25 kDa linear polyethylenimine (LPEI) was used as the transfection reagent, at a GlyT1 DNA:LPEI ratio of 1:2. The cells were typically collected 60 h post-transfection at a viability of around 70%, and stored at \u221280 \u00b0C until purification. GlyT1\u2013Lic was expressed in 20\u201325-litre scale in 50-litre single-use WAVE bioreactors (CultiBag RM, Sartorius Stedim Biotech) at 27 \u00b0C with 18\u201325 rocks per minute in a 40% oxygenated Sf900-III medium (Gibco by Life Technologies). The cells were typically infected with a 0.25% volume of infection of the virus at a density of 2\u20133 \u00d7 10 6 cells per ml and viability of above 95%. The cells were collected 72 h post-infection at a viability of around 80%, and stored at \u221280 \u00b0C until purification. Purification of GlyT1 constructs has been described previously 28 . In brief, the biomass was solubilized in 50 mM Tris-HCl pH 7.5, 150 mM NaCl, 100 \u03bcM Cmpd1 ([5-methanesulfonyl-2-(2,2,3,3,3-pentafluoro-propoxy)-phenyl]-[5-tetrahydro-pyran-4-yloxy)-1,3-dihydro-isoindol-2-yl]-methanone) and 15\u201325 \u03bcM brain polar lipids extract (Avanti), containing either 1% (w\/v) lauryl maltose neopentyl glycol (LMNG) or 1% (w\/v) decyl maltose neopentyl glycol (DMNG) and 0.1% cholesteryl hemisuccinate (CHS). The protein was purified by batch purification using TALON affinity resin (GE Healthcare), then treated with HRV-3C protease (Novagen) to cleave the eGFP\u2013His tag and Roche PNGase F (from Flavobacterium meningosepticum ) to trim glycosylation (Supplementary Fig. 2 ). The transporter was concentrated typically to 15\u201330 mg ml \u22121 in the final buffer, containing 50 mM Tris-HCl pH 7.5, 150 mM NaCl, 50 \u03bcM inhibitor and 15\u201325 \u03bcM brain polar lipids extract, and 0.01% LMNG (w\/v) plus 0.001% CHS for GlyT1, and either 0.05% (w\/v) LMNG plus 0.005% CHS or 0.01% DMNG plus 0.001% CHS for the GlyT1\u2013Lic construct. Lipidic cubic phase crystallization Before crystallization, the concentrated GlyT1 was incubated with Sb_GlyT1#7 in a 1:1.2 molar ratio (GlyT1:sybody) and 1 mM inhibitor. The protein solution was reconstituted into mesophase using molten monoolein (Molecular Dimensions) spiked with 5% (w\/w) cholesterol (Sigma) at a 2:3 ratio of protein solution:lipid, using two coupled Hamilton syringes. Crystallization trials were carried out in 96-well glass sandwich plates (VWR) by a Gryphon LCP crystallization robot or a Mosquito LCP dispensing robot in a humidified chamber, using 50\u2013100 nl of mesophase overlaid with 800 nl of crystallization solution. The plates were incubated at 19.6 \u00b0C and inspected manually. Crystals appeared within 3\u201310 days in 0.1 M ADA pH 7, 13\u201325% PEG600 and 4\u201314% v\/v (\u00b1)-1,3-butanediol, with the longest crystal dimension being 2\u20135 \u03bcm. For crystallization of GlyT1 with Sb_GlyT1#7, we also used 3% v\/v dimethyl sulfoxide, 3% v\/v glycerol, 0.2 M NDSB-201, 0.2 M NDSB-211, 0.2 M NDSB-221, 0.05% w\/v 1,2,3-heptanetriol or 4% v\/v 1,3-propanediol (Hampton research) as additives. The micrometre-sized crystals werecollected from the LCP matrix using MiTeGen MicroMounts, and flash frozen in liquid nitrogen. Data collection and structure determination Crystallographic data were collected on the P14 beamline operated by EMBL Hamburg at the PETRA III storage ring (DESY, Hamburg), using the 5 \u00d7 10 \u03bcm 2 (vertical \u00d7 horizontal) microfocus beam, with a total photon flux of 1.3 \u00d7 10 13 photons per second at the sample position. Diffraction data were recorded on an EIGER 16M detector. In our data-collection strategy, we typically defined a region of interest of 60 \u00d7 14 \u03bcm 2 to 290 \u00d7 340 \u03bcm 2 on the loop, containing crystals oriented perpendicularly to the incoming beam. Diffraction data were collected using serial helical line scans 41 , with a sample displacement of 1 \u03bcm along the rotation axis during the acquisition of one frame, an oscillation of 0.2\u00b0, and an exposure time of 0.1 s, with 100% transmission. Dozor 42 , 43 was used for the first step of data processing to identify diffraction patterns within the large set of frames. Each diffraction image was analysed by Dozor, which determined a list of coordinates for diffraction spots and their partial intensities, and generated a diffraction heat map. Diffraction data were indexed and integrated using XDS 44 , 45 , and the resulting partial mini datasets, containing 3\u201320 consecutive images, were scaled with XSCALE 45 . In some cases, mini datasets with adjacent frame numbers were merged into longer datasets (more than 20 frames) manually. One rotation dataset of 20 frames with an oscillation of 1.0\u00b0 is included in the GlyT1\u2013Lic dataset. Our choice of partial mini datasets to be merged into a high-quality complete dataset was guided by an inhouse script, Ctrl-d, which measured the correlation of each mini dataset to the rest of the mini datasets. The important criterion was the requirement for enough collected datasets to have a scaling model for robust estimation of outliers. We carried out a total of 514 two-dimensional (2D) helical scans on 409 mounted loops containing microcrystals of GlyT1, resulting in the collection of 1,365,232 diffraction patterns, of which 30,837 frames contained more than 15 diffraction spots. We indexed and integrated 229 mini datasets, of which 207, containing 3,400 frames, with a correlation of above 0.7 were scaled and merged (Extended Data Figs. 9a, c ). For GlyT1\u2013Lic, a total of 1,733 2D helical scans were performed on 1,222 mounted loops containing microcrystals, resulting in the collection of 3,190,397 diffraction images of which 225,037 contained 15 spots or more. We indexed and integrated 249 mini datasets, of which 213, containing 3,906 diffraction patterns, with a correlation of above 0.5 were scaled and merged (Extended Data Fig. 9b, d ). The structure of the GlyT1\u2013sybody complex was solved by molecular replacement using modified models of MhsT (PDB ID 4US3) and SERT (PDB ID 6DZZ) (with the loops, TM12 and C-terminal tail removed from the original models), as well as an ASC-binding nanobody (PDB ID 5H8D), as separate search models in Phaser. To solve the structure of GlyT1\u2013Lic, we used the lichenase fusion protein structure (PDB ID 2CIT) as the third search model. The models were refined with Buster, followed by visual examination and manual rebuilding in Coot and ISOLDE 46 , 47 , 48 . The final model of GlyT1 lacks the first 8 residues of the N terminus, residues 235\u2013237 in EL2, residues 309\u2013314 in TM5 and the last 20 residues of the C terminus. Of two lichenase fusion proteins in the asymmetric unit of the GlyT1\u2013Lic structure, only one chain (with higher B factors compared with the other protein chains) has been modelled, owing to the high flexibility of the chains and poor density of the region. The final model of GlyT1\u2013Lic further lacks the first 13 residues of N terminus, residues 235\u2013239 in EL2, residues 309\u2013315 in TM5 and the last 34 residues of the C terminus in chain A; and the first 15 residues of the N terminus, residues 235\u2013239 in EL2, residues 309\u2013315 in TM5 and the last 20 residues of the C terminus in chain B. Of the GlyT1 and GlyT1\u2013Lic residues, 95.4% and 95.01%, respectively, are within the Ramachandran favoured region, with 0.15% (one residue) and 0.26% (four residues) being outliers. The final data and refinement statistics are presented in Extended Data Table 1 . Statistics on data collection were calculated using phenix.table_one 49 . Scintillation proximity assays Scintillation proximity assays (SPAs) were carried out in 96-well plates (Optiplate, Perkin Elmer) using copper His-tag YSi SPA beads (Perkin Elmer) and [ 3 H]Org24598 (80 Ci mmol \u22121 ). Reactions took place in assay buffer containing 50 mM Tris-HCl pH 7.5, 150 mM NaCl and 0.001% LMNG supplemented with solubilized GlyT1 cell membrane\/SPA mix (0.3 mg per well) and for competition experiments, a tenfold serial dilution series of nonlabelled inhibitor Cmpd1 (final concentration 0.001 nM to 10 \u03bcM), bitopertin (0.001 nM to 10 \u03bcM), or glycine (0.1 nM to 1 mM). Assays were incubated for 1 h at 4 \u00b0C before values were read out using a top count scintillation counter at room temperature. In thermal shift (TS) scintillation proximity assays (SPA\u2013TS), solubilized protein was incubated for 10 min with a temperature gradient of 23\u201353 \u00b0C across the wells in a Techne Prime Elite thermocycler before mixing with SPA beads. FSEC\u2013TS A fluorescence-detection size-exclusion chromatography thermostability (FSEC\u2013TS) assay was used to evaluate the thermostability of constructs 50 . We dispensed 180-\u03bcl aliquots of solubilized GlyT1-containing cell membrane in a 4 \u00b0C cooled 96-well polymerase chain reaction (PCR) plate (Eppendorf) in triplicates. A gradient of 30\u201354 \u00b0C for 10 min was applied on the plate in a BioRad Dyad thermal cycler. The plate was cooled on ice and 40 \u03bcl of the samples were injected into a 300-mm Sepax column in 50 mM Tris-HCl pH 7.5, 150 mM NaCl and 0.001% LMNG; the SEC profile was monitored using the fluorescence signal from the eGFP tag. Thermofluor stability assay For Thermofluor stability assays, we used a GlyT1 minimal construct (containing N- and C-terminal deletions of residues 1\u201390 and 685\u2013706, respectively), expressed in Sf9 insect cells and purified as above. Purified GlyT1 minimal was diluted in 50 mM Tris-HCl pH 7.5, 150 mM NaCl and 0.001% LMNG to a final concentration of 0.73 \u03bcM and distributed into the wells of a 96-well PCR plate on ice. The inhibitor was added to the wells at a final concentration of 10 \u03bcM, and a corresponding amount of dimethylsulfoxide (DMSO) was added to the control wells. The plate was sealed and incubated for 30 min on ice. A 1:40 (v\/v) working solution of the CPM ( N -[4-(7-diethylamino-4-methyl-3-coumarinyl)phenyl]maleimide) dye stock (4 mg ml \u22121 in DMSO) was prepared; 10 \u03bcl of this solution was added to 75 \u03bcl of protein sample in each well and mixed thoroughly. We adapted a published assay 51 based on CPM dye to perform the stability tests. The melting profiles were recorded using a real-time PCR machine (Rotor-Gene Q, Qiagen) with temperature ramping from 15 \u00b0C to 95 \u00b0C at a heating rate of 0.2 \u00b0C s \u22121 . The melting temperatures ( T m ) were calculated from the point of inflection, based on a fit to the Boltzmann equation. Molecular modelling We used the 3D conformer generator Omega (OpenEye) to generate a conformational ensemble for bitopertin. Each conformer was superimposed via ROCS (OpenEye) 52 onto the transporter-bound conformation of Cmpd1, and the overlay was optimized with respect to similarity of 3D shapes. The highest-scoring conformer was retained and energy-minimized within the binding pocket using MOE 53 . Docking was performed using the software GOLD 54 from the Cambridge Crystallographic Data Centre (CCDC) with default settings and the standard scoring function ChemPLP. An additional energy minimization within the binding pocket was performed using the five best docking poses. Rapido was used for structure superpositions 55 . A total number of 513, 414 and 393 residues were used to align the structures of SERT (PDB ID 6DZZ), LeuT (PDB ID 3TT3) and MhsT (PDB ID 4US3) on that of GlyT1. Residue ranges used for alignment were 104\u2013224, 226\u2013232, 259\u2013306, 316\u2013353, 357\u2013388, 390\u2013433, 438\u2013489, 491\u2013632 and 636\u2013652 of GlyT1 and 83\u2013152, 154\u2013204, 206\u2013212, 222\u2013239, 242\u2013271, 281\u2013318, 322\u2013353, 355\u2013398, 404\u2013597 and 600\u2013616 of SERT in the SERT\u2013GlyT1 superposition; 115, 117\u2013211, 215\u2013219, 262\u2013270, 272\u2013278, 281, 288\u2013307, 317\u2013352, 354\u2013374, 376\u2013387, 390\u2013421, 429\u2013489, 496\u2013519, 522\u2013530, 532\u2013559 and 568\u2013592 of GlyT1 and 21\u201368, 71\u201373, 76\u201380, 82\u201387, 90\u2013123, 126\u2013130, 141\u2013156, 160, 166\u2013185, 196\u2013217, 222\u2013240, 242\u2013257, 259\u2013270, 273\u2013291, 293\u2013305, 307\u2013312, 318\u2013372, 374\u2013406, 408\u2013435 and 444\u2013468 of LeuT in the GlyT1\u2013LeuT superposition; and 119\u2013173, 176\u2013210, 264\u2013271, 318\u2013352, 358\u2013422, 432\u2013487, 532\u2013554, 568\u2013595, 493\u2013517 and 287\u2013306 of GlyT1 and corresponding residues 28\u201382, 88\u2013122, 134\u2013141, 178\u2013212, 218\u2013282, 284\u2013339, 389\u2013411, 421\u2013448, 343\u2013367 and 148\u2013167 of MhsT in the GlyT1\u2013MhsT superposition. [ 3 H]Glycine-uptake assay We carried out glycine-uptake assays for the wild-type and crystallization constructs of GlyT1 and for untransfected cells in n = 5, n = 4 and n = 3 independent experiments, respectively, each performed with 6\u201311 replicate measurements of total and nonspecific uptake. Mammalian HEK293-MSR cells (Invitrogen; not authenticated and not tested for mycoplasma contamination) were plated at a density of 40% in 96-well plates and were transfected with 0.1 \u03bcg of DNA (in pXOON plasmids) per well in complex with Ecotransfect transfection reagent (OZ Bioscience), along with untransfected cells, 48 h before uptake assays. The medium was aspirated after 48 h and the cells were washed with uptake buffer containing 10 mM HEPES-Tris pH 7.4, 150 mM NaCl, 1 mM MgSO 4 , 5 mM KCl and 10 mM (+) d -glucose. The cells were incubated for 30 min at 22 \u00b0C with the uptake buffer containing no inhibitor (total uptake) or 10 \u03bcM Cmpd1 (nonspecific uptake). Glycine uptake was initiated by adding either [ 3 H]glycine (15 Ci mmol \u22121 ) to a final concentration of 1 \u03bcM for total uptake or [ 3 H]glycine (15 Ci mmol \u22121 ) and Cmpd1 to a final concentration of 1 \u03bcM and 10 \u03bcM, respectively, for nonspecific uptake. The plates were incubated for 10 min or for variable time points and radiotracer-uptake reactions were stopped by aspiration of the substrate followed by washing with 200 \u03bcl of the uptake buffer in an automated plate washer. The cells were then lysed with Microscint 20 (Perkin Elmer) and shaken for 1 h; radioactivity was measured by a Topcounter NXT (Packard). Specific uptake was determined by subtracting nonspecific uptake from total uptake. Statistical significance was determined using one sample t -tests with alpha = 0.05. [ 3 H]Glycine-uptake-inhibition assay Glycine-uptake-inhibition assays were performed in quadruplicate and according to a method previously described 24 . In brief, mammalian Flp-in-CHO cells (Invitrogen; authenticated and free of mycoplasma contamination) were transfected with human and mouse GlyT1 and human GlyT2 cDNA and were plated at a density of 40,000 cells per well in complete F-12 medium 24 h before uptake assays. The medium was aspirated the next day and the cells were washed twice with uptake buffer containing 10 mM HEPES-Tris pH 7.4, 150 mM NaCl, 1 mM CaCl 2 , 2.5 mM KCl, 2.5 mM MgSO 4 and 10 mM (+) d -glucose. The cells were incubated for 20 min at 22 \u00b0C with no inhibitor, 10 mM nonradioactive glycine, or a concentration range of the inhibitor to calculate IC 50 value. A solution containing 25 \u03bcM nonradioactive glycine and 60 nM [ 3 H]glycine (11\u201316 Ci mmol \u22121 ) (hGlyT1 and mGlyT1) or 200 nM [ 3 H]glycine (hGlyT2) was then added. Nonspecific uptake was determined with 10 \u03bcM Org24598 (a hGlyT1 and mGlyT1 inhibitor) 27 , or 5 \u03bcM Org25543 (a hGlyT2 inhibitor) 56 . The plates were incubated for 15 min (hGlyT1) or 30 min (mGlyT1 and hGlyT2) with gentle shaking, and reactions were stopped by aspiration of the mixture and washing three times with ice-cold uptake buffer. The cells were lysed and shaken for 3 h; radioactivity was measured by a scintillation counter. The assays were performed in quadruplicate. To evaluate the mode of inhibition of Cmpd1, we carried out glycine-uptake assays for wild-type GlyT1 as described in the section \u2018[ 3 H]Glycine-uptake assay\u2019 above. The assays were performed in four independent experiments, each with two replicate measurements for total uptake and one replicate measurement for nonspecific uptake. Experiments to generate all four K m \u2212 V max curves for the inhibitor were performed simultaneously on the same 96-well plate. Glycine uptake was initiated by adding the specified concentrations of [ 3 H]glycine (15 Ci mmol \u22121 ) mixed with unlabelled glycine in a 1:1,000 ratio (10 \u03bcM, 25 \u03bcM, 50 \u03bcM, 100 \u03bcM, 200 \u03bcM, 350 \u03bcM, 500 \u03bcM and 700 \u03bcM) and mixed with 0 nM, 60 nM, 240 nM and 960 nM of Cmpd1. The plate was incubated for 10 min, and radiotracer-uptake reactions were stopped by aspiration of the substrate followed by washing with 200 \u03bcl of the uptake buffer in an automated plate washer. The cells were then lysed with Microscint 20 (Perkin Elmer) and shaken for 1 h; radioactivity was measured by a Topcounter NXT (Packard). Specific uptake was determined by subtracting nonspecific uptake from total uptake. Statistical significance was determined using one sample t -tests with alpha = 0.05. Data were fitted to Michaelis\u2013Menten kinetics using nonlinear regression and transformed to Eadie\u2013Hofstee plots with subsequent linear regression analysis using GraphPad Prism 9. [ 3 H]Org24598-binding assay [ 3 H]Org24598-binding experiments were performed in quadruplicate as described 24 . Membranes from Chinese hamster ovary (CHO) cells expressing hGlyT1 and membranes extracted from mouse forebrains (expressing mGlyT1) were used for binding assays. Saturation isotherms were determined by adding [ 3 H]Org24598 to mouse forebrain membranes (40 \u03bcg per well) and cell membranes (10 \u03bcg per well) in a total volume of 500 \u03bcl for 3 h at room temperature. Membranes were incubated with 3 nM [ 3 H]Org24598 and ten concentrations of Cmpd1 for 1 h at room temperature. Reactions were terminated by filtering the mixture onto a Unifilter with bonded GF\/C filters (PerkinElmer) presoaked in binding buffer containing 50 mM sodium-citrate pH 6.1, for 1 h and washed three times with 1 ml of the same cold binding buffer. Filtered radioactivity was counted on a scintillation counter. Nonspecific binding was measured in the presence of 10 \u03bcM Org24598. Figure preparation Figures showing protein structures were prepared using the PyMOL 2.3.3 Incentive Product from Schrodinger, LLC. Sequences were aligned using ClustalOmega 57 and the relevant figure prepared using BOXSHADE 3.2. Binding and uptake data were analysed and figures prepared using GraphPad Prism 9. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this paper. Data availability Coordinates and structure factors for the structures of GlyT1 at 3.4 \u00c5 and 3.9 \u00c5 resolution have been deposited in the Protein Data Bank (  ) under accession codes 6ZBV and 6ZPL , respectively. The executable Ctrl-d is available via webapps.embl-hamburg.de. ","News_Body":"Glycine can stimulate or inhibit neurons in the brain, thereby controlling complex functions. Unraveling the three-dimensional structure of the glycine transporter, researchers have now come a big step closer to understanding the regulation of glycine in the brain. These results, which have been published in Nature, open up opportunities to find effective drugs that inhibit GlyT1 function, with major implications for the treatment of schizophrenia and other mental disorders. Glycine is the smallest amino acid and a building block of proteins, and also a critical neurotransmitter that can both stimulate or inhibit neurons in the brain and thereby control complex brain functions. Termination of a glycine signal is mediated by glycine transporters that reuptake and clear glycine from the synapses between neurons. Glycine transporter GlyT1 is the main regulator of neurotransmitter glycine levels in the brain, and also important for e.g. blood cells, where glycine is required for the synthesis of heme. The N-methyl-D-aspartate (NMDA) receptor is activated by glycine, and its poor performance is implicated in schizophrenia. Over the past twenty years, many pharmaceutical companies and academic research laboratories therefore have focused on influencing glycinergic signaling and glycine reuptake delay as a way of activating the NMDA receptor in search of a cure for schizophrenia and other psychiatric disorders. Indeed, several potent and selective GlyT1 inhibitors achieve antipsychotic and pro-cognitive effects alleviating many symptoms of schizophrenia, and have advanced into clinical trials. However, a successful drug candidate has yet to emerge, and GlyT1 inhibition in blood cells is a concern for side effects. Structural insight into the binding of inhibitors to GlyT1 would provide insight in finding new strategies in drug design. To gain better knowledge about the three-dimensional structure and inhibition mechanisms of the GlyT1 transporter, researchers from the companies Roche and Linkster, and from the European Molecular Biology Laboratory (EMBL) Hamburg, University of Zurich and Aarhus University, have therefore collaborated on investigating one of the most advanced GlyT1 inhibitors. Using a synthetic single-domain antibody (Linkster therapeutics' sybody) for GlyT1, the research team managed to grow microcrystals of the inhibited GlyT1 complex. By employing a Serial Synchrotron Crystallography (SSX) approach, the team lead by Assistant Professor Azadeh Shahsavar and Professor Poul Nissen from the Department of Molecular Biology and Genetics\/DANDRITE, Aarhus University, determined the structure of human GlyT1 using X-ray diffraction data from hundreds of microcrystals. The SSX method is particularly suitable as a method fornew, powerful X-ray sources and opens up for new approaches to, among other things, the development of drugs for various purposes. The structure is reported in the leading scientific journal Nature and also unveils a new mechanism of inhibition in neurotransmitter transporters in general. Mechanisms have previously been uncovered for, for example, inhibition of the serotonin transporter (which has many similarities to GlyT1) with antidepressant drugs, but it is a quite different inhibition mechanism now found for GlyT1. It provides background knowledge for the further development of small molecules and antibodies as selective inhibitors targeted at GlyT1 and possibly also for new ideas for the development of inhibitors of other neurotrandmitter carriers that can be used to treat other mental disorders. Azadeh Shahsavar's team continues the studies of GlyT1 and will be investigating further aspects of its function and inhibition and the effect of GlyT1 inhibitors in the body. ","News_Title":"Determination of glycine transporter opens new avenues in development of psychiatric drugs","Topic":"Medicine"}
{"Paper_Body":"Abstract Metal nanoclusters composed of noble elements such as gold (Au) or silver (Ag) are regarded as superatoms. In recent years, the understanding of the materials composed of superatoms, which are often called superatomic molecules, has gradually progressed for Au-based materials. However, there is still little information on Ag-based superatomic molecules. In the present study, we synthesise two di-superatomic molecules with Ag as the main constituent element and reveal the three essential conditions for the formation and isolation of a superatomic molecule comprising two Ag 13\u2212 x M x structures (M = Ag or other metal; x = number of M) connected by vertex sharing. The effects of the central atom and the type of bridging halogen on the electronic structure of the resulting superatomic molecule are also clarified in detail. These findings are expected to provide clear design guidelines for the creation of superatomic molecules with various properties and functions. Introduction Metal nanoclusters (NCs) 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , 14 composed of noble metal elements such as gold (Au) and silver (Ag) are stabilised when the total number of valence electrons satisfies the closed-shell electronic structure, as in conventional atoms 15 , 16 . Such metal NCs are regarded as superatoms (artificial atoms). If superatoms are used to assemble materials, it might be possible to create materials with physicochemical properties and functions that are different from those of conventional materials 17 . Regarding such materials composed of superatoms (often called superatomic molecules 18 , 19 ), since the 1980s, there have been many reports of Au-based superatomic molecules, which Teo and Zhang called clusters of clusters 20 . Subsequent work by groups such as Tsukuda 21 , Nobusada 22 , Jin 23 and Zhu 24 has gradually improved our understanding of the types of superatomic molecules that can be produced and the electronic structures that can be created 25 . Ag NCs have multiple properties and functions that are superior to those of Au NCs, including photoluminescence (PL) with high quantum yield 26 and selective catalytic activity for carbon dioxide reduction 27 . However, there are only a limited number of reports, including the report 28 by the authors, on Ag-based superatomic molecules 29 , 30 , 31 , 32 . To construct substances using superatomic molecules and create new materials, it is essential to gain a deeper understanding of the types of superatomic molecules that can be produced and the electronic structures that can be created, even for Ag-based superatomic molecules. In the present study, we focus on Ag-based 13-atom NCs (Ag 13\u2212 x M x ; M = Ag or other metal; x = number of M) as superatoms, and aim to elucidate the key factors in the formation of di-superatomic molecules by vertex sharing 33 and the electronic structure of the obtained di-superatomic molecules. Platinum (Pt) or palladium (Pd) was used as the element that substitutes part of the Ag, and chloride (Cl) or bromide (Br) was used as the bridging ligand to support the connection of the two 13-atom NCs. To achieve our purpose, in addition to two previously reported di-superatomic molecules ([Ag 23 Pt 2 (PPh 3 ) 10 Cl 7 ] 0 ( 1 ); Fig. 1a ; PPh 3 = triphenylphosphine) 31 and ([Ag 23 Pd 2 (PPh 3 ) 10 Cl 7 ] 0 ( 2 ); Fig. 1b ) 28 , we synthesised two new superatomic molecules with Br as the bridging ligand ([Ag 23 Pt 2 (PPh 3 ) 10 Br 7 ] 0 ( 3 ) and [Ag 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 ( 4 ); Table 1 ). We investigated their geometric\/electronic structures and their stabilities with regard to degradation in solution. Consequently, we confirmed that 3 and 4 both have a geometric\/electronic structure that qualifies them as superatomic molecules. Regarding the electronic structure, we further observed that (1) there is a peak attributable to the metal core at approximately 600 nm in the optical absorption spectra of all the superatomic molecules; (2) such peaks shift to longer wavelengths when M is changed from Pt to Pd; (3) all 1 \u2212 4 exhibit PL in visible-to-near infra-red (NIR) region; and (4) PL peaks shift to longer wavelengths when M is changed from Pt to Pd. With respect to the stability of the superatomic molecule described by [Ag 23 M 2 (PPh 3 ) 10 X 7 ] z (M = Ag, Pd, or Pt; X = Cl or Br; z = 2+ or 0), we found that the stability decreases in the order 1 > 3 > 2 > 4 (which can be synthesised) > [Ag 25 (PPh 3 ) 10 X 7 ] 2+ (X = Cl or Br; which are not so stable in solution). Based on these results and reports on the related superatomic molecules, we concluded that the following three conditions are essential for the formation and isolation of a superatomic molecule consisting of two Ag 13\u2212 x M x structures (M = Ag or other metal) connected by vertex sharing ([Ag 25\u2212 x M x (PR 3 ) 10 X y ] z ; PR 3 = phosphine; y = number of X): (1) a halogen ligand of a size that can maintain a moderate distance between two Ag 13\u2212 x M x structures is used as the bridging ligand; (2) an icosahedral core, which is stronger than Ag 13 , is formed by heteroatom substitution; and (3) [Ag 25\u2212 x M x (PR 3 ) 10 X y ] z comprises substituted heteroatoms and bridging halogens such that the total number of valence electrons is 16 when they are cationic or neutral. Fig. 1: Comparison of the geometric structures. a 1 . b 2 . c 3 . d 4 . The geometric structure of 1 and 2 are reproduced from ref. 28 , 31 , respectively (grey = Ag; orange = Pt; blue = Pd; green = Cl; dark grey = Br; magenta = P). The positions of the Pd atoms are the predicted positions based on DFT calculations. Full size image Table 1 NC number, chemical composition, number of bridging halogens, number of total valence electrons, and references to literature on Ag-based di-superatomic molecules described in the present paper. Full size table Results and discussion Synthesis and geometric structure An NC mixture containing 3 was first prepared by adding sodium borohydride (NaBH 4 ) methanol solution to a methanol solution containing silver nitrate (AgNO 3 ), PtBr 2 , PPh 3 , and NaBr in the dark. The by-products were then removed by washing with the solvent, and the product was crystallised to obtain high-purity 3 (Fig. S1a ) 28 . Electrospray ionisation-mass spectrometry (ESI-MS) of the product showed that 3 has a chemical composition of [Ag 23 Pt 2 (PPh 3 ) 10 Br 7 ] 0 (Fig. S2 ). X-ray photoelectron spectroscopy (XPS; Fig. S3 ) confirmed the presence of Pt in 3 . We also obtained 4 as single crystals using a process similar to that used to synthesise 3 , except that PdBr 2 was used instead of PtBr 2 (Fig. S1b ). X-ray photoelectron spectroscopy (XPS; Ag : Pd = 23 : 1.5; Fig. S4 ) confirmed the presence of Pd in 4 . Figure 1c shows the geometric structure of 3 determined by single crystal X-ray diffraction (SC-XRD) analysis (Fig. S5 , Table S1 and Supplementary Data 1 , 2 ). We found that 3 has a geometric structure in which two icosahedral Ag 12 Pt molecules are connected by vertex sharing. Pt was located at the central position in each icosahedral Ag 12 Pt molecule, as often seen in the literatures 34 , 35 . This structure is similar to the geometric structure of 1 , as previously reported (Fig. 1a ) 31 . The SC-XRD analysis of 3 did not confirm the presence of counter ions (Fig. S5 ), again supporting the interpretation that 3 was isolated as a neutral NC ([Ag 23 Pt 2 (PPh 3 ) 10 Br 7 ] 0 ). These results demonstrate that both 1 and 3 have 16 valence electrons (Table 1 ) 16 . Therefore, the two Ag 12 Pt structures in 3 are described as [Ag 12 Pt] 4+ , indicating that both have a closed-shell electronic structure that satisfies the 1S 2 1P 6 superatom orbital (Fig. S6 ) 15 , 19 . We concluded from these results that 3 is a NC that can be regarded as a di-superatomic molecule, similar to 1 . Figure 1d shows the geometric structure of 4 (Fig. S7 , Table S1 and Supplementary Data 3 , 4 ). As you can see, 4 has a geometric structure in which two icosahedral Ag 12 Pd structures are connected by vertex sharing, which is similar to the geometric structure of the previously reported 2 (Fig. 1b ) 28 . In addition, 4 was also isolated as a neutral molecule (Fig. S7 ), indicating that each Ag 12 Pd structure in 4 has a closed-shell electronic structure that satisfies 1S 2 1P 6 (Fig. S6 ) 15 , 19 . We concluded from these results that 4 is also a NC that can be considered a di-superatomic molecule. Unfortunately, it is difficult to determine the Pd position for 4 by SC-XRD alone because Pd ( 46 Pd) and Ag ( 47 Ag) have a similar number of electrons. However, Pd (1.920 J m \u22122 for Pd(111)) 36 has a higher surface energy than Ag (1.172 J m \u22122 for Ag(111)) 36 , and Pd is generally located in the centre of the icosahedral structure in Ag 12 Pd 28 , 34 , 35 , 37 , 38 . We performed density functional theory (DFT) calculations for [Ag 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 with different Pd positions using the Perdew\u2013Burke\u2013Ernzerhof (PBE) functional to confirm that Pd is located at the centre of the two icosahedral structures in 4 as in 2 . The results showed that [Ag 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 is stable for Pd positions in the order of the icosahedral centre (i) > the icosahedral surface (ii) > the shared vertex (iii) (Fig. S8 ). Based on these results, we concluded that the two Pd atoms are located in the centre of the icosahedral structure in 4 (Fig. 1d ), as in 2 . In this way, overall 1 \u2212 4 have similar geometric structures. However, a detailed look at their geometric structures revealed some differences between 1 and 2 , which use Cl as the bridging halogen, as well as between 3 and 4 , which use Br as the bridging halogen. The most striking difference is that there is a twist between the two Ag 12 M structures (M = Pt or Pd) in 3 (dihedral angles \u03b8 = 9.02 \u2212 11.85\u00b0) and 4 ( \u03b8 = 9.90 \u2212 12.97\u00b0), unlike in 1 and 2 (both \u03b8 = 0\u00b0) (Fig. 2a\u2013d and S9 ). Br \u2212 (1.95 \u00c5) 39 has a larger ionic radius than Cl \u2212 (1.81 \u00c5) 39 , and the Ag\u2212Br bond (2.619\u22122.659 \u00c5 for 3 ) has a longer bond length than the Ag\u2212Cl bond (2.444\u22122.532 \u00c5 for 1 ) (Fig. S10 ). Therefore, if there is no twist in the two Ag 12 M structures (M = Pt or Pd) in 3 and 4 , the distance between the two Ag 12 M structures in those molecules should be longer than in 1 and 2 (Fig. S11 ). This would induce: (1) an increase in the distance between the shared Ag and the Ag bonded to it; and (2) a structural distortion of the individual Ag 12 M cores (Fig. S11 ), ultimately leading to the instability of the individual Ag 12 M structures (M = Pt or Pd). For 3 and 4 , it can be considered that the formation of such an unstable geometric structure is suppressed by twisting between the two Ag 12 M structures (M = Pt or Pd) (Fig. 2e and S12 ). Fig. 2: Structural analysis of the twist between the two Ag 12 M structures (M = Pt or Pd). a \u2013 d View from the long-axis direction for 1 , 2 , 3 and 4 , respectively, showing the twist between the two Ag 12 M structures (M = Pt or Pd) in the cores of 3 and 4 (grey = Ag; orange = Pt; blue = Pd; green = Cl; dark grey = Br; magenta= P). The geometric structure of 1 and 2 are reproduced from ref. 28 , 31 , respectively. In a \u2212 d , \u03b8 ave indicates the average dihedral angle between the two Ag 12 M structures. e Comparison of the Ag\u2212Ag bond length between the joining Ag and the neighbouring Ag (green line), showing that the bond lengths are quite similar in 1 \u2212 4 . Full size image Regarding such superatomic molecules using Br as the bridging halogen, a similar twist between two icosahedral metal cores was not observed in [Au 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 ( 5 ) 40 using Au as the base element, as reported by Zhu and colleagues (Fig. S13a ). The Au\u2212Br bond (2.569\u22122.583 \u00c5 for 5 ) is shorter than the Ag\u2212Br bond (2.619\u22122.659 \u00c5 for 3 ) (Fig. S13b ). Therefore, there may be no need for a twist between the two Au 12 Pd structures in 5 to preserve the individual icosahedral structures (Fig. S13c ). Namely, in [Au 23 Pd 2 (PPh 3 ) 10 X 7 ] 0 (X = halogen), the distance between the two Au 12 Pd structures is estimated to be relatively moderate when Br is used as a bridging halogen. Indeed, to the best of our knowledge, there have been no reports of the isolation of [Au 23 M 2 (PPh 3 ) 10 Cl 7 ] 0 (M = Pt or Pd) using Cl, which has a smaller ionic radius to Br, as the bridging halogen. It is assumed that [Au 23 M 2 (PPh 3 ) 10 Cl 7 ] 0 (M = Pt or Pd) is difficult to isolate because the distance between the two Au 12 M structures is too small, and it is excessively structurally stressful for the individual Au 12 M structures. A second notable difference is that the PPh 3 structure is located slightly further from the long axis of the superatomic molecule in 3 and 4 compared with in 1 and 2 (Fig. 3 ). Because Br has a larger ionic radius than Cl, there might be a slight steric hindrance between the terminal Br and PPh 3 in 3 and 4 . This seems to produce less variation in the spread angle of the long axis and PPh 3 of the superatomic molecule (Fig. S14a ), and the length of the Ag\u2212P (P = phosphorus) bond (Fig. S14b ) in 3 and 4 compared with in 1 and 2 . Note that 3 has more variation in Ag\u2212Ag bond length than 1 (Fig. S15 ). Fig. 3: Structural analysis of the ligand positions. a \u2013 d View from the long-axis direction for 1 , 2 , 3 and 4 , respectively, showing the average distance between the central long axis and the positions of the P atoms (Dave) (grey = Ag; orange = Pt; blue = Pd; green = Cl; dark grey = Br; magenta = P). The geometric structures of 1 and 2 are reproduced from ref. 28 , 31 , respectively. Full size image Therefore, the type of bridging halogen induces slight differences in the geometric structures of the obtained superatomic molecules. However, in all of 1 \u2212 4 , Ag 12 M structures (M = Pt or Pd) is bridged by five halogens, which is common to all of the geometric structures in 1 \u2212 4 . Meanwhile, in previous work on the connection of Au 7 Ag 6 , Teo et al. successfully synthesised [Au 13 Ag 12 (P( p -Tol) 3 ) 10 Cl 7 ](SbF 6 ) 2 (P( p -Tol) 3 = tri( p -tolyl)phosphine; SbF 6 \u2212 = hexafluoroantimonate; 6 ) 41 bridged by five Cl atoms, and [Au 13 Ag 12 (PPh 3 ) 10 Cl 8 ](SbF 6 ) ( 7 ) 42 bridged by six Cl atoms. In addition, when Br was used as the bridging halogen, they successfully synthesised [Au 13 Ag 12 (PPh 3 ) 10 Br 8 ](SbF 6 ) ( 8 ) 43 , [Au 12 Ag 13 (P( p -Tol) 3 ) 10 Br 8 ](PF 6 ) (PF 6 \u2212 = hexafluorophosphonate; 9 ) 44 , and [Au 13 Ag 12 (PPh 3 ) 10 Br 8 ]Br ( 10 ) 45 bridged by six Br atoms, and even [Au 13 Ag 12 (PMePh 2 ) 10 Br 9 ] 0 (PMePh 2 = methyldiphenylphosphine; 11 ) 46 bridged by seven Br atoms. Although 7 \u2212 11 are connected by a different number of bridging halogens from 1 \u2212 4 and 6 (Fig. S16 and Table 1 ), the total number of valence electrons is estimated to be 16 in all cases 16 . Therefore, 7 \u2212 11 are also considered to be a di-superatomic molecule with two Au 7 Ag 6 or Au 6 Ag 7 structures connected by vertex sharing. However, in the present study, the formation of superatomic molecules bridging two Ag 12 M structures (M = Pt or Pd) with six or seven halogens (X = Cl or Br), such as [Ag 23 M 2 (PPh 3 ) 10 X 8 ] \u2212 (the total number of valence electrons = 16) or [Ag 23 M 2 (PPh 3 ) 10 X 9 ] 2\u2212 (the total number of valence electrons = 16), was not observed. These anions would be readily oxidised under atmospheric conditions 47 , 48 , leading to a change in the total number of valence electrons of [Ag 23 M 2 (PPh 3 ) 10 X 8 ] 0 and [Ag 23 M 2 (PPh 3 ) 10 X 9 ] 0 from 16 16 to 15 or 14, respectively. In these cases, the individual Ag 12 M structures do not necessarily have closed-shell electronic structures. This explains why [Ag 23 M 2 (PPh 3 ) 10 X 8 ] \u2212 and [Ag 23 M 2 (PPh 3 ) 10 X 9 ] 2\u2212 were not produced in our study. Similarly, Teo et al. only reported the formation of [Au 11 Ag 12 Pt 2 (PPh 3 ) 10 Cl 7 ] 0 ( 12 ) bridged by five Cl atoms for a superatomic molecule with Pt at the centre of the metal core 49 . Kappen et al. also only reported [Au 10 Ag 13 Pt 2 (PPh 3 ) 10 Cl 7 ] 0 ( 13 ) 50 bridged by five Cl atoms for superatomic molecules containing Pt at the centre of the metal core. It is assumed that [Au 11 Ag 12 Pt 2 (PPh 3 ) 10 Cl 7 ] \u2212 and [Au 10 Ag 13 Pt 2 (PPh 3 ) 10 Cl 6 ] \u2212 could not be isolated in their study for the same reason. Electronic structure Figure 4a\u2013d shows the optical absorption spectra of dichloromethane solutions of 1 \u2212 4 , respectively. The optical absorption spectra are generally similar in shape, but the peak structure shifts to a longer wavelength when the central atom is changed from Pt to Pd. Fig. 4: Optical absorption spectra and analyses. a \u2013 d Optical absorption spectra of 1 , 2 , 3 and 4 , respectively. e , f Density of states of 3\u2032 and 4\u2032 , respectively. In e and f , a, b, a\u2032 and b\u2032 correspond to the peaks labelled as such in c and d (red = ligand; green = Ag 23 M 2 (sp); blue = Ag 23 M 2 (d)). Full size image Both 1 and 2 belong to the D 5h point group 28 , 31 , 32 . Based on the calculated electronic structures of [Ag 23 Pt 2 (PPh 3 ) 10 Cl 7 ] 0 ( 1\u2032 ) and [Ag 23 Pd 2 (PPh 3 ) 10 Cl 7 ] 0 ( 2\u2032 ), the peak of the first absorption band on the longer wavelength side is attributed to an allowed transition between the orbitals originated from the core ( a 2 \u02b9\u02b9\u2192 a 1 \u02b9) (Fig. 5 ) 28 , 31 , 32 . The second peak that appears on the shorter wavelength side in the absorption spectrum is attributed to a charge transfer transition from the a 2 \u02b9\u02b9 orbital originating from the core to the orbital with charge distribution around PPh 3 (Table S2 ). With regard to the change in peak position due to the difference in the central atom, our previous studies have shown that changing the central atom from Pt to Pd causes a red shift in the peak structure due to a decrease in the energy of the orbitals near the lowest unoccupied molecular orbital (LUMO) 28 . Fig. 5: Orbital energies and Kohn\u2013Sham orbital diagram related to the first peak in the optical absorption spectrum. a , b , c , and d are Kohn\u2013Sham orbital diagram of 1\u2019 , 2\u2019 , 3\u2019 , and 4\u2019 , respectively. The transition dipole moment from HOMO to LUMO becomes zero. This is the reason why the HOMO\u2212LUMO transition is forbidden for 1 \u2212 4 . Full size image We also performed DFT calculations for 3 and 4 in the present study. The geometric structures ([Ag 23 Pt 2 (PPh 3 ) 10 Br 7 ] 0 ( 3\u2032 ; Fig. S17 ) and [Ag 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 ( 4\u2032 ; Fig. S18 )), and the electronic structures (Fig. S19 ) calculated using the PBE functional both reproduced the experimental results well. Both 3\u2032 and 4 \u2032 belong to the D 5 point group. Based on the calculated electronic structures of 3\u2032 and 4\u2032 , the peak of the first absorption band on the longer wavelength side is attributed to an allowed transition between orbitals originating from the core ( a 2 \u2192 a 1 ) (Fig. 5 ). The second peak that appears on the shorter wavelength side is attributed to a charge transfer transition from the a 2 orbital originating from the core to the orbital with charge distribution around PPh 3 (Table S2 ). There was no significant difference in the energy of the highest occupied molecular orbital (HOMO) between 3\u2032 and 4\u2032 , and the HOMOs were similar in energy compared with those of 1\u2032 and 2\u2032 . However, the orbital ( a 1 ) energy on the LUMO side was much lower in 4\u2032 than in 3\u2032 (Figs. 4e, f , 5c, d ). The red shift in the peak structure caused by the change of the central atom from Pt to Pd was found to be due to these factors, similar to the case when Cl is used as the bridging halogen. We have also investigated the possibility that the red shift in the peak structure is caused by the twist due to the use of Br as a bridging halogen. Specifically, we have calculated the optical absorption spectra also for [Ag 23 M 2 (PPh 3 ) 10 Br 7 ] 0 (M = Pt or Pd) without distortion (Figs. S20 , S21 ). The results demonstrated that the optical absorption spectrum changes only a little depending on the twist, supporting the above interpretation that the main reason for the red shift in the peak structure is the change of the central atom from Pt to Pd. For 1 \u2212 4 , it is difficult to estimate the HOMO\u2212LUMO gap of each superatomic molecule from its optical absorption spectrum because the HOMO\u2212LUMO transition is forbidden (Fig. 5 ). Therefore, we estimated the HOMO\u2212LUMO gap of each di-superatomic molecule based on the 1\u2032 \u2212 4\u2032 electronic structure obtained by DFT calculations. As a result, 1\u2032 \u2212 4\u2032 were estimated to have HOMO\u2212LUMO gaps of 1.66, 1.55, 1.66 and 1.52 eV, respectively (Table S3 ). These results indicate that the change of the central atom from Pt to Pd also induces a decrease in the HOMO\u2212LUMO gap. Although we also attempted to conduct the electrochemical experiment 11 to experimentally determine the HOMO\u2212LUMO gap, unfortunately, we could not obtain a reliable voltammogram due to the lack of the required amount of the obtained crystals. Regarding the electronic structure, we also measured PL spectra of 1 \u2212 4 (Fig. 6 ). The results demonstrated that (1) 1 \u2212 4 exhibit PL in the visible-to-NIR region and (2) PL peak positions of 2 and 4 are red-shifted compared to those of 1 and 3 . This trend is well consistent with that of optical absorption, implying that HOMO and LUMO regions (Fig. 5 ) are related to the PL of 1 \u2212 4 . Fig. 6: PL spectra obtained for the toluene solution of 1 \u2212 4 at 25 \u00b0C. The toluene solutions of 1 \u2212 4 were excited by the light of 451, 487, 459 and 496 nm, respectively (orange = 1 , dark blue = 2 , yellow = 3 , purple = 4 ). In this figure, the vertical axis is normalised to eliminate the effect of the difference of the concentration of the 1 \u2212 4 on the PL intensity. Full size image Stability We investigated the stabilities of 1 \u2212 4 with regard to degradation in toluene or dichloromethane solution by optical absorption spectroscopy. Ag NCs generally have low photostability 31 . Furthermore, in the present study, we also dealt with less stable and less easily formed superatomic molecules to clarify the necessary conditions for the formation of a superatomic molecule composed of Ag 13\u2212 x M x . Therefore, the solutions were kept in the dark during the stability measurements. None of the superatomic molecules was very stable in the dichloromethane solution, and the shapes of their spectra changed dramatically over time (Fig. S22 ). Figure 7a\u2013d shows the time-dependent changes of the optical absorption spectra of the toluene solutions of 1 \u2212 4 , respectively. As shown in Fig. 7a , 1 was quite stable in the toluene solution for three days. On the other hand, the shapes of the spectra of 2 \u2212 4 changed gradually over time (Fig. 7e ). We found that 2 and 4 were particularly unstable. Figure 7 demonstrates that the stability of 1 \u2212 4 decreases in the order 1 > 3 > 2 > 4 . In the present study, we also attempted synthesis using only AgNO 3 as the metal salt. The results demonstrated that somethings were synthesised just after adding NaBH 4 into the solution even when the precursor salt of heteroatoms (H 2 PtCl 6 , Pd(PPh 3 )Cl 6 , PtBr 2 or PdBr 2 ) was not included in the solution: the solution colour became yellow just after adding NaBH 4 into solution. However, the solution soon became colourless and the black precipitate was obtained. According to these results, it can be considered that the stability of [Ag 25 (PPh 3 ) 10 X 7 ] 2+ (X = Cl or Br) is quite low even if those clusters could be formed in solution. Similar results were reported by ref. 31 . Taking into account all the results mentioned above, the superatomic molecules described by [Ag 23 M 2 (PPh 3 ) 10 X 7 ] z (M = Ag, Pd, or Pt; X = Cl or Br; z = 2+ or 0) are interpreted to decrease in stability in the order [Ag 23 Pt 2 (PPh 3 ) 10 Cl 7 ] 0 ( 1 ) > [Ag 23 Pt 2 (PPh 3 ) 10 Br 7 ] 0 ( 3 ) > [Ag 23 Pd 2 (PPh 3 ) 10 Cl 7 ] 0 ( 2 ) > [Ag 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 ( 4 ) (which are experimentally synthesisable) > [Ag 25 (PPh 3 ) 10 X 7 ] 0 (X = Cl or Br; which are not so stable in solution). Fig. 7: Time dependence of the optical absorption spectra. a \u2013 d are time-dependent optical absorption spectra of 1 , 2 , 3 and 4 , respectively. e Time dependence of plots of absorbance of the first and second peaks in the optical absorption spectra of 1 (446 and 560 nm), 2 (489 and 630 nm), 3 (452 and 560 nm) and 4 (497 and 630 nm) (orange = 1 , dark blue = 2 , yellow = 3 , purple = 4 ). In these spectra, the peak positions are little shifted compared to those in Fig. 4 , probably due to the difference in solvent (dichloromethane for Fig. 4 vs. toluene for Fig. 7). Full size image Key factors for formation and isolation The substitution of the central atom of each icosahedral core by Pt or Pd is very effective for forming a superatomic molecule consisting of two Ag 13\u2212 x M x structures (M = Ag or other metal) connected by vertex sharing. Based on the DFT calculations by Baraiya et al., the Pt or Pd substitution of the central atom of Ag 13 leads to an increase in the average binding energy in the NCs 37 . The fact that it was possible to generate 1 \u2212 4 , whereas [Ag 25 (PPh 3 ) 10 X 7 ] 2+ (X = Cl or Br) was difficult to isolate, seems to be largely related to the individual icosahedral cores of 1 \u2212 4 being stronger than those of [Ag 25 (PPh 3 ) 10 X 7 ] 2+ (X = Cl or Br) owing to the increase in the average binding energy. The fact that [Ag 23 Pt 2 (PPh 3 ) 10 X 7 ] 0 is more stable than [Ag 23 Pd 2 (PPh 3 ) 10 X 7 ] 0 (X = Cl or Br) can also be explained by the difference in average binding energy. Regarding these heteroatomic substitutions of the central atom, Kang et al. pointed out that (1) they also affect the charge state of Ag in the L4 and L6 layers in Fig. S23a ; and (2) without central atom substitution, [Ag 25 (PPh 3 ) 10 Cl 7 ] 2+ would not form stably owing to high charge repulsion between the L4 and L6 layers 40 . We therefore estimated the natural charges 51 , 52 of Ag in the L4 and L6 layers for 1\u2032 \u2212 4\u2032 , and [Ag 25 (PPh 3 ) 10 Cl 7 ] 2+ ( 5\u2032 ; Table 1 ) and [Ag 25 (PPh 3 ) 10 Br 7 ] 2+ ( 6\u2032 ). The results showed no strong correlation between the charge repulsion in the L4\u2212L6 layer and the stability of 1 \u2212 4 and [Ag 25 (PPh 3 ) 10 X 7 ] 2+ (X = Cl or Br): the magnitude of charge repulsion was estimated to be in the order 1\u2032 > 5\u2032 > 2\u2032 = 3\u2032 > 4\u2032 = 6\u2032 (Fig. S23b ) and this order does not consistent with that of the stability ( 1 > 3 > 2 > 4 ). These results suggest that, although central atom substitution certainly affects the charge state of Ag in the L4 and L6 layers, such charge repulsion is not the main reason of the fact that [Ag 25 (PPh 3 ) 10 X 7 ] 2+ (X = Cl or Br) is difficult to isolate. The present study also revealed that a superatomic molecule consisting of two Ag 13\u2212 x M x structures can be formed even when Br is used as the bridging halogen instead of Cl. As mentioned above, in 3 and 4 , the twist between the two Ag 12 M structures (Fig. 2c, d ) prevents each Ag 12 M structure from becoming unstable. Based on the results obtained in the present study, the type of bridging halogen appears to have little effect on whether superatomic molecules can be formed or not, as long as the bridging halogen is large enough to maintain a moderate distance between the two Ag 12 M structures. It should be noted that the type of bridging halogen has a slight effect on the binding energy of the Ag\u2212X bond. That 1 is slightly more stable than 3 , and 2 is slightly more stable than 4 may be related to the Ag\u2212Cl bond (314 kJ mol \u22121 ) being stronger than the Ag\u2212Br bond (293 kJ mol \u22121 ) 53 . In addition, the type of bridging halogen also has a slight effect on the variation in Ag\u2212Ag bond length within each Ag 12 M molecules (Fig. S15 ). These results suggest that the type of bridging halogen affects the stability depending on the binding energy of the Ag\u2212halogen bond and the variation in Ag\u2212Ag bond length within each Ag 12 M molecules. In [Ag 23 M 2 (PPh 3 ) 10 X 7 ] z , in addition to the bridging sites, the halogen is also coordinated at both ends of the long axis of the superatomic molecule. Therefore, the type of halogen also affects the length of the Ag\u2212P bond (Fig. 3 ), and consequently, for example, some of the Ag\u2212P bonds are longer in 1 (Fig. S14b ). This means that some Ag\u2212P bonds are more easily dissociated in [Ag 23 Pt 2 (PPh 3 ) 10 Cl 7 ] 0 . However, as mentioned above, 1 is the most stable against degradation among 1 \u2212 4 . These results indicate that the slight difference in Ag\u2212P bond length caused by the difference in the halogen species at both ends in the superatomic molecule does not determine the stability order of [Ag 23 M 2 (PPh 3 ) 10 X 7 ] z , although the detachment of PPh 3 seems to be also included in the degradation of the superatomic molecules, as shown in Fig. 7 (Fig. S24 ). Finally, in the present study, we were only able to confirm the formation of superatomic molecules with five bridging halogens. This is considered to be largely because when the number of bridging halogens ( y \u2212 2) is higher than five in [Ag 25\u2212 x M x (PR 3 ) 10 X y ] z (M = Pt or Pd; X = Cl or Br; y = number of X), the total number of valence electrons is 16 only if the molecule is an anion. Anions are generally not highly resistant to oxidation in air 47 , 49 . These results suggest that isolatable [Ag 25\u2212 x M x (PR 3 ) 10 X y ] z must have a substitutional heteroatom species and a certain number of bridging halogens such that the total number of valence electrons is 16 in the cationic or neutral state (Table 1 ). The factors discussed above suggest that the following three conditions are required to stabilise superatomic molecules consisting of two Ag 13\u2212 x M x structures (M = Ag or other metal) connected by vertex sharing ([Ag 25\u2212 x M x (PR 3 ) 10 X y ] z ): (1) a halogen of sufficient size to maintain a moderate distance between the two Ag 13\u2212 x M x structures is used as the bridging halogen (Fig. 8a ); (2) an icosahedral core, which is stronger than Ag 13 , is formed by heteroatom substitution (Fig. 8b ); and (3) the combination of the substituted heteroatoms and the number of bridging halogens is such that the total number of valence electrons is 16 when the molecule is cationic or neutral (Fig. 8c ). For (1), halogens with ionic radii equal or larger than that of Cl fall into this category, and for (2), the central atom substitution with Pt or Pd satisfies this condition. Based on the reports by refs. 41 , 42 , 43 , 44 , 45 , 46 , condition (2) is also satisfied by multiple atom substitution with Au 37 . For (3), the number of bridging halogens ( y \u2212 2) is limited to 5 when Pt or Pd is the heteroatom, but when Au is the heteroatom 41 , 42 , 43 , 44 , 45 , 46 , the number of bridging halogens can be in a range of 5 to 7 (Fig. S16 ). As long as these three essential conditions are met at the same time, it is possible to stabilise and thereby isolate a superatomic molecule with two Ag 13\u2212 x M x structures connected by vertex sharing. To increase the stability of the resulting superatomic molecule, it is preferable to use Cl as the bridging halogen, and to combine multiple heteroatomic substitutions to stabilise the metal core. Therefore, it is assumed that 12 and 13 are even more stable than 1 . Moreover, it is expected that it will be possible to isolate [Ag 23 PtPd(PPh 3 ) 10 X 7 ] 0 , [Ag 23 Ni 2 (PPh 3 ) 10 X 7 ] 0 , [Ag 23 PtNi(PPh 3 ) 10 X 7 ] 0 , and [Ag 23 PdNi(PPh 3 ) 10 X 7 ] 0 (X = Cl or Br) superatomic molecules in the future 20 , 37 , 54 . According to our experiment on the stability (Fig. S24 ), the addition of an excess PPh 3 to the solution seems to help isolate new superatomic molecules. So far, we have only discussed cases in which PR 3 and X are used as ligands. However, if recently reported multidentate ligands 29 , 30 are used as bridging ligands, it may be possible to create even more types of superatomic molecules, such as those consisting of two Ag 13 structures connected by vertex sharing. The knowledge obtained in this study is expected to be also useful to stabilise and thereby isolate the longer superatomic molecules composed of three or four superatoms. Fig. 8: Indispensable requirements for stabilising and thereby isolating [Ag 25\u2212 x M x (PR 3 ) 10 X y ] z . a Use of a bridging X with a relevant ion radius (grey = Ag or M; red = bridging X). b Metal substitution for strengthening each Ag 13\u2212 x M x (grey = Ag; red = M). c Combination of types of M and the number of X (y \u2212 2) for a charge state (z) with high resistance to oxidation (grey = Ag; red = M or bridging X). Full size image Although the present study is concerned with superatomic molecules composed of Ag 13\u2212 x M x (M = Ag or other metal), the above-mentioned conditions ( 1 \u2013 3 ) also seem to be requirements for stabilising and thereby isolating superatomic molecules consisting of two Au 13\u2212 x M x structures (M = Ag or other metal) connected by vertex sharing. The point of difference from the case of Ag 13\u2212 x M x is the threshold in (1). In the case of Au 13\u2212 x M x , the halogens with ionic radii equal or larger than that of Br are assumed to fall under condition 1) 40 . Because Au differs from Ag in its formation of strong bonds with thiolate (SR) and selenolate (SeR) 55 , 56 , 57 , for superatomic molecules composed of Au 13\u2212 x M x , even more stable superatomic molecules can be obtained if SR and SeR are used as bridging ligands 58 . In fact, it has been reported that [Au 25 (PPh 3 ) 10 (SR) 5 Cl 2 ] 2+ (SR = alkanethiolate 21 or PET 59 ) and [Au 25 (PPh 3 ) 10 (SePh) 5 Br 2 ] +\/2+ (SePh = phenylselenolate) 60 connected with Au 13 without heteroatom substitution can also be isolated when SR or SeR is used as the bridging ligand. We have also successfully isolated [Au 24 Pd(PPh 3 ) 10 (PET) 5 Cl 2 ] + in which Pd substitution occurred only in one icosahedral core 61 . It is expected that such superatomic molecules connected with Au 13\u2212 x M x by SR or SeR, [Au 24 Pt(PR 3 ) 10 (SR) 5 ] + and [Au 23 PtPd(PR 3 ) 10 (SR) 5 ] 0 (PR 3 = PPh 3 , P( p -Tol) 3 41 , 43 , 45 or PMePh 2 46 ), will be isolated in the future. Methods Synthesis [Ag 23 Pt 2 (PPh 3 ) 10 Br 7 ] 0 (3) All syntheses were performed at 25 \u00b0C. First, 30 mg (0.18 mmol) of AgNO 3 and 5.1 mg (0.05 mmol) of NaBr were dissolved in 5 mL of methanol, and then 5 mL of methanol containing 2.1 mg (0.006 mmol) of PtBr 2 was added to the solution. The mixed solution was stirred for 15 min, and then 30 mL of methanol containing 262 mg (1 mmol) of PPh 3 , which was sonicated to disperse in methanol, was added. After stirring for 15 min, 1 mL of a methanol solution containing 20 mg (0.529 mmol) of NaBH 4 was rapidly added to the solution and the resulting solution was stirred for another 24 h. All experiments up to this point were performed in the dark. The solvent was then removed from the solution by rotary evaporation. Then, toluene was added to extract the product, and then water was added to the solution. After centrifugation, the toluene layer was separated to eliminate the excess NaBH 4 , and the solvent of the solution was evaporated using an evaporator to obtain the desired NC ( 3 ) (Fig. S25 ) (See S1.1 for chemicals). The chemical composition was confirmed by ESI-MS (Fig. S2 ), XPS (Fig. S3 ) and SC-XRD (See S1.2 for crystallographic method). [Ag 23 Pd 2 (PPh 3 ) 10 Br 7 ] 0 (4) First, 30 mg (0.18 mmol) of AgNO 3 was dissolved in 5 mL of methanol, and then 5 mL of methanol containing 1.6 mg (0.006 mmol) of PdBr 2 was added to the solution. After 15 min of stirring, 30 mL of methanol containing 262 mg (1 mmol) of PPh 3 , which was sonicated to disperse in methanol, was added to the solution. After stirring for 15 min, 1 mL of a methanol solution containing 20 mg (0.529 mmol) of NaBH 4 was rapidly added to the solution and the resulting solution was stirred for another 24 h. All experiments up to this point were performed at 0 \u00b0C in the dark. Note that, unlike in the synthesis of 3 , it was not necessary to increase the quantity of Br ions in the solution by adding TOABr during the synthesis of 4 . The solvent was then evaporated from the mixed solution using a rotary evaporator. Then, toluene was added to extract the product, and then water was added to the solution. After centrifugation, the toluene layer was separated to eliminate the excess NaBH 4 , and the solvent of the solution was evaporated using an evaporator to obtain the desired NC ( 4 ) (Fig. S26 ) (See S1.1 for chemicals). SC-XRD (See S1.2 for crystallographic method) was used for confirming the geometry and composition of 4 except Pd atoms, which were confirmed by XPS (Fig. S4 ) and ICP-MS. ESI-MS of 4 was not succeeded owing to the instability of 4 . Crystallisation Compounds 3 and 4 were crystallised using the liquid\u2212liquid diffusion method. 3 or 4 was first dissolved in ethanol and the solution was placed in a crystallisation vial. Six equivalent amount of hexane was then gently placed on the ethanol solution of 3 or 4 . The crystallisation vial was covered with a lid and the vial was allowed to stand at 25 \u00b0C. Orange needle-like crystals were obtained after a few days. Characterisation ESI-MS was performed with an ESI-Qq-TOF-MS compact (Bruker, MA, USA). In the experiment, first, multiple crystals of 3 were dissolved in toluene with PPh 3 (1 mM), which suppresses the detachments of PPh 3 from the superatomic molecules in the solution (Fig. S24 ). Then, methanol was added to this solution (toluene:methanol = 3 : 1 (v\/v)). Finally, 5 mM caesium carbonate (Cs 2 CO 3 ) methanol solution was added to the solution. The obtained solution was electrosprayed at a flow rate of 200 \u00b5L\/min. The SCs were immersed in cryoprotectant Parabar 10312 (Hampton Research, California, USA) and mounted on a MicroLoops E Inclined Assortment\u2122 (MiTeGen, New York, USA). The SC-XRD data sets were collected in a Bruker D8 QUEST, using monochromated MoK\u03b1 radiation ( \u03bb = 0.71073 \u00c5). Bruker Apex 3 62 suite was used for solving preliminary structures by following the sequential steps: indexing, data integration, reduction, absorption correction (multi-scan), space group determination and structure solution (with the intrinsic-phasing method). Final refinement was performed by SHELXL -2018\/3 63 using the Olex 2 platform 64 (Tables S1 , S2 ). The optical absorption spectra of the dichloromethane solutions of 3 and 4 were obtained at 25 \u00b0C using a V-630 spectrometer (JASCO, Tokyo, Japan). Multiple crystals were dissolved in dichloromethane for the measurement. PL spectra of the toluene solution of 1 \u2212 4 were measured using an FP-6300 spectrofluorometer (JASCO, Tokyo, Japan) at 25 \u00b0C. PL intensity ( F nor. ( \u03bb )) was normalised using the following equation to eliminate the effect of the difference in the concentration of 1 \u2212 4 on the PL intensity. $${F}_{{{{{{\\rm{nor}}}}}}.}(\\lambda )=F({\\lambda }_{{{{{{\\rm{em}}}}}}})\/[1{-}{10}^{{-}A(\\lambda {{{{{\\rm{ex}}}}}})}]$$ Where \u03bb em , \u03bb ex , A and F represent the emission wavelength, excitation wavelength, absorbance and PL intensity, respectively. XPS spectra were collected using a JPS-9010MC electron spectrometer (JEOL, Tokyo, Japan) at a base pressure of \u223c 2 \u00d7 10 \u22128 Torr. X-rays from the Mg-K\u03b1 line (1253.6 eV) were used for excitation. An indium plate was used as a substrate. The spectra were calibrated with the peak energies of In 3d 3\/2 (451.2 eV) 65 . Stability experiments To investigate the stability of 1 \u2212 4 with regard to decomposition in solution, solutions of 1 , 2 , 3 or 4 were prepared and measured in the following three different ways. 1. Dichloromethane solutions of each sample were placed in the glass cell of a spectrophotometer at 25 \u00b0C. The optical absorption spectrum of each solution was regularly measured for 1 h (Fig. S22 ). 2. Toluene solutions of each sample were left in a test tube with a lid at 30 \u00b0C. The optical absorption spectra were measured regularly for 3 days of the solutions of 1 and 3 , 1 day of the solution of 2 , and 8 h of the solution of 4 . (Fig. 7 ). 3. Toluene solutions with PPh 3 (95 mM) of each sample were placed in the glass cell of a spectrophotometer at 25 \u00b0C. The optical absorption spectrum of each solution was regularly measured for 1 week (Fig. S24 ). DFT calculations We performed DFT calculations on 3\u2032 , 4\u2032 and 6\u2032 using the structures of the experimentally synthesised 3 and 4 ; Pd was replaced with Ag for the calculation of 6\u2032 . All DFT calculations were performed with TURBOMOLE 66 under the resolution of identity approximation with the PBE 67 functional using the def-SV(P) basis sets 68 along with the relativistic effective core potentials for Pd, Ag, and Pt 69 . Optimised structures with different Pd positions were obtained at the same level of theory. The electronic absorption spectra were simulated in the framework of time-dependent DFT 70 , 71 , 72 , 73 , in which the line spectra were convoluted by a Lorentz function with a width of 10 nm. PBE was used as a function to calculate the absorption spectrum. Optimised structures with different Pd positions ( 3\u2032 ) and absorption spectra ( 3\u2032 , 4\u2032 and 6\u2032 ) were also calculated using CAM-B3LYP as a functional (Figs. S27 , S28 ), which produced similar overall results to those obtained using PBE as a functional. Data availability The X-ray crystallographic coordinates for structures reported in the present study have been deposited at the Cambridge Crystallographic Data Centre (CCDC), under deposition numbers 2195306\u22122195307. These data can be obtained free of charge from The Cambridge Crystallographic Data Centre via  . cif and cif check of 3 and 4 are provided in supplementary Data 1 \u2013 4 . The atomic coordinates of the DFT-optimised structures of 1\u2032-4\u2032, 6\u2032 have been provided in Supplementary Data 5 \u2013 9 , respectively. All other data are available from the corresponding authors on reasonable request. ","News_Body":"Superatomic molecules containing noble metal elements like gold and silver are studied for their potential in the synthesis of superatomic materials. However, the understanding of silver-based superatomic molecules has been limited. Addressing this gap, researchers from Japan studied two bimetallic superatomic molecules with silver as a main constituent to determine the key factors that enabled their formation. Their findings are expected to advance the development of novel materials in the future. In the past few decades, metal nanoclusters composed of noble metal elements such as gold (Au) and silver (Ag) have gained attention as superatoms for the synthesis of materials with unique properties and potential new applications. These superatoms (also known as \"artificial atoms\") typically consist of a cluster of a few to several hundred atoms and exhibit properties that are significantly different from their bulk, conventional counterparts. However, much like real atoms, the stability of these superatoms is determined by the formation of a closed-shell electron structure. Ag-based superatoms are known for their superior properties and functions, including photoluminescence and selective catalytic activity, compared to those of Au-based superatoms. However, most of the research in this field has been primarily focused on Au-based superatomic molecules. To overcome this research gap, researchers from Japan studied the formation of superatomic molecules composed of Ag and evaluated the factors involved in this formation. This study was published in the journal Communications Chemistry on March 28, 2023. Speaking of the motivation behind studying Ag-based superatoms, Prof. Negishi says, \"So far, we humans have created a variety of useful materials from the elements available to us on Earth. However, looking at a future with complex energy and environmental issues, the development of materials with new properties and functions is desired.\" To this end, the researchers synthesized two di-superatomic molecules with bromine (Br) as the bridging ligand: ([Ag23Pt2(PPh3)10Br7]0 and [Ag23Pd2(PPh3)10Br7]0 (PPh3 = Triphenylphosphine). The former consisted of two icosahedral Ag12Pt superatoms connected by vertex sharing with platinum atoms (Pt) occupying the central position in each superatom. In contrast, the other superatomic molecule consisted of two icosahedral Ag12Pd structures with palladium (Pd) as the central atom. The geometric\/electronic structure and stability of these two nanoclusters was then analyzed and compared with [Ag23Pt2(PPh3)10Cl7]0 (1) and [Ag23Pd2(PPh3)10Cl7]0 (2)\u2014two nanoclusters with geometrical similarity to the synthesized nanoclusters, consisting of chlorine (Cl) as the bridging atom. On examining the geometric structures of the four nanoclusters, the researchers observed a twist between the two icosahedral structures containing Br as the bridging ligand. The researchers suggest that this twist stabilizes the nanocluster by shortening the distance between the two icosahedral structures. Additionally, the larger Br atom was found to introduce steric hindrance in the molecule, causing both the PPh3 molecule to be positioned further from the long axis of the metal nanocluster and, a change in the bond length of the Ag-P and Ag-Ag bonds. These findings indicate that although the type of bridging halogen slightly affects the geometric structures of the metal nanoclusters, it does not hinder their formation. \"The type of bridging halogen appears to have little effect on whether superatomic molecules can be formed or not, as long as the bridging halogen is large enough to maintain a moderate distance between the two Ag12M structures,\" explains Prof. Negishi. However, the stability of the nanocluster was largely dependent on the number of bridging halogens attached to it. Like atoms, stable metallic nanoclusters require a filled valence shell. In the case of the prepared nanoclusters\u2014which had a total of 16 valence electrons\u2014the researchers were only able to attach a maximum of five bridging halogens to maintain the metal nanocluster in a stable neutral or cationic state. The presence of Pd and Pt central atoms was found to be due to the formation of metallic nanoclusters. Substituting the central atom of Ag13 with Pt or Pd led to an increase in the average binding energy within the nanoclusters, making it favorable for the formation of superatomic molecules. Overall, the researchers identified three key requirements for the formation and isolation of superatomic molecules consisting of two Ag13\u2212xMx structures connected by vertex sharing. These include the presence of a bridging halogen that can maintain an optimal distance between the two structures, a combination of heteroatoms and bridging halogens that results in 16 valence electrons, and the formation of an icosahedral core that is stronger than Ag13. In the words of Prof. Negishi, \"These findings offer clear design guidelines for the creation of molecular devices with various properties and functions, and can potentially contribute to resolving pressing concerns regarding clean energy and the environment.\" ","News_Title":"New study reveals design clues for silver-based superatomic molecules","Topic":"Nano"}
{"Paper_Body":"Abstract The human genome reference sequence remains incomplete owing to the challenge of assembling long tracts of near-identical tandem repeats in centromeres. We implemented a nanopore sequencing strategy to generate high-quality reads that span hundreds of kilobases of highly repetitive DNA in a human Y chromosome centromere. Combining these data with short-read variant validation, we assembled and characterized the centromeric region of a human Y chromosome. Main Centromeres facilitate spindle attachment and ensure proper chromosome segregation during cell division. Normal human centromeres are enriched with AT-rich \u223c 171-bp tandem repeats known as alpha satellite DNA 1 . Most alpha satellite DNAs are organized into higher order repeats (HORs), in which chromosome-specific alpha satellite repeat units, or monomers, are reiterated as a single repeat structure hundreds or thousands of times with high (>99%) sequence conservation to form extensive arrays 2 . Characterizing both the sequence composition of individual HOR structures and the extent of repeat variation is crucial to understanding kinetochore assembly and centromere identity 3 , 4 , 5 . However, no sequencing technology (including single-molecule real-time (SMRT) sequencing or synthetic long-read technologies) or a combination of sequencing technologies has been able to assemble centromeric regions because extremely high-quality, long reads are needed to confidently traverse low-copy sequence variants. As a result, human centromeric regions remain absent from even the most complete chromosome assemblies. Here we apply nanopore long-read sequencing to produce high-quality reads that span hundreds of kilobases of highly repetitive DNA ( Supplementary Fig. 1 ). We focus on the haploid satellite array present on the Y centromere (DYZ3), as it is particularly suitable for assembly owing to its tractable size, well-characterized HOR structure, and previous physical mapping data 6 , 7 , 8 . We devised a transposase-based method that we named 'longboard strategy' to produce high-read coverage of full-length bacterial artificial chromosome (BAC) DNA with nanopore sequencing (MinION sequencing device, Mk1B, Oxford Nanopore Technologies). In our longboard strategy, we linearize the circular BAC with a single cut site, then add sequencing adaptors ( Fig. 1a ). The BAC DNA passes through the pore, resulting in complete, end-to-end sequence coverage of the entire insert. Plots of read length versus megabase yield revealed an increase in megabase yield for full-length BAC DNA sequences ( Fig. 1b and Supplementary Fig. 2 ). We present more than 3,500 full-length '1D' reads (that is, one strand of the DNA is sequenced) from ten BACs (two control BACs from Xq24 and Yp11.2; eight BACs in the DYZ3 locus 9 ; Supplementary Table 1 ). Figure 1: BAC-based longboard nanopore sequencing strategy on the MinION. ( a ) Optimized strategy to cut each circular BAC once with transposase results in a linear and complete DNA fragment of the BAC for nanopore sequencing. ( b ) Yield plot of BAC DNA (RP11-648J18). ( c ) High-quality BAC consensus sequences were generated by multiple alignment of 60 full-length 1D reads (shown as blue and yellow for both orientations), sampled at random with ten iterations, followed by polishing steps (green) with the entire nanopore long-read data and Illumina data. ( d ) Circos representation 20 of the polished RP11-718M18 BAC consensus sequence. Blue arrowheads indicate the position and orientation of HORs. Purple tiles in yellow background mark the position of the Illumina-validated variants. Additional purple highlight extending from select Illumina-validated variants are used to identify single-nucleotide-sequence variants and mark the site of the DYZ3 repeat structural variants (6 kb) in tandem. Full size image Correct assembly across the centromeric locus requires overlap among a few sequence variants, meaning that accuracy of base-calls is important. Individual reads (MinION R9.4 chemistry, Albacore v1.1.1) provide insufficient sequence identity (median alignment identity of 84.8% for control BAC, RP11-482A22 reads) to ensure correct repeat assembly 10 . To improve overall base quality, we produced a consensus sequence from 10 iterations of 60 randomly sampled alignments of full-length 1D reads that spanned the full insert length for each BAC ( Fig. 1c ). To polish sequences, we realigned full-length nanopore reads to each BAC-derived consensus (99.2% observed for control BAC, RP11-482A22; and an observed range of 99.4\u201399.8% for vector sequences in DYZ3-containing BACs). To provide a truth set of array sequence variants and to evaluate any inherent nanopore sequence biases, we used Illumina BAC resequencing (Online Methods ). We used eight BAC-polished sequences (e.g., 209 kb for RP11-718M18; Fig. 1d ) to guide the ordered assembly of BACs from p-arm to q-arm, which includes an entire Y centromere. We ordered the DYZ3-containing BACs using 16 Illumina-validated HOR variants, resulting in 365 kb of assembled alpha satellite DNA ( Fig. 2a and Supplementary Data 1 ). The centromeric locus contains a 301-kb array that is composed of the DYZ3 HOR, with a 5.8-kb consensus sequence, repeated in a head-to-tail orientation without repeat inversions or transposable element interruptions 6 , 11 , 12 . The assembled length of the RP11 DYZ3 array is consistent with estimates for 96 individuals from the same Y haplogroup (R1b) ( Supplementary Fig. 3 ; mean: 315 kb; median: 350 kb) 13 , 14 . This finding is in agreement with pulsed-field gel electrophoresis (PFGE) DYZ3 size estimates from previous physical maps, and from a Y-haplogroup matched cell line ( Supplementary Fig. 4 ). Figure 2: Linear assembly of the RP11 Y centromere. ( a ) Ordering of nine DYZ3-containing BACs spanning from proximal p-arm to proximal q-arm. The majority of the centromeric locus is defined by the DYZ3 conical 5.8-kb HOR (light blue). Highly divergent monomeric alpha satellite is indicated in dark blue. HOR variants (6.0 kb) indicated in purple. ( b ) The genomic location of the functional Y centromere is defined by the enrichment of centromere protein A (CENP-A), where enrichment ( \u223c 5\u20136\u00d7) is attributed predominantly to the DYZ3 HOR array. Full size image Pairwise comparisons among the 52 HORs in the assembled DYZ3 array revealed limited sequence divergence between copies (mean 99.7% pairwise identity). In agreement with a previous assessment of sequence variation within the DYZ3 array 6 , we detected instances of a 6.0-kb HOR structural variant and provide evidence for seven copies within the RP11 DYZ3 array that were present in two clusters separated by 110 kb, as roughly predicted by previous restriction map estimates 8 . Sequence characterization of the DYZ3 array revealed nine HOR haplotypes, defined by linkage between variant bases that are frequent in the array ( Supplementary Fig. 5 ). These HOR haplotypes were organized into three local blocks that were enriched for distinct haplotype groups, consistent with previous demonstrations of short-range homogenization of satellite-DNA-sequence variants 6 , 15 , 16 . Functional centromeres are defined by the presence of inner centromere proteins that epigenetically mark the site of kinetochore assembly 17 , 18 , 19 . To define the genomic position of the functional centromere on the Y chromosome, we examined the enrichment profiles of inner kinetochore centromere protein A (CENP-A), a histone H3 variant that replaces histone H3 in centromeric nucleosomes, using a Y-haplogroup-matched cell line that offers a similar DYZ3 array sequence ( Fig. 2b and Supplementary Data 2 ) 5 , 14 , 19 . We found that CENP-A enrichment was predominantly restricted to the canonical DYZ3 HOR array, although we did identify reduced centromere protein enrichment extending up to 20 kb into flanking divergent alpha satellite on both the p-arm and q-arm side. Thus, we provide a complete genomic definition of a human centromere, which may help to advance sequence-based studies of centromere identity and function. We applied a long-read strategy to map, sequence, and assemble tandemly repeated satellite DNAs and resolve, for the first time to our knowledge, the array repeat organization and structure in a human centromere. Previous modeled satellite arrays 14 are based on incomplete and gapped maps, and do not present complete assembly data across the full array. Our complete assembly enables the precise number of repeats in an array to be robustly measured and resolves the order, orientation, and density of both repeat-length variants across the full extent of the array. This work could potentially advance studies of centromere evolution and function and may aid ongoing efforts to complete the human genome. Methods BAC DNA preparation and validation. Clones of bacterial artificial chromosomes (BACs) used in this study were obtained from BACPAC RPC1-11 library, Children's Hospital Oakland Research Institute in Oakland, California, USA (  ). BACs that span the human Y centromere, RP11-108I14, RP11-1226J10, RP11-808M02, RP11-531P03, RP11-909C13, RP11-890C20, RP11-744B15, RP11-648J16, RP11-718M18, and RP11-482A22, were determined based on previous hybridization with DYZ3-specific probes, and confirmed by PCR with STSs sY715 and sY78 (ref. 9 ). Notably, DYZ3 sequences, unlike shorter satellite DNAs, have been observed to be stable and cloned without bias 5 , 21 . The RP11-482A22 BAC was selected as our control since it had previously been characterized by nanopore long-read sequencing 22 , and presented \u223c 134 kb of assembled, unique sequence present in the GRCh38 reference assembly to evaluate our alignment and polishing strategy. BAC DNA was prepared using the QIAGEN Large-Construct Kit (Cat No.\/ID: 12462). To ensure removal of the Escherichia coli genome, it was important to include an exonuclease incubation step at 37 \u00b0C for 1 h, as provided within the QIAGEN Large-Construct Kit. BAC DNAs were hydrated in TE buffer. BAC Insert length estimates were determined by pulsed-field gel electrophoresis (PFGE) (data not shown). Longboard MinION protocol. MinIONs can process long fragments, as has been previously documented 22 . While these long reads demonstrate the processivity of nanopore sequencing, they offer insufficient coverage to resolve complex, repeat-rich regions. To systematically enrich for the number of long reads per MinION sequencing run, we developed a strategy that uses the Oxford Nanopore Technologies (ONT) Rapid Sequencing Kit (RAD002). We performed a titration between the transposase from this kit (RAD002) and circular BAC DNA. This was done to achieve conditions that would optimize the probability of individual circular BAC fragments being cut by the transposase only once. To this end, we diluted the 'live' transposase from the RAD002 kit with the 'dead' transposase provided by ONT. For PFGE-based tests, we used 1 \u03bcl of 'live' transposase and 1.5 \u03bcl of 'dead' transposase per 200 ng of DNA in a 10-\u03bcl reaction volume. This reaction mix was then incubated at 30 \u00b0C for 1 min and 75 \u00b0C for 1 min, followed by PFGE. Our PFGE tests used 1% high-melting agarose gels and were run with standard 180\u00b0 field inversion gel electrophoresis (FIGE) conditions for 3.5 h. An example PFGE gel is shown in Supplementary Figure 6 . For MinION sequencing library preparation, we used 1.5 \u03bcl of 'live' transposase and 1 \u03bcl of 'dead' transposase (supplied by ONT) per 1 \u03bcg of DNA in a 10-\u03bcl reaction volume. Briefly, this reaction mix was then incubated at 30 \u00b0C for 1 min and 75 \u00b0C for 1 min. We then added 1 \u03bcl of the sequencing adaptor and 1 \u03bcl of Blunt\/TA Ligase Master Mix (New England BioLabs) and incubated the reaction for 5 min. This was the adapted BAC DNA library for the MinION. R9.4 SpotON flow cells were primed using the protocol recommended by ONT. We prepared 1 ml of priming buffer with 500 \u03bcl running buffer (RBF) and 500 \u03bcl water. Flow cells were primed with 800 \u03bcl priming buffer via the side loading port. We waited for 5 min to ensure initial buffering before loading the remaining 200 \u03bcl of priming buffer via the side loading port but with the SpotON open. We next added 35 \u03bcl RBF and 28 \u03bcl water to the 12 \u03bcl library for a total volume of 75 \u03bcl. We loaded this library on the flow cell via the SpotON port and proceeded to start a 48 h MinION run. When a nanopore run is underway, the amplifiers controlling individual pores can alter voltage to get rid of unadapted molecules that can otherwise block the pore. With R9.4 chemistry, ONT introduced global flicking that reversed the potential every 10 min by default to clear all nanopores of all molecules. At 450 b.p.s., a 200 kb BAC would take around 7.5 min to be processed. To ensure sufficient time for capturing BAC molecules on the MinION, we changed the global flicking time period to 30 min. This is no longer the case with an update to ONT's MinKNOW software, and on the later BAC sequencing runs we did not change any parameters. We acknowledge that generating long (>100 kb) reads presents challenges, given the dynamics of high-molecular-weight (HMW) DNA for ligation, chemistry updates, and delivery of free ends to the pore, reducing the effective yield. We found that high-quality and a large quantity of starting material (i.e., our strategy is designed for 1 \u03bcg of starting material that does not show signs of DNA shearing and\/or degradation when evaluated by PFGE) and reduction of smaller DNA fragments were necessary for the longboard strategy. Protocol to improve long-read sequence by consensus and polishing. BAC-based assembly across the DYZ3 locus requires overlap among a few informative sequence variants, thus placing great importance on the accuracy of base-calls. Therefore, we employed the following strategy to improve overall base quality. First, we derived a consensus from multiple alignments of 1D reads that span the full insert length for each BAC. Further, polishing steps were performed using realignment of all full-length nanopore reads for each BAC. As a result, each BAC sequencing project resulted in a single, polished BAC consensus sequence. To validate single-copy variants, useful in an overlap-layout-assembly strategy, we included Illumina data sets for each BAC. Illumina data were not used to correct or validate variants observed multiple times within a given BAC sequence due to the reduced mapping quality. MinION base-calling. All of the BAC runs were initially base-called using Metrichor, ONT's cloud basecaller. Metrichor classified reads as pass or fail using a Q-value threshold. We selected the full-length BAC reads from the pass reads. We later base-called all of the BAC runs again using Albacore 1.1.1, which included significant improvements on homopolymer calls. This version of Albacore did not contain a pass\/fail cutoff. We reperformed the informatics using Albacore base-calls for full-length reads selected from the pass Metrichor base-calls. We selected BAC full-length reads as determined by observed enrichment in our yield plots (shown in Supplementary Fig. 7 the read versus read length plots converted to yield plots to identify BAC length min-max selection thresholds). Full-length reads used in this study were determined to contain at least 3 kb of vector sequence, as determined by BLASR 23 ( -sdpTupleSize 8 -bestn 1 -nproc 8 -m 0 ) alignment with the pBACe3.6 vector (GenBank Accession: U80929.2 ). Reads were converted to the forward strand. Reads were reoriented relative to a fixed 3-kb vector sequence, aligning the transition from vector to insert. Derive BAC consensus sequence. Reoriented reads were sampled at random (blasr_output.py). Multiple sequence alignment (MSA) was performed using kalign 24 . We determined empirically that sampling greater than 60 reads provided limited benefit to consensus base quality ( Supplementary Fig. 8 ). We computed the consensus from the MSA whereas the most prevalent base at each position was called. Gaps were only considered in the consensus if the second most frequent nucleotide at that position was present in less than ten reads. We performed random sampling followed by MSA iteratively 10\u00d7, resulting in a panel of ten consensus sequences, observed to provide a \u223c 1% boost in consensus sequence identity ( Supplementary Fig. 8 ). To improve the final consensus sequence, we next performed a final MSA on the collection of ten consensus sequences derived from sampling. Polish BAC consensus sequence. Consensus sequence polishing was performed by aligning full-length 1D nanopore reads for each BAC to the consensus (BLASR 25 , -sdpTupleSize 8 -bestn 1 -nproc 8 -m 0 ). We used pysamstats (  ) to identify read support for each base call. We determined the average base coverage for each back, and filtered those bases that had low-coverage support (defined as having less than half of the average base coverage). Bases were lower-case masked if they were supported by sufficient sequence coverage, yet had <50% support for a given base call in the reads aligned. Variant validation. We performed Illumina resequencing (MiSeq V3 600bp; 2 \u00d7 300 bp) for all nine DYZ3-containing BACs to validate single-copy DYZ3 HOR variants in the nanopore consensus sequence. Inherent sequence bias is expected in nanopore sequencing 22 , therefore we first used the Illumina matched data sets to evaluate the extent and type of sequence bias in our initial read sets, and our final polished consensus sequence. Changes in ionic current, as individual DNA strands are read through the nanopore, are each associated with a unique 5-nucleotide k-mer. Therefore in an effort to detect inherent sequence errors due to nanopore sensing, we compared counts of 5-mers. Alignment of full-length HORs within each polished BAC sequence to the canonical DYZ3 repeat demonstrated that these sequences are nearly identical, where in RP11-718M18 we detected 1,449 variant positions (42% mismatches, 27% deletions, and 31% insertions) across 202,582 bp of repeats (99.5% identity). Although the 5-mer frequency profiles between the two data sets were largely concordant ( Supplementary Fig. 9 ), we found that poly(dA) and poly(dT) homopolymers were overrepresented in our initial nanopore read data sets, a finding that is consistent with genome-wide observations. These poly(dA) and poly(dT) over-representations were reduced in our quality-corrected consensus sequences especially for 6-mers and 7-mers. K-mer method. Using a k-mer strategy (where k = 21 bp), we identified exact matches between the Illumina and each BAC consensus sequence. Illumina read data and the BAC-polished consensus sequences were reformatted into respective k-mer library (where k = 21 bp, with 1 bp slide using Jellyfish v2 software 25 ), in forward and reverse orientation. K-mers that matched the pBACe3.6 sequence exactly were labeled as 'vector'. K-mers that matched the DYZ3 consensus sequence exactly 14 were labeled as 'ceny'. We first demonstrated that the labeled k-mers were useful in predicting copy number. Initially, we showed how the ceny k-mer frequency in the BACs predicted the DYZ3 copy number, relative to the number observed in our nanopore consensus ( Supplementary Fig. 10a ). DYZ3 copy number in each consensus sequence derived from nanopore reads was determined using HMMER3 (ref. 26 ) (v3.1b2) with a profile constructed from the DYZ3 reference repeat. By plotting the distribution of vector k-mer counts ( Supplementary Fig. 10b for RP11-718M18), we observed a range of expected k-mer counts for single-copy sites. DYZ3 repeat variants (single-copy satVARs) were determined as k-mers that (1) did not have an exact match with either the vector or DYZ3 reference repeat, (2) spanned a single DYZ3-assigned variant in reference-polished consensus sequence (i.e., that particular k-mer was observed only once in the reference), (3) and had a k-mer depth profile in the range of the corresponding BAC vector k-mer distribution. As a final conservative measure, satVARs used in overlap-layout-consensus assembly were supported by two or more overlapping Illumina k-mers ( Supplementary Fig. 10 ). To test if it was possible to predict a single-copy DYZ3 repeat variant by chance, or by error introduced in the Illumina read sequences, we ran 1,000 simulated trials using our RP11-718M18 Illumina data. Here, we randomly introduced a single variant into the polished RP11-718M18 DYZ3 array (false positive). We generated 1,000 simulated sequences, each containing a single randomly introduced single-copy variant. Next, we queried if the 21-mer spanning the introduced variant was (a) found in the corresponding Illumina data set and (b) if so, we monitored the coverage. Ultimately, none of the simulated false-positive variants (21-mer) met our criteria of a true variant. That is, although the simulated variants were identified in our Illumina data, they had insufficient sequence coverage to be included in our study. Greater than 95% of the introduced false variants had \u2264100\u00d7 coverage, with only one variant observed to have the maximum value of 300\u00d7. True variants were determined using this data set with values from 1,100\u20131600\u00d7, as observed in our vector distribution. Alignment method. We employed a short-read alignment strategy to validate single-copy variants in our polished consensus sequence. Illumina-merged reads (PEAR, standard parameters 27 ) were mapped to the RP11 Y-assembled sequence using BWA-MEM 28 . BWA-MEM is a component of the BWA package and was chosen because of its speed and ubiquitous use in sequence mapping and analysis pipelines. Aside from the difficulties of mapping the ultra-long reads unique to this work, any other mapper could be used instead. This involves mapping Illumina data to each BAC consensus sequence. After filtering those alignments with mapping quality less than 20, single-nucleotide DYZ3 variants (i.e., a variant that is observed uniquely, or once in a DYZ3 HOR in a given BAC) were considered \u201cvalidated\u201d if they had support of at least 80% of the reads and had sequence coverage within the read depth distribution observed in the single-copy vector sequence for each BAC data set. To explore Illumina sequence coverage necessary for our consensus polishing strategy we initially investigated a range (20\u2013100\u00d7) of simulated sequence coverage relative to a 73-kb control region (hg38 chrY:10137141\u201310210167) within the RP11-531P03 BAC data. Simulated paired read data using the ART Illumina simulator software 29 was specified for the MiSeq sequencing system (MiSeq v3 (250 bp), or 'MSv3\u2032), with a mean size of 400 bp DNA fragments for paired-end simulations. Using our polishing protocol, where reads are filtered by mapping quality score (i.e., at least a score of 20: that the probability of correctly mapping is log 10 of 0.01 * -10, or 0.99), base frequency was next determined for each position using pysamstats, and a final, polished consensus was determined by taking the base call at any given position that is represented by sufficient coverage (at least half of the determined average across the entire BAC) and is supported by a percentage of Illumina reads mapped to that location (in our study, we required at least 80%). If we require at least 80% of mapped reads to support a given base call, we determine that 30\u00d7 coverage is sufficient to reach 99% sequence identity (or the same as our observed identity using our entire Illumina read data set, indicated as a gray dotted line in Supplementary Fig. 11 ). If we require at least 90% of mapped reads to support a particular variant it is necessary to increase coverage to 70\u00d7 to reach an equivalent polished percent identity. To evaluate our mapping strategy, we performed a basic simulation using an artificially generated array of ten identical DYZ3 (5.7 kb) repeats. We then randomly introduced a single base change resulting in a new sequence with nine identical DYZ3 repeats and one repeat distinguished by a single-nucleotide change ( Supplementary Fig. 12 ). We first demonstrate that we are able to confidently detect the single variant by simulating reads from the reference sequence containing the introduced variant of varying coverage and Illumina substitution error rate. Additionally, we investigated whether we would detect the variant as an artifact due to Illumina read errors. To test this, we next simulated Illumina reads from a DYZ3 reference array that did not contain the introduced variant (i.e., ten exact copies of the DYZ3 repeat). We performed this simulation 100\u00d7, thus creating 100\u00d7 reference arrays each with a randomly placed single variant. Within each evaluation we mapped in parallel simulated Illumina reads from (a) the array containing introduced variant sequence and (b) the array that lacked the variant. In experiments where reads containing the introduced variant were mapped to the reference containing the variant, we observed the introduced base across variations of sequence coverage and increased error rates. To validate a variant as \u201ctrue,\u201d we next evaluated the supporting sequence coverage. For example in 100\u00d7 coverage, using the default Illumina error rate we observed 96 \u201ctrue\u201d calls out of 100 simulations, where in each case we set a threshold such that at least 80% of reads that spanned the introduced variant supported the base call. We found that Illumina quality did influence our ability to confidently validate array variants by reducing the coverage. When the substitution error was increased by 1\/10th we observed a decrease to only 75 \u201ctrue\u201d variant calls out of 100\u00d7 simulations. Therefore, we suspect that Illumina sequencing errors may challenge our ability to completely detect true-positive variants. In our alternate experiments, although simulated Illumina reads from ten identical copies of the DYZ3 repeat were mapped to a reference containing an introduced variant, we did not observe a single simulation and\/or condition with sufficient coverage for \u201ctrue\u201d validation. We do report an increase in the percentage of reads that support the introduced variant as we increase the Illumina substitution error rate, however, the range of read depth observed across all experiments was far below our coverage threshold. We obtained similar results when we repeated this simulation using sequences from the RP11-718M18 DYZ3 array. Finally, standard quality Illumina-based polishing with pilon 21 was applied strictly to unique (non-satellite DNA) sequences on the proximal p and q arms to improve final quality. Alignment of polished consensus sequences from our control BAC from Xq24 (RP11-482A22) and non-satellite DNA in the p-arm adjacent to the centromere (Yp11.2, RP11-531P03) revealed base-quality improvement to >99% identity. Prediction and validation of DYZ3 array. BAC ordering was determined using overlapping informative single-nucleotide variants (including the nine DYZ3 6.0 kb structural variants) in addition to alignments directly to either assembled sequence on the p-arm or q-arm of the human reference assembly (GRCh38). Notably, physical mapping data were not needed in advance to guide our assembly. Rather these data were provided to evaluate our final array length predictions. Full-length DYZ3 HORs (ordered 1\u201352) were evaluated by MSA (using kalign 24 ) between overlapping BACs, with emphasis on repeats 28\u201335 that define the overlap between BACs anchored to the p-arm or q-arm ( Supplementary Fig. 13 ). RPC1-11 BAC library has been previously referenced as derived from a known carrier of haplogroup R1b 30 , 31 . We compared our predicted DYZ3 array length with 93 R1b Y-haplogroup-matched individuals by intersecting previously published DYZ3 array length estimates for 1000 Genome phase 1 data 13 , 14 with donor-matched Y-haplogroup information 32 . To investigate the concordancy of our array prediction with previous physical maps of the Y-centromere we identified the positions of referenced restriction sites that directly flank the DYZ3 array in the human chromosome Y assembly (GRCh38) 6 , 7 , 33 . It is unknown if previously published individuals are from the same population cohort as the RPC1-11 donor genome, therefore we performed similar PFGE DYZ3 array PFGE length estimates using the HuRef B-lymphoblast cell line (available from Coriell Institute as GM25430), previously characterized to be in the R1-b Y-haplogroup 34 . PFGE alpha satellite Southern. High-molecular-weight HuRef genomic DNA was resuspended in agarose plugs using 5 \u00d7 10 6 cells per 100 \u03bcL 0.75% CleanCut Agarose (CHEF Genomic DNA Plug Kits Cat #: 170-3591 BIORAD). A female lymphoblastoid cell line (GM12708) was included as a negative control. Agarose plug digests were performed overnight (8\u201312 h) with 30\u201350 U of each enzyme with matched NEB buffer. PFGE Southern experiments used 1\/4\u20131\/2 agarose plug per lane ( \u223c 5\u201310 \u03bcg) in an 1% SeaKem LE Agarose gel and 0.5\u00d7 TBE. CHEF Mapper conditions were optimized to resolve 0.1\u20132.0 Mb DNAs: voltage 6V\/cm, runtime: 26:40 h, in angle: 120, initial switch time: 6.75 s, final switch time: 1 m 33.69 s, with a linear ramping factor. We used the Lambda (NEB; N0340S) and Saccharomyces cerevisiae (NEB; N0345S) as markers. Methods of transfer to nylon filters, prehybridization, and chromosome-specific hybridization with 32P-labeled satellite probes have been described 35 . Briefly, DNA was transferred to nylon membrane (Zeta Probe GT nylon membrane; CAT# 162-0196) for \u223c 24 h. DYZ3 probe (50 ng DNA labeled \u223c 2 c.p.m.\/mL; amplicon product using previously published STS DYZ3 Y-A and Y-B primers 36 ) was hybridized for 16 h at 42 \u00b0C. In addition to standard wash conditions 35 , we performed two additional stringent wash (buffer: 0.1% SDS and 0.1\u00d7 SSC) steps for 10 min at 72 \u00b0C to remove non-specific binding. Image was recovered after 20 h exposure. Sequence characterization of Y centromeric region. The DYZ3 HOR sequence and chromosomal location of the active centromere on the human chromosome Y is not shared among closely related great apes 37 . However, previous evolutionary dating of specific transposable element subfamilies (notably, L1PA3 9.2\u201315.8 MYA 38 ) within the divergent satellite DNAs, as well as shared synteny of 11.9 kb of alpha satellite DNA in the chimpanzee genome Yq assembly indicate that the locus was present in the last common ancestor with chimpanzee ( Supplementary Fig. 14 ). Comparative genomic analysis between human and chimpanzee were performed using UCSC Genome Browser liftOver 39 between human (GRCh38, or hg38 chrY:10,203,170\u201310,214,883) and the chimpanzee genome (panTro5 chrY:15,306,523\u201315,356,698, with 100% span at 97.3% sequence identity). Alpha satellite and adjacent repeat in the chimpanzee genome that share limited sequence homology with human were determined used UCSC repeat table browser annotation 40 . The location of the centromere across primate Y-chromosomes was determined by fluorescence in situ hybridization (FISH) ( Supplementary Fig. 14 ). Preparation of mitotic chromosomes and BAC-based probes were carried according to standard procedures 41 . Primate cell lines were obtained from Coriell: Pan paniscus (Bonobo) AG05253; Pan troglodytes (Common Chimpanzee) S006006E. Male gorilla fibroblast cells were provided by Stephen O'Brien (National Cancer Institute, Frederick, MD) as previously discussed 42 . The HuRef cell line 34 (GM25430) was provided through collaboration with Samuel Levy. BAC DNAs were isolated from bacteria stabs obtained from CHORI BACPAC. Metaphase spreads were obtained after a 1 h 15 min colcemid\/karyomax (Gibco) treatment followed by incubation in a hypotonic solution. Cells were counterstained with 4\u2032,6-diamidino-2-phenylindole (DAPI) (Vector). BAC DNA probes were labeled using Alexa flour dyes (488, green and 594, red) (ThermoFisher). The BAC probes were labeled with biotin 14-dATP by nick translation (Gibco). And the chromosomes were counterstained with DAPI. Microscopy, image acquisition, and processing were performed using standard procedures. Epigenetic mapping of centromere proteins. To evaluate similarity between the HuRef DYZ3 reference model (GenBank: GJ212193) and our RP11 BAC-assembly we determined the relative frequency of each k-mer in the array (where k = 21, with a 1-bp slide taking into account both forward and reverse sequence orientation using Jellyfish software) normalized by the total number of observed k-mers ( Supplementary Fig. 15 ), with the Pearson correlation coefficient. Enrichment across the RP11 Y assembly was determined using the log-transformed relative enrichment of each 50-mer frequency relative to the frequency of that 50-mer in background control (GEO Accession: GSE45497 ID: 200045497 ), as previously described 5 . If a 50-mer is not observed in the ChIP background the relative frequency was determined relative to the HuRef Sanger WGS read data (AADD00000000 WGSA) 34 . Average enrichment values were calculated for windows size 6 kb ( Fig. 2 ). Additionally, CENP-A and C paired read data sets (GEO Accession: GSE60951 ID: 200060951 ) 43 were merged (PEAR 27 , standard parameters) and mapped to all alpha satellite reference models in GRCh38. Reads that mapped specifically to the DYZ3 reference model were selected to study enrichment to the HOR array. The total number of bases mapped from CENP-A and CENP-C data versus the input controls was used to determine relative enrichment. Second, reads that mapped specifically to the DYZ3 reference model were aligned to the DYZ3 5.7 kb in consensus (indexed in tandem to avoid edge-effects), and read depth profiles were determined. To characterize enrichment outside of the DYZ3 array CENP-A, CENP-C and Input data were mapped directly to the RP11 Y-assembly. Reads mapping to the DYZ3 array were ignored. Read alignments were only considered outside of the DYZ3 array if no mismatches, insertions, or deletions were observed to the reference and if the read could be aligned to a single location (removing any reads with mapping score of 0). Sequence depth profiles were calculated by counting the number of bases at any position and normalizing by the total number of bases in each respective data set. Relative enrichment was obtained by taking the log-transformed normalized ratio of centromere protein (A or C) to Input. Statistics. The Pearson correlation coefficient was used to determine a positive linear relationship in our data sets (as shown in Supplementary Figs. 10a and 15a ). Simulation experiments using Illumina short read data were performed using 100 replicates. Representative gel image shown ( Supplementary Fig. 6 ) was repeated ten times, or once for each BAC in our study, with consistent results. Representative Southern Blot (shown in Supplementary Fig. 4a ), was repeated twice with different restriction enzymes with the same results. Centromere Y position analysis using FISH on a panel of primates were repeated at least two times, and results were invariable between experiments and between hybridization patterns within multiple metaphase spreads within a given experiment. Code availability. This study used previously published software: alignments were performed using BLASR 23 (version 1.3.1.124201) and BWA MEM 28 (0.7.12-r1044). Consensus alignments were obtained using kalign 24 (version 2.04). Global alignments of HORs used needle 44 (EMBOSS:6.5.7.0). Repeat characterization was performed using RepeatMasker (Smit, AFA, Hubley, R & Green, P. RepeatMasker Open-4.0 . 2013-2015;  ). Satellite monomers were determined using profile hidden Markov model (HMMER3) 26 . Jellyfish (version 2.0.0) 25 was used to characterize k-mers. Illumina read simulations was performed using ART (version 2.5.8) 29 . PEAR 27 (version 0.9.0) was used to merge paired read data. Comparative genomic analysis between human and chimpanzee were performed using UCSC Genome Browser liftOver 39 . Additional scripts used in preparing sequences before consensus generation are deposited in GitHub:  . Life Sciences Reporting Summary. Further information on experimental design is available in the Life Sciences Reporting Summary . Data availability. Sequence data that support the findings of this study have been deposited in GenBank with reference to BioProject ID PRJNA397218 , and SRA accession codes SRR5902337 and SRR5902355 . BAC consensus sequences and RP11-CENY array assembly are deposited under GenBank accession numbers MF741337 \u2013 MF741347 . Additional information Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Accession codes Primary accessions BioProject PRJNA397218 GenBank\/EMBL\/DDBJ MF741337 MF741347 Sequence Read Archive SRR5902337 SRR5902355 Referenced accessions GenBank\/EMBL\/DDBJ U80929.2 Gene Expression Omnibus 200045497 200060951 GSE45497 GSE60951 ","News_Body":"Fifteen years ago, the Human Genome Project announced they had cracked the code of life. Nonetheless, the published human genome map was incomplete and parts of our DNA remained to be deciphered. Now, a new study published in the journal Nature Biotechnology brings us closer to a complete genetic blueprint by using a nanotechnology-based sequencing technique. Like ancient Egyptian ruins covered in mysterious hieroglyphics, the letters and words in our genetic code remained unutterable for a long time. In an effort to solve this genetic cipher, the Human Genome Project, a collaborative international consortium, was created. The goal was to read out the DNA sequence \u2013 made up of four letters, or bases, A,T,G and C \u2013 of all human genes (genome). In 2003, a near-complete map of the human genome was reported. The scientific community hailed the momentous event as a turning point, perhaps overshadowed only by the discovery of the double-helix structure of DNA. Indeed, for the first time in human history, we could read and understand the language of our \"being\". Yet, the assembled genome represented only 92% of all human genes. Gaps remained that could not be easily decrypted. For many researchers, that elusive 8% of the genome is a holy grail. The dark matter inside us all The unmappable genome is associated with \"heterochromatin\" (dark matter of the genome, highly condensed), unlike \"euchromatin\" (light matter, more loosely wound part of the genome). Euchromatin is gene-rich while heterochromatin refers to the silent, repressed regions of our DNA. Euchromatin is full of unique DNA sequences. This means that finding a single- or low-copy DNA sequence, with all the same DNA bases in the same order, at more than one location in our genome is highly unlikely. These discrete DNA sequences are easily distinguishable and serve distinct purposes within our cells. No wonder the human genome has almost 20,000 different genes with limited redundancy. Now, visualize a human chromosome as a big \"X\", made of coiled-up DNA, with two arms attached at a constriction. Heterochromatin is mostly localised near the point of attachment (centromere) and the tips of the arms (telomeres). In fact, the centromere becomes indispensable when cells divide, dragging along one chromosome arm into each of the newly formed daughter cells. DNA sequencing technologies operate by reading each base of DNA, one at a time, and spitting out short \"reads\" that spell out the sequence being read. Thus, decoding unique, non-identical euchromatic DNA is facile because one stretch apart from other with little ambiguity. The problem arises when we try to enunciate heterochromatic sequences comprising strings of DNA that look like each other. Arranged in tandem arrays or dispersed throughout our genome, these highly repetitive stretches of DNA amount to garbled gibberish after conventional DNA sequencing. One small chunk of DNA (monomer) at the centromere resembles other identical chunks flanking it and so on. In the resulting quagmire, the base-composition & precise position of any given repeated sequence cannot be ascertained in a long polymer of repeats. Made up of millions of repeating A,T,G,C bases, the centromeres of human chromosomes evaded biologists and explain holes in our current DNA map. Threading the genome into a tiny needle The new study, from the team of Dr. Karen Miga at University of California (Santa Cruz), has managed to uncover the centromere of the Y chromosome \u2013 the male-specific chromosome and also the smallest chromosome in our genome (something worth thinking about). The researchers were able to insert a longer stretch of DNA into a nano-pore (like thread passed through the eye of a needle), \"resulting in complete, end-to-end sequence coverage of the entire insert\". Using this nanopore-sequencing method, the researchers can now decipher a long, muddled DNA stretch full of repeats. This \"long-read\" strategy allowed them to string together longer pieces of DNA (made up of variable repeat monomer lengths). It turns out that when all these chunks are laid out, certain clues help reconstruct the repetitive-sequence. Walking along the centromere, from left to right, context is provided by surrounding monomers in the same tandem array and by flanking non-repetitive DNA. Like a neatly laid section of railroad, the authors pieced together a chain of contiguous DNA sequences and solved the jigsaw puzzle of the Y chromosome centromere. This recent work, published in Nature Biotechnology journal, plugs holes in the existing human DNA map. In the future, finding out the DNA sequences that define other centromeres will allow researchers to rewrite, manipulate, alter or duplicate these key structures. Given that the centromere is essential for cells to divide and segregate their genetic content to future generations, the Y centromere assembly represents an exciting step forward in modern biology. ","News_Title":"Reading the entire human genome \u2013 one long sentence at\u00a0a\u00a0time","Topic":"Biology"}
{"Paper_Body":"Abstract Dirac semimetals, the materials featuring fourfold degenerate Dirac points, are critical states of topologically distinct phases. Such gapless topological states have been accomplished by a band-inversion mechanism, in which the Dirac points can be annihilated pairwise by perturbations without changing the symmetry of the system. Here, we report an experimental observation of Dirac points that are enforced completely by the crystal symmetry using a nonsymmorphic three-dimensional phononic crystal. Intriguingly, our Dirac phononic crystal hosts four spiral topological surface states, in which the surface states of opposite helicities intersect gaplessly along certain momentum lines, as confirmed by additional surface measurements. The novel Dirac system may release new opportunities for studying elusive (pseudo) and offer a unique prototype platform for acoustic applications. Introduction The discovery of new topological states of matter has become a vital goal in fundamental physics and material science 1 , 2 . A three-dimensional (3D) Dirac semimetal (DSM) 3 , 4 , 5 , 6 , 7 , 8 , 9 , 10 , 11 , 12 , 13 , accommodating many exotic transport properties such as anomalous magnetoresistance and ultrahigh mobility 14 , 15 , is an exceptional platform for exploring topological phase transitions and other novel topological quantum states. It is also of fundamental interest to serve as a solid-state realization of a (3 + 1)-dimensional Dirac vacuum. A DSM phase may appear accidentally at the quantum transition between normal and topological insulators 16 , 17 . The approach to such a single critical point demands the fine-tuning of the alloy\u2019s chemical composition, which limits the experimental accessibility to the fascinating physics of 3D Dirac fermions. 3D DSMs can also emerge without fine-tuning parameters and are distinguished into two classes 3 , 4 . The first one, already realized in Na 3 Bi 7 , 8 and Cd 3 As 2 9 , 10 , occurs due to band inversion 5 , 6 . The Dirac points, lying on the generic momenta of a specific rotation symmetry axis, always come in pairs and could be eliminated by their merger and pairwise annihilation through the continuous tuning of parameters 3 , 4 that preserve the symmetry of the system. The second class features Dirac points that are pinned stably to discrete high-symmetry points on the surface of the Brillouin zone (BZ). Markedly different from the first class of DSMs, the occurrence of Dirac points is an unavoidable result of the nonsymmorphic space group of the material 11 , 12 , 13 , which cannot be removed without changing the crystal symmetry. Although some solid-state candidate materials have been proposed 4 , 11 , 12 , symmetry-enforced 3D DSMs have never been experimentally realized because of the great challenge in synthesizing materials 4 , 7 . Recently, numerous distinct topological states have been demonstrated in classical wave systems 18 , 19 , such as photonic crystals 20 , 21 , 22 , 23 , 24 , 25 , 26 , 27 , 28 and phononic crystals 29 , 30 , 31 , 32 , 33 , 34 , which offer opportunities for exploring topological physics in a fully controllable manner. Here, we report an experimental realization of a 3D phononic crystal that hosts symmetry-enforced Dirac points at the BZ corners. The fourfold degeneracy is protected by a nonsymmorphic space group that couples point operations (rotations and mirrors) with nonprimitive lattice translations. In addition to the Dirac points identified directly by angle-resolved transmission measurements, highly intricate quad-helicoid surface states are unveiled by our surface measurements and associated Fourier spectra. Specifically, the surface states are composed of four gaplessly crossed spiral branches 13 and thus are strikingly different than the double Fermi arc surface states observed recently in electronic 8 and photonic systems 28 . Excellent agreement is found between our experiments and simulations. As illustrated in Fig. 1a , our Dirac phononic crystal has a body-centered-cubic (bcc) lattice associated with the lattice constant a = 2.8 cm. The main body of the building block consists of four inequivalent resin cylinders, which are labeled with different colors and oriented along different bcc lattice vector directions. All the cylinders have a regular hexagonal cross section with a side length of 0.42 cm. To facilitate sample fabrication, these cylinders are connected with short hexagonal bars with side lengths of 0.21 cm. The remainder of the volume is filled with air. Numerically, the photosensitive resin material used for printing the acoustic structure is treated as rigid, and sound propagates only in air (at speed 342 m\/s), considering the great acoustic impedance mismatch between the resin and air. Fig. 1: Symmetry-enforced Dirac points and quad-helicoid topological surface states in a nonsymmorphic phononic crystal. a Schematics of the bcc unit (left panel) of the phononic crystal and its (010) surface (right panel) featured with two glide mirrors G x and G z . b 3D bcc BZ and its (010) surface BZ. The colored spheres highlight the bulk Dirac points with equal frequency and their projections onto the surface BZ. c Bulk bands simulated along several high-symmetry directions. d Schematic of the quad-helicoid surface state dispersions (color surfaces), where the gray cone labels the projection of bulk states. e Surface bands simulated along a circular momentum loop of radius 0.4 \u03c0 \/ a (as shown in f ) centered at \\({\\bar{\\mathrm P}}\\) . The shadow regions indicate the projected bulk states. f 3D plot of the surface dispersion simulated in the first quadrant of the surface BZ. Bulk band projections are not shown for clarity Full size image The crosslinked network structure belongs to the nonsymmorphic space group 230 \\((Ia\\bar 3d)\\) , featuring inversion symmetry and multiple screw rotations and glide reflections. The crystal symmetry enables rich point and line degeneracies (see Supplementary Materials). Interestingly, the small group at P and P\u2019, a pair of time-reversal related Brillouin zone (BZ) corners (Fig. 1b ), has 24 group elements and supports only fourfold degeneracy. This finding is confirmed by the band structure in Fig. 1c , which is stabilized with two distinct kinds of Dirac points at P (P\u2019). The first kind of Dirac points, crossed with bands of different slopes and thus called generalized Dirac points 22 (e.g., the lowest ones at P and P\u2019 in Fig. 1c ), corresponds to a four-dimensional irreducible representation, whereas the second kind, crossed with bands of identical slopes (see Fig. S1 in Supplementary Materials), corresponds to two inequivalent two-dimensional irreducible representations stuck with time-reversal symmetry. Hereafter, we focus on the latter case (as specified with color spheres in Fig. 1c ), around which the bands are rather clean and carry a wide frequency window of linear dispersion. The system can be captured by a simple four-band effective Hamiltonian derived from \\(k \\cdot p\\) theory, \\({\\cal{H}} = \\left( {\\begin{array}{*{20}{c}} O & H \\\\ {H^\\dagger } & O \\end{array}} \\right)\\) , where \\(H = \\eta \\left( {\\delta k_y\\sigma _x - \\delta k_x\\sigma _y + \\delta k_z\\sigma _z} \\right)\\) , \u03b7 is a complex parameter determined by the acoustic structure, \\((\\delta k_x,\\delta k_y,\\delta k_z)\\) characterizes the momentum deviation from P, and \u03c3 i are Pauli matrices (see Supplementary Materials). The Dirac model gives isotropic linear dispersions around the Dirac point, which are much different from those anisotropic ones observed previously 7 , 8 , 9 , 10 , 28 . A nontrivial Z 2 topological invariant, defined on a momentum sphere enclosing the Dirac points, can be used to depict the topology of such fourfold band closing points 13 . This invariant is derived by considering the pseudo anti-unitary symmetry ( \\(\\vartheta\\) ) composited by a glide reflection ( G ) and time-reversal symmetry ( T ), i.e., \\(\\vartheta = G \\ast T\\) with \\(\\vartheta ^2 = - 1\\) . In addition, markedly different from the Dirac points created by band inversion 5 , 6 , which can be annihilated pairwise without changing the crystal symmetry, here the Dirac points are guaranteed completely by the nonsymmorphic symmetries. The topological robustness of the Dirac points against symmetry-preserving perturbations has been identified numerically by two detailed examples (Supplementary Materials, Fig. S2 ). Unlike Weyl semimetals that host topologically nontrivial Fermi arcs on their surfaces 35 , the presence of topological surface states in a DSM is more subtle because the Dirac points carry a zero Chern number 3 , 4 , 36 . However, for a nonsymmorphic DSM that has Dirac points featuring a nontrivial Z 2 index, the band crossing points will be pairwise connected by symmetry-protected Fermi arcs on the surface, associated with a unique connectivity determined by the nontrivial Z 2 topological charge 13 . The dispersion of the topological surface states can be mapped to an intersecting multihelicoid structure, where the intersections between the helicoids are protected from being gapped by the glide symmetries preserved on the specific surface. In our case, the Dirac phononic crystal supports elusive quad-helicoid surface states 13 if truncated with the (010) surface or its equivalents, which can be characterized by the wallpaper group p 2 gg . Below, we focus on the (010) surface that preserves the two glide mirrors \\(G_x = \\left\\{ {M_z\\left| {\\left( {a\/2} \\right)\\hat x + \\left( {a\/2} \\right)\\hat z} \\right.} \\right\\}\\) and \\(G_z = \\left\\{ {M_x\\left| {\\left( {a\/2} \\right)\\hat z} \\right.} \\right\\}\\) of the bulk crystal (Fig. 1a ). For this specific crystal surface, the two inequivalent Dirac points are projected onto the four equivalent surface BZ corners \\({\\bar{\\mathrm P}}\\) (Fig. 1b ). As schematically illustrated in Fig. 1d , the quad-helicoid surface states feature two crucial signatures. First, there are four branches of spiral surface states for any given momentum loop enclosing \\({\\bar{\\mathrm P}}\\) : two with positive helicities and two with negative helicities. Figure 1e shows the gapless surface bands simulated along a circular loop centered at \\({\\bar{\\mathrm P}}\\) . Second, the surface states of opposite helicities intersect along certain momentum lines, in which the intersecting double degeneracies are protected by the glides G x and G z assisted with time-reversal symmetry 13 . This effect is exhibited clearly in the simulated global dispersion profile (Fig. 1f ), which shows nodal line degeneracies along the surface BZ boundaries \\({\\bar{\\mathrm {P}}}{\\bar{\\mathrm {X}}}\\) and \\({\\bar{\\mathrm {P}}}{\\bar{\\mathrm {Z}}}\\) . (Only \u00bc of the surface BZ is provided due to the presence of the two glides.) For a generic selection of the crystal surface, the nodal line degeneracy of the surface dispersion disappears due to the absence of glide symmetries (Supplementary Materials, Fig. S3 ). The presence of symmetry-enforced Dirac points was confirmed by angle-resolved transmission measurements. Figure 2a demonstrates our experimental setup. The sample, fabricated precisely by the 3D printing technique, has a size of 47.6 cm, 14.0 cm, and 47.6 cm along the x , y, and z directions, respectively. A rectangular acoustic horn was used to launch a collimated beam upon the (010) surface of the sample, where the incident direction can be characterized by the angles \u03b8 and \u03c6 . As illustrated in Fig. 2b , a bulk state is expected to be excited when its in-plane momentum \\(\\vec{k} _{||}\\) matches that of the incident wavevector \\(\\vec{k} _{in}\\sin \\theta\\) at the same frequency. The transmitted sound signal was scanned by a 1\/4 inch microphone (B&K Type 4958-A) and recorded by a multi-analyzer system (B&K Type 3560B). The averaged sound intensities were normalized to those measured in the absence of the sample. The bulk states were mapped out by varying \u03b8 and \u03c6 . Here, only \\(\\varphi \\in [0,45^\\circ ]\\) was focused thanks to the multiple glide mirrors of the system. (For completeness, similar data for \\(\\varphi \\in [45^\\circ ,90^\\circ ]\\) are provided in Supplementary Materials, Fig. S4 .) Specifically, at \u03c6 = 45\u00b0, the incident beam scans through the Dirac point. Figure 2c shows the transmission data measured for six representative \u03c6 values compared with the numerical bulk dispersions projected along the corresponding directions (insets). All the transmission spectra agree reasonably well with the numerical band structures, where the low transmission near the sound cone can be attributed to the smaller effective cross-sectional area of the sample at large \u03b8 . In particular, as expected in the case of \u03c6 = 45\u00b0, a conic touch is observed at approximately 15.3 kHz in frequency and 0.71 \u03c0 \/ a in the wavevector. The point crossing is lifted gradually as \u03c6 decreases from 45\u00b0. Fig. 2: Experimental identification of the symmetry-enforced Dirac points. a Experimental setup for measuring sound transmission. b Schematic of exciting bulk states according to the momentum conservation \\(\\vec{k} _{||} =\\vec{k} _{in}{\\rm{sin}} \\theta\\) . c \u03b8 -resolved transmission spectra measured for different \u03c6 values. The slanted boundary (green line) in each panel corresponds to the \u2018sound cone\u2019 \\(|\\vec{k} _{||}| = |\\vec{k} _{in}|\\) , beyond which no transmission can be measured. Insets: Simulated bulk states (shadow regions) projected along the y direction, scaled to the same range and ratio as the measured data Full size image Furthermore, we performed surface measurements to identify the highly intricate topological quad-helicoid surface states, which have not been experimentally observed in any topological system to date. Figure 3a shows our experimental setup. To mimic the rigid boundary condition involved in our simulations, an additional resin plate with a thickness of 0.2 cm was integrated on the (010) surface, which served as a trivial acoustic insulator to guarantee the presence of topological surface states. Since the typical air channels of the sample are too narrow to accommodate the sound source and probe directly, the plate was perforated with a square lattice of holes (see inset), one of which was reserved for inserting sound source, and one of which was reserved for locating the probe during the measurement; the other holes not in use were sealed to avoid coupling with the air background surrounding the sample. To excite surface states, a broadband point-like sound source launched from a subwavelength-sized tube was injected into one hole near the center of the sample surface. The localized surface field was scanned hole-by-hole by manually moving the probe, where the scanning step was given by the lattice spacing of the holes (1.4 cm). By Fourier transforming the surface pressure field, we mapped out the nontrivial surface arc for any desired frequency 31 . Figure 3b shows such data for a sequence of frequencies. As predicted by the simulations, the measured surface arcs (bright color) exhibit clear crossings at the surface BZ boundaries \\({\\bar{\\mathrm {X}}}{\\bar{\\mathrm {X}}}^{\\prime}\\) and \\({\\bar{\\mathrm {Z}}}{\\bar{\\mathrm {Z}}}^{\\prime}\\) . Our experimental results effectively capture the simulated isofrequency contours of the topological surface states (black lines), despite the band broadening due to the finite-size effect. Note that the amplitude signals of the bulk states (enclosed by white dashed lines) are much weaker than those of the topological surface states that are highly confined to the surface. To further identify the gapless quad-helicoid surface states, we present the surface spectra (Fig. 3c ) measured along the momentum loop specified in the first panel of Fig. 3b . Compared with the loop used in Fig. 1e , this square loop enclosing the \\({\\bar{\\mathrm P}}\\) point is larger and favored to demonstrate the gapless intersection of the surface bands in a wide bulk gap. As expected, two pairs of surface bands with opposite helicities traverse the bulk gap and cross stably at the high-symmetry momenta \\({\\bar{\\mathrm X}}\\) ( \\({\\bar{\\mathrm {X}}}^{\\prime}\\) ) and \\({\\bar{\\mathrm Z}}\\) ( \\({\\bar{\\mathrm {Z}}}^{\\prime}\\) ). Again, excellent agreement is found between our experiment and simulation. Fig. 3: Experimental observation of quad-helicoid topological surface states. a Experimental setup for the surface field measurements. The inset shows the details of the cover plate with circular holes opened or sealed. The plugs that sealed the holes were opened one-by-one during the measurement. b Isofrequency contours plotted in one surface BZ centered at \\({\\bar{\\mathrm P}}\\) (see the first panel). The color scale shows the experimental data compared with the corresponding simulation results (black curves). The orange spheres label the projected Dirac points, and the white dashed lines enclose the bulk band projections. c Frequency-dependent surface spectra (color scale) measured along the momentum path specified in the first panel of b Full size image In conclusion, we have constructed and identified a spinless Dirac crystal working for airborne sound, which exhibits highly intricate properties in both the bulk and surface states, in sharp contrast to those realized previously in condensed matter systems 7 , 8 , 9 , 10 . The topological origin of quad-helicoid surface states deserves to be further investigated. Notably, in a very recent study, S. Zhang et al. have made the first step towards an experimental study of 3D Dirac points in classical wave systems 28 . Interestingly, the Dirac points are constructed by electromagnetic duality symmetry (which is unique in electromagnetic systems), which is also strikingly different from the crystalline symmetry involved here. Starting with our structure, one can design various interesting 3D acoustic topological states (e.g., Weyl points 29 , 31 and line nodes 37 , 38 ) through symmetry reduction. This study may open up new manners for controlling sound, such as realizing unusual sound scattering and radiation, considering the conical dispersion and vanishing density of states around the Dirac points. Last but not least, the dispersion around the Dirac point is isotropic, and thus, our macroscopic system serves as a good platform to simulate relativistic Dirac physics. Methods Numerical simulations All simulations were performed using COMSOL Multiphysics, a commercial solver package based on the finite element method. The bulk band structure in Fig. 1c was calculated by a single unit cell imposed with specific Bloch boundary conditions. Similar calculations gave the projected bulk states along the y direction (Fig. 2c , shadowed region). A ribbon structure was used to calculate the surface band for a desired surface (Fig. 1e, f and Fig. 3b, c ), imposed with Bloch boundary conditions along the x and z directions and a rigid boundary condition along the y direction, respectively. The ribbon was long enough to avoid coupling between the opposite surfaces. Surface states were distinguished from the projected bulk states by inspecting the surface localizations of the eigenstates. Experimental measurements Our experiments were performed for airborne sounds at audible frequencies. The slab-like sample, consisting of 17 \u00d7 5 \u00d7 17 structural units along the x , y, and z directions, was prepared by photosensitive resin via 3D printing. The macroscopic characteristics of our acoustic system enable precise sample fabrication and less demanding signal detection. To excite the bulk states, a rectangular acoustic horn (with a surface area of 24.0 cm \u00d7 10.0 cm) was used to launch Gaussian beams at controllable orientations (Fig. 2a ), whereas a narrow tube (with a diameter of 0.8 cm) was used to export point-like sound signals to excite the topological surface states (Fig. 3a ). During both measurements, a portable microphone was moved on the x\u2013z plane to scan the pressure fields, together with another identical microphone fixed for phase reference. Both the amplitude and phase information of the input and output signals, swept from 11.8 kHz to 18.2 kHz with an increment of 0.032 kHz, were recorded and analyzed by a multi-analyzer system. To map out each surface arc of a given frequency (Fig. 3b ), two-dimensional Fourier transformation was performed on the scanned surface field; this further gave the frequency-dependent surface spectra along the specific momentum loop (Fig. 3c ). Data availability The data that support the plots within this paper and other findings of this study are available from the corresponding author upon reasonable request. Code availability All codes that support this study are available from the corresponding author upon reasonable request. ","News_Body":"Dirac semimetals are critical states of topologically distinct phases. Such gapless topological states have been accomplished by a band-inversion mechanism, in which the Dirac points can be annihilated pairwise by perturbations without changing the symmetry of the system. Here, scientists in China report an experimental observation of Dirac points that are enforced completely by the crystal symmetry using a nonsymmorphic phononic crystal. Novel topological surface states are demonstrated in their experiments. The discovery of new topological states of matter has become a vital goal in fundamental physics and material science. A three-dimensional (3-D) Dirac semimetal (DSM), accommodating many exotic transport properties such as anomalous magnetoresistance and ultrahigh mobility, is an exceptional platform for exploring topological phase transitions and other novel topological quantum states. It is also of fundamental interest to serve as a solid-state realization of a (3+1)-dimensional Dirac vacuum. So far the realized Dirac points always come in pairs and could be eliminated by their merger and pairwise annihilation through the continuous tuning of parameters that preserve the symmetry of the system. In a new paper published in Light Science & Applications, scientists from the Key Laboratory of Artificial Micro- and Nano-Structures of the Ministry of Education and School of Physics and Technology, Wuhan University, China, we report an experimental realization of a 3-D phononic crystal that hosts symmetry-enforced Dirac points at the Brillouin zone corners. Markedly different from existing DSMs, the occurrence of Dirac points is an unavoidable result of the nonsymmorphic space group of the material, which cannot be removed without changing the crystal symmetry. In addition to the Dirac points identified directly by angle-resolved transmission measurements, highly intricate quad-helicoid surface states are unveiled by our surface measurements and associated Fourier spectra. Specifically, the surface states are composed of four gaplessly crossed spiral branches and thus are strikingly different than the double Fermi arc surface states observed recently in electronic and photonic systems. \"This study may open up new manners for controlling sound, such as realizing unusual sound scattering and radiation, considering the conical dispersion and vanishing density of states around the Dirac points. The dispersion around the Dirac point is isotropic, and thus, our macroscopic system serves as a good platform to simulate relativistic Dirac physics,\" the scientists forecast. ","News_Title":"Symmetry-enforced three-dimension Dirac phononic crystals","Topic":"Physics"}
{"Paper_Body":"Abstract Stomata are orifices that connect the drier atmosphere with the interconnected network of more humid air spaces that surround the cells within a leaf. Accurate values of the humidities inside the substomatal cavity, w i , and in the air, w a , are needed to estimate stomatal conductance and the CO 2 concentration in the internal air spaces of leaves. Both are vital factors in the understanding of plant physiology and climate, ecological and crop systems. However, there is no easy way to measure w i directly. Out of necessity, w i has been taken as the saturation water vapour concentration at leaf temperature, w sat , and applied to the whole leaf intercellular air spaces. We explored the occurrence of unsaturation by examining gas exchange of leaves exposed to various magnitudes of w sat \u2212 w a , or \u0394 w , using a double-sided, clamp-on chamber, and estimated degrees of unsaturation from the gradient of CO 2 across the leaf that was required to sustain the rate of CO 2 assimilation through the upper surface. The relative humidity in the substomatal cavities dropped to about 97% under mild \u0394 w and as dry as around 80% when \u0394 w was large. Measurements of the diffusion of noble gases across the leaf indicated that there were still regions of near 100% humidity distal from the stomatal pores. We suggest that as \u0394 w increases, the saturation edge retreats into the intercellular air spaces, accompanied by the progressive closure of mesophyll aquaporins to maintain the cytosolic water potential.     Main The question of whether the internal spaces of a leaf can become undersaturated under high evaporative conditions has remained unresolved for decades. In such a situation, the transpiration rate has to be reduced by mechanisms other than stomatal closure. Jarvis and Slatyer 1 discussed mechanisms proposed to account for non-stomatal control of transpiration, should it occur. The preferred option was \u201cincipient drying\u201d 2 \u2014the retreat of evaporation sites into the mesophyll cell walls, relying on increasingly smaller pore throats and the Kelvin effect. Jarvis and Slatyer 1 measured the resistances to the diffusion of nitrous oxide introduced to one side of a cotton leaf and compared them with the corresponding water vapour resistances of the same leaf, and suggested that relative humidity (RH) inside the leaf could be as low as 70%. This RH would require a water potential of the liquid water of \u221249 MPa, but most plants lose turgor at \u22122 to \u22125 MPa and reach a lethal leaf water potential not much after the turgor loss point. These researchers expressed the reduction in humidity as the product of the transpiration flux and a \u201cwall resistance\u201d but gave no explanation of the resistance. In contrast, Farquhar and Raschke 3 performed a similar experiment with cotton and other species using helium but saw no evidence of a resistance to transpiration within the leaves, suggesting that the humidity inside the substomatal cavity, w i , was near saturation and the water potential was close to zero. Egorov and Karpushkin 4 measured transpiration rates in air and in mixtures of helium and oxygen and concluded that the intercellular RH could be 90% to 85%. Canny and Huang 5 collected Eucalyptus pauciflora leaf discs at midday during late summer and concluded that intercellular RH could be as low as 90%. If the substomatal cavities are unsaturated, then in the standard gas exchange calculations 6 , 7 , the estimation of critical values such as apparent leaf conductance to water vapour, g , and the CO 2 concentration in substomatal cavities, c i , would give lower values than the true ones. In 2008, we found gradients of c i in several species that seemed to be incompatible with the 100% RH assumption of the gas exchange calculations. This led to further efforts over subsequent years to corroborate these results. As a consequence, Cernusak et al. 8 measured the oxygen isotope composition of transpiration and CO 2 assimilation and concluded that water-stressed conifers were experiencing intercellular RH as low as 80%. This was followed by the examination, using the same isotopic techniques, of wild-type Populus leaves as well as a transgenic variety insensitive to abscisic acid that fails to close stomata at high transpiration rates. As humidity decreased, the abscisic-acid-insensitive plants lost saturation 9 . Holloway-Phillips et al. 10 used a two-source method of contrasting oxygen isotopic composition and similarly found that in some cases intercellular RH appeared less than 100%. Despite these recent findings, there has been considerable scepticism 11 because of the lack of a known mechanism 12 to enforce the very low water potentials required to sustain unsaturation in the intercellular mesophyll air space. Theory For evaporation to occur, there will be a water vapour concentration gradient from the sites of evaporation through the stomatal pores to the ambient air. The question becomes: at what depth in the interior of the leaf is the air space saturated? The need for humidity gradients within the substomatal cavity to support vapour flux to the stomatal pore implies that where the saturation water vapour concentration at leaf temperature, w sat , is found depends on the stomatal aperture and the difference between w sat and the humidity in the air ( w a ), \u0394 w . However, it is reasonable to assume that under low \u0394 w (for example, \u0394 w < 8 mmol mol \u22121 ), the saturation edge is a surface around the entry of the stomatal pore within the leaf. We will refer to this surface as the saturation front ( w sat ). Thus, under a low \u0394 w , we assume that the whole intercellular air space is saturated so that w i = w sat (Fig. 1a ). In this condition, we assume that the pathway for water vapour and CO 2 between w sat and the atmosphere over the leaf surface ( w s ) is the same 13 , through the stomatal pore (Fig. 1a ). We can then estimate the stomatal resistance to water vapour ( r sw ) directly and estimate the stomatal resistance to CO 2 ( r sc ) from the ratio of water vapour and CO 2 diffusivities in air as r sc = 1.6 r sw . We denote the resistance to diffusion between stomatal cavities inside the upper and lower surfaces of the leaf as the intercellular air space resistance ( R ias ). A list of the abbreviations used is found in Supplementary Section 1 . Fig. 1: Diagram of the saturation front moving deeper within the leaf as \u0394 w increases. ( a ) represents the resistances in the water vapour path at low \u0394 w , ( b ) under moderate \u0394 w and ( c ) under high \u0394 w . \u0394 w stands for w sat - w a , the subscripts u and l denote upper and lower, w a is air humidity, w s is humidity at the leaf surfaces, w i is humidity at the substomatal cavity, w sat is the saturation front, r bw is the boundary layer resistance to water vapour, r sw is the stomatal resistance to water vapour and r unsat is the unsaturated mesophyll air space resistance. Full size image We postulate that under high \u0394 w , w sat moves farther into the leaf, departing from w i (Fig. 1b,c ), so when w i is assumed equal to w sat , we are adding to the estimation of r sw part of the resistance within the intercellular air space that is now unsaturated ( r unsat )\u2014that is, incorrectly estimating r sw as r sw + r unsat . This generates an overestimation of r sc by the addition of an equivalent resistance to CO 2 , resulting in an underestimated c i . Note that the apparent CO 2 concentration is unlikely to be realized at the same place as w sat , as the CO 2 and water probably follow different pathways after crossing the stomatal pore. However, by measuring the two surfaces of a leaf separately, we can track the apparent CO 2 concentration in the upper and lower substomatal cavities (Fig. 2 ). Fig. 2: Uncorrected gas exchange measurements of a cotton leaf using a double-sided, clamp-on chamber. a , A , E and g as functions of increasing \u0394 w = w sat \u2212 w a . b , The apparent c i of the upper and lower substomatal cavities, and the c i difference (upper minus lower) between them, are plotted as functions of \u0394 w = w sat \u2212 w a . The dotted line denotes zero difference between upper (adaxial) c i and lower (abaxial) c i . Photosynthetically active radiation was fixed at 1,000 \u00b5mol m \u22122 s \u22121 . Full size image Experimental approach to reconciling unsaturation Our approach was to follow Sharkey et al. 14 and Parkhurst et al. 15 by measuring gas exchange separately on the two sides of a leaf, and then reducing the ambient CO 2 concentration ( c a ) at the lower surface until the assimilation rate at that surface was zero. By repeating this measurement with both sides of the leaf exposed to an increasing w sat \u2212 w a (\u0394 w ), the gradient of [CO 2 ] from the upper \u2018fed\u2019 surface to the lower surface ( c iu \u2212 c il , where \u2018u\u2019 and \u2018l\u2019 denote upper and lower) could be examined as a function of \u0394 w . The gradient should be positive, but the results were striking. As \u0394 w increased, the apparent c iu \u2212 c il gradient decreased, even becoming negative at large \u0394 w , which is impossible, as the CO 2 has been fed through the upper surface. As w i is the only input in the calculation that is not directly measured, we argue that this is evidence of unsaturation in the substomatal cavity\u2014that is, that w i is less than w sat , and therefore w i \u2212 w a < \u0394 w . The true c iu \u2212 c il at each \u0394 w should reflect the normal diffusion of gases, assuming there are no structural changes in the leaf. Thus, here we took the observed, apparent value of c iu \u2212 c il at low \u0394 w , multiplied by the CO 2 assimilation rate ( A ) at the current \u0394 w , divided by the value of A at lowest \u0394 w (Supplementary Section 2 ). This yielded the target value of c iu \u2212 c il , and so w i was then adjusted in the gas exchange calculations until the calculated value of c iu \u2212 c il matched the target. Figure 2a shows the rates of CO 2 assimilation ( A ), transpiration ( E ) and g of a cotton leaf, plotted as functions of \u0394 w . The c au in the upper leaf chamber was around 400 \u03bcmol mol \u22121 . The c al was reduced to set A l to 0, while A u remained relatively constant at around 22 \u03bcmol CO 2 m \u22122 s \u22121 until \u0394 w exceeded 20.9 mmol mol \u22121 . E increased initially as \u0394 w increased and then began to decrease after \u0394 w reached 20.9 mmol mol \u22121 . Apparent water vapour conductance ( g ) decreased almost linearly as \u0394 w increased. Figure 2b presents the calculated c i of the upper and lower sides and the apparent c i difference ( c iu \u2212 c il ) of the cotton leaf plotted against \u0394 w . At \u0394 w of 7.1 mmol mol \u22121 , the cotton c i difference was 20.4 \u03bcmol mol \u22121 . The difference in c i decreased as \u0394 w increased, and at \u0394 w of 26.2 mmol mol \u22121 , the apparent c i difference became negative (\u221218.8 \u03bcmol mol \u22121 ). Similar data for a sunflower leaf are shown in Extended Data Fig. 1 . The experiment has been replicated 48 times, also by feeding the CO 2 on either the upper or lower surface, finding consistent results (see Supplementary Data 1 for ten examples). Other plant species tested, in which unsaturation was induced, include Phaseolus vulgaris, Xanthium strumarium, Eucalyptus pauciflora and Glycine max . Table 1 shows the values of c a , A , E , apparent g and c i at various \u0394 w for the cotton leaf shown in Fig. 2 , and Table 2 shows these values for the sunflower leaf shown in Extended Data Fig. 1 , as well as the values corrected to provide the target magnitude of c iu \u2212 c il . For the cotton leaf, at \u0394 w of 20.9 mmol mol \u22121 , the calculated RH of the substomatal cavity was 96%, and this decreased to 90% at 26.2 mmol mol \u22121 . Similarly, for the sunflower leaf, at \u0394 w of 10.1 mmol mol \u22121 , RH was 95%, and this decreased to 80% at 24 mmol mol \u22121 . Table 1 Gas exchange values estimated in a cotton leaf using routine calculations ( w i = w sat ) and corrected values adjusting w i to match the target c iu \u2212 c il Full size table Table 2 Gas exchange values estimated in a sunflower leaf using routine calculations ( w i = w sat ) and corrected values adjusting w i to match the target c iu \u2212 c il Full size table Measurements combining the c iu \u2212 c il technique (gas exchange) with the oxygen isotope technique presented by Cernusak et al. 8 and Holloway-Phillips et al. 10 (measuring 18 O in CO 2 and water) show close agreement in estimating the unsaturation of w i (Extended Data Fig. 2 ). Intercellular air space resistance to diffusion We measured the diffusion resistances across leaves using He, Ne and Ar. These gases have no biological interactions and are only trivially soluble in the leaf tissues. The noble gas was fed into the upper chamber and crossed through the upper and lower boundary layers, the upper and lower stomata, and the mesophyll air space resistances ( r bu , r su , r bl , r sl and R ias ) in series, giving a total resistance to diffusion ( R x , where x is the noble gas): $$\\begin{array}{l}R_{{x}} = r_{{{{\\mathrm{bu}} - x}}} + r_{{{{\\mathrm{su}} - x}}} + R_{{{{\\mathrm{ias}} - x}}} + r_{{{{\\mathrm{sl}} - x}}} + r_{{{{\\mathrm{bl}} - x}}}\\\\ R_{{x}} = R_{{{{\\mathrm{b}} - x}}} + R_{{{{\\mathrm{s}} - x}}} + R_{{{{\\mathrm{ias}} - x}}}.\\end{array}$$ (1) We rearranged equation ( 1 ) to estimate mesophyll air space resistances to the noble gas as $$R_{{{{\\mathrm{ias}} - x}}} = R_{{x}} - R_{{{{\\mathrm{b}} - x}}} - R_{{{{\\mathrm{s}} - x}}}.$$ (2) From water vapour measurements, correcting w i as in Table 1 and Table 2 , we obtain the upper plus lower corrected stomatal resistance ( \\(cR_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = cr_{{\\mathrm{su}} - {\\mathrm{H}}_2{\\mathrm{O}}} + cr_{{\\mathrm{sl}} - {\\mathrm{H}}_2{\\mathrm{O}}}\\) ) and the boundary layer resistance ( \\(R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) ). Then, using the ratio of the diffusion coefficient of water in air ( \\(D_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) ) over the diffusion coefficient of the noble gas in air ( D x ), and the corrected stomatal resistances to water vapour, we estimated the stomatal resistances to the noble gas as $$R_{{{{\\mathrm{s}} - x}}} = cR_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\left( {\\frac{{D_{{\\mathrm{H}}_2{\\mathrm{O}}}}}{{D_{{x}}}}} \\right),$$ (3) and, using the 2\/3 power of this ratio, we calculated the boundary layer resistances to the noble gas from those known for water vapour as $$R_{{{{\\mathrm{b}} - x}}} = R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\left( {\\frac{{D_{{\\mathrm{H}}_2{\\mathrm{O}}}}}{{D_{{x}}}}} \\right)^{2\/3}.$$ (4) Then, from our measurement of R x , we used equation ( 2 ) to deduce R ias\u2212 x : $$R_{{{{\\mathrm{ias}} - x}}} = R_{{x}} - R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\left( {\\frac{{D_{{\\mathrm{H}}_2{\\mathrm{O}}}}}{{D_{{x}}}}} \\right)^{2\/3} - cR_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\left( {\\frac{{D_{{\\mathrm{H}}_2{\\mathrm{O}}}}}{{D_{{x}}}}} \\right),$$ (5) and through multiplying by ( \\(D_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) \/ D x ) \u22121 , we estimated the mesophyll air space resistance to water from noble gas diffusion measurements ( \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) ): $$R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = R_{{{{\\mathrm{ias}} - x}}}\\left( {\\frac{{D_{{\\mathrm{H}}_2{\\mathrm{O}}}}}{{D_{{x}}}}} \\right)^{ - 1}.$$ (6) Analogous to equation ( 1 ), using equation ( 6 ), we obtained the total resistance to water including internal air space resistance from the noble gas measurements (Fig. 3 , squares): $$R_{{{x - {\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} + cR_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} + R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}},$$ (7) and the resistance of the intercellular air spaces, \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) , can be rewritten as $$R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = R_{{{x - {\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} - cR_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}},$$ (8) where \\(cR_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = cR_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} + R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) . \\(cR_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) is less than the uncorrected value ( \\(R_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = R_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} + R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) ) by an amount that Jarvis and Slatyer 1 called the \u201cwall resistance\u201d. We note that the term suggested in the 1970s might now be better thought of as \u2018unsaturated mesophyll air space\u2019 resistance ( R unsat ), as explained below. R unsat is then calculated as $$R_{{{{\\mathrm{unsat}}}}} = R_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} - cR_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}.$$ (9) Fig. 3: Series resistances to the diffusion of water vapour across a cotton leaf. a , b , Series resistances (upper case notation) to the diffusion of water vapour across a cotton leaf as inferred from measurements of neon ( R neon , squares); uncorrected water measurements, assuming w i = w sat , ( \\(R_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) , open circles); \u2018corrected\u2019 water measurements ( \\(cR_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) , solid circles); intercellular air space ( \\(R_{\\mathrm{ias}} = R_{\\mathrm{neon}} - cR_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) , diamonds); and unsaturated mesophyll ( \\(R_{\\mathrm{unsat}} = R_{{\\mathrm{H}}_2{\\mathrm{O}}} - cR_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) , triangles) versus \u0394 w ( a ) and E ( b ). Full size image The inclusion of the unsaturated mesophyll air space resistance leads to $$R_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} = cR_{{{{\\mathrm{s - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} + R_{{{{\\mathrm{b - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}} + R_{{{{\\mathrm{unsat}}}}}$$ (10) In Fig. 3a , we see in a cotton leaf that as \u0394 w increases, \\(R_{{{{\\mathrm{neon - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) remains the greatest resistance because it includes \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) . However, \\(R_{{{{\\mathrm{H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) approaches \\(R_{{{{\\mathrm{neon - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) as R unsat increases from zero at low \u0394 w . The feature of real interest is that R unsat almost reaches \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) at the greatest \u0394 w , indicating that unsaturation is spreading through the entire mesophyll air space. Our interpretation of the results is that as the vapour pressure difference increases, the saturated front behind the stomatal pores retreats from the substomatal cavities into the intercellular spaces. The extra pathlength for the water flux causes an extra resistance: an increasing R unsat . In amphistomatous leaves, the retreating front can go only as far as the front retreating from the other surface, and the maximum R unsat occurs when it equals \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) . Stomatal resistance is thought to respond to transpiration rates rather than humidity, at least at low and moderate \u0394 w (ref. 16 ). Figure 3b shows the dependencies of the series of resistances on E . The responses are not simple at high \u0394 w , presumably reflecting processes causing the reduction in E with increasing \u0394 w . Similar results are shown in Extended Data Fig. 3a,b in cotton using argon rather than neon. Similar results are also found in sunflower using neon (Extended Data Fig. 3c,d ). The data with helium (Extended Data Fig. 3e,f ) show an analogous pattern, but R unsat , while always less than \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) , does not reach it at the greatest \u0394 w . The binary diffusivities with air ( D x ) at 20 \u00b0C of He, Ne and Ar are 69.7, 31.3 and 18.9 mm 2 s \u22121 , respectively, so the conversions of noble gas data to water equivalence are likely to be more reliable with Ne and Ar than with He, as He diffusivity is almost three times H 2 O diffusivity in air ( \\(D_{{\\mathrm{H}}_2{\\mathrm{O}}}\\) = 24.7 mm 2 s \u22121 ). We confirmed that the resistances are of the correct order of magnitude, using an independent experiment to estimate \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) where A l is held at zero and \\(R_{{{{\\mathrm{ias - CO}}}}_{{{\\mathrm{2}}}}}\\) is calculated from c iu \u2212 c il (Supplementary Section 2 ). The value of \\(R_{{{{\\mathrm{ias - CO}}}}_{{{\\mathrm{2}}}}}\\) was then converted to a water basis using the factor 1.6 ( \\(D_{{\\mathrm{H}}_2{\\mathrm{O}}}\/D_{{\\mathrm{CO}}_2}\\) ), \\(R_{{{{\\mathrm{ias - H}}}}_2{{{\\mathrm{O}}}}} = R_{{{{\\mathrm{ias - CO}}}}_2}\/1.6\\) . Extended Data Fig. 4 shows a comparison of \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) derived from noble gas measurements with that derived more crudely but more simply from c iu \u2212 c il measurements and confirms the general magnitudes. Our calculations presented in Tables 1 and 2 assume that there are no structural changes in the leaf and that \\(R_{{{{\\mathrm{ias - CO}}}}_{{{\\mathrm{2}}}}}\\) remains constant. In Fig. 3 and Extended Data Fig. 3 , we can see small increases in \\(R_{{{{\\mathrm{ias - H}}}}_{{{\\mathrm{2}}}}{{{\\mathrm{O}}}}}\\) with the increase of \u0394 w (thereby increasing \\(R_{{{{\\mathrm{ias - CO}}}}_{{{\\mathrm{2}}}}}\\) ), which might indicate structural changes. However, the modest decreases in leaf water potential of about 0.2 MPa as \u0394 w increases (Table 3 ) suggest that these changes are small or negligible. If we impose the increase in \\(R_{{{{\\mathrm{ias - CO}}}}_{{{\\mathrm{2}}}}}\\) shown in Fig. 3 on our calculations, it impacts the estimation of w i \/ w sat by less than 0.01. Table 3 Bulk leaf water potential, \u03a8 bulk , and wall water potential, \u03c8 wall , in cotton leaves Full size table Implications of w i < w sat Turning now to the required modification of gas exchange equations 17 , the calculation for relating transpiration rate to stomatal resistance for one surface becomes (Supplementary Section 3 ) $$\\begin{array}{l}E = E_{\\mathrm{c}} + E_{\\mathrm{s}} = \\frac{{w_{{\\mathrm{sat}}} - w_{\\mathrm{s}}}}{{r_{\\mathrm{c}}}}\\\\\\qquad + \\frac{{\\left( {w_{{\\mathrm{sat}}} - w_{\\mathrm{s}}} \\right)\\left( {1 - w_{\\mathrm{i}}} \\right)}}{{\\left( {r_{\\mathrm{s}} + r_{{\\mathrm{unsat}}}} \\right)\\left( {1 - \\frac{{\\left( {w_{\\mathrm{i}} + w_{\\mathrm{s}}} \\right)}}{2}} \\right)\\left( {1 - \\frac{{\\left( {w_{{\\mathrm{sat}}} + w_{\\mathrm{i}}} \\right)}}{2}} \\right)}},\\end{array}$$ (11) where E c and E s are the cuticular and stomatal transpiration rates, respectively; w sat signifies the water vapour mole fraction saturation humidity at leaf temperature; w s is the water vapour mole fraction over the leaf surface; w i is the mole fraction humidity inside the substomatal cavity; and r c , r unsat and r s denote the cuticular, unsaturated mesophyll and stomatal resistances, respectively. In the special case where w i = w sat , then r unsat = 0. Note that R unsat = r unsatu + r unsatl . The occurrence of unsaturation within the leaf impacts the foundations of the current estimation of gas exchange, influencing our interpretation of gas exchange data and the information deduced from it. For instance, we examined the effects of correcting w i on the estimation of mesophyll conductance to CO 2 ( g m ) during experiments in which \u0394 w was increased (Fig. 4 ) in the cotton leaf featured in Fig. 2 and Table 1 . Fig. 4: Mesophyll conductance to the diffusion of CO 2 ( g m ) as a function of \u0394 w . We calculated g m using online 13 CO 2 fractionation and gas exchange data. The open circles represent g m calculated using the original data. The solid squares represent g m recalculated using the same dataset but with adjustment for the unsaturation of substomatal air spaces. The data are for the same cotton leaf as in Fig. 2 . Full size image The values of g m calculated using the original gas exchange data showed a sudden increase when \u0394 w increased beyond 20.9 mmol mol \u22121 . This was the same turning point where A and E began to decrease and the c i difference changed from +9 to \u221218 \u00b5mol mol \u22121 (Fig. 2 ). Using the corrected w i shown in Table 1 , the recalculated g m was fairly insensitive to \u0394 w . Hydraulic mechanism (aquaporins) Measurements of bulk leaf water potential ( \u03a8 bulk ), which account for the water in the whole tissue, in our plants were between \u22121.35 and \u22121.54 MPa; then, from the Kelvin effect, the expected RH within the substomatal cavity ( w i \/ w sat ) was about 99%. However, w i \/ w sat calculated from gas exchange measurements varied from 97% to 90% (Table 3 ). To achieve the latter, the expected water potential would be between \u22124.2 and \u221213.5 MPa, unlikely values for the cytosol of the cells, particularly considering that the turgor loss point of cotton leaves is about \u22121.5 to \u22122 MPa (ref. 18 ). In practice, water evaporates from the liquid phase of mesophyll cell walls to the air space within the leaf; thus, only the water potential in the liquid phase of the cell wall ( \u03c8 wall ) must be at equilibrium with the RH in the substomatal cavity ( w i \/ w sat ), and the cytosol appears to be largely protected from this stress. The cell wall is a thin structure formed by microfibrils creating an interconnected porous medium with pore diameters varying from about 0.05 \u03bcm to less than 0.006 \u03bcm, averaging 0.02 \u03bcm (refs. 19 , 20 ). From Jurin\u2019s law and a contact angle of water on cell wall fibrils of about 50\u00b0 (ref. 21 ), the cell wall can be considered fully saturated until a water potential of about \u22124.5 to \u22127 MPa and it has an air entry tension higher than \u221225 MPa, as the core of the cell wall microfibrils are in a more dense arrangement 22 . In practice, the cell wall will always have liquid water in living tissue. It is reasonable to assume that the water potential in the cytosol ( \u03c8 cyt ) is close to \u03a8 bulk , as this represents the great majority of the water in the tissue (including vacuoles), suggesting that \u03c8 wall and \u03c8 cyt of the cell are substantially different. Current models of leaf gas exchange assume that the vapour phase of the intercellular air space is in equilibrium with \u03c8 cyt or the leaf water potential (see Buckley 23 , Damour et al. 24 and Buckley and Mott 25 ), and therefore \u03c8 wall = \u03c8 cyt . However, our data indicate that \u03c8 wall must be noticeably lower than \u03c8 cyt . For cells to be able to hold this pressure difference between the cytosol and the cell wall (about \u22123 to \u221212 MPa) without drying out, they need to control the flow of water from the cytosol to the cell wall. This means effectively controlling the hydraulic conductivity of the cell membrane ( L p ). Under steady-state gas exchange conditions, the water evaporated from the cell wall surfaces ( E w ) is an upper limit on the flow of water across the membrane ( J m ) and thus can be expressed as J m \u2248 E w = L p ( \u03c8 cyt \u2212 \u03c8 wall ). The precise value of E w is unknown, but if we assume that it must be, say, ten times smaller than E to allow for the ratio of leaf surface to internal surface area, then L p should have values at high \u0394 w in the vicinity of 0.1 to 0.4 mmol m \u22122 s \u22121 MPa \u22121 . Martre et al. 26 report minimum measurements of L p in isolated protoplasts of 0.27 mmol m \u22122 s \u22121 MPa \u22121 with an average lower bound of 0.55 mmol m \u22122 s \u22121 MPa \u22121 , and similar minimum values have been reported in other studies 27 , 28 , 29 . The occurrence of unsaturation in the substomatal cavity and the gradual increase of R unsat while \u0394 w increases support our theory of w sat retreating from the substomatal cavity deeper into the mesophyll intercellular air space. The retreat of the saturation front adds diffusive length to the water vapour pathway, with the extra resistance having an upper bound equal to that of the full intercellular air space, meaning that R unsat will always be less than the intercellular air space resistance ( R ias ). This was commented on directly by Farquhar and Raschke 3 and appears to be the case with the \u2018wall\u2019 resistance data of Jarvis and Slatyer 1 , restricted to water contents greater than 80% where leaves retain turgor. Our data obtained from noble gases and measurements of \u03a8 bulk suggest that reducing leaf water potential increases R ias , though the leaves never lost turgor. The occurrence of a plateau in the increases of E with a later drop as \u0394 w rises suggests that there is active control of L p that is linked to the transpiration rate. Aquaporins seem to be a logical target to investigate as being responsible for controlling L p (ref. 30 ), as there is evidence that some aquaporins may close in dry air 31 , protecting water in those cells. This hypothesis would also explain the finding presented by Farquhar and Raschke 3 : when a leaf was stripped of its epidermis and exposed to dry air, E decreased rapidly without the expected reduction of leaf water potential, presumably because the aquaporins had already closed. This cellular level of water control requires further examination using novel methods. For example, Jain et al. 32 recently presented one such exciting technique using fluorescent powder to measure water potential. Our data show the assimilation rate to be little affected by unsaturation, which suggests an active control of cell membrane water conductivity without substantially impacting the cell membrane CO 2 permeability. Some aquaporins are permeable to CO 2 33 , 34 , and there is evidence of different pathways for water and CO 2 through the cell membrane 35 . This raises the question: if plants can substantially restrict water loss without affecting CO 2 permeability, why do they not replace stomata with such membranes, maximizing water\/carbon optimization? The membrane would need the water pathway restrained to avoid desiccation with whatever metabolic and structural costs required and mechanical support similar to that offered by cell walls. Additionally, a single layer of chloroplasts would have to be seated immediately behind the membrane to avoid any lengthy pathways through water, which slows diffusion, just as chloroplasts avoid mesophyll walls that abut on a neighbouring cell 36 . On the other hand, with stomata and a humid intercellular air space network instead of a single membrane, plants have multiple layers of membranes (mesophyll cells) with chloroplasts separated from the gas phase by only a thin cell wall with liquid water. In this way, membranes having aquaporin-like properties effectively wrapping the mesophyll cells, nature has added an exquisite mechanism to be called on when evaporative demand is high. This mechanism not only reduces evaporation with little apparent effect on carbon assimilation, at least initially, but also preserves a modest water potential of the symplast. Methods Plant materials Plants were grown in a glasshouse under natural sunlight in Canberra, during the austral spring\u2013summer and late autumn. During spring\u2013summer the midday photosynthetically active radiation ranged from 1,400 to 2,000 \u03bcmol m \u22122 s \u22121 , and during late autumn the midday photosynthetically active radiation was around 1,200 \u03bcmol m \u22122 s \u22121 . The air temperature was 28 \u00b1 2 \u00b0C during the day and 20 \u00b1 2 \u00b0C at night. The RH of the air was 40% during the day and 80 to 90% at night. Seeds of cotton ( Gossypium hirsutum L.) and sunflower ( Helianthus annuus L.) were sown in 10 l plastic pots containing steam-sterilized potting medium. Slow-release fertilizer (Osmocote, Scotts Australia) was added to the potting medium. To get uniform plants, the seedlings were thinned from four to one per pot after germination. Gas exchange device A double-sided, clamp-on chamber was used to measure gas exchange separately at the adaxial and abaxial surfaces. The leaf chamber was equipped with a fan for each side of the chamber. The boundary layer conductances to water vapour for the upper and lower cuvettes were 2.35 and 1.66 mol m \u22122 s \u22121 , respectively. The chamber enclosed a leaf area of 4.9 cm 2 . For the noble gas diffusion measurements, the leaf area enclosed by the chamber was 10 cm 2 , and the boundary layer conductances to water vapour for both the upper and lower cuvettes were 2.44 mol m \u22122 s \u22121 . Leaf temperature was measured with an infrared thermometer (model M50, Mikron Infrared). The infrared thermometer had a field of view of 9.5\u00b0, which covered a leaf area of 12 mm 2 . Leaf temperature was controlled by circulating water from a temperature-controlled water bath. Two platinum resistive elements (Pt100) were used to measure the chamber air temperature. The pressure difference between the upper and lower cuvettes was kept within \u00b12 Pa. This was measured with a high-sensitivity differential pressure sensor with a full range of \u00b1250 Pa (Model DP45, Validyne Engineering). Two identical through-flow gas exchange systems were used, one for each side of the leaf. Each system consisted of three mass flow controllers for N 2 , O 2 and CO 2 in air. CO 2 -free air was generated by mixing 79% N 2 with 21% O 2 . For measurements during 2009, the dry gas was humidified by passing through a bubbler. The humidity was controlled by flowing moist gas through a condenser, with the dew point controlled via circulating water from another temperature-controlled water bath. From 2010 onwards, the control of mixing of wet and dry air was modified by adding two mass flow controllers, one for wet air and one for dry air, for each side of the leaf. CO 2 was added to the gas stream after mixing of the wet and dry air. Humidity was measured using a humidity and temperature probe (Model HMP50, Vaisala). The probe was enclosed in a water jacket through which temperature-controlled water circulated from the leaf chamber water bath. Absolute and differential CO 2 concentrations were measured with the same infrared gas analyser (IRGA) (Model Li-6251, Li-Cor) by alternately switching zero gas, reference gas and sample gas to the reference and sample cells of the IRGA. Nafion tubes (Perma Pure) were used to dry gas before entering the IRGA. The two IRGAs were calibrated daily against a calibration gas of known CO 2 concentration. The stability of the two humidity sensors was checked continuously against the dew point of the respective condensers. To check the possible bias of the upper and lower gas exchange instruments, the upper chamber outlet gas line could be switched to the lower chamber IRGA and humidity sensor. Averaged values of the two sets of CO 2 and humidity measurements from each chamber were used in the calculations. A platinum resistive element was used to measure the temperature inside the condenser, and the reading was taken as the dew point of the gas flowing from it. A barometer (BAROCAP PTB 110, Vaisala) was used to measure the pressure inside the condenser. There was an option to bypass the humidifier and condenser when very dry gas was required to achieve a high leaf-to-air water vapour concentration difference, \u0394 w . CO 2 was added to the gas stream after the condenser. Gas exchange technique The gas exchange measuring sequence started at low \u0394 w , usually around 6 to 10 mmol mol \u22121 , and a c a for both upper and lower leaf chambers at around 380 \u03bcmol mol \u22121 or 400 \u03bcmol mol \u22121 . Photosynthetically active radiation was set at 1,000 \u03bcmol m \u22122 s \u22121 . After the rates of gas exchange reached steady state, which normally took one to two hours, the ambient [CO 2 ] for the lower leaf chamber, c al , was reduced until A l was 0 \u00b1 0.3 \u03bcmol m \u22122 s \u22121 (zero). The difference, calculated as upper c i minus lower c i , was taken as the gradient of CO 2 that was required for sustaining A . Increases in \u0394 w from about 6 to 10 mmol mol \u22121 were achieved by increasing the leaf temperature from 24 \u00b0C to 27 \u00b0C. Further increases in \u0394 w to maximum values in both chambers were achieved by decreasing the condenser temperature and bypassing the humidifier and condenser. Gas exchange calculations included ternary effects and those of cuticular conductances 7 , 17 . Minimum leaf surface conductance was measured on leaves sprayed with 10 \u22124 M cis - trans abscisic acid (Sigma Chemicals). Minimum leaf conductances for the upper and lower surfaces of cotton were 2 mmol m \u22122 s \u22121 and 3.5 mmol m \u22122 s \u22121 , respectively (6 mmol m \u22122 s \u22121 and 7 mmol m \u22122 s \u22121 , respectively, for sunflower). Minimum leaf surface conductances were approximated as cuticular conductances in the calculations. The flow rates of gas to both the upper and lower leaf chambers, measured with mass flow meters, were between 0.7 l min \u22121 and 1.5 l min \u22121 . Noble gases In our experiments with noble gases, neon (10% Ne in N 2 ), argon (100% Ar) and helium (100% He) were individually fed to the upper chamber. These gases were added to the main gas line via a mass flow controller after the mixing of wet and dry synthetic air and CO 2 . Synthetic air was used to avoid measuring double-ionized argon as neon in the mass spectrometer. The final concentration of noble gas in the main gas line was around 1,000 \u03bcmol mol \u22121 . The concentration of the noble gas was measured by passing the air mixture directly into an isotope ratio mass spectrometer (Isoprime) through a capillary. There were four gas lines linking the gas exchange system to the mass spectrometer: (1) a dry 79% N 2 and 21% O 2 mixture (the zero gas), (2) the noble gas inlet, (3) the upper leaf chamber outlet and (4) the lower leaf chamber outlet. In one measurement with a cotton leaf, both CO 2 and Ne were fed to the lower chamber of the leaf. Oxygen isotopic composition measurements Further experiments were carried out using the gas exchange technique combined with the oxygen isotope techniques of Cernusak et al. 8 and Holloway-Phillips et al. 10 . This involved a cavity-ring-down laser water isotope analyser (Picarro L2130-i, Picarro) that measured the \u03b4 18 O of water vapour and a dual quantum cascade laser (QC-TILDAS) absorption spectroscope that measured the C and O isotopic composition of CO 2 (QCLAS-ISO, Aerodyne Research). The w i was then estimated accounting for the difference between the expected \u03b4 18 O in water 10 and CO 2 and the measured values. Mesophyll conductance Mesophyll conductance to the diffusion of CO 2 , g m , was measured using the online carbon isotope discrimination technique described by Evans et al. 37 as modified by Cernusak et al. 38 , with ternary corrections as per Farquhar and Cernusak 39 . This involved the use of online trapping, and analysing the CO 2 on a dual-inlet isotope ratio mass spectrometer to measure \u03b4 13 C of the CO 2 in the chamber and then to estimate g m from the difference between the expected and the measured \u03b4 13 C. Leaf water potential The gas exchange technique described above was used to generate unsaturation in cotton leaves before sampling them to measure \u03a8 bulk . The \u03a8 bulk was measured from leaf tissue within the chamber. Two circular samples 6 mm in diameter were taken after achieving unsaturation conditions, and each sample was placed in a psychrometer (PSY1 psychrometer, ICT International). Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data Availability All generated and analysed data from this study are included in the published article and its Supplementary Information . ","News_Body":"Scientists from The Australian National University (ANU) and James Cook University (JCU) have identified an \"exquisite\" natural mechanism that helps plants limit their water loss with little effect on carbon dioxide (CO2) intake\u2014an essential process for photosynthesis, plant growth and crop yield. The discovery, led by Dr. Chin Wong from ANU, is expected to help agricultural scientists and plant breeders develop more water-efficient crops. Study co-author Dr. Diego Marquez from ANU said the findings will have significant implications for the agricultural industry and could lead to more resilient crops that are capable of withstanding extreme weather events, including drought. \"Plants continuously lose water through pores in the 'skin' of their leaves. These same pores allow CO2 to enter the leaves and are critical to their survival,\" Dr. Marquez said. \"For every unit of CO2 gained, plants typically lose hundreds of units of water. This is why plants require a lot of water in order to grow and survive. \"The mechanism we have demonstrated is activated when the environment is dry, such as on a hot summer day, to allow the plant to reduce water loss with little effect on CO2 uptake.\" The researchers believe this water preserving mechanism can be manipulated and, in turn, may hold the key to breeding more water-efficient crops. According to lead author Dr. Wong, the ANU team's findings are a \"dream discovery\" from a scientific and agricultural perspective. \"The agriculture industry has long held high hopes for scientists to come up with a way to deliver highly productive crops that use water efficiently,\" Dr. Wong said. \"Plant scientists have been dealing with this big question of how to increase CO2 uptake and reduce water loss without negatively affecting yields. \"Having this mechanism that can reduce water loss with little effect on CO2 uptake presents an opportunity for agricultural scientists and plant breeders researching ways to improve water use efficiency and create drought-tolerant crops.\" Although the researchers have confirmed there is a system in place that is working to limit the amount of water being lost from the leaf, they still don't know what's causing it. \"Our main target now is to identify the structures inside the plant that allow this control. We think that water conduits, called aquaporins, located in the cell membranes are responsible,\" Dr. Marquez said. \"Once we're able to confirm this, we can then start thinking about how we can manipulate these systems and turn them into an asset for the agricultural industry.\" Co-author Distinguished Professor Graham Farquhar from ANU said: \"Finding the mechanism itself was one step, a big one, but there is still work to do to translate this discovery into the industry. \"We expect that both government and industry will see the value of contributing funds to achieve this goal.\" Dr. Wong first alluded to this water preserving mechanism 14 years ago, but the research team has only now been able to officially confirm its existence thanks to years of experimentation and corroboration of their results. The research is published in Nature Plants. ","News_Title":"'Dream' discovery could sow crops better equipped to weather the climate change storm","Topic":"Biology"}
{"Paper_Body":"Abstract Two-dimensional crystals with angstrom-scale pores are widely considered as candidates for a next generation of molecular separation technologies aiming to provide extreme, exponentially large selectivity combined with high flow rates. No such pores have been demonstrated experimentally. Here we study gas transport through individual graphene pores created by low intensity exposure to low kV electrons. Helium and hydrogen permeate easily through these pores whereas larger species such as xenon and methane are practically blocked. Permeating gases experience activation barriers that increase quadratically with molecules\u2019 kinetic diameter, and the effective diameter of the created pores is estimated as \u223c 2 angstroms, about one missing carbon ring. Our work reveals stringent conditions for achieving the long sought-after exponential selectivity using porous two-dimensional membranes and suggests limits on their possible performance. Introduction Two-dimensional (2D) membranes with a high density of angstrom-scale pores can be made by engineering defects in 2D crystals 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 , 9 or, perhaps more realistically in terms of applications, by growing intrinsically porous crystals such as, e.g., graphynes 10 , 11 , 12 . Interest in angstroporous 2D materials is strongly stimulated by potential applications, particularly for gas separation as an alternative to polymeric membranes employed by industry 3 , 13 . On one hand, the atomic thickness of 2D materials implies a relatively high permeability as compared to traditional 3D membranes. On the other hand, angstrom-scale pores with effective sizes d P smaller than the kinetic diameter d K of molecules should pose substantial barriers for their translocation, which is predicted to result in colossal selectivities S > 10 10 , even for gases with fractionally ( \u223c 25%) different d K such as, for example, H 2 and CH 4 1 , 14 , 15 . This unique combination of material properties holds a promise of better selectivity-permeability tradeoffs than those possible by conventional membranes 3 , 13 . At present, this optimistic assessment is based mostly on theoretical modeling. Experimental clarity has so far been achieved only for the classical regime of d P > d K where the flow is governed by the Knudsen equation, and the resulting modest selectivities arise from differences in thermal velocities of gases having different molecular masses m 7 , 8 , 9 , 16 . For smaller pores with d P \u2248 d K , S up to 10\u2013100 have been reported for monolayer graphene 5 , 8 , and even higher selectivities ( \u223c 10 4 ) were found for some defects with an estimated diameter of \u223c 3.5 \u00c5 in bilayer graphene 4 . Still, this is many orders of magnitude smaller than S predicted for the activated-transport regime, d P < d K 1 , 14 , 15 . Little remains known about the latter regime, which has proven to be extremely difficult to reach in experiment 5 , 8 , 9 . Indeed, even monovacancies in dichalcogenide monolayers were suggested to exhibit the conventional Knudsen flow 9 . The experimental difficulties and lack of understanding are further exacerbated by prohibitive computational costs of simulating molecular permeation in the activated regime 17 , 18 , 19 , 20 . In this work, we achieve the activated regime by creating individual angstrom-scale pores in monolayer graphene by its short exposure to a low-energy electron beam. Gas permeation measurements reveal exponentially large selectivities with activation barriers that depend quadratically on gas molecules\u2019 kinetic diameter. Results Experimental devices Our devices were micrometer-size cavities sealed with monolayer graphene (Fig. 1a ). The microcavities were fabricated from graphite monocrystals, using lithography and dry etching, and had internal diameters of 1\u20133 \u03bcm and depth of \u223c 100 nm (\u201cMethods\u201d). Large, exfoliated graphene crystals were then transferred in air on top of the microcavities, creating \u201catomically tight\u201d sealing 21 . The sealing was tested by placing the devices into a He atmosphere and monitoring changes in graphene membrane\u2019s position by atomic force microscopy (AFM) (Fig. 1b ). We selected only the devices that were completely impermeable to He (\u201cMethods\u201d; ref. 21 ). Next, the He-tight membranes were subjected to electron radiation using a scanning electron microscope. The accelerating voltage was chosen to be \u226410 kV, and the beam current was set at 10 pA. In a single exposure lasting 3\u20135 s and using magnification of 700, an area of ~150 \u00d7 150 \u03bcm 2 was radiated, which translated into an electron dose of 0.1\u20130.2 \u03bcC cm \u22122 or only \u223c 1 electron per 100 nm 2 . After the exposure, the devices were He-leak tested again. The procedure was repeated several times, until a leak appeared indicating a damage induced by electrons (Fig. 1c ). Fig. 1: Creating defects in suspended graphene. a Schematic of our devices. Left: Monolayer graphene sealing a microcavity was bombarded with electrons. Initially, the membrane sagged inside the cavity due to adhesion to the side walls 4 , 5 , 21 . Right: After pressurization, defected membranes bulged out. b AFM images of the same device before (left) and after (right) its exposure to 10 keV electrons; dose of 0.5 \u03bcC cm \u22122 . Both images were taken after storing the device in Kr at 3 bar for 10 days. The white curves are height profiles along the membrane diameter 21 . \u03c3 is the membrane\u2019s central position measured with respect to graphite\u2019s top surface. The gray scale is given by \u03c3 \u2248 \u221215 and +24 nm in the left and right images, respectively. c Examples of \u03c3 as a function of radiation dose and acceleration voltage. Each point is taken after pressurizing the devices in 3-bar Kr. Dashed lines: guides to the eye; short black lines: \u03c3 = 0. d \u03c3 ( t ) for a device with the medium-size pore denoted as type 2, after pressurizing it with various gases (color coded). Solid curves: best linear fits. Inset: representative height profiles for a deflating device with Ar inside. Full size image Number of pores We argue that, in most cases, only a single pore was created during such exposures. This conclusion is supported by the following observations. First, the pores appeared after extremely small doses of <0.01 electron per typical pore (its size is determined later in the report). Second, the pores appeared suddenly, usually after not one but several such exposures (Fig. 1c ), and no additional damage or modification normally occurred during further, much longer exposures (>100 times larger doses; Supplementary Fig. 3 ). Third, no pores could be created in ~20% of our devices even after hour-long exposures. This clearly shows that the observed perforation was not a continuous damage process but represented rare single events. They can be attributed to the presence of \u201cweak spots\u201d where damage could be induced by the electron beam. Most of our devices had only one such spot, whereas the others had none or two, which would explain the above observations (Supplementary Fig. 3 ). Fourth, we followed ref. 5 . and sealed the leaking pores with sparsely dispersed Au nanoparticles ( Supplementary Information ; Supplementary Fig. 2 ), an approach used previously to argue the presence of individual pores in graphene membranes. Fifth and most unequivocal, only three discrete pore sizes were ever observed in our experiments rather than their statistical distribution (see below). If several pores were present in a single membrane, a broad distribution of leak rates should have been observed. All this evidence suggests the presence of a single pore in our typical device. As for a mechanism of creating such pores, incident electrons with an energy of 10 keV can transfer at most ~1.8 eV to a carbon atom, which is ten times less than the threshold energy (18\u201320 eV) required for knock-on damage 22 , 23 . Many electrons would be needed to strike the same carbon atom nearly simultaneously to remove it from the graphene lattice, which is statistically impossible especially for the used low doses. Accordingly, we tentatively attribute the pore formation to \u201cchemical etching\u201d of graphene with locally adsorbed water, which was activated by the electron beam, as reported previously 24 , 25 , 26 . We also speculate that further electron-beam exposure protected graphene from continuous water-mediated damage because hydrocarbons adsorbed on graphene became cross-linked 27 , 28 and prevented water molecules from reaching the surface. This would be consistent with our observation of rare damage events and the absence of the pores\u2019 modification during further radiation. It would be interesting to gain further information about the discussed etching processes, which can probably be achieved by numerical simulations. On impossibility of imaging individual atomic-scale pores in graphene Unfortunately, no existing technique can visualize the created pores\u2019 atomic structures. Indeed, let us first compare the doses used in our experiments with those typical for studies of graphene defects by high-resolution transmission electron microscope (HRTEM). In the latter case, beam currents were ~10 5 \u201310 6 electron nm \u22122 s \u22121 with exposure times of many seconds 22 , 27 , 29 , 30 . In contrast, our pores were created using a dose of only ~10 \u22122 electron per square nm, at least seven orders of magnitude lower than needed for HRTEM imaging. Furthermore, we used low-energy (typically 8 kV) beams whereas atomic-resolution TEMs operate at 60 kV or higher. The combination of the high doses and high acceleration voltages required for HRTEM imaging would inevitably result in additional defects in graphene or modification of the existing ones. Even if we were to find a rare angstrom-size pore in our large \u03bcm-scale membranes, it would be impossible to argue that the defect was previously there, created by the low-energy beam rather than emerged during the imaging, leaving aside the fact that defects in graphene are known to be strongly modified by > 60 kV used for HRTEM imaging. The fact that it is practically impossible to visualize the studied pores also applies to AFM and scanning tunneling microscopy (STM). Although AFM allows the atomic resolution using freshly cleaved graphite or multilayer graphene, monolayer graphene presents a much harder challenge, especially because of mechanical instabilities induced by the tip interacting with suspended membranes. Vacancies and other atomic-scale defects were previously imaged by STM using atomically clean graphene 31 , 32 but our membranes after the electron-beam exposure are not clean or flat, being covered by an atomically thin layer of hydrocarbon contamination 4 , 5 , 27 . Not surprisingly, all the previous reports on individual pores in graphene could not visualize them either 4 , 5 . Gas permeation through the atomic-scale pores The defected membranes prepared as described above were subjected to further permeation tests using various gases (namely, He, Ne, Ar, Kr, Xe, H 2 , CO 2 , O 2 , N 2 and CH 4 ). To this end, the devices were placed in a chamber containing a mixture of air at 1 bar (to match the air captured inside during fabrication) and the tested gas at a partial pressure P of typically \u22653 bar. Storage for 2\u201320 days, depending on the gas, allowed pressures inside and outside to equalize so that the membranes reached stable-in-time positions. After taking the devices back into air, graphene membranes would normally bulge out (Fig. 1a, b ) and then gradually deflate, which was monitored by AFM (Fig. 1d ). For quantitative analysis, we recorded the central position \u03c3 of bulged membranes (Fig. 1b ) as a function of time t . Initially, \u03c3 evolved linearly with t , indicating a constant outflow of the tested gas (Fig. 1d ), until its partial pressure inside dropped leading to saturation in \u03c3 ( t ), in agreement with the behavior reported in refs. 4 , 5 . We used the initial slope to evaluate the permeation rate J for each gas, as described in Supplementary Information . Repeating this procedure at different P , we confirmed that J \u221d P (Supplementary Fig. 1 ) and, therefore, the pores could be characterized by their P -independent permeance J* = J \/ P . For slowly permeating gases, our range of J* was limited by observation times of several days, which yielded a permeance of \u223c 10 \u221231 mol s \u22121 Pa \u22121 , that is, less than one gas atom per minute escaping the cavity. It is due to this exceptional sensitivity that we could detect flows through individual pores in the activated-transport regime, which would be difficult if not impossible to access otherwise 4 , 5 , 9 , 21 . As for the upper limit on J* , it was determined by the required time of \u223c 3 min to obtain an AFM image after taking devices from the gas chamber, which translates into \u223c 10 \u221223 mol s \u22121 Pa \u22121 , if using high P = 10 bar and our largest cavities. Our measurements of J* are summarized in Fig. 2 on the basis of more than 40 devices, with each one used to probe several gases. Only three distinct types of pores were observed. This is illustrated by Fig. 2a that compares J* for Ne and Kr (30% different d K ). The measured selectivities S = J* (Ne)\/ J* (Kr) fall into clearly separated groups. Small scattering around the average S within each group can be attributed to random local strain or curvature 21 . We refer to the groups as type 1, 2 and 3 pores, according to their S . Using other acceleration voltages between 4 and 10 kV, again only the same three types of pores were observed. This is the strongest evidence in favor of only one pore per membrane (see the other arguments above). The only possibility we cannot rule out is that, for membranes exhibiting highest permeance, two types of pores could be present. For example, type 3 pore could in principle be also present in some devices referred to as type 1 because the biggest pore should dominate the permeation rate. Even if such statistically unlikely events did happen, this would not change any of our conclusions below. Fig. 2: Gas selectivity for graphene pores created by electron bombardment. a Selectivity between Ne and Kr as a function of the dosage at which the pores appeared under an 8 kV electron beam. Each symbol denotes a different device. Three distinctive groups are emphasized by their color with the solid lines indicating the average S for each group. Vertical lines: guides to the eye indicating typical threshold doses for different pore types. b \u2013 d J* for the three types of pores using ten different gases, as annotated in the panels. Error bars: SD for typically six but minimum three devices. Solid curves in b \u2013 d : best fits to the exponential selectivity J* \u221d exp (\u2212 \u03b1d K ) for noble gases with \u03b1 being constants. Because of the limited range of d K , the data fit equally well with J* \u221d exp (\u2212 \u03b1d K 2 ) (not shown). Dashed curves: guides to the eye for diatomic gases. The arrows in d refer to undetectable permeation for Xe and CH 4 . Full size image Figure 2a also shows that the radiation dose at which a pore appeared can serve as a good predictor of its type, before doing actual gas permeation measurements, with low and high doses favoring type 3 and 2 pores, respectively. The observed nonmonotonic dependence of pores\u2019 permeability on radiation dose seems surprising. Indeed, the appearance of bigger pores for larger doses as in the case of type 1 and 3 pores is what is generally expected. To obtain tighter pores (type 2) using doses higher than those allowing the largest pores (type 1) is somewhat counterintuitive. Note however that, in all the cases, the pores appeared spontaneously at some weak spots and did not evolve further with increasing the dose (Supplementary Fig. 3 ). We speculate that the weak spots are determined by local strain and\/or random adatoms on the graphene surface and, once such a spot is in place, the low-energy beam would eventually activate its damage into a predetermined structural configuration. Characteristics of each pore type are detailed in Fig. 2b\u2013d . All the pores exhibited exponential dependences J* ( d K ) with type 3, the least permeable pores, being most selective, followed by type 2 and 1. Judging by their permeance, type 1 pores are similar to those created by ultraviolet-induced oxidation 5 . Within our sensitivity limits, the smallest (type 3) pores were completely impermeable to Xe and CH 4 yielding selectivity > 10 7 with respect to He or H 2 , which is higher than S for any type of membranes reported in the literature. Surprisingly, diatomic gases exhibited systematically higher J* than noble gases (Fig. 2 ). This cannot be due to the elongated shape of diatomic molecules because d K corresponds to the smallest cross-section 33 , that is, the most favorable orientation for translocation. Furthermore, Fig. 2 shows that the observed permeation was controlled mainly by spatial confinement rather than, e.g., chemical affinity: otherwise, translocation of molecules containing certain atoms like oxygen would fall out of the monotonic sequences. Temperature dependence To investigate the underlying sieving mechanisms, we measured temperature ( T ) dependences of J* for all pore types. An example of such measurements is shown in Fig. 3a whereas Fig. 3b plots the extracted activation energies E A , using J* = \u03bd \/( N A P ) exp(\u2212 E A \/ k B T ) where N A is the Avogadro number, k B is the Boltzmann constant and \u03bd is the impingement rate. If plotted as a function of d K 2 (rather than d K ) our data closely follow E A = \u03b1 ( d K 2 \u2212 d 0 2 ). This dependence allows the following interpretation. The pores have an empty space with the diameter d 0 which is free from graphene\u2019s electron clouds (inset of Fig. 3b ). To \u201csqueeze\u201d through the pore, atoms and molecules must disturb a region of ~\u03c0( d K 2 \u2212 d 0 2 )\/4 in size, and both electronic and elastic contributions are expected to scale with this area (Supplementary Fig. 5 ). The same \u03b1 for all three pore types strongly supports the above interpretation, indicating that \u03b1 is determined by the graphene properties, independently of pores\u2019 configurations and diameters. Fig. 3: Characterizing the angstrom pores. a Example of the measured T dependences for type 2 pores (color coded T ). Symbols: experimental data for Ar. Solid lines: linear fits. Inset: resulting Arrhenius plot (same color-coding). Solid curve: best fit yielding E A \u2248 0.4 eV. b E A for noble gases and different pore types shown as a function of d K (note the nonlinear x axis). Symbols: experimental data with error bars showing SD, using the same set of devices as in Fig. 2 . Solid curves: best fits with E A = \u03b1 ( d K 2 \u2212 d 0 2 ) using same \u03b1 . Inset: One of possible atomic-scale defects ( Supplementary Information ) with d 0 close to that of type 2 pores (blue circle\u2019s diameter is 2.5 \u00c5). c Impingement rates \u03bd at 1 bar for the same gases and E A as in b . The solid line: best fit using 1\/ \u03b2 = 40 meV 34 , 35 . Blue shaded area: impingement rates \u03bd 0 if the noble atoms were coming from the bulk only. Note that, because of the upper limit on J* \u2248 10 \u221223 mol s \u22121 Pa \u22121 , we could not obtain the Arrhenius plots for gases with higher permeability than Ne. All E A and impingement rates that were possible to obtain using our experimental setup are presented in b and c . Full size image Next, we analyze the pre-exponential factors \u03bd (Fig. 3c ), which were found from the measured T dependences such as in Fig. 3a . For atoms arriving from the bulk, their impingement rate is given by \u03bd 0 = AP \/(2\u03c0 mk B T ) 1\/2 where A is the effective pore area 5 , 17 , 18 , 21 , which yields \u03bd 0 of the order of 10 8 s \u22121 at 1 bar for all our pores and gases. In contrast, the experiment yielded several orders of magnitude higher \u03bd (Fig. 3c ). This unambiguously indicates that translocating atoms come not from the bulk but mostly through adsorption and surface diffusion 17 , 18 , 20 . Discussion The impingement rate \u03bd ad due to adsorption\u2013diffusion processes can be expressed as ( Supplementary Information ) $${\\upsilon }_{{ad}}=\\frac{P} {\\sqrt{2{\\pi} m{k}_{B}T}} \\sqrt{\\frac{{k}_{B}T}{2{\\pi} m}}\\frac{C}{{f}_{d}}$$ (1) where C is the circumference of the pore and f d is the desorption frequency of adsorbed gases. The desorption frequency f d is described by the van \u2018t Hoff equation: \\({f}_{d}=\\frac{{k}_{B}T}{h}{{{{{\\rm{exp }}}}}}\\left(\\frac{\\Delta S}{{k}_{B}}\\right){{{{{\\rm{exp }}}}}}\\left(-\\frac{{E}_{{ad}}}{{k}_{B}T}\\right)\\) , where h is the Planck constant, k B T \/ h the vibration frequency of adsorbed gases, \u0394 S the entropy change during the permeation process and E ad is the adsorption energy ( E ad is positive for this notation). The involvement of the adsorption\u2013diffusion mechanism has the following consequences on gas selectivity 8 , 18 . First, the measured E A should be notably lower than the actual translocation barriers, as the former values are reduced by the adsorption energy E ad ( Supplementary Information ). Second, the mechanism should favor permeation of stronger-adsorbed diatomic gases, in agreement with their systematically higher J* as compared to noble gases (Fig. 2 ). In the limit of zero E A , the impingement rate in Fig. 3c extrapolates close to \u03bd 0 , as generally expected because this limit corresponds to the Knudsen flow. On the other hand, the strong dependence \u03bd \u2248 \u03bd 0 exp( \u03b2E A ) in Fig. 3c is rather surprising. We speculate that it can be due to entropy loss during the surface-transport permeation process, as discussed in the literature 34 , 35 , and is a result of an increasingly large area that supplies gas molecules to the pore mouth, which rapidly grows with increasing the barrier 2 , 34 (see Supplementary Information ). Note that polymeric membranes exhibit similar \u03bd ( E A ) dependences with a universal, material-independent coefficient \u03b2 \u2248 1\/(40 meV) 34 , 35 which value also matches well our results (Fig. 3c ). The origins of such universality remain unknown 34 , 35 . Although the importance of the adsorption\u2013diffusion mechanism for small pores is well documented in the literature 17 , 19 , 34 , 36 , it is especially difficult to extrapolate the existing simulations onto our case because of the extreme crowding effects expected for ultimately small, angstrom-scale pores 19 . The surface contamination of any realistic membrane (rather than idealized graphene) complicates perspective theoretical analysis even further. To conclude, our work provides experimental feedback for extensive theoretical studies of molecular transport through angstrom-scale pores and reveals some unexpected features of the activated-transport mechanism. The mechanism critically involves adsorption and surface diffusion, which places strong constraints on the pore sizes required to reach high selectivity. The found pre-exponential factor \u221d exp( \u03b2E A ) counteracts the Arrhenius behavior exp(\u2212 E A \/ k B T ) and strongly reduces selectivity for any given pair of gases. Although atomic structures of the studied pores remain unknown, type 3 pores could be similar in size to hepta-vacancies ( Supplementary Information ) and intrinsic pores in \u03b3 -graphyne. Only if 2D membranes with such angstrom pores of high density are developed, one can envisage separation technologies with selectivities beyond the existing selectivity-permeability bounds (for projections based on our results, see Supplementary Fig. 6 ). Methods Device fabrication and inspection To make our devices and test their atomically tight sealing, we followed the procedures developed in ref. 21 . In brief, monocrystals of graphite with a thickness of >200 nm were prepared by mechanical exfoliation on an oxidized silicon wafer. The crystals were examined in an optical microscope using both dark-field and differential-interference-contrast modes to locate relatively large areas (over tens of microns in size), which were free from wrinkles, folds, atomic-step terraces, and other defects. Then, using electron-beam lithography and dry etching, an array of microwells with internal diameters of 1\u20133 \u03bcm and depth of \u223c 100 nm was fabricated within the found atomically flat areas. After overnight annealing at 400 \u00b0C in H 2 \/Ar atmosphere (volume ratio of 1:10), the microwells were sealed with a large crystal of monolayer graphene, which was transferred in ambient air (Fig. 1 ). The resulting devices were carefully inspected using AFM, and those showing any damage to their sealing were discarded. Such damage could be, for example, extended defects in the atomically flat top surface of the microwells or wrinkles in the graphene sealing 21 . The remaining devices were leak tested by placing them into a stainless-steel chamber containing Ar or Kr at a partial pressure P \u2248 3 bar. After a few days, they were taken out and quickly (typically within 3 min) checked using AFM for any changes in the membrane position (Fig. 1b ). Again, we discarded those devices that exhibited any sign of leakage, namely, if changes in the membrane position after pressurization were >1 nm. Finally, we repeated the same leak test but in an atmosphere of helium at 1 bar. Only devices with no changes in membrane positions were kept for further investigation. Perforating graphene with low-energy electrons Devices that successfully passed the above inspection were exposed to electron irradiation in scanning electron microscope Zeiss EVO. To evaluate the radiation exposure of the studied graphene membranes, we first measured the beam current using a Faraday cup. Then the electron beam was switched off and the membrane device with the known coordinates on the substrate was moved into a central position within a projected exposure area. The beam was then switched on and scanned over this entire area for a few seconds, using magnification 700 with a single area scan lasting \u223c 0.1 s. The simultaneously taken images ensured that membranes were in the center and properly exposed to the beam. After each exposure, the devices were subjected to the same leak tests as described above. We repeated the exposure-test cycle several times until the irradiated container started to exhibit a leak, indicating a defect created in the graphene membrane. In about 20% of cases, we could not create any discernible leak, no matter how long the graphene membranes were exposed to the electron beam. In another 20% of cases, we found an increase in permeation after additional exposures, which probably indicates the creation of the second, larger defect (Supplementary Fig. 3 ). No changes in permeation rates occurred after further prolonged exposures, even those leading to visible hydrocarbon contamination 25 , 26 , 27 , 37 . Data availability All relevant data to support this study are available upon request from the corresponding authors. ","News_Body":"By crafting atomic-scale holes in atomically thin membranes, it should be possible to create molecular sieves for precise and efficient gas separation, including extraction of carbon dioxide from air, University of Manchester researchers have found. If a pore size in a membrane is comparable to the size of atoms and molecules, they can either pass through the membrane or be rejected, allowing separation of gases according to their molecular diameters. Industrial gas separation technologies widely use this principle, often relying on polymer membranes with different porosity. There is always a trade-off between the accuracy of separation and its efficiency: the finer you adjust the pore sizes, the less gas flow such sieves allow. It has long been speculated that, using two-dimensional membranes similar in thickness to graphene, one can reach much better trade-offs than currently achievable because, unlike conventional membranes, atomically thin ones should allow easier gas flows for the same selectivity. Now a research team led by Professor Sir Andre Geim at The University of Manchester, in collaboration with scientists from Belgium and China, have used low-energy electrons to punch individual atomic-scale holes in suspended graphene. The holes came in sizes down to about two angstroms, smaller than even the smallest atoms such as helium and hydrogen. In December's issue of Nature Communications, the researchers report that they achieved practically perfect selectivity (better than 99.9%) for such gases as helium or hydrogen with respect to nitrogen, methane or xenon. Also, air molecules (oxygen and nitrogen) pass through the pores easily relative to carbon dioxide, which is >95% captured. The scientists point out that to make two-dimensional membranes practical, it is essential to find atomically thin materials with intrinsic pores, that is, pores within the crystal lattice itself. \"Precision sieves for gases are certainly possible and, in fact, they are conceptually not dissimilar to those used to sieve sand and granular materials. However, to make this technology industrially relevant, we need membranes with densely spaced pores, not individual holes created in our study to prove the concept for the first time. Only then are the high flows required for industrial gas separation achievable,\" says Dr. Pengzhan Sun, a lead author of the paper. The research team now plans to search for such two-dimensional materials with large intrinsic pores to find those most promising for future gas separation technologies. Such materials do exist. For example, there are various graphynes, which are also atomically thin allotropes of carbon but not yet manufactured at scale. These look like graphene but have larger carbon rings, similar in size to the individual defects created and studied by the Manchester researchers. The right size may make graphynes perfectly suited for gas separation. ","News_Title":"Precision sieving of gases through atomic pores in graphene","Topic":"Nano"}
{"Paper_Body":"Abstract Immune-mediated anti-tumoral responses, elicited by oncolytic viruses and augmented with checkpoint inhibition, may be an effective treatment approach for glioblastoma. Here in this multicenter phase 1\/2 study we evaluated the combination of intratumoral delivery of oncolytic virus DNX-2401 followed by intravenous anti-PD-1 antibody pembrolizumab in recurrent glioblastoma, first in a dose-escalation and then in a dose-expansion phase, in 49 patients. The primary endpoints were overall safety and objective response rate. The primary safety endpoint was met, whereas the primary efficacy endpoint was not met. There were no dose-limiting toxicities, and full dose combined treatment was well tolerated. The objective response rate was 10.4% (90% confidence interval (CI) 4.2\u201320.7%), which was not statistically greater than the prespecified control rate of 5%. The secondary endpoint of overall survival at 12 months was 52.7% (95% CI 40.1\u201369.2%), which was statistically greater than the prespecified control rate of 20%. Median overall survival was 12.5 months (10.7\u201313.5 months). Objective responses led to longer survival (hazard ratio 0.20, 95% CI 0.05\u20130.87). A total of 56.2% (95% CI 41.1\u201370.5%) of patients had a clinical benefit defined as stable disease or better. Three patients completed treatment with durable responses and remain alive at 45, 48 and 60 months. Exploratory mutational, gene-expression and immunophenotypic analyses revealed that the balance between immune cell infiltration and expression of checkpoint inhibitors may potentially inform on response to treatment and mechanisms of resistance. Overall, the combination of intratumoral DNX-2401 followed by pembrolizumab was safe with notable survival benefit in select patients (ClinicalTrials.gov registration: NCT02798406). Main Glioblastoma is the most common and lethal adult primary brain tumor. The standard of care treatment for newly diagnosed patients includes surgical resection followed by concomitant chemoradiotherapy and adjuvant temozolomide 1 . Despite maximal multimodal therapy, patients invariably experience recurrence of their disease 7 months after diagnosis, on average 1 . Unfortunately, treatment options at recurrence are scarce. Existing salvage therapies have very limited efficacy, with median survival being in the range of only 6\u20138 months after tumor progression 2 . Effective treatments for recurrent disease are urgently needed. While immune checkpoint blockade by anti-PD1 or anti-PD-L1 antibodies have improved outcomes with objective responses in a variety of other cancers, including those in the brain such as metastatic melanoma 3 , they have had limited efficacy as monotherapy for recurrent glioblastoma where the microenvironment is innately immunosuppressive (that is, immunologically \u2018cold\u2019) 4 , 5 . Oncolytic viruses are capable of reconditioning the tumor microenvironment toward a \u2018hot\u2019 phenotype, providing rationale for combinatorial therapy with checkpoint inhibitors, which has been shown to improve outcomes in other cancers 6 , 7 . DNX-2401 (tasadenoturev; Delta-24-RGD) is a conditionally replicative oncolytic adenovirus engineered to treat high-grade malignant gliomas 8 , 9 . The virus contains two stable genetic changes in the adenovirus dsDNA genome that cause it to selectively and efficiently replicate in cancerous cells. A dose-escalation phase 1 study demonstrated that stereotactic delivery of DNX-2401 into patients with high-grade gliomas was safe and induced cell death initially by direct oncolysis and subsequently by antitumor response from infiltrated immune cells, with durable responses after a single intratumoral dose 10 . In this Article, we report the results of CAPTIVE (2401BT-002P; KEYNOTE-192; NCT02798406 ), a two-part, phase 1\/2, multicenter, open-label clinical trial of combined intratumoral injection of DNX-2401 with systemic pembrolizumab for patients with recurrent glioblastoma. This is the first in-human investigation of combined oncolytic virus with immune checkpoint blockade for recurrent glioblastoma. Results Patient demographics and baseline characteristics A total of 49 patients from 13 of the 15 participating institutions were enrolled between 28 September 2016 and 17 January 2019 (Fig. 1a ). The demographic and baseline clinical characteristics of all patients enrolled are reported in Table 1 . The median age of patients was 53 years, and 41% were women. The majority of patients (80%) presented after first recurrence, and 18% of patients were using steroids at baseline. All patients had histopathological diagnosis of glioblastomas, except one patient enrolled with gliosarcoma (2%). Most patients (90%, N = 44) had reported IDH1 wild-type tumors, four (8%) had IDH1 mutant tumors and IDH1 mutation status was not known for one patient. All patients had received prior treatment with temozolomide and radiotherapy, six (12%) patients had prior bevacizumab treatment and five (10%) had prior treatment with a tumor-treating fields device. Fig. 1: Survival and response to treatment. a , Patient flow in trial. b , Waterfall plot that displays the maximal change in tumor size for all patients who received full-dose DNX-2401 treatment ( n = 42). Bars represent the maximal tumor change from baseline on the basis of contrast-enhanced MRI. Bars are colored according to responses classified according to mRANO criteria. c , Survival for each patient by DNX-2401 dose. The bar colors show the response to treatment according to the mRANO criteria. Arrows indicate that the patient remains alive. d , Overall survival for the intent to treat population. Crosses denote censored data. Full size image Table 1 Patient demographics and baseline characteristics Full size table Safety Forty-eight of 49 (98%) patients were treated with one dose of DNX-2401 after a standard biopsy, which was then followed by pembrolizumab starting 7 days later. One patient enrolled in the first dose cohort received 5 \u00d7 10 8 viral particles (v.p.) DNX-2401 but did not start pembrolizumab due to delirium, which was attributed by the investigators to anesthesia used during biopsy, unrelated to treatment. This patient was included in the safety analysis set only, per protocol. There were no dose-limiting toxicities observed, and the maximal dose tested (5 \u00d7 10 10 v.p. DNX-2401) was selected as the declared dose for the dose-expansion phase. In total, across both dose-escalation and dose-expansion phases, patients were treated with 5 \u00d7 10 8 ( n = 4), 5 \u00d7 10 9 ( n = 3) and 5 \u00d7 10 10 v.p. DNX-2401 ( n = 42). The median duration of exposure to treatment with DNX-2401 and pembrolizumab was 153 days (range 21\u2013753 days), including three patients (6%) who completed the full 2 year course of pembrolizumab therapy. An overview of adverse events (AEs) in the study is summarized in Extended Data Tables 1 and 2 and Supplementary Table 1 . Overall, DNX-2401 in combination with pembrolizumab was generally well tolerated and AEs were primarily as expected for patients with recurrent glioblastoma, with the majority of these being grade 3 or lower events. There were no AEs related to adenoviral infection. There were no deaths related to AEs that were related to treatment. One patient died approximately 7 months after initiating treatment due to hyperosmolar hyperglycemic nonketotic acidosis, which was considered unrelated to treatment. AEs that were considered to be related to treatment are summarized in Table 2 . The majority of these events were grade 1 or 2 events, with the most common being brain edema (37%), headache (31%) and fatigue (29%). Longitudinal volumetric changes of perilesional edema are shown in Extended Data Fig. 1 . We found that patients with and without symptomatic edema both had increases in volumetric measurements of perilesional edema from 8 weeks to 20 weeks after treatment. Patients who did not develop symptomatic edema begin to have a decrease in volume of perilesional edema after 20 weeks, whereas those who develop symptomatic edema continue to have increases in volume of perilesional edema after 20 weeks. Treatment-related serious AEs that were noted in more than one patient included brain edema (16%), dysphasia (6%) and hemiparesis (6%). Serious cerebral edema was managed with either short-course dexamethasone (89%) and\/or other concomitant supportive medications including bevacizumab (18%; Supplementary Table 2 ). Surgical intervention was not needed for serious cerebral edema in any patient. Pembrolizumab was interrupted or discontinued for four patients who had cerebral edema but resumed after resolution. One patient had grade 3 cerebral edema, somnolence and hemiparesis that started 23 days after initiation of treatment, leading to treatment discontinuation and resolution of the AE. A summary of serious AEs related to treatment is provided in Supplementary Table 3 . Table 2 Summary of AEs related to treatment Full size table Efficacy The efficacy and survival endpoints are summarized in Table 3 . According to modified Response Assessment in Neuro-Oncology (mRANO) criteria, two patients had a complete response and three patients had a partial response (Fig. 1b,c ) yielding an objective response rate of 10.4% (90% CI 4.2\u201320.7%) in the intent-to-treat population and 11.9% (90% CI 4.8\u201323.4%) for patients treated with the declared dose of DNX-2401, which was numerically greater than prespecified historical rate of 5% but did not meet statistical endpoint. One additional patient of interest had a complete response at the lesion where DNX-2401 was delivered approximately 8 months after treatment; however, a new lesion at a distant site was evident at the same assessment and the patient was therefore classified to have progressive disease. The median time to response was 3.0 months (range 1.9\u201317.4 months), and median duration of response was 9.4 months (range 1.8\u201333.7 months) in patients who showed an objective response. An additional 22 patients in the intent-to-treat population and 18 patients in the declared dose population had stable disease lasting longer than 28 days, which resulted in a clinical benefit rate of 56.2% (95% CI 41.1\u201370.5%) and 54.8% (95% CI 38.7\u201370.2%), respectively. The median duration of clinical benefit was 3.7 months (range 1.7\u201337.7 months). A summary of therapies received after treatment and at or after disease progression is presented in Supplementary Table 4 . Table 3 Summary of efficacy endpoints Full size table Patients with objective responses did not universally harbor characteristics that are commonly described in prognostically favorable tumors (Table 4 ). All patients with objective responses had reported IDH1 wild-type tumors by immunohistochemistry (IHC), and only two of them had had tumors with MGMT promoter hypermethylation. Additional targeted sequencing revealed that two patients with objective responses harbored mutations in either IDH1 or IDH2 at low allelic frequencies. Three of the patients with objective responses only had prior radiation and chemotherapy without prior resection of their tumor. The median tumor diameter was similar in patients with and without objective response (32.8 mm, 95% CI 25.2\u201346.6 mm versus 28.4 mm, 95% CI 24.8\u201330.8 mm; Supplementary Fig. 1 ). Table 4 Baseline characteristics of patients with complete or partial responses per mRANO criteria Full size table The two patients with complete response each had over 80% reduction in tumor volume approximately 6 months after treatment, which reached complete response criteria by 15\u201318 months after treatment (Fig. 2 ). These two patients completed 2 year treatment with pembrolizumab with durable responses and remain alive without evidence of disease progression. Fig. 2: Complete responses to DNX-2401 and pembrolizumab. a , Axial T1-weighted MR (top row) and FLAIR images (bottom row) obtained at baseline, 3 months, 6 months, 12 months and 38 months after infusion of DNX-2401 for one complete responder. b , The change of tumor size over time in each patient with a complete response. Dotted black line represents no change relative to baseline. Dashed red line represents the threshold for response according to the mRANO criteria. Both patients showed response to treatment at 3 months after DNX-2401 infusion, with complete response by 15\u201318 months. Full size image Survival analyses The secondary efficacy endpoint of 12 month survival was met. The 12 month overall survival was 52.7% (95% CI 40.1\u201369.2%) in the intent-to-treat population and 53.1% (95% CI 36.8\u201367.0%) in patients who received the declared dose of DNX-2401 (Fig. 1d ), and this was greater than the prespecified threshold of 20% from an approved treatment approach. The median overall survival was 12.5 months (10.7\u201313.5 months) in the intent-to-treat population and 12.5 months (95% CI 10.2\u201313.0 months) in declared dose population. Patients with objective responses had longer survival than patients without objective responses that was statistically significant (hazard ratio (HR) 0.20, 95% CI 0.05\u20130.87, P = 0.02; Extended Data Fig. 2 ). Three patients, all with objective responses (including the two patients with complete response), completed the prespecified pembrolizumab treatment and remain alive at the time this Article was written, beyond the study interval, at 45, 48 and 60 months. Moreover, one patient, with an IDH1 wild-type and MGMT unmethylated tumor received a total of six doses of pembrolizumab with overall stable disease. This patient elected to discontinue participation in the study and remained alive over 34 months after initiation of treatment. Exploratory associations We considered that concurrent use of medications may have impacted outcomes. Physicians were permitted to use low-dose bevacizumab or corticosteroids to address cerebral edema in this trial. Baseline corticosteroid use and corticosteroid use throughout the study were not statistically associated with outcomes, though use of corticosteroids throughout the study approached the threshold for statistical significance in some instances (Extended Data Table 3 ). Moreover, none of the patients with an objective response received bevacizumab during treatment. We also considered that variability in intrinsic patient and tumor factors might be associated with differences in outcomes of patients. To characterize potential biomarkers of treatment response, we obtained gene expression data on 38 patients with biopsy specimens available before treatment. We divided tumors from this study into three tumor microenvironment subtypes (TME high , TME medium and TME low ) on the basis of the degree of immune cell enrichment (Extended Data Fig. 3a ), as recently described 11 . TME high tumors had high scores for multiple different immune cells but also highly expressed multiple complementary suppressive immune checkpoints genes (Extended Data Fig. 4 ). By contrast, TME low tumors had low immune cell scores with low expression of immune checkpoint genes. TME medium tumors had intermediary immune cell scores and expression of PDCD-1 (gene that encodes PD-1) but relatively low expression of other checkpoint proteins. We found that pre-treatment gene expression levels of PDCD-1 , but not CD274 ( gene that encodes PD-L1), was statistically significantly associated with reduction in tumor size (Extended Data Fig. 3b and Supplementary Fig. 2 ). All of the patients who had an objective response had TME medium tumors before treatment (29.4%, 95% CI 10.3\u201355.6%, P = 0.012). Patients with TME medium tumors were more likely to have clinical benefit from treatment (odds ratio (OR) 4.08, 95% CI 1.02\u201319.4, P = 0.036; Extended Data Fig. 3c ), and also had statistically significantly longer survival in our cohort (HR 2.27, 95% CI 1.09\u20134.49, P = 0.027; Extended Data Fig. 3d ). Patient samples from a prior trial investigating adjuvant anti-PD1 monotherapy in recurrent glioblastoma 12 were also divisible into the same three TME subtypes, but associations between TME subtypes and outcomes were less clear in this population treated with monotherapy (Extended Data Fig. 3c,e ). Ten patients also had biopsy specimens at the time of disease progression after treatment allowing for a biological assessment of matched-pair tissues. Of these ten patients, one initially had a partial response to treatment before progression, while the other nine patients did not demonstrate objective responses (three patients with progressive disease as best response and six patients with initially stable disease as best response). Comparing gene expression profiles at disease progression after treatment to those at baseline before treatment revealed several differentially expressed genes (Extended Data Fig. 5a ). Genes that were overexpressed in post-treatment specimens were highly enriched for pathways involved in immune system activation and regulation by functional enrichment analysis (Extended Data Fig. 5b ). The patient with a partial response to treatment showed heightened immune activity after treatment relative to other patients, with the highest levels of interferon gamma and downstream signaling, infiltration of T cells, as well as the highest score for a T-cell inflamed microenvironment (Extended Data Fig. 5c ) 13 . Moreover, the expression of several different immune checkpoint genes such as TIGIT (log 2 fold change (FC) 1.77), LAG3 (log 2 FC 2.05) and CD276 (log 2 FC 2.06) were consistently increased in post-treatment samples, and this was highest for the patient with a partial response to treatment. We performed immunophenotypic characterization of tumors before and after treatment by blinded immunohistochemical and multiplex immunofluorescence analysis. Patients with TME medium and TME high tumors by gene expression subtyping also showed progressively greater density of immune cell infiltrates by IHC and immunofluorescence (Extended Data Fig. 6a\u2013b,d ). Comparing specimens before and after treatment, we found that increases in density of microglia (Iba1), macrophages (CD68) and lymphocytes (CD3, CD4 and CD8) after treatment were most evident in the patient who showed an objective response to treatment (Extended Data Fig. 6c,e ). Certain pathogenic mutations are potentially associated with prognosis and specific response to checkpoint inhibition in glioblastoma 14 . Clinically relevant molecular features were reported by investigators for tumor biopsies analyzed using various assays at each clinical site. Investigators reported MGMT status, IDH1\/2 mutation and, for 42 of 49 subjects, pathogenic mutations. Targeted next-generation sequencing was also separately performed on available tumor biopsies on a subset of patients. A notable number of pathogenic mutations, including those in TP53 , NF1 , PTEN, MTOR and RB1 were detected, as were a few mutations in POLE and POLD1 . There was no clear association between these specific molecular features, including tumor mutational burden, on response to treatment (Table 4 and Supplementary Table 5 ). Anti-adenovirus antibodies were measured by direct immunofluorescence assay in the serum of patients before treatment and throughout the course of the trial. All patients were seropositive for IgG antibodies against adenoviral hexon protein before treatment with DNX-2401, and in general, anti-adenovirus IgG levels increased within 2 months post treatment, with levels sustained longest in patients treated with 5 \u00d7 10 10 v.p. DNX-2401, compared to lower doses (Extended Data Fig. 7a,b ). We considered that variability in systemic immunogenic response to DNX-2401 might have impacted outcomes. The median overall survival of patients with and without a systemic immunogenic response to DNX-2401 delivery, which we defined as a greater than fourfold increase in baseline levels of anti-adenovirus antibodies, were similar (12.5 months, 95% CI 10.8\u201315.9 months versus 12.8 months, 95% CI 10.6 months to not reached). These findings were unchanged using more stringent thresholds of greater than tenfold increase in baseline levels of anti-adenovirus antibodies (12.9 months, 95% CI 12.0 months to not reached versus 12.3 months, 95% CI 8.9\u201316.6 months; Extended Data Fig. 7c,d ) Discussion Glioblastoma is a devastating disease, and recurrence of disease is inevitable after initial treatment with radiotherapy and concurrent and adjuvant temozolomide chemotherapy. At progression, treatment options are very limited and of marginal efficacy. Immune checkpoint blockade in other advanced solid cancers such as melanoma 15 , 16 , 17 and non-small cell lung cancer 18 , 19 has greatly improved outcomes. However, the innately immunologically cold microenvironment in glioblastomas has presumably rendered immune checkpoint blockade less effective for this disease 4 , 5 . DNX-2401 (Delta-24-RGD) is a conditionally replicative oncolytic adenovirus with a 24 base pair deletion in the E1A gene that renders selective replication of the virus in malignant cells with defective retinoblastoma signaling. DNX-2401 also has an RGD peptide insertion into the fiber knob that allows the virus to anchor directly to integrins and improve the infectability of glioblastoma cells 9 . Preclinical studies of DNX-2401 in glioma mouse models showed promising antitumor immune activity as early as 1\u20132 weeks after delivery of a single dose of virus with potential for longer-term antigen-specific memory responses 9 , 20 . This led to the first in human trials of DNX-2401 for glioblastoma, where in addition to direct oncolytic effects, we showed that the delivery of the virus into tumors induced an immunogenic environment with increased T-cell infiltration and also altered the expression of checkpoint proteins 10 . Treatment with oncolytic virus and immune checkpoint blockade combines the initial local effects of the oncolytic virus on the tumor microenvironment with the systemic effects of innate and adaptive immune responses from virus replication and PD-1 inhibition 7 . This combination has led to improved outcomes in other tumors, such as melanoma 6 , pointing to the possibility for therapeutic benefit of combination therapy in glioblastoma. Systematic screening of co-signaling molecules after DNX-2401 treatment in preclinical glioma models revealed significant increases in PD-1 expression that would prime the immune system for effective synergy with subsequent anti-PD-1 therapy 21 . Indeed, combination therapy of a single intratumoral dose of DNX-2401 followed by systemic pembrolizumab 1 week after viral treatment improved survival compared to monotherapy with either virus or pembrozliumab alone in glioma mouse models, providing rationale for further investigation in humans 21 . Here we report the results of a two-part, phase 1\/2, multicenter, open-label clinical trial evaluating the safety and efficacy of combined intratumoral delivery of DNX-2401 with systemic pembrolizumab for patients with recurrent glioblastoma treated at 13 institutions in North America. All centers used purpose-built cannulas to standardize the delivery of virus into the tumor, eliminating backflow and ensuring full administration of virus to the tumor. A total of 48 of 49 patients successfully received treatment with DNX-2401 and pembrolizumab. We tested between 5 \u00d7 10 8 to 5 \u00d7 10 10 v.p. of DNX-2401 when delivered sequentially with pembrolizumab and found that the safety profile was consistent with prior studies reporting on oncolytic viruses or immunotherapies for brain tumors 3 , 10 , 22 . There were no dose-limiting toxicities in the dose-escalation phase of this study, and no deaths that were directly related to the treatment regimen. The most common serious AE reported was neurological symptoms related to increase in peritumoral inflammation (cerebral edema), which occurred in 16% of patients. We anticipated the possibility for treatment-induced cerebral edema when designing this study due to inflammatory responses observed in phase 1 study of DNX-2401 monotherapy 10 , and so we allowed for a short-course steroid or low-dose bevacizumab regimen to mitigate these effects. All serious cerebral edema events were resolved with anticipated medical measures, and surgical intervention to remove tumor due to tissue swelling was not necessary for any patient. We established the time course of edema development in this trial by serial volumetric analysis of changes in perilesional fluid-attenuated inversion recovery (FLAIR) signal on imaging. We found increases in volume of edema as early as 8 weeks after treatment that was sustained to 20 weeks, even in patients who did not become symptomatic with cerebral edema. These data can help inform on the expected time interval of cerebral edema for future trials of immunotherapy in recurrent glioblastoma. The nonneurologic toxicity profile in this study was otherwise comparable to those previously reported for pembrolizumab 5 . In total, five patients had objective responses, with two patients showing durable complete responses >45 months and three patients remaining alive at the writing of this manuscript. The objective response rate was 10.4% (90% CI 4.2\u201320.7%). It is noteworthy that there was one additional patient who received the declared dose of DNX-2401 with complete response at the site of treatment; however, this patient developed a new lesion at a distant site resulting in a classification of progressive disease. This patient remained alive a total of 12.3 months after treatment. In the previous phase 1 trial evaluating DNX-2401 monotherapy in recurrent glioma, there was also one patient with a complete response who developed a distant nodule several years after treatment 10 . Pathological examination of the nodule after resection showed only necrosis and inflammation without evidence of tumor. Although the patient in this trial did not undergo resection for the new nodule, it is possible that the radiographic changes seen reflect a similar adaptive memory antitumor response that was observed in the original phase 1 trial of DNX-2401 monotherapy, and not progressive disease. Beyond this, prior reports of durable responses to immunotherapies have largely been limited to patients with favorable biological characteristics 23 . Patients with objective responses in this study had tumors that did not universally harbor the prognostically favorable mutation in IDH1 and had both MGMT methylated and unmethylated tumors, representing the group of glioblastomas that desperately need efficacious therapies. The median overall survival was 12.5 months (10.7\u201313.5 months) and overall survival at 12 months was 52.7% (95% CI 40.1\u201369.2%), which was greater than the prespecified threshold of 20% using approved treatment of tumor-treating fields by Novo-TTF 24 . The 12 month overall survival was 32% in patients treated with DNX-2401 alone 10 , while median overall survival was as 9.3 months and 9.8 months with DNX-2401 or PD-1 blockade alone in prior trials 5 , 10 . While the primary endpoint of objective response was not met, the secondary endpoint of 12 month survival, which is more clinically meaningful and reliable than response rate, was met and the survival of objective responders are encouraging, suggesting that tumor control led to improved survival. Although this trial was not designed to distinguish the effects of DNX-2401 versus pembrolizumab versus combination therapy, the notable survival data point to the potential of improved efficacy in combining oncolytic virus with checkpoint inhibition. As cross-trial comparisons have limitations, further focused comparative studies are needed. While the use of bevacizumab may complicate response assessment in trials by inducing changes in contrast enhancement seen on imaging, none of the patients with objective responses received bevacizumab during the study. Moreover, we did not find that baseline corticosteroid use was associated with outcomes in our study, confirming the findings in a prior study evaluating neoadjuvant checkpoint blockade in recurrent glioblastoma 25 . This may be explained by the fact that patients using more than 4 mg per day of dexamethasone as baseline were excluded from both studies. Although associations of steroid use throughout this study and outcomes were not statistically significant, some comparisons approached the threshold for significance. Whether this association is reflective of symptom management in disease progression or a potential modulation of antitumor immune responses is unclear and warrants dedicated investigation in larger cohorts. We obtained matched mutational data and gene expression data on tumor specimens from patients, where available. Three of the patients with objective responses (60%) had tumors with mutational burden (TMB) greater than 10 mutations Mb \u22121 , while two patients with objective responses (40%) had tumors with TMB less than 10 mutations Mb \u22121 . Although TMB is a known predictive biomarker of response to checkpoint inhibition in a range of advanced cancers, this relationship is more complex and has been less consistent in prior investigations in glioblastomas 26 . One of the major determinants linking TMB to response to checkpoint inhibition is alterations in mismatch repair proteins or polymerase E and D ( POLE and POLD ) genes 26 . None of the patients who showed objective responses had mutations in POLE or POLD genes. Although this suggests that the antitumor responses after combined oncolytic virus and checkpoint inhibition in glioblastomas may be less dependent on TMB than in other solid cancers, further investigation in much larger cohorts is warranted for definitive conclusions. Using gene expression data, we found that objective responses exclusively occurred in patients with moderately inflamed microenvironment, and modest PD-1 expression (TME medium ) before treatment (29.4%, 95% CI 10.3\u201355.6%). Clinical benefit rates and overall survival was also longer in TME medium tumors in this trial. These findings are consistent with prior investigations and our own findings that show that adjuvant anti-PD1 inhibition as monotherapy does not seem to improve survival in TME high tumors 11 , 25 . While TME high tumors are enriched with immune cell infiltrates, they also highly express multiple different suppressive immune checkpoints leading to an exhaustive immune microenvironment by complementary mechanisms. TME medium tumors are primed with a moderate degree of immune cells and express moderate levels of PD-1. DNX-2401 can induce further infiltration of cytotoxic T cells and expression of PD-1 in these tumors that can be further targeted with subsequent anti-PD-1 treatment without immunosuppression from alternative checkpoint proteins. We also obtained specimens on disease progression after treatment for ten patients in this trial. We found that the expression of several different immune checkpoints such as TIGIT, LAG3 and B7-H3 was elevated after treatment, pointing to the potential for using multiple parallel immune checkpoint inhibitors in TME medium tumors that eventually develop disease progression. A similar approach could potentially be considered for TME high tumors. There are limitations to this study that require further investigation. First, this trial did not include a comparator cohort. Further trials to directly compare combination therapy to monotherapy are needed before considering large-scale randomized trials. Second, this trial evaluated a single dose of intratumoral oncolytic virus. Emerging data since the conception of this study have shown some potential benefit with multiple doses of oncolytic virus 27 . The safety of multiple doses of DNX-2401 with pembrolizumab needs further investigation given the local immune-stimulatory effects of treatment, if the logistical considerations to safely conduct such a trial can be addressed. Third, we did not find that variability in seroconversion, as measured by changes in anti-Ad5 IgG levels, impacted patient outcomes. While changes in anti-Ad5 IgG levels can be a surrogate for seroconversion, a potentially more definitive assessment of seroconversion would have benefited from quantification of neutralizing antibodies against human adenovirus 8 . Lastly, we identified biological correlates of outcome using gene expression, mutational data and immunophenotyping that can be leveraged to identify subsets of patients who might benefit most from treatment. It should be noted that these findings were exploratory, and future trials should consider maximizing collection of specimens before and after treatment to allow for even more comprehensive characterization of biological outcomes. To our knowledge, the present study is the first to report on the combined direct delivery of oncolytic viral therapy and systemic checkpoint inhibition for any brain tumor. We identified a safe dose of DNX2401 combined with pembrolizumab with objective and durable responses, including two complete responses, and survival benefit for select patients across multiple institutions. These results are promising and particularly relevant in this population of patients who did not receive repeat resection of tumor and for whom efficacious and nontoxic treatments are entirely lacking. As well, we demonstrate the value that translational analyses and endpoints can add in advancing our understanding of the molecular mechanisms and biomarkers of response and\/or resistance to treatment in clinical trial settings. Methods Patients Adult patients with histologically confirmed glioblastoma or gliosarcoma, presenting with documented failure of previous surgical resection, chemotherapy and\/or radiation at first or second recurrence, with a Karnofsky performance score of at least 70, were eligible. All patients were required to have a single contrast-enhancing tumor of at least 1 cm in two planes but no more than 4 cm in any single plane, as assessed by magnetic resonance imaging (MRI). Surgical resection must not have been possible or planned as part of the treatment for their presentation, and the tumor must have been accessible for stereotactic delivery of DNX-2401. Patients with multifocal or bilateral disease were excluded. The full inclusion and exclusion criteria are detailed in Supplementary Methods . Design To evaluate the safety of combining DNX-2401 with pembrolizumab, we conducted an initial dose-escalation phase to determine a safe dose of DNX-2401 in combination with pembrolizumab and followed by a dose-expansion phase. All patients received a single dose of DNX-2401 by stereotactic injection at the time of standard tumor biopsy followed by 200 mg pembrolizumab infused intravenously at a dose of 200 mg over 30 min every 3 weeks starting 7 days after DNX-2401. Resection of tumors was not permitted. Treatment with pembrolizumab continued for up to 2 years, or until one of the following occurred: disease progression, unacceptable toxic effects or withdrawal of consent. Dose escalation evaluated 5 \u00d7 10 8 , 5 \u00d7 10 9 and 5 \u00d7 10 10 v.p. DNX-2401 in combination with standard dosing pembrolizumab in a 3 + 3 design. All patients underwent a stereotactic biopsy to document the presence of tumor tissue before delivery of DNX-2401. Immediately after biopsy, a stereotactic-compatible neuro-ventricular cannula (Alcyone MEMS; ClearPoint SmartFlow) was inserted into the tumor to deliver the precise targeted dose of DNX-2401 via a single micro-tip at a rate of 0.9 ml h \u22121 over approximately 1 h. The cannula was left in place for 10 min after administration of virus to allow v.p. to diffuse without backflow before removal. Assessments Patients were continuously monitored throughout the study for safety as outlined in the schedule of assessments in the study Protocol. AEs and serious AEs were graded according to National Cancer Institute-Common Terminology Criteria for Adverse Events, version 4.03, and their relationship to treatment administered was assessed. For the dose-escalation phase, the dose-limiting toxicity (DLT) window of observation was the first 21 days after initial pembrolizumab infusion. The occurrence of any of the following toxicities is considered a DLT, if judged by the Investigator to be possibly, probably or definitely related to administration of DNX-2401 and pembrolizumab (and not to the administration procedure): 1. Grade 4 nonhematologic toxicity (not laboratory) 2. Grade 4 hematologic toxicity lasting \u22657 days 3. Grade 3 nonhematologic toxicity (not laboratory) lasting >3 days despite optimal supportive care 4. Any Grade 3 or Grade 4 nonhematologic laboratory value if: Medical intervention is required to treat the subject, or The abnormality leads to hospitalization, or The abnormality persists for >1 week 5. Febrile neutropenia Grade 3 or Grade 4: Grade 3 is defined as ANC <1,000 mm \u22123 with a single temperature of >38.3 \u00b0C (101 \u00b0F) or a sustained temperature of \u2265 3 \u00b0C (100.4 \u00b0F) for more than 1 h Grade 4 is defined as ANC <1,000 mm \u22123 with a single temperature of >38.3 \u00b0C (101 \u00b0F) or a sustained temperature of \u226538 \u00b0C (100.4 \u00b0F) for more than 1 h, with life-threatening consequences and urgent intervention indicated 6. Thrombocytopenia <25,000 mm \u22123 if associated with: A bleeding event that does not result in hemodynamic instability but requires an elective platelet transfusion, or A life-threatening bleeding event which results in urgent intervention and admission to an Intensive Care Unit. 7. Prolonged delay (>2 weeks) in initiating cycle 2 due to treatment-related toxicity 8. Missing >10% of pembrolizumab doses as a result of AE(s) during the first cycle 9. Grade 5 toxicity Treatment response was determined by serial protocolized contrast-enhanced MRI every 4 weeks for 28 weeks, and afterward at an interval of every 8 weeks for the remainder of the treatment period. Patients who completed the treatment phase entered the long-term response and survival follow-up phase of the study for the rest of life, with MRI every 16 weeks. Objective responses were evaluated by the RANO criteria 28 , 29 and mRANO criteria 30 . Complete and partial responses required confirmation on the consecutive scan 4 weeks after the initial response was observed. Patients with suspected radiological progression were permitted to remain on study until progression was confirmed by follow-up MRI separated by a minimum of 4 weeks. Endpoints and statistical analyses The analyses reported in this study were performed according to the statistical analysis plan. All enrolled patients were included in the safety analysis set, and patients were considered evaluable for efficacy if they received at least one dose, or part of one dose, of either study drug, had measurable tumor at baseline and completed the week 4 follow-up visit. Patients who discontinued study participation for any reason other than progressive disease or study treatment-related toxicity before the 4 week visit were not considered evaluable and were replaced; however, they continued to be monitored for safety. The primary safety objective was to evaluate the safety of escalating doses of DNX-2401 and the overall safety of the declared dose of intratumoral DNX-2401 when followed by sequential intravenous administration of pembrolizumab. AEs and serious AEs were summarized for all patients in the study and were considered treatment related if reported as possibly, probably or definitely related to study drug. The primary efficacy objective was to determine the objective response rate, defined as the percentage of patients that had complete or partial responses based on mRANO criteria 30 . The primary endpoint was tested in a single-arm design. As the sample size estimation was based on a prespecified historical response rate of 5%, with \u03b1 = 0.05, a total of 39 evaluable subjects in the declared dose phase would yield an 80% power for an alternative hypothesis of objective response rate of 18%. Objective response rate was reported as the number and percentage of subjects with an objective response and the corresponding 95% CI based on the exact binomial method (Clopper\u2013Pearson method). Type I error was set at 5% (one-sided), so it was predetermined that the 90% CI would also be provided. Secondary efficacy objectives were to evaluate 12 month overall survival as well as the clinical benefit rate, defined as the proportion of patients treated with DNX-2401 and pembrolizumab who had stable disease, complete response or partial response. Overall survival was defined as the time from the start of treatment (DNX-2401 injection) until death (or last follow-up). Overall survival at 12 months was summarized using Kaplan\u2013Meier methods and outcomes were compared to historical rates of 20% from an approved treatment approach, NovoTTF 24 . Overall survival of patients with objective responses was compared to those without objective responses using the 6 month landmark Kaplan\u2013Meier method to account for potential lead time bias 31 . IDH1 mutation status and MGMT methylation status were assessed locally at each institution. Follow-up of survival for patients remaining alive after database lock was used for descriptive purposes only. Statistical and computations analyses were performed using SAS 9.4 and R 4.1.3. Study organization and oversight The study was conducted in compliance with the Protocol at 15 clinical trial sites in the United States and Canada, as well as recognized international standards including the Good Clinical Practice guidelines of the International Conference on Harmonisation and the principles of the Declaration of Helsinki. The Protocol and its amendments were approved by the institutional review board of each participating trial site. Voluntary written informed consent was obtained from every patient before participation in this study. DNX-2401 preparation, handling and administration followed institutional standards for biosafety level 2 agents. Anti-adenovirus antibodies Anti-hexon IgG antibody levels were determined before and after treatment by ELISA from patient serum samples according to the manufacturer\u2019s instructions (Adenovirus IgG ELISA Kit; DEIA309; Creative Diagnostics). Absorbance at 450 nm was measured using a Synergy H4 plate reader (BioTek), and concentrations calculated on the basis of a standard curve (Gen 5 software Version 3.0, BioTek). Anti-adenovirus IgG serum concentration increases of fourfold or greater were considered seroconversions. A more stringent threshold of tenfold or greater increases in levels of anti-adenovirus IgG serum concentrations was also tested. Targeted mutational sequencing Targeted next-generation sequencing was performed on DNA extracted from formalin-fixed, paraffin-embedded (FFPE) pretreatment tumor biopsies available from 28 patients. Tumor samples from 18 subjects were sequenced by NeoGenomics using NeoType Discovery Profile for Solid Tumor. Tumor samples from ten subjects were sequenced by NovoGene using Novogene PM 2.0. Gene expression profiling and analyses RNA was extracted from FFPE pretreatment tumor biopsies available from 38 patients and analyzed retrospectively on the NanoString nCounter system. For ten patients, there were also tumor biopsy specimens available at the time of disease progression, allowing for an examination of gene expression changes before and after treatment in matched patient samples. The geometric mean of canonical marker genes was used to compute scores for immune cell types 32 , functional orientation markers and signature scores that are reported in this study, unless otherwise explicitly stated. Functional orientation markers and the chemokine and cytolytic signature scores were obtained from previous studies 11 , 33 , 34 . Remaining marker genes are provided in Supplementary Table 6 . A T-cell-inflamed signature was computed as previously described using a weighted sum of normalized expression values of 18 inflammatory genes (CCL5, CD27, CD274 (PD-L1), CD276 (B7-H3), CD8A, CMKLR1, CXCL9, CXCR6, HLA.DQA1, HLA.DRB1, HLA.E, IDO1, LAG3, NKG7, PDCD1LG2 (PD-L2), PSMB10, STAT1 and TIGIT) related to antigen presentation, chemokine expression, cytolytic activity and adaptive immune resistance 13 . Glioblastoma microenvironment subtypes were obtained by partition-around-medoid clustering using immune cell type scores, as previously described 11 . Differentially expressed genes between groups were identified by comparing log 2 FC and Welch\u2019s P values. Genes with absolute value log 2 FC >1 and P < 0.05 were considered differentially expressed, unless otherwise specified. Functional enrichment analysis was performed using gProfiler. Previously published datasets Zhao et al. previously published their transcriptomic data in patients receiving anti-PD-1 therapy in high-grade gliomas 12 . A total of 16 patients had transcriptomic data available before initiation of anti-PD-1 therapy, and 9 patients also had transcriptomic data available at progression after initiating anti-PD-1 therapy. The transcriptomic data from these 25 patients were downloaded from SRAPRJNA482620, and clinical annotation was provided by the authors. Response was considered as stable disease or better in this study. Associations with outcome were based on overall survival after initiating anti-PD-1 therapy. Edema volumetric analysis Digital Imaging and Communications in Medicine files for study MRIs were imported into Horos (version 3.3.6), and a blinded reviewer used non-motion degraded, axial, FLAIR sequences to segment perilesional FLAIR hyperintense signal. The Horos volume generator function was used to determine the total FLAIR signal volume for each study MRI. Volume of edema at each study MRI was normalized relative to baseline levels. Grouped comparisons were made by calculating the mean normalized edema volume with 95% confidence intervals at the timepoints outlined in the protocol every 4 weeks for 28 weeks and then every 8 weeks thereafter. IHC We performed immunohistochemical analyses for myeloid cell markers (Iba-1, CD68 and CD163) and lymphoid cell markers (CD3, CD4 and CD8) in samples with available tissue before and after treatment in this sample. Staining and subsequent annotation and analyses were performed blinded to clinical status. Slides with 5 \u00b5m FFPE tissue sections were rehydrated and a sodium citrate-dihydrate buffer or Tris\u2013EDTA buffer was used for heat-mediated antigen retrieval. A 3% hydrogen peroxide in methanol solution was utilized to block endogenous peroxidase activity. Blocking solution (5% bovine serum albumin in phosphate buffered saline plus 0.1% Triton X-100) was applied to slides for 1 h at room temperature. Subsequently, primary antibodies including anti-CD3 (Agilent, M725401-2, mouse monoclonal, 1:100), anti-IBA1 (Wako, 019-19741, rabbit polyclonal, 1:1,500), anti-CD68 (Agilent, M0514, mouse monoclonal, 1:200), anti-CD4 (abcam, ab133616, rabbit monoclonal, 1:100) and anti-CD8 (abcam, ab93278, rabbit monoclonal, 1:250) were applied overnight at 4 \u00b0C in blocking solution. A 1 h incubation with secondary antibody was performed followed by processing with the DAKO polymer-HRP system and DAB peroxidase kit, counterstaining with hematoxylin, dehydration of the tissue and coverslipping. Whole slide images were digitized, and then for each slide tumor versus non tumor content was annotated and representative images were selected. Proportions of stain-positive cells were quantified using HALO (version 3.0311, Indica Labs) software algorithms that were defined to identify cells with either nuclear or cytoplasmic staining as a fraction of all cells. This algorithm was applied to all annotated tissue sections in an unbiased systematic manner, and the density of immunopositivity per square millimeter was recorded for each antibody. PD-L1 protein expression was performed by NeoGenomics Laboratories (NeoGenomics) under the direction of Merck using FFPE tumor biopsy samples according to standard protocols (PD-L1 IHC 22C3 assay). Multiplex immunofluorescence staining, tissue imaging and cell phenotyping A validated and standardized multiplex immunofluorescence protocol was developed for simultaneous detection of CD3, CD8, CD11b, CD163, GFAP and DAPI in a single FFPE tissue section. The validation pipeline for the multiplex immunofluorescence protocol has been previously described by our group 8 . Briefly, whole-slide tissue sections were deparaffinized and subjected to sequential rounds of antibody staining. Antigen retrieval was performed using Dako PT-Link heat-induced antigen retrieval with low pH (pH 6) or high pH (pH 9) target retrieval solution (Dako). The antibody panel included CD11b (rabbit monoclonal, clone EPR1344, 1:1,000, Abcam, product number ab133357), CD163 (mouse monoclonal, clone MRQ-26, ready-to-use, Cell Marque, product number 760-4437), CD3 (rabbit polyclonal, IgG, ready-to-use, Agilent, product number IR503), CD8 (mouse monoclonal, clone C8\/144B, ready-to-use, Agilent, product number IR623), and GFAP (mouse monoclonal, clone 6F2, 1:500, Agilent, product number M0761). After all sequential rounds, nuclei were counterstained with spectral DAPI (Akoya Biosciences) and sections were mounted with Faramount Aqueous Mounting Medium (Dako). Multiplexed immunofluorescence slides were scanned on a Vectra-Polaris Automated Quantitative Pathology Imaging System (Akoya Biosciences). Spectral unmixing was performed using inForm software (version 2.4.8, Akoya Biosciences), as described. Image analysis was performed using QuPath and Fiji\/ImageJ. Briefly, cells were segmented on the basis of nuclear detection using the StarDist 2D algorithm. A random trees algorithm classifier was trained for each cell marker. Cells were then subclassified as CD3 + , CD8 + , CD11b + and CD163 + cells. CD4 + T cells were defined as CD3 + CD8 \u2212 . Cells negative for these markers were defined as \u2018other cell types\u2019. Measurements were calculated as cell densities (cells mm \u22122 ). GFAP was used to identify tumor areas. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability Pseudononymized participant data, including outcomes and relevant reported patient characteristics, are shared as Supplementary Information . Processed gene expression data that can be linked to pseudonymized participant data are provided at GSE226976 . Previously published data were accessed from SRAPRJNA482620 with clinical annotation provided from authors. Custom algorithms or software were not used to generate the results reported in this manuscript. ","News_Body":"A new international study published in Nature Medicine and presented as a late-breaking abstract at the American Association of Neurological Surgeons (AANS) annual conference, shows great promise for patients with glioblastoma. Drs. Farshad Nassiri and Gelareh Zadeh, neurosurgeons at the University Health Network (UHN) in Toronto, published the results of a Phase 1\/2 clinical trial investigating the safety and effectiveness of a novel therapy which combines the injection of an oncolytic virus\u2014a virus that targets and kills cancer cells\u2014directly into the tumor, with intravenous immunotherapy. The authors found that this novel combination therapy can eradicate the tumor in select patients, with evidence of prolonged survival. Investigative work by the authors also revealed a new genetic signature within tumor samples that has the potential to predict which patients with glioblastoma are most likely to respond to treatment. \"The initial clinical trial results are promising,\" says Dr. Zadeh, who is also Co-Director of the Krembil Brain Institute and a Senior Scientist at the Princess Margaret Cancer Center. \"We are cautiously optimistic about the long-term clinical benefits for patients.\" Glioblastoma is a notoriously difficult-to-treat primary brain cancer. Despite aggressive treatment, which typically involves surgical removal of the tumor and multiple chemotherapy drugs, the cancer often returns, at which point treatment options are limited. Immune checkpoint inhibitors are effective treatments for a variety of cancers, but they have had limited success in treating recurrent glioblastoma. This novel therapy involves the combination of an oncolytic virus and immune checkpoint inhibition, using an anti-PD-1 antibody as a targeted immunotherapy. First, the team delivered the virus by accurately localizing the tumor using stereotactic techniques and injecting the virus through a small hole and a purpose-built catheter. Then, patients received an anti-PD-1 antibody intravenously, every three weeks, starting one week after surgery. \"These drugs work by preventing cancer's ability to evade the body's natural immune response, so they have little benefit when the tumor is immunologically inactive\u2014as is the case in glioblastoma,\" explains Dr. Zadeh. \"Oncolytic viruses can overcome this limitation by creating a more favorable tumor microenvironment, which then helps to boost anti-tumor immune responses.\" The combination of the oncolytic virus and immune-checkpoint inhibition results in a \"double hit\" to tumors; the virus directly causes cancer cell death, but also stimulates local immune activity causing inflammation, leaving the cancer cells more vulnerable to targeted immunotherapy. Dr. Zadeh and colleagues evaluated the innovative therapy in 49 patients with recurrent disease, from 15 hospital sites across North America. UHN, which is the largest research and teaching hospital in Canada and the only Canadian institution involved in the study, treated the majority of the patients enrolled in the trial. The results, published in Nature Medicine, show that this combination therapy is safe, well tolerated and prolongs patient survival. The therapy had no major unexpected adverse effects and yielded a median survival of 12.5 months\u2014considerably longer than the six to eight months typically seen with existing therapies. \"We're very encouraged by these results,\" says Dr. Farshad Nassiri, first author of the study and a senior neurosurgery resident at the University of Toronto. \"Over half of our patients achieved a clinical benefit\u2014stable disease or better\u2014and we saw some remarkable responses with tumors shrinking, and some even disappearing completely. Three patients remain alive at 45, 48 and 60 months after starting the clinical trial.\" \"The findings of the study are particularly meaningful as the patients in the trial did not have tumor resection at recurrence\u2014only injection of the virus\u2014which is a novel treatment approach for glioblastoma. So, it's really remarkable to see these responses,\" says Dr. Zadeh. \"We believe the key to our success was delivering the virus directly into the tumor prior to using systemic immunotherapy. Our results clearly signal that this can be a safe and effective approach,\" adds Dr. Nassiri. The team also performed experiments to define mutations, gene expression, and immune features of each patient's tumor. They discovered key immune features which could eventually help clinicians predict treatment responses and understand the mechanisms of glioblastoma resistance. \"In general, the drugs that are used in cancer treatment do not work for every patient, but we believe there is a sub-population of glioblastoma patients that will respond well to this treatment,\" says Dr. Zadeh. \"I believe this translational work, combining basic bench science and clinical trials, is key to moving personalized treatments for glioblastoma forward.\" This is one of the few clinical trials with favorable results for glioblastoma over the last decade, and it was truly a team effort. \"The trial would not have been possible without our incredible OR teams, research safety teams and researchers\u2014including Dr. Warren Mason and his team at Princess Margaret Cancer Center\u2014and our brave patients and their families. We're also grateful to the Wilkins Family for providing the funds to enable us to complete trials that advance care for our patients,\" says Dr. Zadeh. The next steps for the group are to test the effectiveness of the combination therapy against other treatments in a randomized clinical trial. \"We are encouraged by these results, but there is still a lot of work ahead of us,\" says Dr. Nassiri. \"Our goal, as always, is to help our patients. That's what motivates us to continue this research.\" ","News_Title":"New study using novel approach for glioblastoma treatment shows promising results, extending survival","Topic":"Medicine"}
{"Paper_Body":"Abstract The ability to estimate the distance of objects from one\u2019s self and from each other is fundamental to a variety of behaviours from grasping objects to navigating. The main cue to distance, stereopsis, relies on the slight offsets between the images derived from our left and right eyes, also termed disparities. Here we ask whether the precision of stereopsis varies with professional experience with precise manual tasks. We measured stereo-acuities of dressmakers and non-dressmakers for both absolute and relative disparities. We used a stereoscope and a computerized test removing monocular cues. We also measured vergence noise and bias using the Nonius line technique. We demonstrate that dressmakers\u2019 stereoscopic acuities are better than those of non-dressmakers, for both absolute and relative disparities. In contrast, vergence noise and bias were comparable in the two groups. Two non-exclusive mechanisms may be at the source of the group difference we document: (i) self-selection or the fact that stereo-vision is functionally important to become a dressmaker, and (ii) plasticity, or the fact that training on demanding stereovision tasks improves stereo-acuity. Introduction Depth perception is an important human visual ability allowing people to interact easily with their environment. It relies substantially on the stereoscopic depth information, which itself is based on image binocular disparities. These disparities are caused by the different viewpoints of the two eyes. Monocular cues to depth (e.g. motion parallax, shadows, occlusion) also contribute to depth perception 1 . The functional role of stereopsis has been the subject of much debate. It has been theorized to guide the fine movements of the hands in reaching and grasping 2 , 3 , 4 . Indeed, object placement 5 , 6 and grasping 7 , 8 , 9 , 10 are more precise with binocular viewing than monocular viewing (at least in the centre of the visual field 11 ). However, most of the evidence is based on comparing binocular and monocular viewing conditions, which differ not only in the absence of stereopsis, but also in an absence of binocular vergence and summation, and a decreased field of view. It is known that decreasing the field of view affects reaching 12 . Yet, there remains a binocular advantage in object prehension even when controlling for the field of view 13 . There is also a growing body of confirmatory evidence, including studies showing that binocular cues to depth are crucial to prehension 14 , that binocular cues are given more weight than monocular cues when placing objects 15 , and that the binocular advantage in object placement correlates with stereo-acuity 5 . Previous studies 6 , 16 have shown that binocular vision is more efficient than monocular vision in delicate manual tasks like threading a needle. Some have argued that stereopsis can only be useful for slow motions requiring extreme precision 17 . However, past studies have not shown better stereo-acuities for professions based on slow motions requiring extreme precision, like surgeons 18 or dentists 19 , 20 , although stereoblind surgeons performed a simulated surgical task significantly worse than the stereo-normal ones 21 . Furthermore, stereo-acuity when entering a school of dentistry was not linked with later student grades 22 . In the current study, we tested stereoscopic acuities of a sample of dressmakers, and compared these acuities with those of a non-dressmaker group. Given the likely advantage given by stereopsis in fine eye-hand tasks, we reasoned that dressmakers may display better stereo-acuities. This could result either through self-selection or through the development of expertise given that their daily work involves constantly estimating small changes in visual depth. Indeed, stereoscopic vision is known to undergo some training-dependent plasticity. For example, stereo-perception can be ameliorated by training on a depth task with random dot stereograms 23 , 24 , 25 , or a depth task with local stereograms, involving edges, squares, lines, dots, or Gabor patches 24 , 26 , 27 , 28 , 29 . In addition, persons with strabismus and amblyopia, who often suffer from stereo-blindness, have been trained to recover stereoscopic vision with various rates of success (for a review, see ref. 4 ), using techniques such as patching 30 , monocular 31 or dichoptic perceptual learning 32 , 33 , monocular 34 or dichoptic video gaming 30 , 35 , 36 , and stereo-training 37 , 38 , 39 . However, it is not known whether manual actions, in particular, the kind of fine actions involved in sewing can increase stereoscopic depth perception, or whether having poor (or no) stereopsis would deter individuals from professions such as dressmaking. Although we have discussed stereoscopic acuity as if it were a unitary concept, it is well known that there are two different types of disparity: absolute disparity and relative disparity. An object\u2019s absolute disparity is the difference between the angle subtended by the target at the two entrance pupils of the eyes and the angle of convergence. Absolute disparity is important for judging the depth distance of an object from one\u2019s self (Fig. 1 ). The difference between the absolute disparities of two objects is called relative disparity (Fig. 1 ). Relative disparity is important for judging the depth distance between two (or more) objects. It is well known that human observers are better at judging relative disparity than at judging absolute disparity 40 . We and others have argued that the source of this difference is an absence of conscious readout for absolute disparities. We refer to this as the absolute disparity anomaly 41 . Despite this anomaly, humans should have a high sensitivity for absolute disparities, given that both vergence eye movements 42 , 43 , 44 , 45 and relative disparities are based on absolute disparities 41 , 46 , 47 . The plasticity studies discussed above were all conducted with relative disparities. Therefore, it is not clear whether absolute disparity acuity (or readout) can be improved by learning. On the one hand, in a recent study 41 , we have found very little evidence for rapid learning of absolute disparity sensitivity (or readout), suggesting it may be difficult to change. However, our participant sample was small (n = 6) and we tested learning over only 1200 trials. On the other hand, given the assumed link between absolute disparities and relative disparities, at least under the absolute disparity anomaly view, changes in relative disparity could go hand in hand with changes in absolute disparities. Therefore, we tested both absolute and relative disparities, in order to learn whether expertise in sewing might be associated with better relative or absolute disparity acuity (or readout), or both. Figure 1 Schematic illustration of absolute and relative disparities. Left and right panels show the viewpoints from left and right eyes respectively. The observer fixates on the phone (fixation indicated in red crosshairs). The absolute disparity of the author\u2019s cap is the sum of the distances indicated in blue while the absolute disparity of the tower (Berkeley\u2019s campanile) is the sum of the distances indicated in green. The relative disparity between the cap and the tower is the sum of the distances indicated in yellow, and also the differences of the absolute disparities of the cap and of the tower. A more formal definition can be found in ref. 41 . Full size image Absolute disparity is a cue for vergence. However, it is widely believed that relative disparity acuity is considerably better than absolute disparity acuity, because absolute disparities are corrupted by vergence noise 2 , 42 , 48 . In a recent article 41 , we argued against that idea by showing that vergence noise was too small to explain the difference between absolute and relative disparity acuities. Rather, we suggested that vergence noise is not the limiting factor for absolute disparity measurements. Given that debate, however, we felt it was important to measure vergence ability. Furthermore, we were interested to learn whether dressmakers (who need to converge accurately) would show less vergence noise than non-dressmakers. For that purpose, we measured vergence noise and bias (over-convergence or divergence during fixation) for each participant with the Nonius-line technique. Results We compared absolute and relative disparity thresholds of dressmakers and non-dressmakers. In addition, we measured vergence thresholds under nearly identical conditions and separated the results in two values: fixation noise and fixation bias. All stimuli were briefly presented to minimize eye movements. Stereo-thresholds: Dressmakers are better than non-dressmakers A mixed-model ANOVA on Log-thresholds with group as between-subject factor and disparity task (absolute\/relative) as a within-subject factor established a main effect of task (F(1,32) = 67; p < 10 \u22125 ), and importantly of group, with the dressmakers outperforming the non-dressmakers (F(1,32) = 6.2; p = 0.018; the interaction \u201cdisparity task \u00d7 group\u201d was not significant, p = 0.99). As illustrated in Fig. 2 , the dressmakers displayed better (i.e., lower) absolute (1504 vs. 2714 arcsec; T(32) = 1.78; p = 0.05) and better (i.e., lower) relative disparity acuities (241 vs. 345 arcsec; T(32) = 2.16; p = 0.025) than the non-dressmakers (one-sided post-hoc t-tests with Holm-Bonferroni-corrected p-values for the between-group differences). The effect sizes were relatively small (for the absolute disparity task: Cohen\u2019s d = 0.68; for the relative disparity condition: Cohen\u2019s d = 0.34), mostly because of the large range and variance of performances: dressmakers\u2019 median acuity was 43% better in the relative disparity condition and 80% better in the absolute disparity condition, when compared to non-dressmakers\u2019 acuity. Figure 2 Boxplots of log-transformed thresholds for discrimination of depth from absolute disparities only (left side) and from additional relative disparities (right side), for non-dressmaker and dressmaker groups. The median for each group is in red and the blue box defines the Q1 and Q3 quantiles for each group. The whiskers encompass the entire distribution. Each pink dot is a data point for a female participant and each blue dot is a data point for a male participant. Full size image Vergence noise and bias do not differ between dressmakers and non-dressmakers Neither vergence noise (log-thresholds - Fig. 3 ; t-test T(32) = 1.13, p = 0.27) nor vergence bias (Fig. 3 ; t-test T(32) = 1.64; p = 0.11) differed significantly between dressmaker and non-dressmaker groups. Figure 3 Boxplots of log-transformed vergence thresholds (( a ), noises) and vergence biases ( b ) from Nonius - line method, for non-dressmaker and dressmaker groups. Medians are in red and the blue box defines the Q1 and Q3 quantiles. The whiskers encompass the entire distribution. Each pink dot is a data point for a female participant and each blue dot is a data point for a male participant. Full size image Discussion Dressmakers demonstrate a better overall disparity acuity than non-dressmakers for both absolute and relative disparities. There are two plausible and non-exclusive explanations for the dressmakers\u2019 superior stereo-acuity: selection and experience. First, it is possible that having a high stereo-acuity is highly advantageous for becoming a professional dressmaker. We know that observers differ substantially in the precision of their stereo-acuity 49 . Thus, it could be that dressmaking selects for those individuals endowed with superior stereo-acuity because it makes the dressmakers\u2019 task easier, highlighting an example of the functional importance of stereo-vision. A second plausible explanation is that dressmakers, who spend significant time manually sewing, become accustomed to situations in which they deal with precise visual details and in which the depth matters: sewing requires the dressmaker to put a needle behind or in front of a thread or a cloth. In addition, sewing likely provides immediate and direct feedback when an error is accompanied by negative reinforcement (pain from being pricked by the needle), which may aid perceptual learning. In other words, this could be a form of stereo-plasticity from a manual task. For that reason, it is important to note that our dressmakers were selected because they were hand sewing rather than machine sewing. Interestingly, this interpretation, if confirmed, would also imply that absolute disparity acuity (or readout) can be improved by experience. With the present cross-sectional design, it is not possible to know whether dressmakers\u2019 acuities are better because of learning by experience, or because of an implicit selection for better stereoscopic vision by the profession. The two possible origins of the effect could also be cumulative. In the future, a training study could be carried out to address this issue. A recent study demonstrates a complementary idea: namely, that studying representative arts, a profession involving mostly 2D images, is associated with poorer stereopsis 50 . That study also cannot disentangle between a training effect (while ignoring stereo 3D-information in order to represent 2D) and a selection bias (when impaired stereo 3D-vision helps to represent 2D information). Interestingly, we found no difference between groups for the vergence precision and accuracy (vergence noise and bias). This was not a given, as sewing, which requires high precision, would certainly benefit from better vergence, and may also provide a form of vergence training. Finally, the lack of interaction between the type of disparity (absolute or relative) and the type of observer (dressmaker or non-dressmaker) is consistent with the view that relative disparities are calculated from absolute disparities 41 , 46 , 47 . In this view, improvements in threshold may have originated at the level of the absolute disparity encoding, and then percolated to relative disparities. Note that improved absolute stereo-acuities could be expected to result in improved vergence, as absolute disparities contribute to vergence noise 42 , 43 , 44 , 45 . Yet, similar vergence was measured across groups. This is probably owing to the fact that there are other sources of vergence noise than absolute disparity noise. Among such sources are motor noise and noise in the estimation of the vergence angle from eye muscle tension. Those sources of noise may constitute greater limiting factors for vergence fixation than the absolute disparity noise. We acknowledge that our two groups differed in gender balance with the dressmaker group being predominantly female and the non-dressmaker group male. To further assess a potential gender difference in our sample, we show on each figure which participant is a female or a male (color-coded). No clear pattern in favour of a gender bias appears, with around half of the women in the non-dressmaker group, and half of the men in the dressmaker group fall on either side of the median line (Figs 2 and 3 ). If anything, male participants in the dressmaker group had slightly better stereo-acuity. In addition and importantly, several large - scale studies have investigated gender differences in stereo-acuity and they reported no differences, both for standardized clinical tests 51 and psychophysical measures 52 . Therefore, gender is unlikely to explain the effect we document here. Although all of our participants successfully passed the Randot and Butterfly clinical stereo-tests with an acuity better than 70 arcsec, we were unable to measure relative-disparity stereo-acuity better than 3000 arcsec for four of them (out of 34) with our psychophysical method, which had a greater range and sensitivity than either clinical test. This suggests that clinical measures may still present monocular cues 4 . While clinical tests can be performed quickly, allowing large-scale screening tests, they are unsuitable for detecting group differences of the size we document here. There is a clear need for new, computerized tests of stereopsis for the clinic, that contain no monocular cues, and we are encouraged that some, such as \u201cAsteroid\u201d are currently being developed (J. Read, personal communication). To conclude, we were interested in the role of expertise in the perception of stereoscopic depth. We have shown that dressmakers have better stereoscopic acuity than non-dressmakers for both absolute and relative disparities, and no difference in their vergence abilities. The findings are compatible with two non-exclusive possibilities: either that stereopsis has a clear functional importance (here, to success in dressmaking), or that experience with fine manual tasks can influence the precision of the stereoscopic system. Only a training study could disentangle the two options, with one of them opening a door to new ways of training stereoscopic vision. Methods The stimuli, methods and data have been described in detail elsewhere 41 , 53 , therefore we simply provide a brief overview below. Observers Thirteen professional dressmakers (11 female, 2 male, age range: 21\u201334 years, average: 27.6) and twenty-one non-dressmakers (4 female, 17 male, age mean: 24.1, age range: 19\u201335 years) participated in the study. Only dressmakers with substantial experience (minimum 2 h\/week over the last 3 years) with manual sewing (rather than machine sewing) were included in the study. None of the observers had ever participated in visual studies. Crossed stereoacuity was better than or equal to 70 arcsec on two clinical stereo-tests (Randot circle test and Butterfly circle test) for all observers. All passed the random dot stereogram part of each test. Following recommendations in ref. 54 , we report exclusion of participants at the first stage: no participant was excluded at the clinical stereo-test stage. We collected informed consent for all participants and they all obtained monetary compensation for their participation. Both groups were fully na\u00efve about the computerized tasks in our study. The study was carryout out in accordance with the Declaration of Helsinki and was approved by UNIGE\u2019s Ethics Committee. Stereo-task stimuli and procedures The two stereo tasks that we used to collect thresholds for absolute and relative disparities, used nearly identical stimuli (vertical white lines 20-arcmin long and 26-arcsec wide on a black background). We presented stereoscopic stimuli appearing in depth using a stereoscope in a darkened room. Distance to the screen was 2.1 m and we used a subpixel presentation technique so that binocular disparities as small as 2.6 arcsec could be reliably presented on screen (for full details, see ref. 41 ). A trial started when the fixation point was presented. Observers had to fixate it, and to maintain precise vergence. Vergence feedback was achieved through the perceived horizontal alignment of Nonius lines around the fixation. After aligning the nonius lines, the observer pressed a key which initiated the disappearance of all items on the screen and replaced them with a 10-ms mask made of uniform uncorrelated white noise. The two vertical lines of the stereoacuity stimulus were then presented for 200 ms. Vergence eye movements were precluded by the short presentation time. To minimize the effect of monocular cues, a horizontal jitter in the position of both lines was added. A black screen then replaced the stimulus. Absolute disparity task Participants were shown the two vertical lines at the same depth. We measured the absolute disparity thresholds using the method of single stimuli with an implicit reference 55 , 56 , 57 . For each trial, observers had to decide whether the depth between the (extinguished) fixation point and the lines was smaller or larger than the mean of the same depth over all previous trials seen in the block. The method allowed us to minimize differences in memory load inherent to the task. We estimated that the largest stereo-threshold that we could reliably measure was 3000 arcsec, using a Monte Carlo experiment simulating an ideal observer (2000 repetitions). Larger thresholds could be measured but were most likely under-estimated. Relative disparity task Participants were shown the two vertical lines identical to those in the absolute disparity task. However, when measuring relative disparity thresholds, each line presented a different depth and observers responded about the depth distance between the lines. Observers had to decide whether the depth difference between the two lines was smaller or larger than the mean of the same depth over all trials seen in the block. Vergence measures We measured vergence using the Nonius line method described in ref. 41 . In short, participants were presented with stimuli as identical as possible as the disparity task stimuli. After the initial fixation Nonius lines, whose goal was to ensure the best vergence fixation, another set of Nonius lines was dichoptically flashed with some horizontal jitter. The lines were also shifted horizontally from each other and the shift was varied with a staircase procedure. The task was to judge whether the line above (extinguished) fixation was flashed to the left or to the right of the line below. Vergence data was separated in two aspects: the noise (which reflects the variability of vergence) and the bias (which reflects the accuracy of vergence). Statistical Analyses All statistical tests were conducted at criterion \u03b1 = 0.05, with n = 34. In the absolute disparity task, participants ran a block with a reference at a 5-arcmin disparity and the other with the reference at 10 arcmin. The blocks did not differ significantly (mixed ANOVA model with absolute disparity condition as a within-subject factor, and group as another factor: F(1,32) = 0.47; p = 0.50; and on log scale: F(1,32) = 0.63; p = 0.43). Therefore, we merged them for the rest of the analyses. When studying acuities in absolute and relative disparity tasks, Lilliefors tests showed that the samples were not normally distributed (absolute disparity condition: p = 0.012 for the control group and p = 0.0021 for the dressmaker group; relative disparity condition: p = 0.0015 for control group and p = 0.001 for dressmaker group). However, the log-transformed distributions could not be shown to diverge from normality, using Kolmogorov-Smirnov or Lilliefors tests (all p > 0.22). Cochran test on the log-transformed thresholds demonstrated that the assumption of homoscedasticity was met for all samples (C = 0.31; p = 0.75), therefore we used log-transformed stereo-thresholds. For the stereo-threshold analysis, we use a modified Thompson tau procedure (median as central value, alpha = 0.01), which identified two observers as outliers (one in the non-dressmaker group\/absolute disparity condition and one in the dressmaker group\/relative disparity condition). Their values were replaced with the group median in each condition. Vergence - noise estimates for the non-dressmaker group were not normally distributed (Lilliefors test, p = 0.0011). Log-transformed thresholds were not different from Gaussian distributions (using both Kolmogorov-Smirnov and Lilliefors tests, all p > 0.11), therefore we used log-transformed data for the vergence noise analysis. Outlier detection (modified Thomson tau) also detected 2 outliers (1 in each group). Their values were replaced with the group median in each condition. Distributions of vergence biases were not different from Gaussian distributions (using both Kolmogorov-Smirnov and Lilliefors tests, all p > 0.50) and therefore we used raw data for the analysis. Outlier detection (modified Thomson tau) also detected 4 outliers (2 in each group). Their values were replaced with the group median in each condition. Data Availability The dataset is available online on Figshare public repository. ","News_Body":"Haute couture can be credited for enhancing more than catwalks and red carpets. New research from UC Berkeley suggests that the 3-D or \"stereoscopic\" vision of dressmakers is as sharp as their needles. Stereoscopic vision is the brain's ability to decode the flat 2-D optical information received by both eyes to give us the depth of perception needed to thread a needle, catch a ball, park a car and generally navigate a 3-D world. Using computerized perceptual tasks, researchers from UC Berkeley and the University of Geneva, Switzerland, tested the stereoscopic vision of dressmakers and other professionals, and found dressmakers to be the most eagle-eyed. The results, published in the June 13 issue of the journal Scientific Reports, show dressmakers to be 80 percent more accurate than non-dressmakers at calculating the distance between themselves and the objects they were looking at, and 43 percent better at estimating the distance between objects. \"We found dressmakers have superior stereovision, perhaps because of the direct feedback involved with fine needlework,\" said study lead author Adrien Chopin, a postdoctoral researcher in visual neuroscience at UC Berkeley. What researchers are still determining is whether dressmaking sharpens stereoscopic vision, or whether dressmakers are drawn to the trade because of their visual stereo-acuity, Chopin said. Credit: University of California - Berkeley To experience what it means to have stereoscopic vision, focus on a visual target. Now blink one eye while still staring at your target. Then blink the other eye. The background should appear to shift position. With stereoscopic vision, the brain's visual cortex merges the 2-D viewpoints of each eye into one 3-D image. It has generally been assumed that surgeons, dentists and other medical professionals who perform precise manual procedures would have superior stereovision. But previous studies have shown this not to be the case. That spurred Chopin to investigate which professions would produce or attract people with superior stereovision, and led him to dressmakers. A better understanding of dressmakers' stereoscopic superpowers will inform ongoing efforts to train people with visual impairments such as amblyopia or \"lazy eye\" to strengthen their stereoscopic vision, Chopin said. In addition to helping people with sight disorders, improved stereoscopic vision may be key to the success of military fighters, athletes and other occupations that require keen hand-eye coordination. An estimated 10 percent of people suffer from some form of stereoscopic impairment, and 5 percent suffer from full stereo blindness, Chopin said. For example, the 17th-century Dutch painter Rembrandt, whose self-portraits occasionally showed him with one lazy eye, is thought to have suffered from stereo blindness, rendering him with flat vision. Some vision scientists have posited that painters tend to have poorer stereovision, which gives them an advantage working in 2-D. For the study, participants viewed objects on a computer screen through a stereoscope and judged the distances between objects, and between themselves and the objects. Researchers recorded their visual precision and found that, overall, dressmakers performed markedly better than their non-dressmaker counterparts in visual acuity. ","News_Title":"Dressmakers found to have needle-sharp 3D vision","Topic":"Medicine"}
{"Paper_Body":"Abstract Timescale comparison between optical atomic clocks over ground-to-space and terrestrial free-space laser links will have enormous benefits for fundamental and applied sciences. However, atmospheric turbulence creates phase noise and beam wander that degrade the measurement precision. Here we report on phase-stabilized optical frequency transfer over a 265 m horizontal point-to-point free-space link between optical terminals with active tip-tilt mirrors to suppress beam wander, in a compact, human-portable set-up. A phase-stabilized 715 m underground optical fiber link between the two terminals is used to measure the performance of the free-space link. The active optical terminals enable continuous, cycle-slip free, coherent transmission over periods longer than an hour. In this work, we achieve residual instabilities of 2.7 \u00d7 10 \u22126 rad 2 Hz \u22121 at 1 Hz in phase, and 1.6 \u00d7 10 \u221219 at 40 s of integration in fractional frequency; this performance surpasses the best optical atomic clocks, ensuring clock-limited frequency comparison over turbulent free-space links. Introduction Modern optical atomic clocks have the potential to revolutionize high-precision measurements in fundamental and applied sciences 1 , 2 , 3 , 4 , 5 , 6 , 7 . The ability to realize remote timescale comparison in situations where fiber links are impractical or impossible, specifically, between ground- and space-based optical atomic clocks 8 , 9 , 10 , 11 , 12 , 13 , 14 , 15 , 16 , 17 , 18 , 19 , 20 , 21 , 22 , will enable significant advances in fundamental physics and practical applications including tests of the variability of fundamental constants 23 , 24 , general relativity 25 , 26 , searches for dark matter 27 , geodesy 28 , 29 , 30 , 31 , 32 , 33 , 34 , and global navigation satellite systems 35 among others 36 , 37 , 38 , 39 , 40 , 41 , 42 , 43 , 44 , 45 , 46 . These efforts build on optical timing links developed for timescale comparison between microwave atomic clocks 47 , 48 , 49 , and efforts are underway to develop optical clocks that can be deployed on the International Space Station 50 and on dedicated spacecraft 51 . Similarly, timescale comparisons between mobile terrestrial optical clocks 1 , 52 , 53 , 54 , 55 , where one or more mobile clocks are able to be deployed and moved over an area of interest, enable ground tests of general relativity and local geopotential measurements for research in geophysics, environmental monitoring, surveying, and resource exploration. Comparison of both ground- and space-based clocks, and mobile terrestrial clocks, requires frequency transfer over free-space optical links. Just as with timescale comparison over optical fiber links, free-space frequency transfer should have residual instabilities better than those of the optical clocks. However, atmospheric turbulence induces much greater phase noise than a comparable length of fiber 12 , 19 , 56 , 57 . In addition, free-space links through the turbulent atmosphere must also overcome periodic deep fades of the signal amplitude due to beam wander and scintillation. When the size of the optical beam is smaller than the Fried scale of the atmospheric turbulence, the centroid of the beam can wander off the detector, while in the case where the beam is larger than the Fried scale, destructive interference within the beam (speckle) can result in loss of signal (scintillation) and so loss of timescale synchronization 19 , 58 , 59 . These deep fades can occur 10s to 100s of times per second for vertical links between the ground and space, and also on horizontal links on the order of 10 km 12 , 17 . One method to overcome deep fades of the signal is to transmit a series of optical pulses from an optical frequency comb and compare them with another optical frequency comb at the remote site 21 . While deep fades will result in the loss of some pulses, the time and phase information can be reconstructed from the remaining pulses. Another method to overcome deep fades is to stabilize the spatial noise caused by atmospheric turbulence by active correction of the emitted and received wave front. In general, tip-tilt correction is sufficient when using apertures that are small compared to the Fried scale as beam wander will dominate the deep fades. For large apertures, the effects of speckle scintillation increase and higher-order corrections using adaptive optics may be necessary. Tip-tilt stabilization of beam wander for comparison of atomic clocks has previously been demonstrated over 12 km with 50 mm scale optics 17 and 18 km with larger 250 mm telescopes 8 . A further practical concern for the deployment of free-space links is the ability of the system to acquire and track a moving object 10 , 60 . In that case, tip-tilt capability is mandatory, and additionally such a system must be robust while also having as low a size, weight, and power as possible for ease of deployment in spacecraft, airborne relay terminals, or mobile ground segments. In this work, we describe phase-stabilized optical frequency transfer via a 265 m point-to-point free-space link between two portable optical terminals. Both terminals have 50 mm apertures and utilize tip-tilt active optics to enable link acquisition and continuous atmospheric spatial noise suppression. The terminals are human-portable and ruggedized for daily field deployment to demonstrate the suitability for remote optical timescale comparison. The performance of the phase stabilization system was determined using a separate 715 m, phase-stabilized optical fiber link between the two terminals. The phase-stabilized free-space optical transfer exhibits an 80 dB improvement in phase noise at 1 Hz, down to 2.7 \u00d7 10 \u22126 rad 2 Hz \u22121 , compared to the unstabilized optical transmission. The active spatial stabilization used at each terminal is effective at suppressing beam wander caused by the atmospheric turbulence, allowing continuous, cycle-slip and deep-fade free, coherent transmission over periods longer than an hour. The resulting fractional-frequency stability of the phase-stabilized optical transfer reaches 1.6 \u00d7 10 \u221219 with 40 s of integration. At timescales beyond 100 s, the fractional-frequency stability flattens, which we determine to be caused by unstabilized temperature fluctuations in the uncompensated short fibers in the phase stabilization system. Results Coherent optical stabilization system Figure 1 shows the architecture of the phase stabilization systems, as well as the free-space and fiber links used to compare the phase noise performance. Fig. 1: Point-to-point phase-stabilized optical frequency transfer between buildings. a Block diagram of the experimental link. Two identical phase stabilization systems are implemented across the CNES campus. Both systems have their transmitter located in the Auger building (local site), and both receivers are located in the Lagrange building (remote site). One system transmits the optical signal over a 265 m free-space path between the buildings using tip-tilt active optics terminals while the other transmits via 715 m of optical fiber. The relative stability of the two optical signals is then measured at the remote site. QPD, quad-photodetector; Pol, polarization controller; PD, photodetector; PLL, phase-locked loop; AOM, acousto-optic modulator; FM, Faraday mirror; EDFA, erbium-doped fiber amplifier; Mix, radio frequency electronic mixer. Satellite image adapted from Google (Map data: Google, Maxar Technologies). b Active optical terminal located at the local site. c Transmitter portion of the phase stabilization system located at the local site. d Receiver portion of the phase stabilization system located at the remote site. Full size image A 15 dBm optical signal from a 1550 nm NKT Photonics X15 Laser was split and passed into two independent phase stabilization systems, detailed in \u201cMethods\u201d. One of these phase-stabilized systems operated over the free-space link, and was used to suppress the phase noise resulting from atmospheric turbulence. The second phase stabilization system operated over an optical fiber that ran underground between the local and remote sites, and was used to measure the performance of the free-space transmission. Each side of the free-space link also incorporated tip-tilt active optical terminals (detailed in \u201cMethods\u201d) that were used to suppress the received optical intensity fluctuations and deep fades caused by beam wander due to atmospheric turbulence. The remote terminal additionally had a bi-directional optical amplifier that amplified the incoming optical signal (typically by ~13 dB) before passing it to the phase stabilization system, and amplified the reflected portion of the signal for transmission back over the link. The free-space link spanned 265 m between two buildings at the Centre National d\u2019\u00c9tudes Spatiales (CNES) campus in Toulouse, as shown in Fig. 1 . The link passed over grass, sparse trees, and roads, and was operated during late winter over the course of 2 weeks. The most favorable conditions were when the sky was overcast and wind speed was low. Fully coherent transfer over a true point-to-point link Figure 2 shows the measurements for the fiber noise floor (gray) and phase stabilization off (red) cases made with a Microsemi 3120A Phase Noise Test Probe. Phase noise measurements for the phase-stabilized cases with (orange) and without (blue) tip-tilt were obtained using an Ettus X300 Software Defined Radio operating as a continuous IQ demodulator, and are also shown in Fig. 2 . Fig. 2: Phase and frequency stability of the optical transmission measured at the remote site. Red trace, free-space link phase stabilization off, tip-tilt active optics off (data from Microsemi); blue trace, free-space link phase stabilization on, tip-tilt active optics off (data from Ettus); orange trace, free-space link phase stabilization on, tip-tilt active optics on (data from Ettus); and gray trace, system noise floor with both phase stabilization systems transmitting over parallel optical fiber (data from Microsemi). a Power spectral density of the phase noise ( S \u03d5 ( f )) after transmission. b Fractional frequency stability presented as modified Allan deviation ( \u03c3 y ( \u03c4 )). The dashed traces are calculated from raw data; the solid traces are calculated from data with quadratic drift removed; and the error bars represent a standard fractional frequency measurement confidence interval set at \\(\\pm {\\sigma }_{y}(\\tau )\/\\sqrt{N}\\) , where N is the number of phase measurements. On both plots, the black dashed lines show key gradients of interest. Full size image Further discussion of the measurement equipment architecture and choice may be found in the Supplementary Note 1 . The phase noise Power Spectral Densities (PSD) found using the Ettus X300 shows good agreement with the Microsemi 3120A within overlapping frequency ranges, as shown in Supplementary Fig. 1 . When the phase stabilization and tip-tilt systems are off, the measured noise is expected to be dominated by atmospheric turbulence. In theory 61 , the corresponding PSD is expected to decrease as f \u22128\/3 for low frequencies, before dropping sharply as f \u221217\/3 due to the averaging effect of the optical aperture. The slopes of our measured PSD are compatible with that model. The transition frequency between the two regimes is given in ref. 61 by f c = 0.3 V \/ D , where V is the transverse wind speed and D the aperture diameter. This is not confirmed in our data, as wind speeds were no more than a few tens of m\/s and our beam diameter was about 34 mm. The corresponding theoretical transition frequency is significantly lower than the \u2248400 Hz visible in Fig. 2 . We attribute that discrepancy mainly to the fact that the theoretical calculations in ref. 61 were done for a plane wave impinging on a circular aperture, while our beam is Gaussian and smaller than the receiving aperture, and we note that discrepancies between the theoretical model and experimental measurements have been reported previously (see e.g. Tab. I in ref. 57 ). When the stabilization system is turned on, we see around eight orders of magnitude reduction in phase noise PSD at 1 Hz, down to 2.7 \u00d7 10 \u22126 rad 2 Hz \u22121 . Having the active tip-tilt terminal engaged appears to offer a slight improvement in phase-stability. At frequencies above roughly 2 kHz, the phase noise performance is limited by the residual phase noise of the laser (this is discussed in Supplementary Note 2 ), which also affects the unstabilized measurement above \u224810 kHz. At lower frequency (roughly 200 Hz to 2000 Hz), we are most likely limited by the noise floor resulting from the operation of our compensation system when applied to the atmospheric phase noise, as shown in detail in Supplementary Note 2 . The long-term fractional frequency stability of the stabilized signals is shown in Fig. 2 b in terms of modified Allan deviation (MDEV). This provides an alternative tool for assessing the performance of the stabilized optical transfer, with a particular focus on stability at longer time scales. The MDEV, calculated using the same Ettus X300 data, is shown in Fig. 2 b both in its raw form (dashed traces), as well as after removal of a quadratic fit in phase (solid traces). The linear and quadratic coefficients were 0.15 rad s \u22121 and 6.1 \u00d7 10 \u22127 rad s \u22122 for the tip-tilt on data (and 0.14 rad s \u22121 and \u22121.3 \u00d7 10 \u22127 rad s \u22122 for tip-tilt off). We attribute the linear drift to a known offset (measured as 0.141 rad s \u22121 ) produced in the Ettus. The residual linear drift after accounting for the Ettus is <9 mrad s \u22121 and results in a systematic offset of <1.5 mHz (or a fractional offset of <7.5 \u00d7 10 \u221218 ). This systematic offset does not impact the transfer stability; however, it would need to be taken into account when calibrating a true optical clock comparison. We further conclude that any residual drift is due to thermally induced variations in the differential optical length change of the uncompensated short (~60 cm) fibers between the laser and first splitters on the transmitter side, and last splitters and photo-diode on the receiver side (refer to Fig. 1 ). We expect that modest temperature control can decrease the quadratic effect by about an order of magnitude, hence the drift removed stability (solid lines) is likely to reflect the ultimate potential of our method. The MDEV averages as a combination of \u03c4 \u22123\/2 and \u03c4 \u22121 power laws until an integration time of around 20 s, indicating that the dominant noise at short timescales is white phase and flicker phase noise, in agreement with the phase noise PSD. The optimum stability reached when the active tip-tilt control system was turned off is 3.0 \u00d7 10 \u221219 at 40 s of integration time. When the active tip-tilt terminal is engaged, a slight improvement in stability is seen for integration times longer than 0.02 s (consistent with the phase noise PSD), and the transfer is made more robust. This results in a fractional frequency stability less than 7 \u00d7 10 \u221219 for integration times longer than 10 s, with an optimum stability of 1.6 \u00d7 10 \u221219 achieved at 40 s of integration. This is a factor of two improvement over the case without active tip-tilt control. At longer timescales, the stability does not integrate down further. This is likely due to long-term residual temperature fluctuations in the local and remote sites affecting the uncompensated parts of the two links, as discussed above, and observed in ref. 12 . With better thermal regulation, the fractional frequency stability is expected to continue averaging down to a lower limit. The minimum absolute fiber-to-fiber power loss achieved for the one-way transmission was ~12 dB, though this would quickly degrade with poor alignment. The two free-space beam splitters in the optical terminal account for 6\u20137 dB of the loss, and the remaining is attributed to coupling losses, imperfect alignment, and atmospheric effects. During operation, the relative power of the optical signal received by the remote site was recorded in order to measure the atmospheric induced fluctuations encountered during a one-way pass of the free-space link. Immediately after the active terminal, a fiber splitter was used to send a small portion of the received signal to a fiber-photo-detector with a linear response to the received optical power. The response of this detector was then digitized at 4 kHz. Figure 3 shows the frequency domain power of the received power fluctuations. Without the active tip-tilt terminal engaged, the power fluctuations drop as roughly f \u22122 at low frequency and f \u22123\/2 beyond a few Hz. The tip-tilt active optical terminal improves the stability at frequencies below 4 Hz, with over two orders of magnitude reduction in power fluctuations at 0.1 Hz. The tip-tilt servo bump at ~7 Hz is clearly visible. Beyond that bump, there is not a significant difference between having the tip-tilt compensation on or off, as expected. It is interesting to note that the ~4 Hz crossing point roughly matches the frequency at which the phase noise PSD in Fig. 2 starts improving for tip-tilt on (with respect to off), confirming that the phase noise reduction is related to the reduction in power fluctuations. This also implies that better performance of the active optics system (hence lower power fluctuations) is likely to lead to lower phase noise. The tip-tilt system was based on a commercially available unit and the low bandwidth of the system is due to the low gain setting necessary to mitigate some artifacts in the control system firmware (discussed further in \u201cMethods\u201d). Fig. 3: Normalized power ( P \/\u3008 P \u3009) of the free-space optical signal received at the remote site with over 3 min. Blue trace, tip-tilt active optics off; and orange trace, tip-tilt active optics on. a Power spectral density of the received power. b Time series of received power with tip-tilt active optics off. c Time series of received power with tip-tilt active optics on. d Histogram of the normalized received power values. Full size image The time domain plots and the histogram provide additional representations of the effect of the active terminal. Without tip-tilt, the optical power fluctuates significantly, and at around 100 s, there is a step change in the received power. This was likely due to mechanical movement of the optical terminal, such as mechanisms in the telescope mount suddenly slipping. This step in power can also be seen in the bi-modal distribution of the histogram. When the tip-tilt actuation was activated, step behavior like this was not observed. Taking the bi-modal feature due to movement of the optical terminal into account, the histograms for both the tip-tilt on and off cases exhibit a log-normal distribution, as is expected of power fluctuations caused by turbulence-induced beam wander. The case with the tip-tilt system engaged shows a much narrower distribution in received power, indicating more constant optical power levels delivered to the phase stabilization system. This indicates that the tip-tilt active optics are effective at suppressing power fluctuations caused by atmospheric turbulence or movement of the terminals. For clarity, the optical power time series traces shown in Fig. 3 are normalized to their own average power level. With the tip-tilt system on, the average optical power received at the remote site was 2.4 times higher than the average power level when the tip-tilt system was off. Critically, with the tip-tilt system on, the optical power does not make significant excursions into lower power values, greatly reducing the chance of a cycle slip in the phase stabilization system. Discussion The transfer of stable optical frequency reference signals over free-space is of particular interest to applications involving ad-hoc transmissions between mobile sites. A specific example of interest is chronometric geodesy 28 , 29 , 30 , 31 , 32 , 33 , 34 , where frequency comparisons with a mobile optical atomic clock at different positions over the region of interest provide a direct measurement of the gravitational red-shift caused by changes in gravitational field and height. The requirements are that the transfer system provide sufficiently stable optical transmission so that the uncertainty of the frequency comparison is limited by the uncertainty of the optical atomic clocks themselves, be physically robust and portable, and be light and small enough to allow for easy and rapid set up of the terminals in different locations. The stabilities of the best lab-based optical atomic clocks are approaching 10 \u221218 for averaging times on the order of 10 3 s 2 , 28 , 62 , 63 , 64 , 65 . Bothwell et al. 63 achieve a stability of \\(4.8\\times 1{0}^{-17}\/\\sqrt{\\tau }\\) (represented by the \u03c4 \u22121\/2 gradient line in Fig. 2 ), averaging down to a final systematic uncertainty-dominated stability of 2 \u00d7 10 \u221218 within 10 min. The stability demonstrated using the system described in this paper surpasses this stability by more than an order of magnitude, ensuring that frequency comparison between optical clocks over a turbulent free-space channel such as this will not be limited by the performance of the phase-stabilized link. Our system is also designed to be physically robust and portable (as shown in Fig. 1 ). The optical terminals are securely built within a steel enclosure that provides protection during transport and while the link is operational. Each terminal has a mass of 14.5 kg, and is 49 cm wide, 24 cm deep, and 18 cm high. The optical fiber-based phase stabilization systems are built within 19\" rack-mount steel and aluminum enclosures. The transmitter module is 2U high, 34 cm deep, and has a mass of 11.6 kg; while the receiver module is 1U high, 25 cm deep, with a mass of 5.9 kg. It should be noted that there is scope for significant reduction in size and weight through the use of custom-engineered components. The robustness of the terminals was demonstrated by the fact that they were successfully shipped, via conventional couriers, from Perth, Australia to Toulouse, France without damage or misalignment of the optics. One of the terminals was installed in a telescope dome for the duration of the 2-week trial period, while the other terminal was set up on an open rooftop and was removed and reset every day. A co-aligned visible guide laser and a simple mount scanning algorithm were used to set the link alignment each morning, and initial alignment could be completed within ~15 min. Throughout the day, the link alignment would occasionally need to be re-optimized. We have since commenced the development of a co-aligned camera with a machine vision imaging system to automate link acquisition to under a minute. The long-term operation of the system was limited by the performance of the tip-tilt active optics due to the relatively low sensitivity of the Quadrant Photo Detector (QPD), as discussed further in \u201cMethods\u201d. The lower limit of the QPD\u2019s operational detected power range is \u221210 dBm (0.1 mW), whereas the phase stabilization system is capable of operating with ~\u221254 dBm (4.0 nW) of light returning to the transmitter unit 66 . Thus, large drops in link power would first affect the QPD, causing the tip-tilt system to lose the link alignment, and resulting in a loss of signal and cycle slips in the phase stabilization system. Additionally, the tip-tilt system lacked the ability to consistently recover from loses in link alignment. This resulted in the link being able to consistently achieve cycle-slip and deep-fade-free operation for time periods on the order of 3 \u00d7 10 3 s, before the tip-tilt system lost link alignment. For the system to be able to operate over longer periods of time, the tip-tilt system has to be improved to operate with less stringent power requirements and to be able to effectively re-acquire the link. There are additional challenges associated with extending the link to beyond 265 m, including more severe atmospheric effects and increased power losses. The more severe atmospheric effects will require higher tip-tilt suppression bandwidth and steering range. This will involve improving the feedback transfer function to more effectively deal with the frequency resonances of the tip-tilt mirror, or replacing the mirror and actuators with alternatives that have higher resonances. The decreased optical power associated with longer links will exacerbate the issues caused by the low sensitivity of the QPD. Our plan to overcome this is to replace the QPD with a more sensitive equivalent and increase the power of the transmitted beam with a high-power amplifier. This should allow operation over longer links, without having to significantly increase the complexity of the active optical terminals. An alternative method of dealing with the greater power losses associated with longer links would be to increase the size of the apertures. This however introduces other difficulties. If the size of the apertures increases to much larger than the Fried parameter, then higher-order spatial effects of the atmosphere will start to become significant and lead to speckle and scintillation 19 . Complex and expensive adaptive optics would be required to suppress these higher order effects. Simulations, similar to those published by Robert et al. 19 , indicate that tip-tilt is sufficient at keeping power fluctuations low for links to a stratospheric platform at a 50 km distance, provided the apertures remain below around 10 cm. This reduces the required complexity of the optics, but at the cost of higher absolute link loss. While the focus of our research has been terrestrial links between mobile optical atomic clocks, it is worth noting that the compact nature of the demonstrated system may prove useful for future satellite-to-satellite timing links. The significant weight, and power consumption costs associated with satellite instrumentation, lends itself to the simple system demonstrated in this paper. Additionally, the reduction in atmospheric effects associated with satellite-to-satellite transmission may reduce the challenges associated with the active optics. There are however many other challenges associated with creating a coherent satellite-to-satellite link that have not been captured within the experiment described in this paper. For example, the phase stabilization technique will work only with reduced bandwidth due to the longer transmission time, and be affected by large Doppler shifts. While solutions exist (e.g. corrections in post-analysis), a significant amount of system development and experimentation would be required before translating the system to space-based links. The long-term goal of our collaboration is to work toward a practical system for performing high-precision clock comparisons between mobile atomic clocks for the purposes of chronometric geodesy. This application requires the use of ad-hoc free-space links between mobile optical atomic clocks separated by up to 100 km, and without necessarily having line of sight. For this extreme application, beyond having to overcome the power and atmospheric challenges mentioned above, an active relay off an airborne platform would be required. The results of this paper represent the first steps toward this ambitious long-term goal. Methods Phase stabilization system Two phase stabilization systems, with very similar architectures, are used to stabilize the free-space and fiber paths. For simplicity, we assume negligible propagation delay and consider only link noise in this section, but these assumptions are revisited in Supplementary Note 2 . Equivalent variables relating to the free-space and fiber stabilization systems are identified by superscripts of fs and fb, respectively. The stabilization systems are based on the imbalanced Michelson interferometer design developed by Ma et al. 67 , 68 , where the long arm of the interferometer is sent over the link and the short arm is reflected by a Faraday mirror to provide an optical frequency reference. The frequency of the outgoing optical signal is shifted by a transmission acousto-optic modulator (AOM) with a nominal frequency ( \\({\\nu }_{{\\rm{tr}}}\\) ) which may be varied ( \\({{\\Delta }}{\\nu }_{{\\rm{tr}}}\\) ). The shifted optical signal is then sent over the link. In the free-space system, the signal is passed through the active terminal described below and launched over the free-space link. The signal then reaches the remote site after picking up link phase noise caused mainly by atmospheric turbulence ( \u03b4 \u03bd fs ). This optical signal is received by a second active optical terminal and passed through a bi-directional optical amplifier to offset the signal power lost during transmission. In the fiber system, the signal is passed through an underground fiber running between the two sites. The transmitted signal picks up link noise due to mechanical and thermal fluctuations along this fiber ( \u03b4 \u03bd fb ). At the remote site, each stabilization system passes their received signal through an anti-reflection AOM ( \u03bd ar ), before outputting half the signal to the end user ( \u03bd out ). The output at the remote site of the free-space stabilization system is given by $${\\nu }_{{\\rm{out}}}^{{\\rm{fs}}}={\\nu }_{{\\rm{L}}}+{\\nu }_{{\\rm{tr}}}^{{\\rm{fs}}}+{{\\Delta }}{\\nu }_{{\\rm{tr}}}^{{\\rm{fs}}}+{\\nu }_{{\\rm{ar}}}^{{\\rm{fs}}}+\\delta {\\nu }^{{\\rm{fs}}}\\ ,$$ (1) where \u03bd L is the laser frequency, while the output of the fiber stabilization system is given by $${\\nu }_{{\\rm{out}}}^{{\\rm{fb}}}={\\nu }_{{\\rm{L}}}+{\\nu }_{{\\rm{tr}}}^{{\\rm{fb}}}+{{\\Delta }}{\\nu }_{{\\rm{tr}}}^{{\\rm{fb}}}+{\\nu }_{{\\rm{ar}}}^{{\\rm{fb}}}+\\delta {\\nu }^{{\\rm{fb}}}\\ .$$ (2) The two signals are optically beat together at a photodetector and low-pass filtered to produce a down-converted signal, $${\\nu }_{{\\rm{meas}}}={\\nu }_{{\\rm{out}}}^{{\\rm{fs}}}-{\\nu }_{{\\rm{out}}}^{{\\rm{fb}}},$$ (3) used to measure the relative stability of the optical signals reaching the remote site through free-space and fiber. The residual phase noise from the free-space transmission dominates the residual phase noise from the fiber transmission over most of the Fourier frequency range. The AOM frequencies were chosen so that the measured beat signal ( \u03bd meas ) was at a nominal frequency of 1 MHz. An external 10 MHz signal from a hydrogen maser was shared between the two sites via radio frequency (RF) over fiber and provided a common reference for the transmitter oscillators and remote site measurement equipment. As the frequency of the RF reference is seven orders of magnitude lower than the optical signal, the frequency stability of the RF reference will not significantly degrade the phase measurement taken by the remote site measurement equipment. The other half of the signals reaching the remote site are reflected by Faraday mirrors back through the anti-reflection AOMs and back over the free-space and fiber links. For the free-space link, the return signal also passed back through the bi-directional optical amplifier. At the local site, the signals returning from the fiber and free-space links pass back through their respective transmission AOMs. Each system then performs a self-heterodyne measurement by beating the returned signal against the short arm of the Michelson interferometer on a photodetector. The final electrical beat signal, $${\\nu }_{{\\rm{beat}}}=2{\\nu }_{{\\rm{tr}}}+2{{\\Delta }}{\\nu }_{{\\rm{tr}}}+2{\\nu }_{{\\rm{ar}}}+2\\delta \\nu ,$$ (4) now contains information about the phase noise picked up during the transmission over the link. This signal is then mixed with a local oscillator of frequency ( \\(2{\\nu }_{{\\rm{tr}}}+2{\\nu }_{{\\rm{ar}}}\\) ) and low-pass filtered in order to extract a DC error signal, $${\\nu }_{{\\rm{dc}}}=2{{\\Delta }}{\\nu }_{{\\rm{tr}}}+2\\delta \\nu ,$$ (5) for the phase-locked loop (PLL) that stabilizes the transmission frequency. The PLL then controls the frequency of the transmission AOM in order to drive this error signal to zero, such that \\({{\\Delta }}{\\nu }_{{\\rm{tr}}}=-\\delta \\nu\\) . This has the effect of suppressing the link phase noise from the free-space (Eq. 1 ) and fiber (Eq. 2 ) output signals. Active optical terminals The active terminals (Fig. 1 ) used at each end of the free-space link were reciprocal and identical. The optical signal is passed through a fiber to free-space collimator with a 1\/ e 2 radius of 1.12 mm. This is then passed through a 50\u201350 beam splitter (BS). Half the optical signal is sent to a beam-dump, and the other half is sent to a 15:1 Galilean beam expander (GBE) with a 48 mm clear aperture. The signal from the GBE is reflected off a 50 mm flat mirror with active piezo-electric actuators and launched over the free-space link with a 1\/ e 2 radius and divergence of approximately 16.8 mm and 29 \u03bcrad, respectively. The incoming beam is reflected by the active mirror into the GBE. The BS then sends half the incoming light to the free-space-to-fiber collimator, and the other half to a QPD. This QPD is used to detect first-order spatial fluctuations in the incoming beam. The measured fluctuations are passed through a Proportional Integral (PI) controller and used to drive the piezo-electric actuators on the active mirror in order to suppress these fluctuations and keep the incoming beam centered on the QPD. The QPD is positioned so that the optical signal coupled by the collimator into the fiber is maximized when the beam is centered on the QPD. The QPD and active mirror control system is a commercial off-the-shelf system. The achievable turbulence suppression bandwidth of the system during these tests was limited by the low PI controller gain settings which were necessary to reduce the sensitivity of the tip-tilt system to noise in the QPD when the link optical power dropped below the threshold for effective operation of the QPD. When the optical power dropped below this threshold, the tip-tilt system would attempt to steer to the false beam centroid caused by the detector noise, losing the real beam in the process. The low gain settings prevented the tip-tilt system from steering too far off target before sufficient optical power was restored. Data availability The data that support the findings of this study are available from the corresponding author, B.P.D.-M., upon reasonable request. ","News_Body":"Scientists from the International Centre for Radio Astronomy Research (ICRAR) and the University of Western Australia (UWA) have set a world record for the most stable transmission of a laser signal through the atmosphere. In a study published today in the journal Nature Communications, Australian researchers teamed up with researchers from the French National Centre for Space Studies (CNES) and the French metrology lab Syst\u00e8mes de R\u00e9f\u00e9rence Temps-Espace (SYRTE) at Paris Observatory. The team set the world record for the most stable laser transmission by combining the Aussies' phase stabilization technology with advanced self-guiding optical terminals. Together, these technologies allowed laser signals to be sent from one point to another without interference from the atmosphere. Lead author Benjamin Dix-Matthews, a Ph.D. student at ICRAR and UWA, said the technique effectively eliminates atmospheric turbulence. \"We can correct for atmospheric turbulence in 3-D, that is, left-right, up-down and, critically, along the line of flight,\" he said. \"It's as if the moving atmosphere has been removed and doesn't exist. It allows us to send highly stable laser signals through the atmosphere while retaining the quality of the original signal.\" The result is the world's most precise method for comparing the flow of time between two separate locations using a laser system transmitted through the atmosphere. One of the self-guiding optical terminals on its telescope mount on the roof of a building at the CNES campus in Toulouse. Credit: ICRAR\/UWA ICRAR-UWA senior researcher Dr. Sascha Schediwy said the research has exciting applications. \"If you have one of these optical terminals on the ground and another on a satellite in space, then you can start to explore fundamental physics,\" he said. \"Everything from testing Einstein's theory of general relativity more precisely than ever before, to discovering if fundamental physical constants change over time.\" The technology's precise measurements also have practical uses in earth science and geophysics. \"For instance, this technology could improve satellite-based studies of how the water table changes over time, or to look for ore deposits underground,\" Dr. Schediwy said. There are further potential benefits for optical communications, an emerging field that uses light to carry information. Optical communications can securely transmit data between satellites and Earth with much higher data rates than current radio communications. \"Our technology could help us increase the data rate from satellites to ground by orders of magnitude,\" Dr. Schediwy said. \"The next generation of big data-gathering satellites would be able to get critical information to the ground faster.\" The phase stabilization technology behind the record-breaking link was originally developed to synchronize incoming signals for the Square Kilometer Array telescope. The multi-billion-dollar telescope is set to be built in Western Australia and South Africa from 2021. ","News_Title":"Record-breaking laser link could provide test of Einstein's theory","Topic":"Physics"}
{"Paper_Body":"Abstract For the first time it is shown that carbon black inks on ancient Egyptian papyri from different time periods and geographical regions contain copper. The inks have been investigated using synchrotron-based micro X-ray fluorescence (XRF) and micro X-ray absorption near-edge structure spectroscopy (XANES) at the European Synchrotron Radiation Facility (ESRF). The composition of the copper-containing carbon inks showed no significant differences that could be related to time periods or the geographical locations. This renders it probable that the same technology for ink production was used throughout Egypt for a period spanning at least 300 years. It is argued that the black pigment material (soot) for these inks was obtained as by-products of technical metallurgy. The copper (Cu) can be correlated with the following three main components: cuprite (Cu 2 O), azurite (Cu 3 [CO 3 ] 2 [OH] 2 ) and malachite (Cu 2 CO 3 [OH] 2 ). Introduction Two of the most profound technological advances in human intellectual history were the twin inventions of ink and papyrus, the ancient precursor of modern paper, by the Egyptians about 5.000 years ago. The advent of writing allowed information to be expanded beyond the mental capacity of any single individual and to be shared across time and space. The two inventions spread throughout the ancient Mediterranean to Greece, Rome and beyond. The chemistry of the black inks used in the ancient world has been only scantily studied so far, leaving gaps in our knowledge of one of the fundamental inventions in the history of civilization 1 . Thus, until recently, it was assumed that the ink used for writing was primarily carbon-based at least until the 4 th to the 5 th century CE. However, micro XRF analyses of two papyrus fragments from Herculaneum have shown that lead compounds were added to black ink already in 1 st century CE, thereby modifying our knowledge of ink manufacture in Antiquity 2 , 3 . Here, we report on the chemical composition of black ink inscribed on papyrus fragments from ancient Egypt using micro XRF and XANES. The fragments form parts of larger manuscripts belonging to the Papyrus Carlsberg Collection, University of Copenhagen, and can be divided into two groups: The first group comes from southern Egypt and consists of the private papers of an Egyptian soldier, Horus, who was stationed at the military camp of Pathyris, located at modern Gebelein some 30 km south of Luxor. Pathyris was destroyed in 88 BCE during a civil war and thousands of papyri have been preserved in the ruins until modern times and are now conserved in papyrus collections around the world, including Berlin, Cairo, Heidelberg and Turin, as well as Copenhagen. Our archive consists of 50 Greek and Egyptian papyri that date to the late 2 nd and early 1 st century BCE. They were bought on the antiquities market in 1924 by the manuscript collector Elkan Nathan Adler (1861\u20131941) according to whom they had been found inside a sealed jar at the ancient settlement 4 . This is the only archive from Pathyris that have come down to posterity substantially intact 5 . The second group derives from the only large scale institutional library to survive from ancient Egypt, the Tebtunis temple library. The assemblage includes an estimated 400\u2013500 papyrus manuscripts which span the 1 st through the early 3 rd century CE, with the bulk dating to the late 1 st and 2 nd centuries. It was discovered within two small cellars inside the main temple precinct at Tebtunis, modern Umm el-Breig\u00e2t, which is located in the south of the Fayum depression, some 100 km south-west of Cairo. The dry and brittle manuscripts are all poorly preserved and the material as a whole now consists of many thousands of smaller fragments, which are preserved in papyrus collections around the world, including Copenhagen, Florence, Berlin, Berkeley, Oxford and Yale. Whole columns or pages are only rarely preserved, and the difficult and time consuming process of sorting and identifying fragments of specific manuscripts is still ongoing. Published texts indicate that on average less than 10% of a manuscript is likely to have been preserved. The papyri selected for analysis were acquired for the Papyrus Carlsberg Collection between 1931 and 1938 on the antiquities market in Cairo 6 . Recently, the chemical composition of papyri and ink from the two localities was studied using a combination of laboratory XRF point analysis, Raman spectroscopy and scanning electron microscopy-energy dispersive x-ray spectroscopy (SEM-EDXS). Despite their distance in time, space, and social context, the study concluded that the black inks of Pathyris and Tebtunis revealed similar traits and that \u2013 besides carbon ink \u2013 two other distinct types of black ink were used for at least a period of 300 years: lead-containing carbon ink and copper-containing carbon ink. However, this preliminary characterization was limited to conventional XRF (few points), Raman and SEM-EDXS (small area maps) techniques and the chemical nature of the lead (Pb) and copper (Cu) compounds detected in the black inks could not be ascertained through the experimental setup 7 . Experimental Samples In total, the research was conducted on a corpus of 12 fragments. The papyri are of a light brown color and the inks range from deep black to light grey or brown (cf. the visible light pictures shown in the figures). The papyrus medium itself is approximately 0.3 mm thick and made of two layers of papyrus strips \u2013 in one instance, where two sheets overlap, of four layers (sample 1). The macro XRF elemental maps, discussed below, showed either no contrast between the inked areas and the papyrus, indicating soot or finely powdered charcoal as the origin of the black color, or the presence of Cu or Pb compounds in the pigments. In Fig. 1 , an example of a XRF fit is shown, which demonstrates that the main elements can be identified with certainty. Figure 1 Example of a XRF fit (sample 1). Full size image Out of the 12 samples, five showed no contrast, six contained Cu and a single fragment Pb. Here, we report results obtained from a study of four samples with Cu-containing black inks, two from Pathyris and two from Tebtunis respectively. The four samples were chosen, because they showed an intense Cu signal in the inked areas. Further, as an example (sample 5), a carbon based ink from Pathyris is included in the supporting information (Fig. S1 ). Samples 1 and 2 are Greek contracts from Pathyris that date to 134 BCE (Fig. 2A ) and 101 BCE (Fig. 3A ) respectively. Sample 5 belongs to the same archive and is written demotic, a cursive ancient Egyptian script; it dates to c. 100 BCE. Samples 3 and 4 were found at Tebtunis; they are written in Demotic and can be dated to the 1 st \/2 nd century CE (Figs 4A and 5A ) on the basis of paleography. For synchrotron-based analyses, the papyri were analyzed, without any sample preparation: the fragments were maintained between two 4 \u00b5m thick Ultralene foils (Spec, Certiprep) and mounted vertically in the X-ray microscope. Figure 2 ( A ) Visible light picture of sample 1 (P. Carlsberg 828) ( B ) macro and micro XRF maps of Cu (fitted and normalized by the intensity of incident beam). The areas were XANES spectra were collected are highlighted ( C ) Average XANES spectra from area 2, and its decomposition by LCF. Full size image Figure 3 ( A ) Visible light picture of sample 2 (P. Carlsberg 839) ( B ) macro and micro XRF maps of Cu (fitted and normalized by the intensity of incident beam). The areas were XANES spectra were collected are highlighted. The red-blue maps are the superimposition of Cu and Fe maps, from area 2. The red-green maps are the superimposition of Cu micro XRF maps excited at two specific energies shown in ( C ) (after realignment) ( C ) Average XANES spectra from area 2, \u201cred region\u201d and \u201cgreen region\u201d in red-green dual-energy map, and from area 5 and their decomposition by LCF. Full size image Figure 4 ( A ) Visible light picture of sample 3 (P. Carlsberg 79) ( B ) macro and micro XRF maps of Cu (fitted and normalized by the intensity of incident beam). The areas, where XANES spectra were collected, are highlighted ( C ) Average XANES spectra from area 2 and its decomposition by LCF. Full size image Figure 5 ( A ) Visible light picture of sample 4 (P. Carlsberg 649) ( B ) macro and micro XRF maps of Cu (fitted and normalized by the intensity of incident beam). The areas were XANES spectra were collected are highlighted. The red-blue maps are the superimposition of Cu and Fe maps, from the detailed map ( C ) Average XANES spectra from area 1, and its decomposition by LCF. Full size image The black inks on the analyzed fragments appear black at an IR illumination of 970 nm and show no signs of transparency, as observed for other black pigment materials such as iron-gall ink 6 , 7 , 8 . This suggests that the inks used are based on amorphous carbon obtained through the pyrolysis or macerating of botanicals, which is confirmed by Raman spectroscopy carried out on the same papyri, where the spectra are characterized by two broad bands at ca. 1322 and 1588 cm \u22121 , known as D and G bands of carbon materials 7 , 9 , 10 . Macro XRF and micro XRF XRF measurements were performed at X-ray microscopy beamline ID21 at the ESRF (Grenoble, France) 11 . By the use of a Si (111) monochromator, the primary beam energy was tuned at the Cu-K edge (8979 eV). For general overview mapping over entire fragments (macro XRF), the beam spot size was defined using a pinhole of 100 or 50 \u00b5m diameter. The average beam flux was ~10 9 \u201310 10 ph\/s during the measurements of the samples. An incident beam flux monitoring pin diode was used continuously to monitor and correct for intensity variations (i 0 ). XRF maps were acquired by scanning the sample through the X-ray beam with a single energy of 9.05 keV recording a XRF spectrum at each pixel with an acquisition time of 100 ms. High resolution micro XRF maps were acquired the same way, with a beam focused down to ~0.4 \u00d7 0.7\u00b5m 2 using a Kirkpatrick-Baez mirror system. The microscope was operated in vacuum and samples were mounted vertically under an angle of 62\u00b0 with respect to the primary X-ray beam. No visible modification of any aspect of the samples was observed after analysis. The XRF (and scattered) radiation was detected using a Bruker (Germany) XFlash 5100 silicon drift detector (SDD), equipped with a Moxtex AP3.3 polymer window 12 , and mounted under 69\u00b0 with respect to the primary X-ray beam. An additional Ultralene foil (4 \u00b5m) covered the detector. XRF spectra were processed using the PyMCA software package 13 . Elemental maps shown in the figures below are the batch fitted XRF intensity maps, divided by the i 0 map. Micro XRF maps were acquired for sample 2 at three different energies to map the different Cu species. The small beam shift between these different maps was determined using the Fe maps and Cu maps were realigned accordingly, using the \u201cSpectrocrunch\u201d python software library 11 . Micro XANES The measurements were performed at ID21, at the Cu K-edge (calibrated with a Cu foil, setting the maximum of the derivative spectrum at 8.979 keV). Micro XANES spectra were recorded in XRF mode (using the same set-up described above) with a micro beam of 0.4 \u00d7 0.7\u00b5m 2 . The micro XANES spectra were obtained by scanning the primary energy from 8.9 to 9.15 keV in 260 steps of 0.3 eV. To reduce risks of radiation damage, XANES spectra were acquired as single acquisition (30 s\/point) over many points, instead of cumulating many spectra on few points. Normalized data were employed for Linear Combination Fitting (LCF), using the ATHENA software, to identify and to estimate the amount of copper compounds on the analyzed papyrus (cf. list of reference in Table 1 ) 14 . The reference compounds were prepared as powder and measured in transmission mode. LCF were accomplished within \u221220 < E 0 > 30 eV range, using first all the references (azurite, malachite, chalcanthite, tenorite, cuprite, chalcopyrite and Cu acetate), and then reducing this set to the main 3 or 4 components; all amounts between 0\u20131, but not forced to sum 1 for better alignment (amounts were recalculated); all spectra shared the same E 0 value. The micro XANES spectra were compared to selected reference compounds (Table 1 ), chosen as the most probable compounds according to the micro XRF maps and the available literature 1 , 15 , 16 , 17 , 18 , 19 . In general, the LCFs have good R-factors (0.002\u20130.017). However, it has to be kept in mind that all results below could be biased by this set of references and we cannot exclude that other Cu compounds may be present. Table 1 Results of the LCFs analysis of XANES spectra, calculated as average over n points per area. Areas are located in the different XRF maps (cf. Figs 2B , 3B , 4B and 5B ). Full size table Results Macro XRF maps The papyrus fragments were scanned using X-ray beams of different sizes, from a sub-millimeter to a micrometric scale. Macro-XRF maps of the full fragments were used to identify and localize elements both outside and inside of the ink (cf. the supporting information, where all the XRF elemental maps of the five samples are provided and some specific results are commented). Although the papyri derive from different time periods and geographical areas, the elemental composition detected in the fragments is similar and showed the following distributions: potassium (K) and chlorine (Cl) maps reveal the fibrous structure of the papyri. Silicon (Si) shows a complementary distribution; as if it fills the holes left by the K-Cl based fiber structure. Sodium (Na), magnesium (Mg), aluminum (Al), phosphor (P), sulfur (S), calcium (Ca) and manganese (Mn) are present in a rather homogeneous way on the surface of the papyrus fragments, independently of the fibrous structure. Iron (Fe) is present as spots all over the papyri, independently of the ink, except for sample 4, where Fe-Al-K containing spots are more concentrated in the inked regions (Fig. S 11 ). The Cu elemental distribution (fitted XRF intensity divided by the intensity of incoming beam) for samples 1, 2, 3 and 4 is depicted in Figs 2B , 3B , 4B and 5B . The color scales are identical for the large map of samples 1, 2 and 3 (0\u20130.5, a.u.). Because sample 4 shows lower amounts of Cu in the ink, the scale has been adjusted to 0\u20130.15 a.u. in Fig. 5 . From the maps, it is clear that the copper is concentrated in the letters and signs from where it diffuses out in the papyri and runs along the fibrous structure. As seen in the supporting information, some Cr maps show a peculiar circular structure that is due to the sample holder. This demonstrates that the X-rays penetrated the full depth of the samples (Figs S 5 , S 6 , S 8 ). Micro XRF maps Additional XRF maps were acquired on selected areas with Cu-containing black ink, in order to assess at the micrometer scale the possible co-localization of certain elements that were detected at the macro-scale. Sample 1: as observed in the macro-XRF maps, Cl, K and Cu maps show some correlations, but this may be due to the strong diffusion of Cu in the K-Cl fibrous structure. The other elements do not show a particular co-localization with Cu (Fig. S 4 ). Sample 2: a high resolution micro XRF map revealed significant variation in the distribution of Cu both within and outside the ink area. To further investigate these differences, five areas were examined more closely, encompassing the ink, the surrounding fibers and Cu-rich spots (Figs 3B , S 6 and S 7 ), with a pixel size of 1 or 2 \u00b5m. None of the other detected elements were co-localized with Cu. As an example, a Fe map is shown in Fig. 3C . Sample 3: micro XRF maps confirmed the results obtained from the macro XRF maps (cf. Fig. S 8 ), which showed slightly higher counts of Mg, Al, S, P, Ca, Mn and Pb in the ink than in the papyrus (Fig. S 9 ). The detailed maps show a similar co-localization of Cu with P, S and Pb at the micron-scale. However, considering the diffuse distribution of these elements and the absence of micrometric Cu-based spots, it cannot be concluded that these elements originate from the copper source; they are rather associated with the soot and the binder (Fig. S 10 ). Sample 4: spots containing Mg, Al, Si, P, K, Cr, Mn and Fe are found, but they are not co-localized with Cu (cf. Figs 5B , S 11 and S 12 ). Other spots contain Ca and S, Ca and P, or Ca alone. S, Cl and Ca were more concentrated in ~50 \u00b5m Cu-rich regions (Fig. S 11 ), but without co-localization to Cu at the micron-scale (Fig. S 12 ). Micro XANES In order to examine the Cu speciation and possibly its origin in the Cu-rich inks, Cu K-edge micro XANES spectra were acquired at different points of the four samples. The fact that none of the other detected elements were co-localized with Cu led us to the assumption that Cu was most probably present as elements from the first and second periods of the periodic table, e.g. oxides, hydroxides, carbonates or organic salts. For sample 1, we recorded micro XANES spectra at 17 points located in three different regions highlighted in Fig. 1C . Since the spectra were very similar, they were subsequently averaged. The LCF of the average of Cu-K XANES spectra was done using the above mentioned reference compounds, excluding chalcopyrite, since Cu is not co-localized with Fe. In the three areas, micro XANES spectra show features characteristic of a mixture of Cu 1+ (fitted as cuprite) and Cu 2+ species (fitted as azurite and malachite) (Table 1 and Fig. 2C ). A total of 120 spectra were collected for sample 2 in the five areas shown in Fig. 3C . These spectra showed clear differences from one area to another, but also within a single Cu spot. As an example, some of the spectra acquired over the spots in area 1, 2 and 3 showed a pronounced shoulder at ~8.987 keV, while this shoulder was mostly absent in spots at other locations. To map the distribution of these different species, speciation maps were acquired by collecting micro XRF maps at three different energies: at the shoulder energy (E 1 = 8.987 keV), at the maximum absorption energy (E 2 = 8.997 keV) and above the edge \u2013 in order to map the Cu distribution, independently of its speciation (E 3 = 9.075 keV). The superimposed Cu maps obtained at E 1 and E 2 are shown in Fig. 2B and reveal a tight interlacing of the two Cu-based ingredients. None of these maps is correlated with the Fe map (Fig. 3B ). The LCF of the average spectra exhibiting a strong shoulder at E 1 gives azurite as the main component (the azurite reference contains a pronounced shoulder at E 1 ), together with some malachite and cuprite (Table 1 and Fig. 3C ). The spectra with a less intense shoulder could be fitted with a strong contribution of copper acetate, with smaller amounts of azurite (Table 1 and Fig. 3C ). Spectra acquired in area 4 \u2013 i.e. in the ink, but not in the \u2018spotty\u2019 Cu regions \u2013 were fitted as azurite mainly, together with cuprite and copper acetate. Finally, the spectra acquired in regions where Cu has diffused within papyrus fibers, but outside areas of actual writing, show a lower signal but with a more pronounced shoulder at 9.075 keV, together with a clear shift of the edge energy. These features could be fitted by a high contribution of cuprite (>50%), mixed with malachite and azurite (Table 1 and Fig. 3C ). For sample 3, a total of 21 spectra were acquired: six in the ink in the region with high Cu content, eight in the ink with lower Cu content and seven spectra in a region, where the Cu distribution follows the fibrous structure. The LCF analysis of the average spectra gives cuprite as the main component, together with azurite and malachite (Table 1 and Fig. 4C ). A total of 41 spectra were acquired for sample 4 over four areas (Fig. 5 ): in spots with a high content of Cu both with and without Fe, in ink regions with lower Cu content and in fibrous Cu rich regions, far removed from the ink. In all these areas, cuprite is the main component of the LCFs. Malachite is the second component, and azurite to a lesser extent (Table 1 and Fig. 5C ) Discussion The synchrotron based macro and micro XRF maps confirmed the presence of Cu in the black ink on the four ancient Egyptian papyri studied here. In sample 2, 3 and 4, the ink contains Cu and other lighter elements \u2013 Al, Si, K, Mn, Fe \u2013 and Pb. However, the study of Cu spots at the micron-scale did not reveal any clear local co-localization of these elements with Cu. Micro XANES revealed that Cu in inked areas is present principally as the copper minerals cuprite, azurite and malachite. In Egypt these minerals are present along almost the entire length of the eastern desert and in the Sinai, and their use in the production of green and blue pigments has been amply documented 18 , 20 . In the areas where Cu is diffused into the fibrous structure of the papyri, and in the complex Cu-rich spots in sample 2, malachite occurs as one of the components. It may be present as part of the original pigment or be formed as a result of the degradation of azurite 18 . The copper acetate present in sample 2 could also be a result of a reaction of the copper minerals with the chemical compounds in the surroundings or reactions caused by the conservation procedures. Ancient copper-containing pigments are well-known as a source of the catalyzed degradation of cellulose based materials such as gum-Arabic and papyrus; 16 for instance Egyptian blue and green can \u2018burn\u2019 holes in illustrated papyrus manuscripts 20 . A degraded binder (gum-Arabic) could explain why the inks visually appear to be \u2018cracking\u2019 and the diffuse presence of Cu outside the letters and signs in the fibrous structure of the papyrus. Likely, the migration of the Cu along the fibers was enhanced by the conservation procedures applied to the manuscripts. There is no detailed documentation on the method of conservation applied to the fragments, but it usually consists of a simple process, where the papyri were moistened with water in order to relax the fibers and unfold or unroll them; thereafter, they were mechanically cleaned with a sharp instrument and a sable brush 21 , 22 . With respect to cuprite (Cu 2 O) it could be a result of a reduction of copper carbonate pigments like azurite and malachite, the two other principal Cu compounds detected in the ink and along the fibrous structure of the four fragments 17 . However, there is also evidence that the cuprite present in the ink could have undergone an oxidation reaction in the presence of water and CO 2 in the atmosphere, which transversely would lead to the formation of azurite and malachite 16 . These observations suggest that the source of the Cu compounds found in the black inks and along the fibrous structure are by-products of metallurgy, glaze and glass production, which provided the raw material (soot) for \u201crefined\u201d carbon inks in the ancient Mediterranean. This is supported by the few preserved written formulae from the Hellenistic Period pertaining to the manufacture of black ink 1 . Conclusion Looking at the results, it is likely that the soot\/charcoal of copper-containing carbon inks were obtained during manufacturing processes related to the extraction of copper from sulfurous ores like chalcopyrite. This hypothesis finds confirmation in the particle size (sub-micron) and the fact that another copper-bearing pigment in Egypt, the so-called Egyptian blue (CaSi 2 O 5 \u00b7CuSi 2 O 5 ), was manufactured from scrap or by-product copper obtained at temple workshops that either melted copper or produced glass and faience 23 . It was made by mixing cupric oxides with sand, soda and lime, which thereafter was roasted at about 850\u2013900 \u00b0C to sintered crystalline aggregates rather than glass 18 , 24 , 25 , 26 . Similarly, Egyptian kohl or black eye-paint, which is closely related to the manufacture of lead-containing carbon inks, was produced in workshops, where vitreous materials were manipulated 27 . Since the papyri in question were written over a period of 300 years, the findings cannot represent an accidental event. Moreover, sample 1 is the oldest dated document from the ancient Mediterranean in which the addition of metals to a black ink has been detected. Though the fabrication of ink is likely to have evolved during this time-span, none of the four inks studied here are completely identical and Cu micro XANES showed variations within a single fragment. This demonstrates a variable local ink composition, and by extension production, which precludes the chance to obtain unique signature of the ink based on Cu speciation. This observation complicates the mapping of inks, but might facilitate the identification of fragments belonging to specific manuscripts or sections thereof 7 . Moreover, it should be taken into account that Cu speciation may have evolved since the original preparation and use of the ink. In particular, conservation treatments may have modified the Cu chemistry. Finally, the results will facilitate future strategies of conservation, since knowledge of material composition assists decisions, which remain to be made regarding the proper conservation and storage of the papyri, thereby ensuring their preservation and longevity. Why and when copper-containing carbon inks were introduced in ancient Egypt remain to be explained, but perhaps it is related to the type of pen used for writing the manuscripts, since the four papyri appear to have been written with a Greek reed pen ( kalamos ) rather than an Egyptian reed brush 28 . ","News_Body":"Until recently, it was assumed that the ink used for writing was primarily carbon-based at least until the fourth and fifth centuries AD. But in a new University of Copenhagen study, analyses of 2,000-year-old papyri fragments with X-ray microscopy show that black ink used by Egyptian scribes also contained copper - an element previously not identified in ancient ink. In a study published today in Scientific Reports, a cross-disciplinary team of researchers show that Egyptians used carbon inks that contained copper, which has not been identified in ancient ink before. Although the analysed papyri fragments were written over a period of 300 years and from different geographical regions, the results did not vary significantly: The papyri fragments were investigated with advanced synchrotron radiation based X-ray microscopy equipment at the European Synchrotron Radiation Facility in Grenoble as part of the cross-disciplinary CoNext project, and the particles found in the inks indicate that they were by-products of the extraction of copper from sulphurous ores. \"The composition of the copper-containing carbon inks showed no significant differences that could be related to time periods or geographical locations, which suggests that the ancient Egyptians used the same technology for ink production throughout Egypt from roughly 200 BC to 100 AD,\" says Egyptologist and first author of the study Thomas Christiansen from the University of Copenhagen. No unique ink signature The studied papyri fragments all form part of larger manuscripts belonging to the Papyrus Carlsberg Collection at the University of Copenhagen, more specifically from two primary sources: the private papers of an Egyptian soldier named Horus, who was stationed at a military camp in Pathyris, and from the Tebtunis temple library, which is the only surviving large-scale institutional library from ancient Egypt. \"None of the four inks studied here was completely identical, and there can even be variations within a single papyrus fragment, suggesting that the composition of ink produced at the same location could vary a great deal. This makes it impossible to produce maps of ink signatures that otherwise could have been used to date and place papyri fragments of uncertain provenance,\" explains Thomas Christiansen but adds: \"However, as many papyri have been handed down to us as fragments, the observation that ink used on individual manuscripts can differ from other manuscripts from the same source is good news insofar as it might facilitate the identification of fragments belonging to specific manuscripts or sections thereof.\" According to the researchers, their results will also be useful for conservation purposes as detailed knowledge of the material's composition could help museums and collections make the right decisions regarding conservation and storage of papyri, thus ensuring their preservation and longevity. ","News_Title":"Ink from ancient Egyptian papyri contains copper","Topic":"Other"}
{"Paper_Body":"Abstract The \u00c5ngstr\u00f6m-sized probe of the scanning transmission electron microscope can visualize and collect spectra from single atoms. This can unambiguously resolve the chemical structure of materials, but not their isotopic composition. Here we differentiate between two isotopes of the same element by quantifying how likely the energetic imaging electrons are to eject atoms. First, we measure the displacement probability in graphene grown from either 12 C or 13 C and describe the process using a quantum mechanical model of lattice vibrations coupled with density functional theory simulations. We then test our spatial resolution in a mixed sample by ejecting individual atoms from nanoscale areas spanning an interface region that is far from atomically sharp, mapping the isotope concentration with a precision better than 20%. Although we use a scanning instrument, our method may be applicable to any atomic resolution transmission electron microscope and to other low-dimensional materials. Introduction Spectroscopy and microscopy are two fundamental pillars of materials science. By overcoming the diffraction limit of light, electron microscopy has emerged as a particularly powerful tool for studying low-dimensional materials such as graphene 1 , in which each atom can be distinguished. Through advances in aberration-corrected scanning transmission electron microscopy 2 , 3 (STEM) and electron energy loss spectroscopy 4 , 5 , the vision of a \u2018synchrotron in a microscope\u2019 6 has now been realized. Spectroscopy of single atoms, including their spin state 7 , has together with Z-contrast imaging 3 allowed the identity and bonding of individual atoms to be unambiguously determined 4 , 8 , 9 , 10 . However, discerning the isotopes of a particular element has not been possible\u2014a technique that might be called \u2018mass spectrometer in a microscope\u2019. Here we show how the quantum mechanical description of lattice vibrations lets us accurately model the stochastic ejection of single atoms 11 , 12 from graphene consisting of either of the two stable carbon isotopes. Our technique rests on a crucial difference between electrons and photons when used as a microscopy probe: due to their finite mass, electrons can transfer significant amounts of momentum. When a highly energetic electron is scattered by the electrostatic potential of an atomic nucleus, a maximal amount of kinetic energy (inversely proportional to the mass of the nucleus, \u221d ) can be transferred when the electron backscatters. When this energy is comparable to the energy required to eject an atom from the material, defined as the displacement threshold energy T d \u2014for instance, when probing pristine 11 or doped 13 single-layer graphene with 60\u2013100 keV electrons\u2014atomic vibrations become important in activating otherwise energetically prohibited processes due to the motion of the nucleus in the direction of the electron beam. The intrinsic capability of STEM for imaging further allows us to map the isotope concentration in selected nanoscale areas of a mixed sample, demonstrating the spatial resolution of our technique. The ability to do mass analysis in the transmission electron microscope thus expands the possibilities for studying materials on the atomic scale. Results Quantum description of vibrations The velocities of atoms in a solid are distributed based on a temperature-dependent velocity distribution, defined by the vibrational modes of the material. Due to the geometry of a typical transmission electron microscopy (TEM) study of a two-dimensional material, the out-of-plane velocity v z , whose distribution is characterized by the mean square velocity , is here of particular interest. In an earlier study 11 this was estimated using a Debye approximation for the out-of-plane phonon density of states 14 (DOS) g z ( \u03c9 ), where \u03c9 is the phonon frequency. A better justified estimate can be achieved by calculating the kinetic energy of the atoms via the thermodynamic internal energy, evaluated using the full phonon DOS. As a starting point, we calculate the partition function Z =Tr{ e \u2212 H \/( kT ) }, where Tr denotes the trace operation and k is the Boltzmann constant and T the absolute temperature. We evaluate this trace for the second-quantized Hamiltonian H describing harmonic lattice vibrations 15 : where \u0127 is the reduced Planck constant, k the phonon wave vector, j the phonon branch index running to 3 r ( r being the number of atoms in the unit cell), \u03c9 j ( k ) the eigenvalue of the j th mode at k , and n j ( k ) the number of phonons with frequency \u03c9 j ( k ). After computing the internal energy from the partition function via the Helmholtz free energy F =\u2212 kT ln Z , we obtain the Planck distribution function describing the occupation of the phonon bands (Methods). We must then explicitly separate the energy into the in-plane U p and out-of-plane U z components, and take into account that half the thermal energy equals the kinetic energy of the atoms. This gives the out-of-plane mean square velocity of a single atom in a two-atom unit cell as where M is the mass of the vibrating atom, \u03c9 z is the highest out-of-plane mode frequency, and the correct normalization of the number of modes is included in the DOS. Phonon dispersion To estimate the phonon DOS, we calculated through density functional theory (DFT; GPAW package 16 , 17 ) the graphene phonon band structure 18 , 19 via the dynamical matrix using the \u2018frozen phonon method\u2019 (Methods; Supplementary Fig. 1 ). Taking the density of the components corresponding to the out-of-plane acoustic (ZA) and optical (ZO) phonon modes ( Supplementary Data 1 ) and solving equation 2 numerically, we obtain a mean square velocity m 2 s \u22122 for a 12 C atom in normal graphene. This description can be extended to \u2018heavy graphene\u2019 (consisting of 13 C instead of a natural isotope mixture). A heavier atomic mass affects the velocity through two effects: the phonon band structure is scaled by the square root of the mass ratio (from the mass prefactor of the dynamical matrix), and the squared velocity is scaled by the mass ratio itself (equation 2). At room temperature, the first correction reduces the velocity by 3% in fully 13 C graphene compared with normal graphene, and the second one reduces it by an additional 10%, resulting in m 2 s \u22122 . Electron microscopy In our experiments, we recorded time series at room temperature using the Nion UltraSTEM100 microscope, where each atom, or its loss, was visible in every frame. We chose small fields of view ( \u223c 1 \u00d7 1 nm 2 ) and short dwell times (8 \u03bcs) to avoid missing the refilling of vacancies (an example is shown in Fig. 1 ; likely this vacancy only appears to be unreconstructed due to the scanning probe). In addition to commercial monolayer graphene samples (Quantifoil R 2\/4, Graphenea), we used samples of 13 C graphene synthesized by chemical vapour deposition (CVD) on Cu foils using 13 C-substituted CH 4 as carbon precursor, subsequently transferred onto Quantifoil TEM grids. An additional sample consisted of grains of 12 C and 13 C graphene on the same grid, synthesized by switching the precursor during growth (Methods). Figure 1: Example of the STEM displacement measurements. The micrographs are medium angle annular dark field detector images recorded at 95 kV. ( a ) A spot on the graphene membrane, containing clean monolayer graphene areas (dark) and overlying contamination (bright). Scale bar, 2 nm. ( b ) A closer view of the area marked by the red rectangle in ( a ), with the irradiated area of the following panels similarly denoted. Scale bar, 2 \u00c5. ( c \u2013 g ) Five consecutive STEM frames ( \u223c 1 \u00d7 1 nm 2 , 512 \u00d7 512 pixels (px), 2.2 s per frame) recorded at a clean monolayer area of graphene. A single carbon atom has been ejected in the fourth frame ( f , white circle), but the vacancy is filled already in the next frame ( g ). The top row of ( c \u2013 g ) contains the unprocessed images, the middle row has been treated by a Gaussian blur with a radius of 2 px, and the coloured bottom row has been filtered with a double Gaussian procedure 3 ( \u03c3 1 =5 px, \u03c3 2 =2 px, weight=0.16). Full size image From each experimental dataset (full STEM data available 20 ) within which a clear displacement was observed, we calculated the accumulated electron dose until the frame where the defect appeared (or a fraction of the frame if it appeared in the first one). The distribution of doses corresponds to a Poisson process 12 whose expected value was found by log-likelihood minimization (Methods; Supplementary Fig. 2 ), directly yielding the probability of creating a vacancy (the dose data and statistical analyses are included in Supplementary Data 2 ). Figure 2 displays the corresponding displacement cross sections measured at voltages between 80 and 100 kV for normal (1.109% 13 C) and heavy graphene ( \u223c 99% 13 C), alongside values measured earlier 11 using high-resolution TEM (HRTEM). For low-probability processes, the cross section is highly sensitive to both the atomic velocities and the displacement threshold energy. Since heavier atoms do not vibrate with as great a velocity, they receive less of a boost to the momentum transfer from an impinging electron. Thus, fewer ejections are observed for 13 C graphene. Figure 2: Displacement cross sections of 12 C and 13 C measured at different acceleration voltages. The STEM data is marked with squares, and earlier HRTEM data 11 with circles. The error bars correspond to the 95% confidence intervals of the Poisson means (STEM data) or to previously reported estimates of statistical variation (HRTEM data 11 ). The solid curves are derived from our theoretical model with an error-weighted least-squares best-fit displacement threshold energy of 21.14 eV. The shaded areas correspond to the same model using the lowest DFT threshold T d \u2208 [21.25, 21.375] eV. The inset is a closer view of the low cross section region. Full size image Comparing theory with experiment The theoretical total cross sections \u03c3 d ( T , E e ) are plotted in Fig. 2 for each voltage (Methods; Supplementary Table 1 , Supplementary Data 2 ). The motion of the nuclei was included via a Gaussian distribution of atomic out-of-plane velocities P ( v z , T ) characterized by the DFT-calculated , otherwise similar to the approach of ref. 11 . A common displacement threshold energy was fitted to the data set by minimizing the variance-weighted mean square error (the 100 kV HRTEM point was omitted from the fitting, since it was underestimated probably due to the undetected refilling of vacancies, also seen in Fig. 1 ). The optimal T d value was found to be 21.14 eV, resulting in a good description of all the measured cross sections. Notably, this is 0.8 eV lower than the earlier value calculated by DFT, and 2.29 eV lower than the earlier fit to HRTEM data 11 . Different exchange correlation functionals we tested all overestimate the experimental value (by <1 eV), with the estimate T d \u2208 [21.25, 21.375] closest to experiment resulting from the C09 van der Waals functional 21 (Methods). Despite DFT overestimating the displacement threshold energy, we see from the good fit to the normal and heavy graphene data sets that our theory accurately describes the contribution of vibrations. Further, the HRTEM data and the STEM data are equally well described by the theory despite having several orders of magnitude different irradiation dose rates. This can be understood in terms of the very short lifetimes of electronic and phononic excitations in a metallic system 22 compared with the average time between impacts. Even a very high dose rate of 10 8 e \u2212 \u00c5 \u22122 s \u22121 corresponds to a single electron passing through a 1 nm 2 area every 10 \u221210 s, whereas valence band holes are filled 23 in <10 \u221215 s and core holes 24 in <10 \u221214 s, while plasmons are damped 25 within \u223c 10 \u221213 s and phonons 26 in \u223c 10 \u221212 s. Our results thus show that multiple excitations do not contribute to the knock-on damage in graphene, warranting another explanation (such as chemical etching 11 ) for the evidence linking a highly focused HRTEM beam to defect creation 27 . Each impact is, effectively, an individual perturbation of the equilibrium state. Local mapping of isotope concentration Finally, to test the spatial resolution of our method, we studied a sample consisting of joined grains of 12 C and 13 C graphene. Isotope labelling combined with Raman spectroscopy mapping is a powerful tool for studying CVD growth of graphene 28 , which is of considerable technological interest. Earlier studies have revealed the importance of carbon solubility into different catalyst substrates to control the growth process 29 . However, the spatial resolution of Raman spectroscopy is limited, making it impossible to obtain atomic-scale information of the transition region between grains of different isotopes. The local isotope analysis is based on fitting the mean of the locally measured electron doses with a linear combination of doses generated by Poisson processes corresponding to 12 C and 13 C graphene using the theoretical cross section values. Although each dose results from a stochastic process, the expected doses for 12 C and 13 C are sufficiently different that measuring several displacements decreases the errors of their means well below the expected separation ( Fig. 3c ). To estimate the expected statistical variation for a certain number of measured doses, we generated a large number of sets of n Poisson doses, and calculated their means and standard errors as a function of the number of doses in each set. The calculated relative errors scale as 1\/ n and correspond to the precision of our measurement, which is better than 20% for as few as five measured doses in the ideal case. Although our accuracy is difficult to gauge precisely, by comparing the errors of the cross sections measured for isotopically pure samples to the fitted curve ( Fig. 2 ), an estimate of roughly 5% can be inferred. Figure 3: Local isotope analysis. ( a ) A STEM micrograph of a hole in the carbon support film (1.3 \u03bcm in diameter), covered by a monolayer of graphene. In each of the overlaid spots, 4\u201315 fields of view were irradiated. The dimensions of the overlaid grid correspond to the pixels of a Raman map recorded over this area. ( b ) Isotope concentration map where the colours of the grid squares denote 12 C concentration based on the fitting of the Raman 2D band response (Methods; Supplementary Fig. 3 ). The overlaid spots correspond to ( a ), with colours denoting the concentration of 12 C estimated from the mean of the measured doses. ( c ) Locally measured mean doses and their standard errors plotted on a log scale for each grid square. The horizontal coloured areas show the means\u00b1s.e. of doses simulated for the theoretical 12 C and 13 C cross sections. Note that a greater variation in the experimental doses is expected for areas containing a mix of both carbon isotopes. Full size image Working at 100 kV, we selected spots containing areas of clean graphene (43 in total) each only a few tens of nanometers in size ( Fig. 1a ), irradiating 4\u201315 (mean 7.8) fields of view 1 \u00d7 1 nm 2 in size until the first displacement occurred ( Fig. 1f ). Comparing the mean of the measured doses to the generated data, we can estimate the isotope concentration responsible for such a dose. This assignment was corroborated by Raman mapping over the same area, allowing the two isotopes to be distinguished by their differing Raman shift. A general trend from 12 C-rich to 13 C-rich regions is captured by both methods ( Fig. 3b ), but a significant local variation in the measured doses is detectable ( Fig. 3c ). This variation indicates that the interfaces formed in a sequential CVD growth process may be far from atomically sharp 30 , instead spanning a region of hundreds of nanometers, within which the carbon isotopes from the two precursors are mixed together. Discussion It is interesting to compare our method to established mass analysis techniques. In isotope ratio mass spectrometry precisions of 0.01% and accuracies of 1% have been reported 31 . However, these measurements are not spatially resolved. For spatially resolved techniques, one of the most widely used is time-of-flight secondary ion mass spectroscopy (ToF-SIMS). It has a lateral resolution typically of several micrometers, which can be reduced to around 100 nm by finely focusing the ion beam 32 . In the case of ToF-SIMS, separation of the 13 C signal from 12 C 1 H is problematic, resulting in a reported 33 precision of 20% and an accuracy of \u223c 11%. The state-of-the-art performance in local mass analysis can be achieved with atom-probe tomography 34 (APT), which can record images with sub-nanometer spatial resolution in all three dimensions. A recent APT study of the 13 C\/ 12 C ratio in detonation nanodiamonds reported a precision of 5%, but biases in the detection of differently charged ions limited accuracy to \u223c 25% compared to the natural isotope abundances 35 . A limitation of ToF-SIMS is its inability to discriminate between the analyte and contaminants and that it requires uniform isotope concentrations over the beam area for accurate results. APT requires the preparation of specialized needle-like sample geometries, a laborious reconstruction process to analyse its results 36 , and its detection efficiency is rather limited 37 . In our case, we are only able to resolve relative mass differences between isotopes of the same element in the same chemical environment. While we do not need to resolve mass differences between different elements, since these differ in their scattering contrast, we do need to detect the ejection of single atoms, limiting the technique to atomically thin materials. However, our method captures the isotope information concurrently with atomic resolution imaging in a general-purpose electron microscope, without the need for additional detectors. We have shown how the \u00c5ngstr\u00f6m-sized electron probe of a scanning transmission electron microscope can be used to estimate isotope concentrations via the displacement of single atoms. Although these results were achieved with graphene, our technique should work for any low-dimensional material, including hexagonal boron nitride and transition metal dichalcogenides such as MoS 2 . This could potentially extend to van der Waals heterostructures 38 of a few layers or other thin crystalline materials, provided a difference in the displacement probability of an atomic species can be uniquely determined. Neither is the technique limited to STEM: a parallel illumination TEM with atomic resolution would also work, although scanning has the advantage of not averaging the image contrast over the field of view. The areas we sampled were in total less than 340 nm 2 in size, containing \u223c 6,600 carbon atoms of which 337 were ejected. Thus, while the nominal mass required for our complete analysis was already extremely small (131 zg), the displacement of only five atoms is required to distinguish a concentration difference of less than twenty per cent. Future developments in instrumentation may allow the mass-dependent energy transfer to be directly measured from high-angle scattering 39 , 40 , further enhancing the capabilities of STEM for isotope analysis. Methods Quantum model of vibrations The out-of-plane mean square velocity can be estimated by calculating the kinetic energy via the thermodynamic internal energy using the out-of-plane phonon DOS g z ( \u03c9 ), where \u03c9 is the phonon frequency. In the second quantization formalism, the Hamiltonian for harmonic lattice vibrations is ref. 15 where k is the phonon wave vector, j is the phonon branch index running to 3 r ( r being the number of atoms in the unit cell), \u03c9 j ( k ) the eigenvalue of the j th mode at k , and and b k j are the phonon creation and annihilation operators, respectively. Using the partition function Z =Tr{ e \u2212 H \/( kT ) }, where Tr denotes the trace operation and k is the Boltzmann constant and T the absolute temperature, and evaluating the trace using this Hamiltonian, we have where is the number of phonons with frequency \u03c9 j ( k ). The Helmholtz free energy is thus and the internal energy of a single unit cell, therefore, becomes 15 where in the last step the sum is expressed as an average over the phonon DOS. Using the identity yields the Planck distribution function describing the occupation of the phonon bands, and explicitly dividing the energy into the in-plane U p and out-of-plane U z components, we can rewrite this as where the number of modes is included in the normalization of the DOSes, that is, , corresponding to the out-of-plane acoustic (ZA) and optical (ZO) modes (the in-plane DOS g p ( \u03c9 ) being correspondingly normalized to 4), and \u03c9 d is the highest frequency of the highest phonon mode. Since half of the thermal energy equals the average kinetic energy of the atoms, and the graphene unit cell has two atoms, the out-of-plane kinetic energy of a single atom is Thus, the out-of-plane mean square velocity of an atom becomes where \u03c9 z is now the highest out-of-plane mode frequency. This can be solved numerically for a known g z ( \u03c9 ). For the in-plane vibrations, we would equivalently get Frozen phonon calculation To estimate the phonon DOS, we calculated the graphene phonon band structure via the dynamical matrix, which was computed by displacing each of the two primitive cell atoms by a small displacement (0.06 \u00c5) and calculating the forces on all other atoms in a 7 \u00d7 7 supercell (\u2018frozen phonon method\u2019; the cell size is large enough so that the forces on the atoms at its edges are negligible) using DFT as implemented in the grid-based projector-augmented wave code (GPAW) package 17 . Exchange and correlation were described by the local density approximation 41 , and a \u0393-centered Monkhorst-Pack k -point mesh of 42 \u00d7 42 \u00d7 1 was used to sample the Brillouin zone. A fine computational grid spacing of 0.14 \u00c5 was used alongside strict convergence criteria for the structural relaxation (forces <10 \u22125 eV\u00c5 \u22121 per atom) and the self-consistency cycle (change in eigenstates <10 \u221213 eV 2 per electron). The resulting phonon dispersion ( Supplementary Fig. 1 ) describes well the quadratic dispersion of the ZA mode near \u0393, and is in excellent agreement with earlier studies 18 , 19 . Supplementary Data 1 contains the out-of-plane phonon DOS. Graphene synthesis and transfer In addition to commercial monolayer graphene (Graphenea QUANTIFOIL R 2\/4), our graphene samples were synthesized by CVD in a furnace equipped with two separate gas inlets that allow for independent control over the two isotope precursors 29 (that is, either \u223c 99% 12 CH 4 or \u223c 99% 13 CH 4 methane). The as-received 25 \u03bcm thick 99.999% pure Cu foil was annealed for \u223c 1 h at 960 \u00b0C in a 1:20 hydrogen\/argon mixture with a pressure of \u223c 10 mbar. The growth of graphene was achieved by flowing 50 cm 3 min \u22121 of CH 4 over the annealed substrate while keeping the Ar\/H 2 flow, temperature and pressure constant. For the isotopically mixed sample with separated domains, the annealing and growth temperature was increased to 1,045 \u00b0C and the flow rate decreased to 2 cm 3 min \u22121 . After introducing 12 CH 4 for 2 min the carbon precursor flow was stopped for 10 s, and the other isotope precursor subsequently introduced into the chamber for another 2 min. This procedure was repeated with a flow time of 1 min. After the growth, the CH 4 flow was interrupted and the heating turned off, while the Ar\/H 2 flow was kept unchanged until the substrate reached room temperature. The graphene was subsequently transferred onto a holey amorphous carbon film supported by a TEM grid using a direct transfer method without using polymer 42 . Scanning transmission electron microscopy Electron microscopy experiments were conducted using a Nion UltraSTEM100 scanning transmission electron microscope, operated between 80 and 100 kV in near-ultrahigh vacuum (2 \u00d7 10 \u22127 Pa). The instrument was aligned for each voltage so that atomic resolution was achieved in all of the experiments. The beam current during the experiments varied between 8 and 80 pA depending on the voltage, corresponding to dose rates of \u223c 5\u201350 \u00d7 10 7 e \u2212 \u00c5 \u22122 s \u22121 . The beam convergence semiangle was 30 mrad and the semi-angular range of the medium-angle annular-dark-field detector was 60\u2013200 mrad. Poisson analysis Assuming the displacement data are stochastic, the waiting times (or, equivalently, the doses) should arise from a Poisson process with mean \u03bb . Thus the probability to find k events in a given time interval follows the Poisson distribution To estimate the Poisson expectation value for each sample and voltage, the cumulative doses of each data set were divided into bins of width w (using one-level recursive approximate Wand binning 43 ), and the number of bins with 0, 1, 2... occurrences were counted. The goodness of the fits was estimated by calculating the Cash C-statistic 44 (in the asymptotically- \u03c7 2 formulation 45 ) between a fitted Poisson distribution and the data: where N is the number of occurence bins, n i is the number of events in bin i , and e i is the expected number of events in bin i from a Poisson process with mean \u03bb . An error estimate for the mean was calculated using the approximate confidence interval proposed for Poisson processes with small means and small sample sizes by Khamkong 46 : where is the estimated mean and Z 2.5 is the normal distribution single tail cumulative probability corresponding to a confidence level of (100\u2212 \u03b1 )=95%, equal to 1.96. The statistical analyses were conducted using the Wolfram Mathematica software (version 10.5), and the Mathematica notebook is included as Supplementary Data 2 . Outputs of the Poisson analyses for the main data sets of normal and heavy graphene as a function of voltage are additionally shown as Supplementary Fig. 2 . Displacement cross section The energy transferred to an atomic nucleus from a fast electron as a function of the electron scattering angle \u03b8 is ref. 47 which is valid also for a moving target nucleus for electron energies >10 keV as noted by Meyer and co-workers 11 . For purely elastic collisions (where the total kinetic energy is conserved), the maximum transferred energy E max corresponds to electron backscattering, that is, \u03b8 = \u03c0 . However, when the impacted atom is moving, E max will also depend on its speed. To calculate the cross section, we use the approximation of McKinley and Feshbach 48 of the original series solution of Mott to the Dirac equation, which is very accurate for low-Z elements and sub-MeV beams. This gives the cross section as a function of the electron scattering angle as where \u03b2 = v \/ c is the ratio of electron speed to the speed of light (0.446225 for 60 keV electrons) and \u03c3 R is the classical Rutherford scattering cross section Using equation 14 this can be rewritten as a function of the transferred energy 49 as Distribution of atomic vibrations The maximum energy (in eV) that an electron with mass m e and energy E e = eU (corresponding to acceleration voltage U ) can transfer to a nucleus of mass M that is moving with velocity v is where and are the relativistic energies of the electron and the nucleus, and E n = Mv 2 \/2 the initial kinetic energy of the nucleus in the direction of the electron beam. The probability distribution of velocities of the target atoms in the direction parallel to the electron beam follows the normal distribution with a standard deviation equal to the temperature-dependent mean square velocity , Total cross section with vibrations The cross section is calculated by numerically integrating equation 17 multiplied by the Gaussian velocity distribution (equation 19) over all velocities v where the maximum transferred energy (equation 18) exceeds the displacement threshold energy T d : where E max ( v , E e ) is given by equation 18, the term \u0398[ E max ( v , E e )\u2212 E d ] is the Heaviside step function, T is the temperature and E e is the electron kinetic energy. The upper limit for the numerical integration v max =8 was chosen so that the velocity distribution is fully sampled. Displacement threshold simulation For estimating the displacement threshold energy, we used DFT molecular dynamics as established in our previous studies 12 , 13 , 50 , 51 . The threshold was obtained by increasing the initial kinetic energy of a target atom until it escaped the structure during the molecular dynamics run. The calculations were performed using the grid-based projector-augmented wave code ( GPAW ), with the computational grid spacing set to 0.18 \u00c5. The molecular dynamics calculations employed a double zeta linear combination of atomic orbitals basis 52 for a 8 \u00d7 6 unit cell of 96 atoms, with a 5 \u00d7 5 \u00d7 1 Monkhorst-Pack k -point grid 53 used to sample the Brillouin zone. A timestep of 0.1 fs was used for the Velocity-Verlet dynamics 54 , and the velocities of the atoms initialized by a Maxwell\u2013Boltzmann distribution at 50 K, equilibrated for 20 timesteps before the simulated impact. To describe exchange and correlation, we used the local density approximation 41 , and the Perdew-Burke-Ernzerhof (PBE) 55 , Perdew-Wang 1991 (PW91, ref. 41 ), RPBE 56 and revPBE 57 functionals, yielding displacement threshold energies of 23.13, 21.88, 21.87, 21.63 and 21.44 eV (these values are the means of the highest simulated kinetic energies that did not lead to an ejection and the lowest that did, respectively). Additionally, we tested the C09 (ref. 21 ) functional to see whether inclusion of the van der Waals interaction would affect the results. This does bring the calculated threshold energy down to [21.25, 21.375] eV, in better agreement with the experimental fit. However, a more precise algorithm for the numerical integration of the equations of motion, more advanced theoretical models for the interaction, or time-dependent DFT may be required to improve the accuracy of the simulations further. Varying mean square velocity with concentration Since the phonon dispersion of isotopically mixed graphene gives a slightly different out-of-plane mean square velocity for the atomic vibrations, for calculating the cross section for each concentration, we assumed the velocity of mixed concentration areas to be linearly proportional to the concentration where c is the concentration of 12 C and v 12\/13 are the atomic velocities for normal and heavy graphene, respectively. Raman spectroscopy A Raman spectrometer (NT MDT Ntegra Spectra) equipped with a 532 nm excitation laser was used for Raman measurements. A computer-controlled stage allowed recording a Raman spectrum map over the precise hole on which the electron microscopy measurements were conducted, which was clearly identifiable from neighboring spot contamination and broken film holes. The frequencies \u03c9 of the optical phonon modes vary with the atomic mass M as \u03c9 \u221d M \u22121\/2 due to the mass prefactor of the dynamical matrix. This makes the Raman shifts of 13 C graphene (12\/13) \u22121\/2 times smaller, allowing the mapping and localization of 12 C and 13 C domains 28 with a spatial resolution limited by the size of the laser spot (nominally \u223c 400 nm). The shifts of the G and 2D bands compared with a corresponding normal graphene sample are given by , where \u03c9 12 is the G (2D) line frequency of the normal sample, c 0 13 =0.01109 is the natural abundance of 13 C, and c is the unknown concentration of 12 C in the measured spot. Due to background signal arising from the carbon support film of the TEM grid, we analyzed the shift of the 2D band, where two peaks were in most locations present in the spectrum. However, in many spectra these did not correspond to either fully 12 C or 13 C graphene 58 , indicating isotope mixing within the Raman coherence length. To assign a single value to the 12 C concentration for the overlay of Fig. 3 , we took into account both the shifts of the peaks (to estimate the nominal concentration for each signal) and their areas (to estimate their relative abundances) as follows: where are the nominal concentrations of 12 C determined from the measured higher and lower 2D Raman shift peak positions, \u03c9 A\/B are the measured peak centers of the higher and lower 2D signals, and A and B are their integrated intensities. The peak positions of fully 12 C and 13 C graphene were taken from the highest and lowest peak positions in the entire mapped area (covering several dozen Quantifoil holes), giving \u03c9 12 =2,690 cm \u22121 and \u03c9 13 =2,600 cm \u22121 . The fitted 2D spectra, arranged in the same 6 \u00d7 6 grid as the overlay, can be found as Supplementary Fig. 3 Data availability The full STEM time series data on which the determination of the 12 C and 13 C displacement cross sections ( Fig. 2 ) are based are available on figshare with the identifier  (ref. 20 ). The STEM data of Fig. 3 are available upon request. All other data are contained within the article and its Supplementary Information files. Additional information How to cite this article: Susi, T. et al . Isotope analysis in the transmission electron microscope. Nat. Commun. 7, 13040 doi: 10.1038\/ncomms13040 (2016). Change history 30 August 2017 A correction has been published and is appended to both the HTML and PDF versions of this paper. The error has not been fixed in the paper. ","News_Body":"The different elements found in nature each have their distinct isotopes. For carbon, there are 99 atoms of the lighter stable carbon isotope 12C for each 13C atom, which has one more neutron in its nucleus. Apart from this natural variation, materials can be grown from isotope-enriched chemicals. This allows scientists to study how the atoms arrange into solids, for example to improve their synthesis. Yet, most traditional techniques to measure the isotope ratio require the decomposition of the material or are limited to a resolution of hundreds of nanometers, obscuring important details. In the new study, led by Jani Kotakoski, the University of Vienna researchers used the advanced scanning transmission electron microscope Nion UltraSTEM100 to measure isotopes in nanometer-sized areas of a graphene sample. The same energetic electrons that form an image of the graphene structure can also eject one atom at a time due to scattering at a carbon nucleus. Because of the greater mass of the 13C isotope, an electron can give a 12C atom a slightly harder kick, knocking it out more easily. How many electrons are on average required gives an estimate of the local isotope concentration. \"The key to making this work was combining accurate experiments with an improved theoretical model of the process\", says Toma Susi, the lead author of the study. Publishing in Nature Communications allowed the team to fully embrace open science. In addition to releasing the peer review reports alongside the article, a comprehensive description of the methods and analyses is included. However, the researchers went one step further and uploaded their microscopy data onto the open repository figshare. Anyone with an Internet connection can thus freely access, use and cite the gigabytes of high-quality images. Toma Susi continues: \"To our knowledge, this is the first time electron microscopy data have been openly shared at this scale.\" The results show that atomic-resolution electron microscopes can distinguish between different isotopes of carbon. Although the method was now demonstrated only for graphene, it can in principle be extended for other two-dimensional materials, and the researchers have a patent pending on this invention. \"Modern microscopes already allow us to resolve all atomic distances in solids and to see which chemical elements compose them. Now we can add isotopes to the list\", Jani Kotakoski concludes. The lighter the atom, the fewer electrons are on everage needed to eject it. Credit: Copyright: Koponen+Hild\u00e9n, Creative Commons BY 4.0 ","News_Title":"'Weighing' atoms with electrons","Topic":"Nano"}
{"Paper_Body":"Abstract An apparent absence of Silurian fishes more than half-a-metre in length has been viewed as evidence that gnathostomes were restricted in size and diversity prior to the Devonian. Here we describe the largest pre-Devonian vertebrate ( Megamastax amblyodus gen. et sp. nov.), a predatory marine osteichthyan from the Silurian Kuanti Formation (late Ludlow, ~423 million years ago) of Yunnan, China, with an estimated length of about 1 meter. The unusual dentition of the new form suggests a durophagous diet which, combined with its large size, indicates a considerable degree of trophic specialisation among early osteichthyans. The lack of large Silurian vertebrates has recently been used as constraint in palaeoatmospheric modelling, with purported lower oxygen levels imposing a physiological size limit. Regardless of the exact causal relationship between oxygen availability and evolutionary success, this finding refutes the assumption that pre-Emsian vertebrates were restricted to small body sizes. Introduction The Devonian Period has been considered to mark a major transition in the size and diversity of early gnathostomes (jawed vertebrates), including the earliest appearance of large vertebrate predators 1 . In contrast to the rich Devonian fossil record, gnathostomes from earlier strata have long been represented by scarce and highly fragmentary remains 2 . Traditional depictions of Silurian marine faunas typically either lack fish altogether 3 or are dominated by diminutive jawless forms 4 . In addition to this apparent low diversity, the maximum size of pre-Devonian gnathostomes and vertebrates in general, has been noted as being considerably smaller than later periods 1 . Until recently, the largest known Silurian gnathostomes were the osteichthyan Guiyu 5 and the antiarch placoderm Silurolepis 6 from the Ludlow Kuanti Formation of Yunnan, both with total body lengths of roughly 35 cm. Beyond the Silurian, the Ordovician agnathan Sacabambaspis from Bolivia is of comparable size 7 . The absence of pre-Devonian gnathostomes more than a few tens of centimeters in length, coupled with an apparent increase in size and diversity in the Early Devonian, has led to suggestions that jawed vertebrates were minor components of aquatic faunas prior to the Emsian 1 , 8 . Such an extended period of time with no apparent increase in body size is striking, given that the gnathostome fossil record may extend as far back as the Ordovician 9 , 10 . Recent discoveries reveal that Silurian gnathostomes were far more diverse and widely distributed than previously recognized 10 , 11 . Of particular importance is Xiaoxiang fauna of Yunnan Province, southwestern China, based on fossils from a series of marine sediments of which the Kuanti Formation is by far the most productive 12 , 13 . This unit has produced a diverse assemblage of early fishes, including the only articulated specimens of pre-Devonian gnathostomes. Here we present a bony fish from the Kuanti Formation ( Fig. 1 ) with an estimated length of about 1 meter, revealing that pre-Devonian gnathostomes could attain comparatively large sizes. The likely specialized predatory feeding habits of this form and anatomical disparity to other early osteichthyans, reinforce earlier indications of a significant degree of morphological and ecological diversity among gnathostomes well before the Devonian 10 , 14 . Figure 1 Silurian sequence in Qujing (Yunnan, China) with stratigraphic position of Megamastax amblyodus gen. et sp. nov. and other vertebrate taxa (modified from ref. 5 , using Adobe Illustrator 10). Full size image The apparent small size and limited diversity of Silurian gnathostomes has recently been employed as a constraint in paleoatmospheric reconstruction 1 , 8 . Models of atmospheric history based on geochemical data indicate a mid-Palaeozoic episode of global oceanic oxygenation, likely linked to the formation of a global terrestrial vascular flora and the concurrent widespread burial of organic matter 15 , 16 and roughly coinciding with the appearance of large gnathostomes in the fossil record. Our new finding refutes suggestions that there were significant environmental constraints to vertebrate body size prior to the Emsian (~400 Ma). Results Systematic palaeontology Gnathostomata, Gegenbaur, 1874 Osteichthyes, Huxley, 1880 Sarcopterygii, Romer, 1955 Megamastax amblyodus gen. et sp. nov. Etymology Genus named from megalos and mastax (Greek), meaning \u201cbig mouth\u201d. The specific epithet is derived from amblys and odous (Greek) meaning \u201cblunt tooth\u201d. Holotype Institute of Vertebrate Paleontology and Paleoanthropology (IVPP) V18499.1, complete left mandible. Referred material IVPP V18499.2, partial left mandible; IVPP V18499.3, right maxilla. Type locality and horizon The Kuanti Formation, at a hill close to the Xiaoxiang Reservoir, Qujing, Yunnan, southwestern China ( Fig. 1 ), dating to the late Ludlow (Ludfordian Stage) 11 , 12 , 13 , with a youngest age of ~423 million years ago 17 . The fossils were collected from a horizon immediately below the first appearance of the conodont Ozarkodina crispa . Other fishes from this horizon include the galeaspid Dunyu 18 , the remarkable placoderm Entelognathus 19 and the osteichthyan Guiyu 5 , 20 . Diagnosis Osteichthyan with multiple rows of closely packed conical teeth on the marginal jaw bones and widely spaced pairs of blunt teeth fused to each of the four coronoids. Coronoids fused to the lingual face of the mandible with the posterior three flanked by an elongate anterior ramus of the prearticular. Outer surfaces of the mandible and maxilla covered in cosmine with numerous embedded pores. Description The external faces of the mandible ( Fig. 2A, F ) and maxilla ( Fig. 2I ) have a cosmine surface with numerous pores, as in Achoania and Psarolepis 21 . The mandible is long and low in overall shape, tapering anteriorly as in some Devonian limbed tetrapods 22 . It is gently convex in longitudinal and vertical axes, with slight medial curvature in dorsal view suggesting a narrow tapering snout. The sutured margins of the dermal bones are not clearly visible, although a small notch on the anteroventral jaw margin likely marks the posteromedial boundary of the splenial as in Achoania and Psarolepis 21 . There is a shallow semi-lunate overlap area for the maxilla and quadratojugal, while a horizontal pit-line runs almost end to end in the upper portion of the mandible. Internally, a narrow flange runs along the dorsal margin of the dentary, bearing at least two longitudinal rows of conical, slender teeth ( Fig. 2E ). All marginal teeth on the holotype are of roughly uniform height, but those on the inner-most row are broader and more sparsely arranged. The teeth extend almost to the tip of the jaw, well past the level of the parasymphysial articulation. On V18499.2 the marginal dentition is reduced to weathered stumps and empty tooth sockets. It is unclear if this feature is pre- or post-mortem. Figure 2 Fossils of Megamastax amblyodus gen. et sp. nov. (A\u2013E) Holotype mandible (IVPP V18499.1) in (A) lateral, (B) lingular and (C) dorsal views; close-up of prearticular bone, showing surface ridges (D) and close-up of the marginal dentition in lingual view (E). (F\u2013H) Partial mandible (V18499.2) in (F) lateral, (G) lingular and (H) dorsal views. (I) Right maxilla (V18499.3) in lateral view. (J) Reconstruction of (i1) Guiyu oneiros (ref. 13 ) alongside hypothetical silhouettes of (J2\u20133) Megamastax with superimposed fossil outlines (drawn by B.C.). The (J2) smaller fish is based on the V18499.1 and V18499.3, the (J3) larger on V.18499.2. ar.psym, knob-like parasymphysial structure; Co 1\u20134, coronoids 1\u20134; coT 1\u20138, coronoid teeth 1\u20138; De, dentary; fo.add, adductor fossa; fo.gl, glenoid fossa; fo.Mk, Meckelian foramen; Id, infradentary; mpl, mandibular pit line; maT, marginal teeth; oaMx, overlap area for maxilla and quadratojugal; Pat, prearticular; sym, area for parasymphysial plate; tr, indented track bordering splenial. Full size image Antero-medially there is a knob-like articular structure and symphysial overlap area for a small parasymphysial dental plate. The knob is not as strongly developed as in Psarolepis , Achoania 21 or Guiyu 5 and is concealed by the dentary in lateral view. The large prearticular is devoid of denticles, but is covered in numerous parallel ridges ( Fig. 2D ) as in Styloichthys 21 . The broad posterior section covers the dorsal and medial face of the Meckelian ossification near the adductor fossa, terminating posteriorly just behind the level of the glenoid fossa. Anteriorly, it narrows to an elongate ramus, mesially flanking the coronoid series to terminate against the posteromedial margin of the 1 st coronoid. The Meckelian cartilage is ossified for most of its length, although a large oval cavity anteroventral of the adductor fossa may indicate a region of incomplete ossification. The Meckelian bone extends ventrally beyond the prearticular with a series of small fenestra piercing the posteroventral margin. Posteriorly, it contributes to the rim of the adductor fossa and a small bipartite glenoid fossa. It anteriorly tapers to a narrow shelf that is fused to the knob-like parasymphysial area and the anterior tip of the prearticular. The four coronoids are smooth save for a row of widely-spaced blunt, semi-circular teeth, with two on each coronoid ( Fig. 2B, C, G, H and Fig. 3F, G ). The dentition is ankylosed to a continuous median ridge, with no sockets. Tooth surfaces are smooth and lack infolding, with weathered sections on V18499.2 exposing the pulp cavity. Figure 3 Lingual views of mandibles from selected pre-Emsian osteichthyans. Except for Megamastax , all are from the Lochkovian Xitun Formation, Qujing, eastern Yunnan, China. (A) Psarolepis romeri , IVPP V8138 (reversed). (B) Achoania jarviki , IVPP V12492.1 (reversed). (C) Jaw tentatively assigned to Meemannia eos , IVPP V14536.5. (D) Styloichthys changae , IVPP V8143.1. (E) Partial dentary of an indeterminable osteichthyan, IVPP V12493 (reversed). (F) Megamastax amblyodus , IVPP V18499.1 (holotype), dark grey = matrix-filled areas. (G) Megamastax amblyodus , IVPP V18499.2 with restored silhouette in black. (A\u2013G) drawn by B.C. ar.psym, knob-like parasymphysial articular structure; ar.Co1\u20134, articulation for coronoid 1\u20134; Co1\u20135, coronoid 1\u20135; coT1\u20138, 1 st \u20138 th coronoid tooth; fo.add, adductor fossa; fo.gl, glenoid fossa; Pat, Prearticular; sym, area for parasymphysial tooth plate. Scale bars = 5 mm. Full size image The 9.5 cm long maxilla (V18499.3, Fig. 2I ) represents an individual of similar size to the holotype. It has identical ornamentation and corresponding contours of the occlusal margins. The biting margin is straight with no posteroventral flexion. In overall shape, the maxilla is most suggestive of porolepiforms 23 in lacking a posterior expansion that is known in actinopterygians, onychodonts and stem sarcopterygians 5 . Multiple rows of closely packed conical teeth are arranged over the entire ventral margin. Comparisons In possessing true marginal teeth, cosmine, coronoids, prearticular and a biconcave glenoid, Megamastax is unambiguously an osteichthyan. The presence of cosmine, the shape of the maxilla and the configuration of the prearticular relative to the coronoids indicate sarcopterygian affinities. Porous cosmine is found in many crown sarcopterygians 23 , 24 as well as Psarolepis and Achoania , taxa that are usually resolved as stem sarcopterygians 25 , 26 , 27 , although a stem-osteichthyan position is also suggested 20 , 27 , 28 . While not universally distributed among sarcopterygians 24 , cosmine is unknown in actinopterygians. The maxilla lacks the pronounced posteroventral curvature and posterior expansion of Guiyu 5 , Psarolepis , onychodonts 29 , 30 and early actinopterygians 31 , 32 , 33 , 34 and in this respect is more similar to porolepiforms 35 . As in early sarcopterygians, the prearticular extends anteriorly to mesially flank the coronoids 5 , 21 , 36 , differing from the condition in primitive actinopterygians where the prearticular sutures against the posterior margin of the most posterior coronoid 31 , 37 . The dentition is highly unusual. As in crown osteichthyans, the marginal teeth are discrete structures unlike the enlarged denticles of Lophosteus and Andreolepis 38 . However the marginal dentition of most early tooth-bearing osteichthyans is segregated into a single inner row of large conical teeth bordered laterally by sharpened denticles 5 , 21 , 29 , 31 , 32 , 33 , 34 . The dentary and the maxilla of Megamastax exhibit at least two parallel rows of sharp conical teeth of roughly uniform length. The 4-bone coronoid series of Megamastax , with large blunt teeth fused to the dermal surface, is unlike that of other osteichthyans where the teeth, if present, are discrete structures demarcated at the base from the adjacent bone. Psarolepis 21 and Guiyu 5 have five coronoids per jaw, each with sharp fangs housed in semi-lunate sockets. Those of early actinopterygians 31 , actinistians and onychodonts 29 possess numerous minute teeth or denticles. Porolepiforms and early tetrapodomorphs have a 3-coronoid series, bearing sharp tusks with infolded surfaces and an additional row of small denticles 35 . Dipnoans lack discrete coronoids. The coronoids of Megamastax share a striking similarity to the dentigerous jaw bones of some acanthodians, notably the Ischnacanthiformes and Acanthodopsis 39 , 40 and to a lesser extent, the infragnathals of certain arthrodires with purported teeth 41 . As the coronoids of unambiguous stem-osteichthyans are unknown, it is unclear if this is a convergence with non-osteichthyans, or is instead a plesiomorphic relict. Examining purported ischnacanthiform jaw fragments in museum collections may yield additional early osteichthyan coronoids. A previously described 6 cm long section of a dentary (V12493, Fig. 3E ) from the Lochkovian Xitun Formation, Yunnan, is superficially similar to Megamastax in its large size, ornamentation and prominent marginal tooth-bearing flange 21 . It differs in the greater degree of anterodorsal curvature and in bearing only a single row of conical marginal teeth. The unpreserved coronoids were evidently not fused to the dentary. Regardless of its relationships, the specimen provides additional evidence of large osteichthyans well before the Emsian. Discussion The size of Megamastax To determine the maximum size of Megamastax ( Fig. 2J ), the total length of the large but incomplete V18499.2 was extrapolated based on the complete holotype jaw, using the distance between the 2 nd and 8 th coronoid teeth as landmarks. Fusion of the dermal bones suggests that both represent adult or near-adult specimens despite the roughly 35% difference in size. V18499.2 has a preserved length of 109 mm, missing most of the posterior section, including the adductor fossa and the front of the jaw anterior to the second coronoid tooth. The apices of the 2 nd and 8 th coronoid teeth are 70 mm apart. V18499.1 has a total mandibular length of 129 mm with a 52 mm distance between the 2 nd \u20138 th coronoid teeth. V18499.2 is thus calculated to be 1.346 times longer than the holotype, with a restored total length of 173.65 mm ( Fig. 3G ). While errors in scaling due to ontogenetic or individual variation cannot be ruled out, mandibles of Achoania and Psarolepis from the Lower Devonian Xitun Formation exhibit an even greater degree of relative size differences; the jaws of Achoania ranging from 32.5 to 72 mm in length 21 . While larger specimens exhibit a proportionally greater depth, due primarily to a deepening of the infradentaries, they do not exhibit consistent differences in the relative anteroposterior proportions of the glenoid fossa, adductor fossa and coronoid series, regardless of the size of the mandible 21 . To provide an estimate for the total body length of Megamastax , comparisons were made with more completely known Siluro-Devonian osteichthyans ( Fig. 2J ). Calculations based on isolated jaws must be tentative as relative mandible-to-body size is subject to individual and ontogenetic variation. Guiyu is currently the only Silurian osteichthyan known from reasonably complete remains, with the holotype (V15541) measuring about 260 mm from snout-to-anal fin for a likely total length of roughly 350 mm ( Fig. 2J1 ); the lower jaw accounting for about 1\/7 th of that length 20 . Excluding tetrapods, Devonian osteichthyans, both sarcopterygians and actinopterygians, share a conservative fusiform anatomy with no unusually elongate or truncated body configurations. This includes Dialipina , an Early Devonian taxon usually resolved as a stem-osteichthyan in recent analyses 19 , 25 and thus likely a more basal taxon than Megamastax , suggesting a fusiform-body via phylogenetic bracketing. Mandibular lengths in Devonian bony fishes generally account from between 1\/5 th of body length in forms like Strunius 30 and Miguashaia 42 to 1\/7 th in more elongate taxa like Howqualepis 34 and Gogosardina 32 . Extrapolating from this provides estimates of between 645 to 903 mm for V18499.1 with a 129 mm jaw and between 868 to 1215 mm for V18499.2 with a 173.65 mm jaw ( Fig. 2J2\u20133 ). The earliest durophagous predatory osteichthyan? The coronoid teeth ( Fig. 2B, C, G, H ) differ from the sharp tusks of other Silurian bony fishes from the South China block, notably Guiyu 5 and Psarolepis 21 , 27 . When coupled with the much larger size of Megamastax , this suggests widely divergent feeding strategies and alludes to a considerable degree of trophic specialisation well before the Devonian. Nothing is currently known of the palate, but the rounded coronoid dentition is suggestive some sort of crushing role, perhaps against a complimentary row on the dermopalatine. Among extant fishes, dentition combining grasping and crushing morphologies is common in durophagous predators. These target hard-shelled prey, which require processing prior to injestion 43 . Such forms usually employ anterior conical teeth for initial prey capture before food is passed posteriorly to flattened or rounded molariform teeth. The shell-crushing dentition may be located on the marginal jaws as in hornsharks 44 and wolf-eels 43 , or set within pharyngeal batteries as in many wrasses 45 . Megamastax differs from extant forms in that the processing dentition is on the coronoids, medial to rather than posterior to the conical teeth, which are distributed throughout the jaw margins rather than anteriorly restricted. However the contrasting tooth-form suggests a separation of activity (capture vs processing) that is broadly analogous to extant piscine durophages, possibly making it the earliest osteichthyan with specific adaptations for such a diet. The sub-tidal marine invertebrate fauna of the Ludlow of Yunnan included a rich variety of potential prey, including brachiopods, molluscs and trilobites 12 , 13 , 46 . Megamastax may have also consumed the heavily armoured fishes whose fossils are well represented in the Kuanti Formation ( Fig. 4 ), including placoderms 19 and galeaspids 18 . Given its great size, Megamastax could have potentially eaten any other animal in the assemblage and may thus represent the earliest vertebrate apex-predator. As an apparently specialised predator that differs substantially from contemporary osteichthyans, Megamastax correlates well with a documented initial increase in the functional disparity of the earliest gnathostomes which had stabilized by the Early Devonian 14 . Figure 4 Life reconstruction of Megamastax amblyodus consuming the galeaspid Dunyu longiforus (drawn by B.C.). Full size image Implications for palaeoatmospheric modelling The role of oxygen availability as a significant factor in the appearance of large animals in the mid-late Palaeozoic has been the subject of considerable scrutiny, although the exact causal relationships are ambiguous and controversial due to the likely influence of other variables such as trophic tiering and cascades, temperature and biotic interactions 47 , 48 , 49 . Recent advances in geochemistry 1 , 10 , 15 , 50 , 51 , 52 have provided a wealth of data on early Phanerozoic climate and atmospheric conditions, allowing for correlation with key biological events. Earlier attempts at palaeoatmospheric modelling suggest consistently low Silurian O 2 concentrations, substantially below the current atmospheric level of 21% 53 , 54 , 55 . Of the two most recent models, GEOCARBSULF 51 , 52 is based new isotopic data of carbon and sulphur. It indicates a gradual increase of atmospheric O 2 from the end of Ordovician with a peak exceeding modern levels towards the end of the Silurian, followed by a decrease in the Early-Middle Devonian with a low point during the Frasnian ( Fig. 5A ). This correlates with the relative abundance of charcoal during the Silurian to Permian 12 . Figure 5 Competing models of mid-Palaeozoic oxygenation from 500 Ma to 300 Ma. Vertical blue line indicates minimum age of the Kuanti Formation and Megamastax . (A) From figure 2 in ref. 51 . Estimates of atmospheric O2 over time based on calculations from the GEOCARBSULF model (solid line = modern O2%). (B) From figure 3B in ref. 8 . Mo sediment samples with seawater (SW) values inferred from highly euxinic (red) and mildly euxinic sediments (pink). \u03b4 98 Mo is a measure of the relative proportions of heavy and light Mo isotopes, with higher values inferring oxygenated oceans (\u03b4 98 Mo modern SW = 2.3). Solid lines represent 90% percentiles while values above the dashed line require a substantial oxic Mo sink. Full size image An alternative model based on molybdenum (Mo) isotopes 1 , 8 ( Fig. 5B ), while with initial results spanning a broad possible time range of ~430\u2013390 Ma, suggests a peak in the later part of the Early Devonian (~400 Ma, during the Emsian Stage) based in part on calibration with the vertebrate fossil record 1 . As body size in extant predatory marine fishes has been claimed to scale positively with both oxygen demand and uptake, with vulnerability to hypoxic mortality in large predatory forms being considerably greater than their smaller kin 1 while fishes in general have been recorded as less tolerant of hypoxia than many marine invertebrates 56 . These observations have served as a proxy for the Emsian oxygenation scenario, with earlier limitations to oxygen availability, estimated to have been 15\u201350% of present atmospheric levels (PAL), imposing physiological constraints on maximal body size 8 . A date of ~400 Ma for O 2 concentration attaining to 40% PAL (the minimum estimated requirement for predatory fishes above 1 m) was favoured when correlated against the low maximum length of Silurian gnathostomes (no taxa more than a few tens of centimeters) and the apparent rise of large predatory fishes, with presumably greater metabolic requirements, during the Devonian 1 . Although a simple causal relationship between size and hypoxia tolerance has been challenged 49 , 57 , 58 , extant marine fishes in general are also known to be less tolerant of hypoxic conditions than many marine invertebrates 1 , 56 , 59 . This suggests that low oxygen levels would have imposed some degree of extrinsic constraint on the maximum body size and available niche opportunities of the earliest gnathostomes. Bambach 60 proposed that the emergence of large predatory fish in the Devonian was linked to the rise of a global terrestrial flora, with an expanded trophic pyramid fuelled by phosphate-laden runoff from plant-covered continental zones. However, recent palaeobotanical discoveries have brought the timing of the evolution of vascular plants into question 61 and indicate a well established terrestrial flora by the latest Silurian 62 . Cryptospore records suggest a floristic invasion of the land as far back as the latest Ordovician 63 . As such, the benefits of terrestrial vegetation to aquatic biotas may have been active for considerably longer than initially thought, accounting for the large size of Megamastax and the rich diversity in the Xiaoxiang fauna. While it might be argued that Megamastax , being presumably a foraging predator of slow-moving or sessile shelled prey, likely had lower oxygen requirements than a fast midwater piscivore, it has been demonstrated that even relatively sedate benthic fishes in modern coastal communities exhibit high vulnerability to hypoxia 64 , 65 , whereas some modern foraging reef omnivores, such as the picasso triggerfish 66 , employ highly energetic forms of locomotion. A recent time-calibrated phylogenetic analysis of a broad sample of living actinopterygians presented a striking correlation between speciation and increases in body size 67 . Based on this result, it could be argued that the large size of Megamastax is a simple corollary of early gnathostome diversification, rather than an indicator of extrinsic environmental factors such as oxygen level. Regardless, the existence of a metre-long predatory fish in the Ludlow raises doubts on the use of restricted vertebrate body size as a proxy for low Silurian O2 levels. This discovery does not necessarily dispute the use of Mo isotopes in palaeoatmospheric reconstruction as the ~423 Ma Kuanti Formation falls within the lower extreme of the estimated time interval of the mid-Phanerozoic peak, although it suggests that the present model requires recalibration in light of this new datum. The size of Megamastax and the emerging diversity of late Silurian gnathostomes based on ongoing fossil discoveries are not indicative of any significant restrictions on pre-Devonian gnathostome size and diversity. While not in itself a reliable indicator of ancient atmospheric conditions, these fossils are at least consistent with the high Silurian oxygen levels predicted by GEOCARBSULF. Given the presence of big osteichthyans in the Kuanti and Xitun formations, the purported absence of large pre-Emsian jawed fishes is seen to be a sampling artefact at least partially due to preservational and environmental biases 68 . Methods All fossils are housed at the Institute of Vertebrate Paleontology and Paleoanthropology (IVPP), Chinese Academy of Sciences, Beijing. The blocks were collected from the Kuanti Formation (late Ludlow) in Qujing, Yunnan, China and prepared mechanically at IVPP using pneumatic air scribes and needles under microscopes. Nomenclatural acts This published work and the nomenclatural acts it contains have been registered in ZooBank, the proposed online registration system for the International Code of Zoological Nomenclature (ICZN). The ZooBank LSIDs (Life Science Identifiers) can be resolved and the associated information viewed through any standard web browser by appending the LSID to the prefix \u2018  \u2019. The LSID for this publication is: urn:lsid:zoobank.org:pub:4FA91224-FF35-4DD1-9618-BA950DF073FE, urn:lsid:zoobank.org:act:6A4DA6A3-B675-4B4D-8F79-5CA5F9366327 and urn:lsid:zoobank.org:act:89729F30-1562-4FD9-9801-6604003C514B. ","News_Body":"A team of researchers working at China's Kuanti formation has unearthed the largest known example of a jawed vertebrate from the early Dvonian, commonly known as the Silurian period. In their paper published in Scientific Reports, the team describes the predatory fish as being approximately 1 meter long with two types of teeth, one for catching prey, the other for crushing hard shells. The discovery adds new evidence to the theory that animals with backbones and jaws first developed in what is now China and also disrupts current theories regarding atmospheric oxygen levels during early Earth history. The researchers believe the fish, Big Mouth, Blunt Tooth (Megamastax amblyodus), lived approximately 423 million years ago\u2014a time period that until this new discovery was thought to be characterized by low atmospheric oxygen levels. But a large fish such as Megamastax couldn't survive under such conditions, thus, levels must have been higher. The find actually consisted of three fossils from three different fish\u2014one a whole lower jaw, the other two, both fragments of an upper jaw\u2014all found at the Yunnan province dig site. The size of the jaw and teeth allowed the researchers to suggest the entire fish, when alive, would have been approximately 1 meter long. The teeth in front were sharp, for grabbing, while those in the back were clearly meant for grinding, likely hard shelled prey. The jaw was approximately 16 cm in length. Megamastax lower jaw: Holotype mandible (IVPP V18499.1) of Megamastax amblyodus gen. et sp. nov. in lateral, lingular, and dorsal views. Credit: Min Zhu Fossils of Megamastax amblyodus gen. et sp. nov. (A\u2013E) Holotype mandible (IVPP V18499.1) in (A) lateral, (B) lingular, and (C) dorsal views; close-up of prearticular bone, showing surface ridges (D), and close-up of the marginal dentition in lingual view (E). (F\u2013H) Partial mandible (V18499.2) in (F) lateral, (G) lingular, and (H) dorsal views. (I) Right maxilla (V18499.3) in lateral view. (J) Reconstruction of (i1) Guiyu oneiros alongside hypothetical silhouettes of (J2\u20133) Megamastax with superimposed fossil outlines. The (J2) smaller fish is based on the V18499.1 and V18499.3, the (J3) larger on V.18499.2. Credit, Min Zhu The researchers believe the fish was likely the largest predator in its environment\u2014about triple the size of any other known fish from that time period\u2014making it the dominant fish in the sea. During the Silurian period, the part of China where the fish was unearthed was part of the South China Sea. Fossil finds from the region predate jawed vertebrates found anywhere else thus far, suggesting the area was the birthplace of such creatures. They also believe that the reason Megamastax grew so large was because of intense competition between the many types of fish that existed at the time. But making it possible was the amount of oxygen available. Prior to the Silurian period, levels would have been too low. Interestingly, the most recent climate models used to depict early Earth conditions during the same period have also indicated higher atmospheric oxygen levels\u2014this latest fossil find now backs that up. ","News_Title":"Researchers unearth largest Silurian vertebrate to date\u2014meter long Megamastax","Topic":"Other"}
{"Paper_Body":"Abstract Understanding the mortality impact of COVID-19 requires not only counting the dead, but analyzing how premature the deaths are. We calculate years of life lost (YLL) across 81 countries due to COVID-19 attributable deaths, and also conduct an analysis based on estimated excess deaths. We find that over 20.5 million years of life have been lost to COVID-19 globally. As of January 6, 2021, YLL in heavily affected countries are 2\u20139 times the average seasonal influenza; three quarters of the YLL result from deaths in ages below 75 and almost a third from deaths below 55; and men have lost 45% more life years than women. The results confirm the large mortality impact of COVID-19 among the elderly. They also call for heightened awareness in devising policies that protect vulnerable demographics losing the largest number of life-years. Introduction The large direct and indirect effects of the COVID-19 pandemic have necessitated the delivery of policy responses that, when reasonable, are a balancing act between minimizing the immediate health impact of the pandemic, and containing the long-term damage to the society that may arise from the protective policies. A key input parameter in the calculation of how restrictive policies might be justified is the mortality impact of COVID-19. Attempts to evaluate the total mortality impact of COVID-19 are proceeding on several fronts. Progress is being made in estimating the infection fatality rate of COVID-19 and how this might vary across sub-populations 1 . Large, coordinated international collaborations have been set up to collect data that records COVID-19 attributable deaths. Attempts to estimate total excess mortality related to the COVID-19 are underway, and emphasized as an important measure 2 , 3 . Each of these research avenues and their associated health measures (infection rate, deaths and excess deaths) is important in informing the public and policymakers about the mortality impact of COVID-19. However, each come with their own limitations. Infection fatality rates apply only to the relatively small sub-population that has been confirmed to have the disease, and without knowledge about the true number of infected, these rates are inherently difficult to estimate. COVID-19 attributable deaths may over- or underestimate the true number of deaths that are due to the disease, as both policies and practices about coding the deaths are only being developed and standardized. Excess death approaches that compare mortality rates during the COVID-19 outbreak to a baseline depend on correctly estimating the baseline. The most important limitation in COVID-19 attributable death or excess death approaches, however, is that these approaches do not provide information on how many life years have been lost. Deaths at very old ages can be considered to result in fewer life years lost, when compared to deaths at very young ages. In fact, several policy responses (or non-responses) have been motivated with the argument that COVID-19 is mostly killing individuals who, even in the absence of COVID-19, would have had few life years remaining. However, comprehensive evaluation of the true mortality impact of COVID-19 has not been conducted. We analyze the premature mortality impact of COVID-19 by calculating the amount of life-years lost across 81 countries covering over 1,279,866 deaths. We base our analysis on two large recently established and continuously growing databases 4 , 5 and on two different methodological approaches, one based on COVID-19 attributable deaths, and, for selected countries, one based on estimated excess deaths comparing recent mortality levels to an estimated baseline. We are not able to solve the measurement limitations of either of these approaches, but the complementary nature of the two ways of measuring COVID-19 deaths makes these concerns explicit and allows us to evaluate the implications. This study is also limited to premature mortality only; a full health impact evaluation might consider for instance, the burden of disability associated with the disease 6 . This latter dimension requires thorough understanding of sequelae associated with COVID-19, for which data are limited at this point on a cross-country, comparable level. As such, we focus on premature mortality here. Methods Country death counts by age and gender due to COVID-19 come from the COVerAge-DB 4 ; the analysis includes all countries with at least one COVID-19 related death in 4 at the time of the study. Population data are drawn from the Human Mortality Database 5 and the World Population Prospects 7 . Country life expectancies are from the life tables in the World Population Prospects for the period 2015-2020. The dates at which data are collected, and death counts by country are reported in the Supplementary information materials (SI Table S1 ). Projections for total number of deaths due to COVID-19 by country are from the Imperial College 8 . Death counts due to other causes of mortality are from data in Global Burden of Disease 9 . Finally, we use weekly excess mortality data from the Short-Term Mortality Fluctuations Database (STMF, from the Human Mortality Database 5 ). A full description of the data, its sources, and the methodology is provided in the Supplementary Information. Results In total, 20,507,518 years of life have been lost to COVID-19 among the studied 81 countries, due to 1,279,866 deaths from the disease. The average years of life lost per death is 16 years. As countries are at different stages of the pandemic trajectory, this study is a snapshot of the impacts of COVID-19 on years of life lost (YLL) as of January 6, 2021 (a complete list of countries and their dates at measurement is in the Supplementary Information). In 35 of the countries in our sample, coverage of the data spans at least 9 months; in such cases, this suggests that the full impacts of the pandemic in 2020, or at least the first waves of the pandemic, are likely captured. For other countries still on an upwards incline of transmission rates or for which data is yet forthcoming for end of 2020, the YLL experienced are likely to further increase substantially in the next few months. We encourage context-based interpretation of the results presented here, especially when used for evaluation of the effectiveness of COVID-19 oriented policies. Figure 1 Panels A through C report the ratio of COVID-19 YLL rates over influenza YLL rates (in median\/maximum deadly years by country), traffic accidents, and heart conditions respectively. Panel D reports, countries with available data, the ratio of YLL rates of COVID-19 deaths over YLL rates of excess deaths. When two causes of mortality affect YLL equally, the ratio is precisely 1; larger ratio values suggest COVID-19 YLL rates are higher than the alternative cause. Average ratios in vertical lines in each panel. Each country name is followed by (in parentheses) the number of days passed since the country\u2019s first official COVID-case up to the last day of available COVID-19 deaths data for that country. Countries always sorted by ratio of COVID-19 YLL vs seasonal flu (in median years) across panels for ease of reading. Full size image Comparisons with other causes of mortality To put the impacts of COVID-19 on YLL in perspective, we compare it against the premature mortality impacts of three other global common causes of death: heart conditions (cardiovascular diseases), traffic accidents (transport injuries), and the seasonal \u201cflu\u201d or influenza (see the Supplementary Information for definitions and cause ids). Heart conditions are one of the leading causes of YLL 6 , while traffic accidents are a mid level cause of YLL, providing sensible medium and high cause comparison baselines. Finally, common seasonal influenza has been compared against COVID-19, as both are infectious respiratory diseases (though see 10 , which suggests vascular aspects to the disease). We compare YLL rates (per 100,000) for COVID-19 against YLL rates for other causes of death. There is substantial variation in the mortality burden of seasonal influenza by country across years and so we compare YLL rates for the worst and median influenza years for each country in the period 1990\u20132017. Comparisons of YLL rates for COVID-19 over YLL rates for other causes are presented in Fig. 1 . We find that in heavily impacted highly developed countries, COVID-19 is 2\u20139 times that of the common seasonal influenza (as compared to a median flu year for the same country), between 2 and 8 times traffic related YLL rates, between a quarter and a half of the YLL rates attributable to heart conditions in countries (with rates as high as parity to twice that of heart conditions in Latin America). Variation across countries is large, as many countries have YLL rates due to COVID-19 still at very low levels. Results in our Supplementary Information show that these countries are often countries where relatively fewer days have passed since first confirmed case of COVID-19. A noted problem in attributing deaths to COVID-19 has been systematic undercounting of deaths due to COVID-19, as official death counts may reflect limitations in testing as well as difficulties in counting in out-of-hospital contexts. In order to asses the importance of undercounting in our results, we compute excess deaths for 19 countries with available weekly mortality data. A mortality baseline is estimated for each country and age group for weekly all-cause mortality since the first week of 2010. Our results (Fig. 1 , fourth panel) support the claim that the true mortality burden of COVID-19 is likely to be substantially higher. Comparisons of COVID-19 attributable deaths and excess deaths approaches to calculating YLL suggests that the former on average may underestimate YLL by a factor of 3. Variation across countries is large, in Belgium the two approaches deliver comparable results, but for Croatia, Greece and South Korea the excess deaths approach suggests that we may underestimate the YLL by a factor of more than 12. Figure 2 Panel A displays the country-specific proportions of YLL traced back to each age group. The global average proportion is presented at the top, and countries are in decreasing proportion of YLL in the under 55 age bracket. Panel B reports the ratio of male YLL rates to female YLL rates for countries with available gender specific COVID-19 death counts. Countries with genders equally affected by YLL rate are closer to the parity line at 1, while countries with women more affected have points lying on the left; countries with men more severely affected display points lying to the right. Global average and global weighted average of male to female YLL are presented at the top. Full size image Age specific years of life lost As has been noted early on in the pandemic, mortality rates for COVID-19 are higher for the elderly 11 , with postulations that this may be due to correlations with the greater likelihood of these individuals suffering from underlying risk factors 12 , 13 . This study\u2019s sample presents an average age-at-death of 72.9 years; yet only a fraction of the YLL can be attributed to the individuals in the oldest age brackets. Globally, 44.9% of the total YLL can be attributed to the deaths of individuals between 55 and 75 years old, 30.2% to younger than 55, and 25% to those older than 75. That is, the average figure of 16 YLL includes the years lost from individuals close to the end of their expected lives, but the majority of those years are from individuals with significant remaining life expectancy. Across countries, a substantial proportion of YLL can be traced back to the 55\u201375 age interval, however there remain stark differences in the relative contribution of the oldest and youngest age groups (Fig. 2 , Panel A). These patterns account for the proportion of YLL for each age group out of the global YLL (see Table S7 ). In higher income countries, a larger proportion of the YLL is borne by the oldest group compared to the youngest age groups. The opposite pattern appears in low and mid-income countries, where a large fraction of the YLL are from individuals dying at ages 55 or younger. Gender specific years of life lost It has also become apparent that there are gender disparities in the experience of COVID-19 14 ; our study finds this to be true not only in mortality rates, but in absolute years of life lost as well. In the sample of countries for which death counts by gender are available, men have lost 44% more years than women. Two causes directly affect this disparity: (1) a higher average age-at-death of female COVID-19 deaths (71.3 for males, 75.9 for females), resulting in a relatively lower YLL per death (15.7 and 15.1 for males and females respectively); and (2) more male deaths than female deaths in absolute number (1.39 ratio of male to female deaths). Though this general pattern is shared by most countries, the size of the disparity varies, as well as the importance of the two above causes. The ratio of male YLL rates (per 100,000) to female YLL rates for COVID-19 spans from near parity, such as in Finland or Canada, to more than double the YLL rates countries like Peru or quadruple like in Taiwan (Fig. 2 , Panel B). For countries that present highly skewed male to female YLL rates (most prevalent in low-income countries), the death count differences across genders contribute the most to this imbalance. Yet, the substantial imbalances remain starkly present among high-income countries as well (see Supplementary Information for details). Discussion Understanding the full health impact of the COVID-19 pandemic is critical for evaluating the potential policy responses. We analyzed the mortality impact of COVID-19 by calculating the amount of life-years lost across 81 countries covering over 1,279,866 deaths. From a public health standpoint, years of life lost is crucial in that it assesses how much life has been cut short for populations affected by the disease. We considered COVID-19 attributable deaths throughout in identifying patterns of years of life lost, and as an important robustness check, conducted analysis based on estimated excess deaths comparing recent mortality levels to a (estimated) baseline. Our results deliver three key insights. First, the total years life lost (YLL) as of January 06, 2021 is 20,507,518, which in heavily affected countries is between 2 and 9 times the median YLL of seasonal influenza or between a quarter and a half of heart disease. This implies 273,947 \u201cfull lives lost\u201d \u2013 or over two hundred thousand lives lived from birth to the average life expectancy at birth in our sample (74.85 years). Second, three quarters of the YLL are borne by people dying in ages below 75. Third, men have lost 45% more years of life than women. These results must be understood in the context of an as-of-yet ongoing pandemic and after the implementation of unprecedented policy measures. Existing estimates on the counterfactual of no policy response suggest much higher death tolls and, consequently, YLL. Our calculations based on the projections by 8 yield a total impact several orders of magnitude higher, especially considering projections based on a complete absence of interventions (see Supplementary Information for details on projections). This is in line with further evidence of the life-saving impacts of lockdowns and social distancing measures 15 . There are two key sources of potential bias to our results, and these biases operate in different directions. First, COVID-19 deaths may not be accurately recorded, and most of the evidence suggests that on the aggregate level, they may be an undercount of the total death toll. As a result, our YLL estimates may be underestimates as well. We compare our YLL estimates to estimates based on excess death approaches that require more modeling assumptions but are robust to missclassification of deaths. The results of this comparison suggest that on average across countries, we might underestimate COVID-19 YLL rates by a factor of 3. Second, those dying from COVID-19 may be an at-risk population whose remaining life expectancy is shorter than the average person\u2019s remaining life expectancy 16 , 17 , 18 . This methodological concern is likely to be valid, and consequently our estimate of the total YLL due to COVID-19 may be an overestimate. However, our key results are not the total YLL but YLL ratios and YLL distributions which are relatively robust to the co-morbidity bias. Indeed, this bias also applies to the YLL calculations for the seasonal influenza or heart disease. Thus, the ratio of YLL for COVID-19 compared to other causes of death is more robust to the co-morbidity bias than the estimate on the level of YLL as the biases are present in both the numerator and the denominator. Likewise, the age- and gender distributions of YLL would suffer from serious co-morbidity bias only if these factors vary strongly across the age or gender spectrum. As noted earlier, our analysis is limited to premature mortality. A full health impact evaluation ought to consider the burden of disability associated with the disease.Indeed, YLL are often presented jointly with years lived with disability (YLD) in a measure known as disability-adjusted life year (DALY), constructed by adding YLD to YLL 19 . In order to compute YLD, though, we must have a thorough understanding of the sequela associated with the disease, as well their prevalence. Several sequelae have been linked to COVID-19 recently 20 , 21 in China, but we still lack the full understanding of the extent that would be needed to compute reliable cross-national YLD measures at the scale of this article. We see collection of such measures as therefore of key importance in next steps in advancing our understanding of the magnitude of the COVID-19 effects on public health. Some of our findings are consistent with dominant narratives of the COVID-19 impact, others suggest places where more nuanced policy-making can affect how the effects of COVID-19 might be spread among society. Our results confirm that the mortality impact of COVID-19 is large, not only in terms of numbers of death, but also in terms of years of life lost. While the majority of deaths are occurring at ages above 75, justifying policy responses aimed at protecting these vulnerable ages, our results on the age pattern call for heightened awareness of devising policies protecting also the young. The gender differential in years of life lost arises from two components: more men are dying from COVID-19, but men are also dying at younger ages with more potential life years lost than women. Holding the current age distribution of deaths constant, eliminating the gender differential in YLL would require on average a 34% reduction in male death counts; this suggests that gender-specific policies might be equally well justified as those based on age. Data availability All study code and data are fully replicable and available in the following Open Science Framework (OSF) repository:  . Change history 14 April 2021 A Correction to this paper has been published:  ","News_Body":"Over 20.5 million years of life may have been lost due to COVID-19 globally, with an average of 16 years lost per death, according to a study published in Scientific Reports. Years of life lost (YLL)\u2014the difference between an individual's age at death and their life expectancy\u2014due to COVID-19 in heavily affected countries may be two to nine times higher than YLL due to average seasonal influenza. H\u00e9ctor Pifarr\u00e9 i Arolas, Mikko M\u00ffrskyla and colleagues estimated YLL due to COVID-19 using data on over 1,279,866 deaths in 81 countries, as well as life expectancy data and projections for total deaths of COVID-19 by country. The authors estimate that in total, 20,507,518 years of life may have been lost due to COVID-19 in the 81 countries included in this study\u201416 years per individual death. Of the total YLL, 44.9% seems to have occurred in individuals between 55 and 75 years of age, 30.2% in individuals younger than 55, and 25% in those older than 75. In countries for which death counts by gender were available, YLL was 44% higher in men than in women. Compared with other global common causes of death, YLL associated with COVID-19 is two to nine times greater than YLL associated with seasonal flu, and between a quarter and a half as much as the YLL attributable to heart conditions. The authors caution that the results need to be understood in the context of an ongoing pandemic: they provide a snapshot of the possible impacts of COVID-19 on YLL as of 6 January, 2021. Estimates of YLL may be over- or under-estimates due to the difficulty of accurately recording COVID-19-related deaths. ","News_Title":"Over 20.5 million years of life may have been lost due to COVID-19","Topic":"Medicine"}
{"Paper_Body":"Abstract In connectomics, the study of the network structure of connected neurons, great advances are being made on two different scales: that of macro- and meso-scale connectomics, studying the connectivity between populations of neurons, and that of micro-scale connectomics, studying connectivity between individual neurons. We combine these two complementary views of connectomics to build a first draft statistical model of the micro-connectome of a whole mouse neocortex based on available data on region-to-region connectivity and individual whole-brain axon reconstructions. This process reveals a targeting principle that allows us to predict the innervation logic of individual axons from meso-scale data. The resulting connectome recreates biological trends of targeting on all scales and predicts that an established principle of scale invariant topological organization of connectivity can be extended down to the level of individual neurons. It can serve as a powerful null model and as a substrate for whole-brain simulations. Introduction The study of connectomics has to date largely taken place on two separate levels with disjunct methods and results: macro-connectomics, studying the structure and strength of long-range projections between brain regions, and micro-connectomics, studying the topology of individual neuron-to-neuron connectivity within a region. In macro-connectomics, the absence or presence and strength of projections between brain regions are measured using for example, histological pathway tracing, retrograde 1 , 2 or anterograde 3 tracers, or MR diffusion tractography 4 , 5 . While recent advances made it possible to turn such data into connectome models with a resolution of 100 \u03bcm 6 , this is still far away from single-neuron resolution. In micro-connectomics, two complementary approaches prevail: stochastic models and direct measures of synaptic connectivity using, for example, electron microscopy. The first uses biological findings to formulate principles that rule out certain classes of wiring diagrams and prescribe probabilities to the remaining ones, while with electron microscopy, snapshots of individual biological wiring diagrams are taken 7 , 8 , 9 , 10 , 11 , 12 , 13 . However, published reconstructed volumes at this point only contain incomplete dendritic trees, and therefore incomplete connectivity. To gain a full understanding of, for example the role of an individual neuron or small groups of neurons in a given behavior, we will have to integrate the advantages of both scales: single-neuron resolution on a whole-brain or at least whole-neocortex level. This has been recognized before 14 , but steps toward this goal have until now remained limited. At this point, electron-microscopic reconstructions at that scale are not viable, leaving only statistical approaches to dense micro-connectivity, based on identifying biological principles in the data. Scaling it up to a whole-neocortex level will amplify the uncertainty about the biological accuracy of the results, as many of the resulting connections will be between rarely studied brain regions with little available biological data. Nevertheless, it can serve as a first draft micro-connectome defining a null model to compare and evaluate future findings against. It will also allow us to perform full-neocortex simulations at cellular resolution to gain insights, as to which brain function can or cannot be explained with a given connectome. We have completed such a first-draft connectome of mouse neocortex by using an improved version of our previously published circuit and connectivity modeling pipeline 15 . It has been improved to place neurons in brain-atlas defined 3d spaces instead of hexagonal prisms, taking into account the geometry and cellular composition of individual brain regions. However, this did not include long-range connections between brain regions, especially the ones formed via projections along the white matter. We therefore set out to identify possible principles, hypotheses of rules constraining the long-range connectivity, and develop stochastic methods to instantiate micro-connectomes fulfilling them. A first constraint was given by the data on macro- or mesoscale connectivity, which is often reported as a region-to-region connection matrix, yielding a measure proportional to the total number of synapses forming a projection between pairs of brain regions 1 , 14 , 16 , 17 . We used for this purpose, the recently published mesoscale mouse brain connectome of Harris et al. 3 . This data set splits the mouse neocortex into 86 separate regions (43 per hemisphere) and further splits each region when considered as a source of a projection into five individual projection classes, by layer or pathway (Layer 23IT, Layer 4IT, Layer 5IT, Layer 5PT, and Layer 6CT). IT refers to intratelencephalic projections, targeting the ipsilateral and contralateral cortex and striatum; PT refers to pyramidal tract projections, predominantly targeting subcortical structures, but also ipsilateral cortex; CT refers to corticothalamic projections. From here on, we will leave out this additional distinction for projections from layers 2\/3, 4, and 6, where only one class is specified in the data of ref. 3 . While the data set does not include GABAergic projection neurons 18 , it provides the most comprehensive information on connection strengths of individual projection classes to date. We further constrained the spatial structure of each projection within the target region. Along the vertical axis (orthogonal to layer boundaries), this was achieved by assigning a layer profile to each projection, as provided by Harris et al. 3 . Along the horizontal axes, we assumed a generalized topographical mapping between regions, parameterized using a voxelized (resolution 100 \u03bcm) version of the data provided by Knox et al. 6 . As a final constraint, we applied rules on the number and identity of brain regions innervated by individual neurons in a given source region. To this end, we analyzed the brain regions innervated by individual in vivo reconstructions of whole-brain axons in a published data set (MouseLight project at Janelia, mouselight.janelia.org 19 ). Based on the analysis, we conceptualized and parameterized a decision tree of long-range axon targeting that reproduced the targeting rules found in the in vivo data. This approach was generalized to other brain regions for which few or no axonal reconstructions are available. Finally, we implemented a stochastic algorithm that connected morphologically detailed neurons in a 3d-volume representing the entire mouse neocortex. Synapses were placed onto the dendrites of target neurons according to all the derived constraints by a modified version of a previously used algorithm 15 . Analyzing the results, we found that the constraints we added on top of the region-to-region projection matrices led to a surprisingly complex and non-random micro-structure of neuron-to-neuron connectivity. We characterized this structure to be an extension of an established principle of hierarchical organization of modular connectivity 20 to the level of individual neurons. Results Neuronal composition and local connectivity We placed around 10 million morphological neuron reconstructions in a 3d space representing the entirety of a mouse neocortex. Neuron densities and excitatory to inhibitory ratios at each location were taken from a voxelized brain atlas 21 , which is consistent with version 3 of the brain parcellation of the Allen Brain Atlas 22 , 23 . The composition in terms of morphological neuron types was as in Markram et al. 15 . Reconstructed morphologies were placed in the volume according to densities for individual, morphologically defined subtypes, and correctly oriented with respect to layer boundaries. For simplicity, we made a strict distinction between local and long-range connectivity, defining local connectivity to comprise any connection where source and target neuron were in the same brain region according to the parcellation in Harris et al. 3 , and derived it using previously published methods 24 . All other connections were considered long-range and were derived using the methods described below. Constraining the anatomical strengths of projections For long-range connectivity, we handled each combination of a projection class (Layer 23, Layer 4, Layer 5IT, Layer 5PT, and Layer 6), a source region, and a target region as a conceptually separate projection. As a first constraint, we determined the average volumetric density of synapses in each projection using published data 3 , 25 , using a programmatic interface provided by the authors. Two further steps were required to apply their data: scaling from projection strength to synapse density, and splitting into densities for individual projection classes. The biological data provided a measure proportional to the mean volumetric density of projection axons in the target region. Assuming a uniform mean density of synapses on axons across projections, the volumetric synapse density is simply a scaled version of this. We calculated a scaling factor such that the resulting total synapse density in ipsilateral and contralateral projections matches previously published results 26 . From their measured average synapse density (0.72 \u03bcm \u22123 ), we subtracted the synapses we predicted in local connectivity within a region. While a part of the remaining synapses is formed by projections from the hippocampus and extracortical structures, their total number is unclear, but likely comparatively small. For example, the density of synapses in the prominent pathway from VPM into the barrel field 27 , when averaged over the whole-cortical depth, is only ~1.5% of the average total density. For now we left no explicit space for synapses from such projections due to the difficulties in parameterizing it for all potential sources. We then generated matrices of synapse densities for different projection classes by considering projection strengths derived only from tracer experiments in cre-lines associated with a given projection class. Unfortunately, there were no experiments available for some combinations of cre-line and source region. Instead we generated individual matrices by first averaging the reported projection strengths of a line associated with a projection class over modules of several contiguous brain regions (see Supplementary Table 1 ), and then using that information to generate scaled versions of the wild-type matrix (see the Methods section). Each combination of source and target module were scaled individually, and we enforced the sum of matrices over projection types to be equal to the wild-type. The result is a prediction of the mean volumetric synapse densities from the bottom of layer 6 to the top of layer 1 for all projections (Fig. 1 ). Fig. 1 Predicted synapse densities in target regions. Modules are labeled: PF: prefrontal, AL: anterolateral, SoM: somatomotor, Vis: visual, Med: medial, Temp: temporal. Exact order of brain regions and assignment to modules by Harris et al. 3 are also listed in Supplementary Table 1 . White regions indicate no projections placed for that combination of source and target region Full size image Constraining layer profiles So far, we have constrained density and consequently the total number of synapses formed by each individual projection. This reproduces the spatial structure of projections on the macroscale. However, it is likely that there is also spatial structure within a projection, on the mesoscale or microscale. One such structure, acting along the vertical axis is a distinct targeting of specific layers 28 . To constrain the layer profiles of projections, we once more tended to the data published in Harris et al. 3 . The authors provide extensive data on layer profiles, measured hundreds of them, and then clustered them into six prototype profiles using unsupervised hierarchical clustering using spearman correlation and average linkages. As they demonstrate that these prototypes occur in significantly different numbers in feedforward against feedback projections and for the various projection classes and modules, we concluded that they capture sufficient biological detail. We therefore decided to follow this classification and assign one of the prototype profiles to each projection. Harris et al. 3 already measured the relative frequencies of their prototypical layer profiles for individual projection classes (their Fig. 5o) and for individual source modules, within and across modules (their Fig. 8c, d). They also classified profiles as belonging to feedforward or feedback projections. We combined the constraints by first calculating which layer profiles are overexpressed or underexpressed between pairs of modules, relative to the base profile frequencies for projection classes (see the Methods section). We then classified each projection as feedforward or feedback, based on the hierarchical position of the participating regions, and cut the assumed frequencies of profiles belonging to the other type in half. Finally, we picked for each combination of projection class, source, and target region the layer profile with the highest derived frequency. We chose to pick the single most likely profile for each projection and ignore the others, as mixing several profiles would have diminished their sharp, distinguishable peaks and troughs. The approach resulted in a prediction, where each profile is used for between 10 and 20% of the projections (Fig. 2a ). Based on the prediction, we calculated the resulting relative frequencies of layer profiles per module and per projection class and compared them against the data (Supplementary Fig. 1 ). We found that in spite of the simplifying step of picking only the most likely profile, the trends in the data were well preserved, although the peaks and troughs were more exaggerated in the model. Fig. 2 Predicted layer profiles. a Predictions for all projection classes. Exact order of brain regions and assignment to modules by Harris et al. 3 are also listed in Supplementary Table 1 . b \u2013 f Relative error of the predicted synapse densities in all layers. That is, the difference between prediction and the mean of the raw biological data, divided by the standard deviation of the biological data. b For projections from L2\/3. c From L4. d From L5IT. e From L5PT. f From L6. Dashed black lines indicate the biological variability of density under the assumption that it is Gaussian distributed. We used only projections where more than five raw data points to establish the biological variability were available. g Fraction of projections where with the relative error under two standard deviations for each source layer Full size image We have demonstrated that our simplified predictions recreate the tendencies demonstrated in Harris et al. 3 , but the question remains, how do they compare against the raw biological data? As we moved through two consecutive simplifications\u2014from the raw data to six prototypical profiles and from six profiles to a single profile per projection\u2014how much biological detail was lost? To address this question, we generated raw layer profiles from the voxelized experimental data on projection strength of individual cre-lines 3 using a programmatic interface provided by the authors 6 , and compared them to our single prediction. We obtained the connection strength measured with individual tracer experiments in voxels with a resolution of 100 \u03bcm 3 and grouped them by cre-lines associated with the projection classes (see Methods). For each experiment, we calculated a profile of the connection strengths in each layer of a region, relative to the mean across all layers. As a representative example, Supplementary Fig. 2 depicts the model for projections from MOs (blue line) and the data from individual experiments (gray lines). We see an overall fair match between simplified prediction and data, albeit with some errors. For example, the data for 5PT projections show very shallow profiles in four regions that were not predicted. For 5IT projections to visual regions, the data flattens out in layer 1 instead of peaking, although this may be partly artificial, because the data resolution of 100 \u03bcm 3 is close to the width of layer 1, leading to unreliable sampling. Overall, we find a substantial degree of variability in the biological data, especially for projections from layer 2\/3. For example, the density in layer 4 of VISpor due to projections from layer 2\/3 of MOs varies between 0.2 and 2.5 times mean. As such, we evaluated the overall match of our predictions relative to the biological variability by calculating the deviation from the biological mean in multiples of the biological standard deviation (z-score, Fig. 2b\u2013f ). As a certain number of samples is required to estimate the biological variability, we limited this validation to projections where data from at least five experiments were available. Under the assumption of a Gaussian profile, the data randomly sampled from the biological distribution would follow a standard normal distribution of z-scores (Fig. 2b\u2013f , black dashed lines). We found that the bulk of our predictions fall within that distribution, although a significant number have a z-score exceeding four standard deviations, especially for projections from layer 4 (Fig. 2c ). Yet, 75% of z-scores fall within two standard deviations (Fig. 2g ). We conclude that the predicted layer profiles fall within the range of biological variability for most projections, but do result in imperfect densities in individual cases. We judge this to be sufficient for a first draft null model of a white matter micro-connectome, but refinement should be attempted in the future, as more data, such as whole-brain axonal reconstructions become available. Constraining the mapping of projections The previous section constrained projection by imposing a spatial structure along the vertical axis, a layer profile. Yet, it is likely that there is also a structure along the other two spatial dimensions. That is, that neurons around a given point in the source region project not equally to all points in the target region, but with certain spatial preferences, which we assumed can be expressed by a topographical mapping. To define the mapping, we once more used the voxelized version of the mouse meso-connectome model 6 . As each brain region comprised many voxels in the model, we could use this data to determine whether any given part of a brain region projected more strongly to some part of the target region than to other parts. This would indicate a structured, nonrandom mapping that we would have to recreate to preserve the biologically accurate cortical architecture. We started by projecting 3d representations of the source and target regions into 2d, preserving distances along the cortical surface (as in Harris et al. 3 ). This effectively collapsed the vertical axis, as we had constrained structure along that axis in the previous step. Next, we defined a local barycentric coordinate system in the 2d representation of the source region by picking three points inside the region that maximize the sum of pairwise distances between them, then moving them 25% toward the center. We visualized the result by setting each of the red, green, and blue color channels of an image of the source region to one of the three barycentric coordinates (Fig. 3a , C src ). By extension, we also associated each voxel of the macro-connectome model ( x , y , z ) with a color ( B x , y , z ) by first projecting its center into the 2d plane, then looking up the barycentric coordinate. Next, we considered the strengths of projections from each source voxel and visualized the results by coloring each target pixel according to the product of the 2d-projected projection strength and the color associated with the source voxel: $$I^{{\\mathrm{raw}}} = f \\cdot \\mathop {\\sum}\\limits_{x,y,z \\in V_{src}} {B_{x,y,z}} \\cdot F\\left( {p_{x,y,z}} \\right),$$ (1) where p x , y , z refers to the voxelized projection strength from the voxel at x , y , z , F ( p x , y , z ) to its 2d projection and f to a scaling factor effectively deciding the overall lightness of the resulting image. The result is a two-dimensional image with three color channels \\(\\left( {I_R^{{\\mathrm{raw}}},I_G^{{\\mathrm{raw}}},I_B^{{\\mathrm{raw}}}} \\right)\\) . To more clearly reveal the structure of projections, we ignored source voxels associated with a color saturation below 0.5. Fig. 3 Projection mapping in the visual system. a The primary visual area (VISp) and its defined source coordinate system. The three points defining the barycentric system are indicated as colored triangles. Each coordinate is associated with the indicated red, green, or blue color channel to decide the color of each pixel in the region. b The spatial structure of projections from VISp is indicated by coloring pixels in the surrounding regions according to the color in a of the area they are innervated from. c Center: as in b , but the color of each pixel is normalized such that the sum of the red, green, and blue channels is constant. Periphery: target coordinate systems for the surrounding regions were fit to recreate the color scheme of the center, when colored as in a Full size image The results showed a clear nonrandom structure of targeting in the other regions (e.g., for projections from VISp: Fig. 3b ). To parameterize this structure, we first normalized the color values of each pixel, dividing them by the total projection strength reaching that pixel from src . We set the denominator to a minimum of 25% of the maximum strength from src in the target region to ensure that weakly innervated parts of tgt would be depicted as such. $$N_{src}^{tgt}[a,b] = \\frac{{I^{{\\mathrm{raw}}}[a,b]}}{{{\\mathrm{max}}\\left( {I_R^{{\\mathrm{raw}}}[a,b] + I_G^{{\\mathrm{raw}}}[a,b] + I_B^{{\\mathrm{raw}}}[a,b],\\sigma _{25}} \\right)}},$$ (2) where I [ a , b ] denotes the pixel of image I at coordinates a , b and $$\\sigma _{tgt} = 0.25 \\cdot {\\mathrm{max}}_{[a,b] \\in tgt}\\left( {I_R^{{\\mathrm{raw}}}[a,b] + I_G^{{\\mathrm{raw}}}[a,b] + I_B^{{\\mathrm{raw}}}[a,b]} \\right)$$ (3) This represented a projection as pixels with normalized lightness, that faded to black in weakly innervated parts of the target region (Fig. 3c , center, \\(N_{src}^{tgt}\\) ). Next, we optimized a barycentric coordinate system in the 2d-projected target region to most closely recreate the color scheme observed in \\(N_{src}^{tgt}\\) (Fig. 3c , periphery, \\(M_{src}^{tgt}\\) ). We then assume that a neuron at any coordinate in C src is mapped to neurons at the same coordinate in \\(M_{src}^{tgt}\\) . Thus, the two local coordinate systems, each parameterized by three points, together define the topographical mapping between regions src and tgt . We validated our predicted mapping against established data on the retinotopic mapping in the visual system. This is functional data on the mapping between a brain region and locations in the visual field instead of anatomical data on the projections between brain regions. Yet, we can use it for validation under the assumption that areas corresponding to the same location in the visual field are preferably projecting to each other. Analyzing the retinotopy, Wang and Burkhalter 29 found certain trends: In adjacent regions, points close to the boundary between them on both sides are mapped together. A counter-clockwise cycle in one area is mapped to a clockwise cycle in an adjacent one. This change in chirality indicates that the mapping must contain a reflection operation. Juavinett et al. 30 utilize this to identify borders between brain areas from intrinsic signal imaging of retinotopy. When we systematically examined the reflections and rotations in our predicted mapping (Table 1 ), we found identical results. Table 1 Validation of predicted mapping Full size table Finally, we quantified to what degree barycentric coordinate systems in source and target region can capture the biological trends present in the projection data. As this type of mapping is always continuous and cannot capture nonlinear trends, biological accuracy could be lost. To this end, we calculated the difference between the image of the target region, colored according to the target coordinate system, \\(M_{src}^{tgt}\\) , and the normalized image of the target region according to the projection data, \\(N_{src}^{tgt}\\) . We defined the relative error of a target coordinate system as the sum of absolute differences of the two images, divided by their average and the number of pixels (Fig. 4 ). We found that for over half of the projections the error was below 5% and the maximum error was 17%. Fig. 4 Validation of predicted mapping. Relative error of the mapping defined by the barycentric coordinate systems in the target area, compared with the data. Values along the main diagonal: for contralateral mapping; all others: ipsilateral mapping. The data shown where the sum of densities from all projection classes is above 0.025 \u03bcm \u22123 Full size image Constraining projection types Thus far, we have considered constraints on the spatial structure of projections on a global scale (the macro-connectome matrix) and a local scale (the layer profiles and the mapping). The topographical mapping also limited which individual neurons in a target region can be reached by a given neuron in a source region, severely constraining the topology of the potential connectome graphs on a local scale. Yet, an important aspect of neocortical connectivity not yet considered is which combinations of regions are innervated by single-source neurons 31 . Even if we know which regions are innervated by a population of neurons in a given region, each individual neuron is likely to innervate only a subset of those regions. We call that subset its projection type or p-type . It is unclear to what degree the process is pre-determined or stochastic, and if it is stochastic, what mechanisms further shape and constrain the randomness. This is a complex problem, as a region such as SSp-tr innervates 27 other regions, yielding 2 27 = 134217728 potential p-types. To tackle this problem, we analyzed that reconstructed axons made available by the MouseLight project at Janelia 19 . These are whole-brain neuron reconstructions of cortical neurons that include their long-range projections. We first classified their neuron types, then placed the axons in the context of the Allen Brain Atlas and finally evaluated the amount of axonal length projecting into the 43 ipsilateral and 43 contralateral brain regions. Figure 5a shows an example of 61 analyzed axons originating in MOs. The scale of the p-type problem is clear at first glance: only a single combination of innervated regions is repeated in this data set, all others represent unique p-types. Yet, a structure is also apparent: while only 11 out of the 61 axons innervate the visual or medial modules, the ones that do tend to innervate more than a single of their regions. Moreover, it appears that the projection strength (Fig. 5a , first row) is a strong predictor of the probability that any given axon innervates a region (innervation probability), indicating that a projection is strong because many neurons participate in it, not because of few participating neurons with large axonal trees in the target region. Fig. 5 Innervation of brain regions by individual axons. a Projection density according to Harris et al. 3 (top row), ranging from no projection (white) to strong projections (black), and brain regions innervated by 61 reconstructed axons (rows) indicated by gray squares. b Probability to innervate individual brain regions, predicted from the normalized projection strength from MOs, against the observed innervation probability (L2\/3: calculated from n = 25 axons, L5: n = 61 axons, L6a: n = 35 axons). c Normalized projection strength against the mean total length of axon branches in individual brain regions ( n as in b ). d Observed interactions between the innervation of individual brain regions, i.e., increase in innervation probability of one region when the other is known to be innervated. e Increase in innervation probability as in d against the innervation probability of a pair of regions under the assumption of independence. Gray dotted line indicates the point where the product of independent probability and increase is one that can logically not be exceeded. All innervations and projection strengths in this figure are for projections from MOs Full size image Next, we analyzed these observations systematically. We only had for the source region MOs a sufficient number of reconstructed axons to robustly estimate the innervation probabilities. We found that innervation probability was proportional to the normalized projection strength, i.e., the amount of axon in the target region, normalized by the volume of the source region. We determined projection class-specific constants of proportionality with a linear fit, resulting in a predicted innervation probability \\(P = 0.5 \\cdot \\sqrt {nps}\\) for projections from L2\/3 and L6, \\(0.33 \\cdot \\sqrt {nps}\\) from L4 and L5PT and \\(0.22 \\cdot \\sqrt {nps}\\) from L5IT. Figure 5b compares the innervation probability predicted this way to the one observed in 25 samples for L2\/3 of MOs, 61 samples for L5 of MOs, and 35 samples of its L6 ( p = 3 \u00b7 10 \u22129 , two-tailed pearsonr, n = 3\u00b786, i.e., one sample per region x hemisphere x projection class). Conversely, the projection strength was less a predictor of the axon length in a target region for individual axons innervating the region (Fig. 5c ). Projection strength being a predictor of innervation probability is in line with the findings of Han et al. 31 . Assuming the principle holds for other brain regions as well, we were able to predict the first-order innervation probabilities for all combinations of source and target region. Next, we analyzed statistical interactions of the innervation probabilities for axons originating in MOs. For pairs of target regions, we evaluated the null hypothesis that their innervations are statistically independent, and if it was rejected ( p \u2265 0.05; see the Methods section) calculated the strength of the statistical interaction as the conditional increase in innervation probability \\(\\left( {\\frac{{P(s \\to t_1|s \\to t_2)}}{{P(s \\to t_1)}}} \\right)\\) . We found significant interactions for 283 pairs (Fig. 5d ), with some strengths exceeding a 15-fold increase. However, there were several problems preventing us from simply using these observed interactions to constrain connectivity. First, we only had data for axons originating from one of 43 brain regions and it is likely that interactions differ for source regions. Second, the data were incomplete, as some targeted regions were not innervated by a single reconstructed axon (Fig. 5d , white patches), and others were based on only a single or two axons. Third, evaluating 86\u00b7(86\u22121)\/2 = 3655 potential interactions based on only 61 data points (i.e., axons) are statistically inherently unstable and likely to dramatically overfit. A model to generate projection types Instead, we tried to use the available axon data to develop a conceptual model of how the interactions arise. We first observed that the largest interactions strengths occurred for target regions in the medial and visual modules that are otherwise only weakly innervated. Evaluating this observation systematically, we found that indeed the strength of an interaction was strongly negatively correlated with the product of the first-order innervation probabilities of the pair (Fig. 5e ). Second, we observed only conditional increases in innervation probability (values \u22651), i.e., innervation of pairs of brain regions is not mutually exclusive. One model explaining both our observations is the following: consider a tree with the brain regions in both hemispheres as the leaves. Let each edge in the tree be associated with a probability that the edge is successfully crossed by an axon, these probabilities can be different in both directions of the edge. To generate the set of innervated regions for a random axon, start at the leaf representing its source region and then consecutively spread to other nodes further into the tree along its edges with the probabilities associated with the edges (Fig. 6a ). Once it has been decided that an edge is not crossed, it cannot be crossed in future steps. Every leaf reached this way is then considered to be innervated by the axon. Fig. 6 A model to generate p-types. a Toy example of a p-type generating model with four regions (A\u2013D). The regions are associated with the leaves of a directed tree (black), edges of the tree are associated with a probability to cross it. Two exemplary axons (orange, blue) spread from region D either crossing an edge (dashed lines) or not (dashed X-marks). Inset: resulting p-types; black regions are innervated; s indicates the source region. b Examples of innervation of brain regions predicted by the full model for L5IT (left column) and of reconstructed axons (right column). Sampled axons along the y -axis, brain regions along the x -axis. A black pixel indicates that an axon is innervating a region. Top row: axons originating from L5 of MOs; bottom row: from MOp. c Pairwise distances (hamming distance) between the profiles of brain region innervation. Blue: the data from reconstructed axons (see a ); orange: from 10,000 profiles sampled from the tree-based model; green: from 1000 profiles sampled from a naive model taking only the first-order innervation probabilities into account. Left: for axons originating from MOp; right: from MOs. d Increase in innervation probability against the basic innervation probability as in Fig. 5e Full size image If we set the length of an edge in this model to the negative logarithm of the associated probability, then the first-order probability that a region T is innervated by an axon originating in region S is easily calculated: $$P(S \\to T) = 10^{ - L(S,T)},$$ (4) Where L ( S , T ) denotes the length of the shortest path between S and T . Similarly, the increase in conditional innervation probability of T 1 and T 2 is given as: $$I(S,T): = \\frac{{P(S \\to T_1|S \\to T_2)}}{{P(S \\to T)}} = \\frac{{10^{ - L(lca(T_1,T_2),T_2)}}}{{10^{ - L(S,T_2)}}},$$ (5) Where lca ( T 1 , T 2) is the lowest common ancestor of T 1 and T 2 . Due to the underlying tree structure, the lowest common ancestor is always an inner node that is closer or of equal distance to T 2 , therefore the strengths of interactions are always larger than one indicating an increase of innervation probability, which is in line with our earlier observations. Fitting the model consisted of two steps: first, we generated the topology of the tree using the normalized connection density of projections, i.e., the amount of signal (axon) in the target region normalized by the volume of both source and target region. Specifically, we used the Louvain heuristics 32 with successively decreasing values for the gamma parameter to detect successively larger communities in the matrix of normalized connection densities (see Methods). Next, we replaced each edge with two directed edges, one in each direction. Then we optimized the probabilities associated with edges using the first-order innervation probabilities predicted from the normalized connection strength of projections as in Fig. 5b . These predictions then served as constraints on the path lengths between leaves. Specifically, we locally optimized the edges in small motifs consisting of two sibling nodes and their parent, based on differences in the distances of the siblings to all leaves (see Methods). As the pair of edges between nodes can have different associated probabilities, the predicted statistical interactions are not symmetric (see Supplementary Fig. 4 ) and there can be region-specific differences in the number of regions innervated or innervated from. We used the fitted model to generate 10,000 profiles of brain region innervation for axons originating from L5 of MOs and MOp. Figure 6b compares a number of randomly picked profiles against the data from reconstructed axons for both regions. As the model was constrained with the predicted first-order innervation probabilities, it manages to recreate the observed high-level trends: strong innervation of the ipsilateral and contralateral prefrontal, anterolateral, and somatomotor modules; weaker, but highly correlated innervation of the other modules. To test the model further, we calculated the pairwise hamming distances between innervation profiles from reconstructed axons and from the model (Fig. 6c ). We also compared the data against a naive model using only the observed first-order innervation probabilities and assuming no interactions. We found that the naive model resulted in a narrow, symmetrical distribution with a single peak at around 9 (MOp) or 13 (MOs). In contrast, the axon data led to a much wider, asymmetrical, and long-tailed distribution that were much better approximated by the tree-based model. The difference between the distribution resulting from the tree-based model and the axon data was, in fact, not statistically significant (MOp: p = 0.44, n = 9 axons; MOs: p = 0.12, n = 61 axons; kstest). Using the tree-model, we could predict the strengths of interactions as described in Eq. ( 5 ) (Supplementary Fig. 4 ). When comparing the strength of the interactions against the naive innervation probabilities without interactions, we found in the model the strong negative correlation that was present in the axon data (Figs. 5e, 6d ). For the model, we found more data points toward the lower left corner of the plot that indicates low naive probability and low increase. The lack of such points in the data from axon reconstructions can be explained by the fact that points associated with extremely low probabilities are unlikely to show up in a relatively small sample of reconstructed axons. As a final validation, we compared the model against the results of Han et al. 31 , which considered brain region targeting of single axons originating from VISp. We have not taken into account axons from this source region when we formulated or fitted the model, making this a powerful validation of the generalization power of the model (Fig. 7 ). Comparing the number of visual regions innervated (out of VISli, VISl, VISal, VISpm, VISam, and VISrl) by individual axons originating in layer 2\/3 of VISp, we find comparable results (Fig. 7a ). Although in the model, the mean number of regions innervated is slightly higher (1.84 vs 1.7 (fluorescence-based) or 1.56 (MAPseq)) we find the same roughly binomial distribution where fractions decrease with increasing number of innervated regions. Fig. 7 Validation of the tree-model. Validation against the results of Han et al. 31 . a Top, results of 31 in terms of the number of visual areas innervated by single axons originating in layer 2\/3 of VISp. Bottom, corresponding results of the tree-model for axons originating in layers 2\/3, 4, 5, and 6 (top left to bottom right; n = 10,000 innervation profiles each). b Top, results of Han et al. 31 in terms of common innervation of pairs of visual brain areas by axons originating in layer 2\/3 of VISp. Bottom, corresponding results of the tree-model, based on n = 10,000 innervation profiles Full size image We were also able to predict this distribution for axons from other layers using our model. We predict similar shapes of the distribution with an even higher mean for layer 5 and a lower mean for layer 4 and especially layer 6. Next, we also considered the statistical interactions between the six visual target regions (Fig. 7b ). Again, we found overall comparable conditional probabilities, with a comparable structure, although strong common innervations of regions VISl and VISal and VISpm and VISam were underestimated. Connectome instantiations and their micro-structure Finally, we developed a stochastic algorithm to generate instances of a neuron-to-neuron connectome that fulfills all constraints in the long-range projection recipe and used it to connect a model of the entire mouse neocortex (see the Methods section). We considered slender tufted and untufted pyramidal cells in layer 5 to participate in projection class L5IT and half of the thick tufted layer 5 pyramidal cells in L5PT, with the other half participating in L5CT, which is not covered by the present, purely cortical model. Pyramidal cells in other layer all participated in the corresponding projection class. As a result, we obtained connectome instances with 88 billion modeled synapses, each associated with a presynaptic neuron, postsynaptic neuron, and an exact location on the postsynaptic morphology (Supplementary Fig. 6 ). This allowed us to analyze the microstructure emerging from the constraints we added on top of the matrix of connection strengths. While the additional constraints on layer profiles and topographical mapping were arguably on the meso- rather than microscale, and the p-types governed the targeting of regions rather than individual neurons, they were together likely to affect measurements of the microstructure. For example, an overexpression of reciprocally connected neuron pairs is traditionally a measure of microstructure 33 , 34 . Topographical mapping between regions A and B can lead to such an overexpression for pairs where one neuron is in A and the other in B . This occurs when a location in A is mapped to a location in B that is in turn mapped back to the same location in A , leading to reciprocal connectivity of neurons in those locations that is higher than expected from the average unidirectional probabilities between the regions. In order for this trend to emerge in an experiment, neurons would have to be sampled over sufficiently large volumes for the mapping to have a significant effect. We evaluated the strength of this effect in an exemplary pair of connected regions, VISa and VISam (Fig. 8 ). We calculated unidirectional and reciprocal connection probabilities between parts of the regions, where we first defined a subvolume of VISam with increasing radius, then found the center of its projection to VISa according to the mapping and defined a subvolume with the same radius around the center (Fig. 8a , sampling radius). We found that the connection probabilities decreased with increased radius, as more and more parts of the regions are considered that are not mapped to each other (Fig. 8b ). However, the expected reciprocal connection probability obtained from multiplying the unidirectional probabilities fell off faster than the measured one. Indeed for all radii over 150 \u03bcm, the reciprocal overexpression, i.e., the measured divided by the expected reciprocal probability, was in three connectivity instances larger than one, reaching values as high as 2.5 for radii over 500 \u03bcm (Fig. 8c ). Fig. 8 Bidirectional micro-connectivity and modularity. a Connectivity between individual neurons in VISa and VISam was sampled by defining a subvolume with various radii in VISam (sampling radius), then by finding the center of the projection from the subvolume to VISa according to the mapping (dashed arrow), moving it (sampling offset) and defining a subvolume with the same radius around it. b Unidirectional (red) and reciprocal connection probabilities for various sampling radii with zero-sampling offset. Gray: expected from unidirectional connectivity; black: model. c Ratio of reciprocal connectivity measured in the model over the expected value. Gray: three instances; black: mean of n = 3 instances. d As b , but for a sampling radius of 150 \u03bcm with various sampling offsets. e As c , but for sampling offsets. f Bottom: edge density, i.e., the number of connections over the number of pairs, of the microconnectivity between within-region modules that were defined by clustering the connectivity within the two brain regions (see the Methods section). Top: neuron-to-neuron connectivity between 7 \u00d7 7 within-region modules outlined in green. Gray lines indicate boundaries between within-region modules. g Distribution of edge densities in f (top right quadrant) compared with a random control. h Width of the distribution of edge densities (as in g ) at half height, model against control, for projections with a density over 0.02 \u03bcm \u22121 . Circles: projection originating in the prefrontal module; stars: anterolateral module; left-pointing triangles: medial; downward-pointing triangles: somatomotor; right pointing: temporal; upward pointing: visual; dark blue: intramodule projection; light blue: intermodule Full size image In addition, we found that measuring connection probabilities not at the center of the projection of the subvolume, but offset from it (Fig. 8a , sampling offset) lead to an overexpression of reciprocally connected pairs. For a sampling radius of 150 \u03bcm, we shifted the center of the subvolume in VISa in a random direction by various amounts, finding that it decreased all connection probabilities while simultaneously leading to an increase in the reciprocal overexpression (Fig. 8d, e ). Motif counts in neuron triplets is another traditional measure of microstructure 33 , 34 ; its equivalent in long-range connectivity is motif counts in triplets where each neuron is in a different brain region. The p-types dictate that certain pairs of regions tend to be innervated together, which would lead to overexpression of the corresponding motifs. Using the same method of sampling from subvolumes as above (Supplementary Fig. 5a ), we performed such an analysis for three regions that are strongly connected to each other, FRP, MOs, and MOp, confirming the trend. Based on 100,000 triplets in the subvolumes, we found that motifs where a neuron in FRP innervates only a neuron in MOp or a neuron in MOs only innervates a neuron in FRP where significantly underexpressed in favor of motifs where they innervate neurons in both other regions (Supplementary Fig. 5b ). The constraints on topographical mapping and the p-types are specific implementations of a principle of structured connectivity on various levels; not only between modules and regions, but also successively smaller subregions, leading to a scale-invariant structure, previously identified in human MRI data 20 . The topographical mapping generates a structure of subregions, as outlined above, while the p-types generate larger structures of groups of regions that tend to be innervated together. As such, the the micro-connectome instances can be thought of as extending this principle\u2014so far demonstrated for voxelized connectivity\u2014further down to the level of individual neurons. Taylor et al. 20 quantified this structure for diffusion imaging voxels by detecting modules in the internal connectivity structure of two contiguous brain regions, and then considering the connectivity between the brain regions in terms of connection strengths between pairs of such within-area modules. They found that the distribution of strengths was much wider than in a random control, indicating that the within-area modules also structure the connectivity between areas. We replicated this experiment on the microstructure, i.e., the predicted neuron-to-neuron connection matrices within and between VISa and VISam (Fig. 8f, g ). Upon grouping individual neurons in the two regions into 93 (VISa) and 179 (VISam) within-area modules and comparing the connectivity between them to a random control preserving individual neuron in- and out-degrees, we found comparable results. Repeating the analysis for all sufficiently strong projections (Fig. 8h ), we found the same, predicting that the principle extends down to the level of individual neurons. Taken together, we conclude that the constraints and principles we identified lead to a highly nonrandom microstructure of connectivity. While the structure is a prediction that will need to be validated, this demonstrates the utility of generating statistical connectome instances, as they reveal and quantify the interactions between mesoscale and micro-scale connectivity. Discussion We have developed a way to generate statistical instances of a whole-neocortex mouse micro-connectome. This approach takes into account the current state of knowledge on region-to-region connectivity strengths, the laminar pattern of projection synapses, the structure of topographical mapping between regions and the logic of regional targeting of individual projection axons as derived from over 100 whole-brain axon reconstructions, and a comprehensive mesoscale model of projections, built from thousands of experiments 3 . Combining these data with a morphologically detailed model of neocortex 21 has allowed us to statistically predict connections with sub-cellular resolution, i.e., including the the locations of individual synapses on dendritic trees. Our approach is timely, as it leverages and integrates three very recent, publicly available data sets. Furthermore, its flexibility and modularity will allow it to readily use future data sets in place and in addition to the currently used ones. The resulting wiring diagram allows fundamental questions to be addressed, such as the nature and dynamics of clinically relevant brain rhythms as well as hierarchical interactions in the cortex, which are fundamental for understanding cortical coding and whole-brain regional dynamics. As the available data on this topic remains sparse, our approach was as follows: we considered the formation of the connectivity as a stochastic process selecting one out of a space of possible wiring diagrams, and then sought out biological principles and rules that consecutively restrict this space of biologically viable wiring diagrams. The principles we identified were not only based on the biological data but also a number of assumptions. The assumptions were necessary to break down the scale of the problem, to interpret the data (data assumptions) and structure it into principles (structuring assumptions), to formulate principles mathematically (modeling assumption), and to apply them to infer missing data (generalizing assumption). In order to interpret the resulting micro-connectome and predictions, one needs to first understand these assumptions. While we have made them explicit in this paper, they are also summarized in Table 2 and discussed in the Supplementary Discussion . Table 2 List of assumptions used in the formulation of the model Full size table As in any model, there is the implicit assumption of completeness, that our model captures all pertinent biological principles. We make no claim that this is true. This assumption is formally necessary for us to achieve the following modeling goal: given the assumptions, find the most general model that completely describes the data. In this context, we have drastically improved the strength of the null model of the microstructure of long-range connectivity. Previously, the most general model of the data was the null model implicit in long-range connection matrices\u2014that of unstructured connectivity beyond the region-to-region level\u2014or with at most some layer targeting rules. We have not only systematically integrated the data on this level but also added constraints that lead to a nonrandom microstructure with testable predictions. Comparing potential experimental data against our improved model will lead to a better interpretation of the results. For example, we have demonstrated that an increased reciprocal microconnectivity between regions does not only necessarily imply a mechanism selectively stabilizing such motifs but can to some degree be explained by the mechanisms leading to topographical mapping. We have further demonstrated that in the presence of strong mapping, reciprocity must be evaluated relative to the trends present in the mapping to be correctly understood. Findings violating the naive, unstructured null model but in line with our improved model can be explained by the principles of connectivity we implemented. For data points invalidating the model, for example conflicting triplet motif counts, we can try to pinpoint which assumption it violates and thus provide it context. Alternatively, data contradicting the model can be simply a result of biological variability between individuals. At this stage, we positioned the model to represent an average adult mouse where such false positives are least likely. Further, some constraints\u2014such as the mapping and p-types\u2014remained statistical and consequently captured a large degree of variability between individual instances. For the other constraints\u2014such as average synapse density and layer profiles\u2014we can estimate an upper bound on variability in the future by running our programmatic pipeline to parameterize connectome constraints on outlier data points instead of averaged data. Similarly, other ages or specific strains can be modeled by using different data in the same pipeline. We can already hypothesize about additional principles that might have to be added in the future. In terms of targeting of connectivity, we have implemented many aspects of spatial targeting of brain regions and locations within a region, and we have demonstrated that this leads to a highly nonrandom microstructure. However, it is possible that similar rules apply for the incoming long-range projections, i.e., which set of brain regions individual neurons are innervated by, and possible interactions between incoming and outgoing. In that case, we will be able to extent our definition of p-types to be the concatenation of incoming p-types and outgoing p-types. In terms of the large-scale inter-area connectivity trends, i.e., the macro-connectome, our approach does not make any predictions, but is instead explicitly recreating the input data used. While Harris et al. 3 provided sufficient data for five projection classes, it missed for example a GABAergic projection class 18 . Additional sources could be used in the future to add such a type. In principle, completely different data sets could be used to define projection strengths. For example, G\u0103m\u0103nu\u0163 et al. 2 report a cortical mouse macro-connectome that recreates biological trends, such as a lognormal distribution over several orders of magnitude of projection strengths. They argue that their data captures several projections that are missed by Oh et al. 25 (and consequently also potentially by Harris et al. 3 , which is based on similar computational methods). As their data provides potential sub-area resolution (see their Fig. S2 ), it could be used to also constrain the mapping and consequently serve as the basis of a stochastic micro-connectome predicted with our method, albeit without distinction of projection classes. The assumption of a continuous, linear mapping between regions appears to solidly recreate the projection data, with only three regions leading to significant error (Fig. 4 ; MOs, MOp, and SSs). One explanation for the error would be that these regions contain subregions that each send and receive their own, continuous projections. Indeed, for the projections from SSp-ll and SSp-ul to SSs (Supplementary Fig. 3b , right), we see several peaks of the green and blue color channels in the data, whereas a single continuous mapping can only generate single peaks. This is not surprising, as MOs, MOp, and SSs are not broken up by body part, unlike SSp that it strongly interacts with. In the future, the projection data could thus be used to further break up these regions, at least for the purpose of analyzing projections. With more advanced analyses and more data it may even become possible to hypothesize a brain parcellation scheme ab initio based on projection data. Even with the imperfections outlined above, the present model will lead to advances in our understanding of brain function, when employed in simulations of whole-neocortex activity. The explicit parameterization of the constraints will allow us to change parameters to assess their impact. For example, it is at this point unclear whether the targeting rules for individual axons (p-types) will have an effect on high-level brain activity. Similarly, we can investigate to what degree the relatively simple topographical mapping in the model is sufficient for the upstream propagation of spatial information from VISp. Steps into that direction can be undertaken both in morphologically detailed models and point neuron models using the publicly available model connectome. Methods Accessing the mouse connectivity model Unless noted otherwise, the data from the voxelized mouse connectivity model of the Allen Institute was accessed using the mcmodels python package provided by the authors (  ). Volumetric synapse densities of projections We formulated a target mean density of synapses of 0.72 \u03bcm \u22123 in the model, as measured by Sch\u00fcz and Palm 26 . Multiplied with the neocortex volume of the isocortex in the Allen mouse brain atlas (123.2 mm 3 ), this yielded a target number of 88.74 billion synapses. From this number, we subtracted 36 billion synapses we predicted in local connectivity within a brain region. This local connectivity was predicted by detecting axo-dendritic appositions in the model and filtering them to fulfill biological constraints, such as bouton density and synapses per connection 24 . We then derived a matrix of synapse densities in all projections between pairs of brain region by scaling the wild-type connection density matrices provided by Harris et al. 3 in the following way: Let M i and M c be the 43 \u00d7 43 matrices of connection densities in ipsilateral and contralateral projections between brain regions, provided by Harris et al. 3 . Entries along the main diagonal of M i , corresponding to connectivity within a region are set to 0. Furthermore, let V be the vector of region volumes and C t the matrix of target region coverage in Supplementary Fig. 3d . Then we can calculate the scaling factor \u03c3 : $$\\sigma \\cdot \\mathop {\\sum}\\limits_{a,b} {\\left( {M_i[a,b] + M_c[a,b]} \\right)} \\cdot V[b] \\cdot C_t[a,b] = 68.74 \\cdot 10^9$$ (6) This factor was then applied to both M i and M c to convert them into matrices of the average density of synapses in the target region due to a projection, measured in \u03bcm \u22123 . While this left no explicit room for synapses from extracortical sources, we estimate them to contribute comparatively little. For example, the density of thalamic synapses projected from VPM into SSp-bfd 27 , when averaged over the whole-cortical depth, is only about 1.5% of the average total density (0.72 \u03bcm \u22123 ) 26 . Projection density matrices for individual projection types We combined the wild-type projection matrix from Harris et al. 3 with their incomplete information on projections in individual projection classes, to get five individual projection matrices, one for each projection class. As their wild-type experiments affected neurons in all layers and classes of the source region, we assumed that the sum of synapse densities over projection classes is equal to the density for the wild-type. Furthermore, based on qualitative observations, we assumed that the region-to-region connection matrices for each projection class are versions to the wild-type matrix, where individual module-to-module submatrices are scaled by individual values. The modules were six groups of contiguous brain regions (prefrontal, anterolateral, somatomotor, visual, medial, and temporal) identified in Harris et al. 3 . This assumption means that connectivity trends between modules will be preserved for all projection classes, but more fine grained trends for regions within a module will simply replicate the overall trends observed in the wild-type matrix for all classes. Based on these assumptions, we derived matrices of synapse densities for individual projection classes with the following algorithm. First, we digitized the available information for individual projection classes from the Harris paper using the following mapping to cre-lines: 2\/3: Cux2-IRES-Cre; 4: Scnn1a-Tg3-Cre; 5it: Tlx3-Cre_PL56; 5 pt: A93-Tg1-Cre; 6: Ntsr1-Cre_GN220. Then we condensed the information into five 6 \u00d7 6 matrices of average projection strengths between modules and normalized results such that the sum of the five matrices is 1 for each entry. Finally, we generated full-size 43 by 43 matrices for each projection type by scaling module-to-module specific submatrices of the wild-type matrix by the corresponding entry in the condensed and normalized matrix (Supplementary Fig. 7 ). To reduce the computational demand of generating connectome instances, we determined a minimal projection strength and removed projections weaker than the cutoff. The cutoff was calculated as 0.0006 \u03bcm \u22123 , such that <5% of projection synapses would be lost. Projection density assumed symmetrical for both hemispheres As the data in Harris et al. 3 are focused on the right hemisphere, we assumed connectivity to be symmetrical between hemispheres to be able to model both of them. This lead to 5 (projection classes) \u00d7 43 (source regions) \u00d7 86 (ipsilateral and contralateral target regions) potential projections parameterized in terms of their strength by the data. However, we considered the 5 \u00d7 43 ipsilateral projections within the same region to be local connectivity, which we instead derived with our established approach 24 . A number of regions also lack layer 4, rendering projections in that projection class void. Predicting layer profiles To assign one out of six layer profiles to each projection, we digitized the data on profile frequencies of Harris et al. 3 and combined it according to the process illustrated in Supplementary Fig. 8 : first, for a source module we counted the number of intra-module or inter-module projections originating from it in each projection class. The example illustrates inter-module feedforward projections from the prefrontal module (Supplementary Fig. 8 , top left). For the presence of a projection, we defined a minimum projection strength, selected such that <5% of the total number of projection synapses are lost to the cutoff. The counts were then used as weights for a weighted average of the vectors of layer profile frequencies associated with each projection class. The result is a vector of expected profile frequencies for intra- or inter-module projections from the source module, if only the layer profile frequencies associated with projection classes are considered (Supplementary Fig. 8 , top right). Next, we looked up the observed profile frequencies for the source module in the data of ref. 3 and compared them to the expected ones (Supplementary Fig. 8 , bottom left). Dividing the observed by the expected frequencies yielded adjustment factors for each layer profile that expressed which profiles were overexpressed or underexpressed in intra- or inter-module projections from the source module under consideration (Supplementary Fig. 8 , bottom middle). We categorized projections as feedforward or feedback, based on the hierarchical positions of brain regions, reported in Fig. 8e of ref. 3 , and, in accordance with their findings, reduced by 50% the adjustment factors for profiles 1, 3, and 5 when considering feedback projections and of profiles 2, 4, 6 when considering feed-forward projections. Finally, we multiplied the vector of adjustment factors with the vectors of profile frequencies for individual projection classes to get adjusted profile frequencies (Supplementary Fig. 8 , bottom right). The method yielded unique profile frequencies for each combination of source module, projection class and intra- or inter-module projection. To reduce the vectors of adjusted frequencies to a single profile, we simply picked the profile with the highest adjusted frequency (Supplementary Fig. 8 , bottom right). Topographical mapping of projections The topographical mapping of projections was defined by barycentric coordinate systems in the source and target regions and the assumption that a point in one region is mapped to the corresponding point in the other. The local coordinate systems were derived using the methods described in the Results section, implemented in custom python code available at:  . However, due to the potentially large extent in the target region of single-projection axons, the biological mapping is rather point-to-area than point-to-point. Therefore, we additionally predicted for each projection the width of the targeted area. A point-to-area mapping would result in an \\(N_{src}^{tgt}\\) with lower saturation values, i.e., when depicted as in Fig. 3 in an image with slightly washed-out colors. Indeed, we found for most projections low saturation values in \\(N_{src}^{tgt}\\) and consequently the optimal solution for the target coordinate system \\(M_{src}^{tgt}\\) would place all three defining points outside the target region. However, we assumed that low saturation values were rather a result of a large extent of projection axons leading to a weak mapping. We therefore added another objective to the optimization procedure for \\(M_{src}^{tgt}\\) : minimizing the fraction of the source region that is mapped to points outside the target region. To compensate, we defined points in the source region to be mapped to 2d Gaussian kernels at their target location instead of a single point. The width of the Gaussian was optimized such that a convolution of \\(M_{src}^{tgt}\\) with the same Gaussian resulted in the same distribution of saturation values as \\(N_{src}^{tgt}\\) . Analyzing whole-brain axons We acquired 183 neuron reconstructions from the Janelia Mouselight data portal 19 by querying for reconstructions where the soma location is within the neocortex. We first manually annotated the apical dendrite using Neurolucida (MBF Bioscience, Williston, VT, USA) given that it was not available in the original data. Based on this, we classified the neuron as a pyramidal cell or interneuron. Then, we performed a spatial analysis of the axon projection of each neuron by mapping the terminal points of the axon as well as the soma location into the Allen CCFv3 atlas coordinate system 25 . This yielded a complete list of brain regions containing axon terminal branches, as well as the brain region and layer containing the soma. Together with the information previously extracted from the annotated apical dendrite (e.g., shape, layer, number of branches), this spatial information is used to perform classification of the m-type and projection type (p-type) of the neuron. Testing statistical independence of region innervation Let N be the number of analyzed axons (here: 61 for innervation from L5 of MOs). Let n a and n b be the number of them that innervate regions a and b , respectively. Then under the assumption of statistical independence, the number of axons innervating both a and b is distributed according to the hypergeometric distribution with parameters N , n a , n b . We tested where the observed number of dual innervations fell along the cumulative distribution, and rejected the null hypothesis of independence if it was within the first or last 2.5% (two-tailed test). Constructing the p-type generating tree morphology The Louvain algorithm takes a weighted adjacency matrix as input, and then clusters the nodes into communities trying to maximize the weights within a community and minimize the weights across. An additional parameter is \u03b3 , which defines the granularity of the result: The smaller the value, the fewer communities it will result in, until a value of zero resulting in a single community. We began by setting gamma to a value of 6.0, such that every brain region resulted in its own community. Correspondingly, we began constructing the tree topology by associating every brain region with its own leaf node. We then continuously lowered the value of \u03b3 , such that regions and communities began to merge into larger communities. We considered a pair of communities to be merged when through lowering \u03b3 a new community appeared that contained more than half of the regions of each of the original communities. In that case, we placed a new node in the graph representing the new community and connected it with the two nodes representing the original communities. We continued lowering \u03b3 until it reached zero, at which point everything merged into a single community and the root of the tree was placed. We fit the weights of the edges to the predicted innervation probabilities using a recursive algorithm that optimized the local weights in small motifs consisting of two sibling nodes and their parent. It is based on the following observations (Supplementary Fig. 9 ): Let T 1 and T 2 be two sibling nodes and R their parent. In the model, any difference in the innervation probabilities for axons originating in T 1 and T 2 can only be due to differences in the lengths of the edges connecting each of them to their parent. This is because once the parent is reached, the shortest paths to any other region will be identical. Therefore: $$w_{T_1 \\to R} - w_{T_2 \\to R} \\approx |M[T_1,:]| - |M[T_2,:]|$$ (7) $$w_{R \\to T_1} - w_{R \\to T_2} \\approx |M[:,T_1]| - |M[:,T_2]|,$$ (8) Where M denotes the matrix of the negative logarithm of predicted innervation probabilities, M [ x , :] a single row of it (i.e., the probabilities of neurons in x to innervate each other region), and M [:, x ] a single column of it (i.e., the probabilities of x to be innervated by neurons in each other region). Further, the probability a neuron in T 1 innervates T 2 is given by the path from T 1 via R to T 2 : $$M[T_1,T_2] = w_{T_1 \\to R} + w_{R \\to T_2}$$ (9) $$M[T_2,T_1] = w_{T_2 \\to R} + w_{R \\to T_1}$$ (10) We found values for the four edge lengths in the motif by finding the least-squares solution of the system of linear equations. After this, we continued by performing the same step for node R and its sibling, until the root was reached. Generating connectome instances according to the constraints As mentioned previously, a long-range projection recipe is created which describes constraints on the desired connectivity. By doing so, the same recipe can be used to instantiate long-range projections with different circuit models, and to allow for different implementations to create these instantiations. This section describes the implementation used to generate the connectomes published under  . The circuit representation and input data required for this implementation are: A placement of neuron morphologies in space A table describe their morphological types A spatial index, allowing the querying of morphology segments in a bounding region An atlas describing the different regions and layers that are addressed by the recipe A \u201cflat-mapping\u201d from 3d to 2d space the recipe, which has: populations: defining which regions, subregions and morphological types are part of the various source and target populations projections: organized by source population; specifies per target population, the expected synapse density, layer profile, and the barycentric source and target triangles p-types: organized by source population; specifies per target population the first-order innervation probabilities for neurons of the source population, and for pairs of target populations the conditional increase in innervation probabilities The basic circuit representation (first three items) was generated by a scaled-up version of a published algorithm 15 . The atlas was based on the Allen Common Coordinate Framework 35 . For the flat-mapping, we used the Allen Dorsal Flatmap of the mcmodels python package (see above). With this data, the implementation proceeds with the following steps: Neuron allocation : For each source population, the neurons in those populations are allocated to participate in projections to a number of target populations according to specified fractions and statistical interactions. Where no interaction is specified, the overlap (neurons participating in both projections) is calculated from the fractions participating in one projection multiplied by the other; this default value is scaled-up, where interactions are specified. The challenge is then to assign neurons to each of the projections, such that the desired fractions and overlap sizes are reached. A simplistic greedy algorithm was used to perform this allocation. Each source population group is assigned a sampled set of neurons, and pairwise the overlap is calculated, and adjusted based on the first-order interactions. When the overlap is too small, it is enforced by randomly sampling neurons from each group, and replacing neurons in the other group such that the overlap is achieved. Attempts were made to use a SAT solver to perform exact allocations, but the size of the neuron counts and the constraint counts meant the model could not be solved in the available memory. Synapse sampling : Sampling happens at the target region level. The target populations in the region are grouped, and the required densities per incoming projection are computed based on the long-range projections recipe. The densities are translated into counts based on the constrained volume created by intersecting the area occupied by the barycentric triangles, and reversed mapped using the \u201cflat-map\u201d to the voxels of the atlas within the region. Finally, all morphological segments of a target population within this volume are found, and sampled with replacement with weights proportional to their length. Synapses are placed at random offsets within these segments. This structure allows for parallelization, as each combination of target and source populations can be run at the same time, subject to computation and memory limits. In practice, finding all segments within the volume demands significant memory which constrains the implementation. This, in turn, gives rise to per target population order: all samples for this population are loaded, and then all the sources referencing this population are calculated sequentially, with the calculations parallelized when possible\u2014the initial sampling, picking the segments with in the barycentric coordinates, etc. Further parallelization can be achieved by running many of these process on different machines, in a batch style. Mapping : Following the allocation and sampling, the two results are brought together in mapping: source neurons that are allocated to a projection are matched to the synapses created during the sampling of the same projection. Because both these data sets work with 3d coordinates, they are projected into the 2d representation so that the barycentric coordinates, described earlier, can be used to create the desired spatial organization. To that end, since source neurons are less numerous, they are first projected into the flat space, and from there mapped into the barycentric coordinate system of the source region. The same coordinates in the barycentric system of the target region are then mapped back into the flat space and considered the mapped locations of the source neurons in the target region. Synapses in the target region are directly mapped to the flat space. Finally, in parallel, synapses are then stochastically assigned to a target neuron with a weighting based on the distance to their mapped location in the flat space and the specified width of the mapping. To speed this process, the source locations are put in a k-dimensional tree, and only the 100 closest source locations are queried per potential target synapses. Output : The final step is to output the circuit in a format that can be used for simulation. For this, the SONATA file format was chosen:  . In addition, for structural analysis, we output for each target region a connectivity matrix of all incoming connections in the scipy.sparse.csc_matrix format. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability The recipe constraining the long-range connectivity\u2014underlying Figs. 1 \u2013 4 , S1 \u2013 S3 \u2014and stochastic instances fulfilling the constraints\u2014underlying Figs. 6 \u2013 8 , S5 and S6 \u2014can be downloaded from the Mouse whole-neocortex connectome model portal (  ). The reconstructions of individual axons\u2014underlying Fig. 5 \u2014are available at the MouseLight project at Janelia, mouselight.janelia.org . Code availability The model was constructed using python 2.7 with custom code available at  . ","News_Body":"Researchers at EPFL's Blue Brain Project, a Swiss brain research Initiative, have combined two high profile, large-scale datasets to produce something completely new\u2014a first draft model of the rules guiding neuron-to-neuron connectivity of a whole mouse neocortex. They generated statistical instances of the micro-connectome of 10 million neurons, a model spanning five orders of magnitude and containing 88 billion synaptic connections. A basis for the world's largest-scale simulations of detailed neural circuits. Identifying the connections across all neurons in every region of the neocortex The structure of synaptic connections between neurons shapes their activity and function. Measuring a comprehensive snapshot of this so-called connectome has so far only been accomplished within tiny volumes, smaller than the head of a pin. For larger volumes, the long-range connectivity, formed by bundles of extremely thin but long fibers, has only been studied for small numbers of individual neurons, which is far from a complete picture. Alternatively, it has been studied at the macro-scale, a 'zoomed-out' view of average features that does not provide single-cell resolution. In a paper published in Nature Communications, the Blue Brain researchers have shown that the trick lies in combining these two views. By integrating data from two recent datasets\u2014the Allen Mouse Brain Connectivity Atlas and Janelia MouseLight\u2014the researchers identified some of the key rules that dictate which individual neurons can form connections over large distances within the neocortex. This was possible because the two datasets complemented each other in terms of entirety of the neocortex and the cellular resolution provided. Emergence of a surprisingly complex structure at single-cell resolution Building on their previous work in modelling local brain circuits, the researchers were then able to parameterize these principles of neocortical connectivity and generate statistical connectome instances compatible with them. When they studied the resulting structure, they found something fascinating; at cellular resolution, a surprisingly complex structure that had so far only been seen between neighboring neurons now also tied together neurons in different regions and at opposite ends of the brain. This was comparable to a rule of self-similarity that has been previously found in the human brain (MRI data) and predicts that it extends all the way down to the level of individual neurons. \"This made me re-think how I think about these long-range connections,\" reveals lead researcher Michael Reimann. \"They have been depicted as these blunt cables, connecting or synchronizing whole brain regions. But maybe there is more to them, more specific targeting of individual neurons. And this is what we learned from just a few, relatively course-grained principles. I expect that with improved methods we will find more in the future.\" Researchers at EPFL's Blue Brain Project, a Swiss brain research Initiative have combined two high profile, large-scale datasets to produce something completely new - a first draft model of the rules guiding neuron-to-neuron connectivity of a whole mouse neocortex. Credit: Blue Brain Project \/ EPFL Openly accessible connectome can serve as a powerful null model to compare experimental findings \"We have completed such a first-draft connectome of mouse neocortex by using an improved version of our previously published circuit building pipeline (Markram et al., 2015),\" explains Michael Reimann. \"It has been improved to place neurons in brain-atlas defined 3d spaces instead of hexagonal prisms, taking into account the geometry and cellular composition of individual brain regions. The composition was based on data from the open source Blue Brain Cell Atlas. Further constraints were derived from other openly accessible datasets. Additional constraints that are so far unknown are likely to limit long-range connectivity even more. To start a process of iterative refinement, we made the model and data available to the public. The parameterized constraints on projection strength, mapping, layer profiles and individual axon targeting (i.e. the projection recipe), as well as stochastic instantiations of whole-neocortex micro-connectomes can be found under https:\/\/portal.bluebrain.epfl.ch\/resources\/models\/mouse-projections\". This openly accessible connectome can serve as a powerful null model to compare experimental findings to and as a substrate for whole-brain simulations of detailed neural networks. Sparse connection matrices of several instances of the predicted null model of neocortical long-range connectivity have also been publicly available as this result actively demonstrates the power of making datasets available to the public. Further advancing the case for Simulation The simulation (in-silico) method allowed the scientists to target volumes several orders of magnitude smaller, than would be possible with experimental methods, right down to the innervation of individual neurons with sub-cellular resolution. Going forward, this will allow the simulation of the electrical activity of individual neurons, entire regions or of the entire neocortex. \"This paper builds upon Blue Brain's earlier work on evaluating morphological constraints on connectivity, \"Morphological Diversity Strongly Constrains Synaptic Connectivity and Plasticity,\" (Cerebral Cortex, 2017) and \"Reconstruction and Simulation of Neocortical Microcircuitry' (Cell 2015) explains Blue Brain Founder and Director Prof. Henry Markram. \"The findings enable us continue our simulation experiments at an exponentially increasing rate meaning, we can now build biologically accurate brain models of bigger and bigger brain regions and at a higher and higher resolution thereby further advancing the case for simulation.\" ","News_Title":"Blue Brain finds how neurons in the mouse neocortex form billions of synaptic connections","Topic":"Medicine"}
{"Paper_Body":"Abstract The investigation of two-dimensional atomically thin superconductors\u2014especially those hosting topological states\u2014attracts growing interest in condensed-matter physics. Here we report the observation of spin\u2013orbit\u2013parity coupled superconducting state in centrosymmetric atomically thin 2M-WS 2 , a material that has been predicted to exhibit topological band inversions. Our magnetotransport measurements show that the in-plane upper critical field not only exceeds the Pauli paramagnetic limit but also exhibits a strongly anisotropic two-fold symmetry in response to the in-plane magnetic field direction. Furthermore, tunnelling spectroscopy measurements conducted under high in-plane magnetic fields reveal that the superconducting gap possesses an anisotropic magnetic response along different in-plane magnetic field directions, and it persists much above the Pauli limit. Self-consistent mean-field calculations show that this unusual behaviour originates from the strong spin\u2013orbit\u2013parity coupling arising from the topological band inversion in 2M-WS 2 , which effectively pins the spin of states near the topological band crossing and gives rise to an anisotropic renormalization of the effect of external Zeeman fields. Our results identify the unconventional superconductivity in atomically thin 2M-WS 2 , which serves as a promising platform for exploring the interplay between superconductivity, topology and strong spin\u2013orbit\u2013parity coupling.     Main Two-dimensional (2D) crystalline superconductors serve as wonderful platforms 1 , 2 for the search of intriguing quantum phenomena, such as quantum metallic ground state 3 , 4 , non-reciprocal charge transport 5 , 6 , 7 and large in-plane upper critical field \\(B_{{\\mathrm{C2}}}^{||}\\) (refs. 8 , 9 , 10 , 11 , 12 ). In non-centrosymmetric superconductors, the spin\u2013orbit coupling (SOC) lifts spin degeneracies of the electronic bands, which enhances \\(B_{{\\mathrm{C2}}}^{||}\\) and gives rise to the Zeeman-protected superconductivity 2 , 8 , 10 , 11 , 12 , 13 . One particular example is the Ising superconductivity in liquid-gated MoS 2 (molybdenum disulfide) (refs. 8 , 10 ), 2D NbSe 2 (niobium diselenide) (refs. 11 , 12 ) and monolayer TaS 2 (tantalum disulfide) (ref. 13 ). Beyond non-centrosymmetric superconductors, the study of Ising-protected superconductivity has recently been extended to centrosymmetric superconductors, such as stanene 9 and PdTe 2 (palladium ditelluride) (ref. 14 ) thin films, where the SOC induces spin\u2013orbit locking near the \u0393 point 15 and generates enhanced \\(B_{{\\mathrm{C2}}}^{||}\\) . In general, exploring and understanding the microscopic origin of the novel superconducting states that are resilient to large magnetic fields is of great interest to both fundamental and applied physics. When combining superconductivity and topology, topological superconducting states with Majorana fermions can emerge, which is the central component for fault-tolerant quantum computing 16 , 17 , 18 . Moreover, the further presence of inversion symmetry can enrich the topological structure of a system and enable the manifestation of topological crystalline superconductors 19 , 20 , 21 . Recently, a theory proposed that 2D centrosymmetric superconductors with a topological band inversion 22 , such as the 1T\u2032-WTe 2 (refs. 23 , 24 , 25 , 26 , 27 ), exhibit a distinct type of superconductivity termed as spin\u2013orbit\u2013parity coupled superconductivity 28 . As depicted in Fig. 1a , near the topological band inversion where bands with opposite parities invert, a topological gap opens. In this scenario, the conventional SOC terms that involve only spin and momentum are forbidden by inversion symmetry, but the spin, momentum and parities of the electronic states are allowed to couple together near the topological band inversion, referred to as spin\u2013orbit\u2013parity coupling (SOPC). This SOPC is predicted to produce novel superconductivity near the topological band crossing with both largely enhanced \\(B_{{\\mathrm{C2}}}^{||}\\) and anisotropic spin susceptibility with respect to in-plane magnetic field directions 28 . Experimentally, the emergent van der Waals superconductor 2M-WS 2 (2M phase tungsten disulfide) (ref. 29 ) is believed to be a promising candidate for spin\u2013orbit\u2013parity coupled superconductivity. Monolayer 2M-WS 2 shares an identical structure with 1T\u2032-WTe 2 , but it possesses a stacking mode distinct from other transition metal dichalcogenides 30 . Its bulk material exhibits a high superconducting transition temperature T C of 8.8 K (ref. 30 ) and hosts many intriguing phenomena, including the evidence of anisotropic Majorana bound states 31 and topological surface states 32 . Furthermore, theoretical calculations predict that 2M-WS 2 holds topological edge states with band inversion in the atomically thin limit 33 , 34 , making it an attractive platform to explore exotic superconducting states. Fig. 1: Crystal structure and characterizations of 2M-WS 2 . a , Schematic plot of two bands of opposite parity getting inverted at \u0393 with colour indicating different orbitals (represented by dark blue and red, respectively). The spectrum after projection is depicted to show such topological band inversion that can give rise to edge states. The SOPC superconductivity appears when cooper pairs are formed with the states near the topological band crossing (such as near Fermi level E F ), where SOPC is strong and crucial. b , Top and side views of the crystal structure of 2M-WS 2 , where the a axis (purple dashed line), b axis (pink dashed line), c axis (light blue dashed line) and c* axis (dark blue dashed line oriented perpendicular to the {001} planes) are marked. Tungsten atoms are shifted from their octahedral sites due to the strong intermetallic bonding, forming the visible zigzag metal\u2013metal chains along the a axis. c , Density functional theory calculated d states for the tungsten atoms and p states for the sulfur atoms projected onto the monolayer (left) and bilayer (right) electronic bands of the 2M-WS 2 , where a clear band inversion between W and S bands can be observed around the \u0393 point. d , Optical images of few-layer flakes of 2M-WS 2 cleaved on a SiO 2 \/Si substrate. The number of layers (L) is labelled in the left image and the a axis of each crystal is marked by cyan dashed lines in both the left and right images. Scale bars, 4 \u03bcm. e , TEM bright-field image taken from a section of an exfoliated 2M-WS 2 ribbon-like flake, with the inset being the selected area electron diffraction pattern. It shows that the flake long axis is along the <100> direction ( a axis, as marked by the cyan dashed line). Scale bar, 500 nm. f , Experimental annular dark-field scanning transmission electron microscopy image taken from the 2M-WS 2 flake viewed along the c * axis. The inset shows the simulated image. Scale bar, 0.5 nm. Full size image Here, through magnetotransport and tunnelling spectroscopy measurements conducted under high magnetic fields and low temperatures, we demonstrate the observation of spin\u2013orbit\u2013parity coupled superconductivity in centrosymmetric few-layer 2M-WS 2 . Figure 1b shows the 2M-WS 2 monoclinic structure (space group C 2\/ m ) where the inversion symmetry is preserved 30 . Due to strong intermetallic bonding, the tungsten atoms are shifted from their regular octahedral sites in the sulfur octahedrons, forming the zigzag metal\u2013metal chains along the a axis. Monolayer 2M-WS 2 has an identical structure to 1T\u2032-WTe 2 . However, rather than a glide mirror operation in the 1T\u2032 structure, in bulk 2M-WS 2 , the layers stack differently along the c- axis direction with neighbouring layers offset through a translation operation (Supplementary Fig. 1 ). As a result, there always exists a global inversion centre in atomically thin 2M-WS 2 , as marked in Supplementary Fig. 1c,d . Figure 1c shows the d orbitals of tungsten atoms and p orbitals of sulfur atoms projected onto the monolayer and bilayer electronic bands of 2M-WS 2 (see Supplementary Fig. 4 for thicker layers), where the bands are doubly degenerate due to the combination of inversion symmetry and time-reversal symmetry. Notably, the tungsten d- orbital-dominated bands and the sulfur p- orbital-dominated bands get inverted near the Fermi energy. This feature is essential for the appearance of the SOPC superconductivity 28 . Figure 1d depicts an optical image of exfoliated atomically thin 2M-WS 2 on SiO 2 (silicon dioxide)\/Si substrates. It is worth noting that most of the exfoliated samples have a long-ribbon-like shape, with the long axis parallel to the crystal a axis (tungsten\u2013tungsten chains direction). Figure 1e is a transmission electron microscopy (TEM) bright-field image taken from a 2M-WS 2 nanoribbon, and the corresponding selective-area electron diffraction is displayed in the inset, which shows the single-crystalline feature of the sample and confirms that the long axis direction of the ribbon is indeed along [100], that is, the a axis. The high crystal quality of the nanoribbon is further confirmed by atomic-resolution imaging using scanning TEM, as shown in Fig. 1f . To probe the nature of superconductivity in atomically thin 2M-WS 2 , four-terminal contacts were fabricated ( Methods ). Figure 2a illustrates the temperature-dependent normalized resistance R\/R N of a 2M-WS 2 device (device 01, thickness approximately 4 nm; Supplementary Table 1 ) with an out-of-plane magnetic field B \u22a5 changing from 0 T to 9 T. R N is the normal state resistance right above the superconducting transition. At B \u22a5 = 0 T, the device becomes superconducting at T C = 7.62 K, where T C is defined as the temperature corresponding to 50% R N . Compared with R\/R N \u2013T behaviour measured under the in-plane magnetic field B || (Fig. 2b , \u03b3 = 0\u00b0. Here, \u03b3 is the angle between the in-plane magnetic field and the positive direction of the x axis), an obvious suppression of the superconductivity is observed when the B \u22a5 are applied. This substantially large magnetic anisotropy of the superconductivity is also observed in the temperature-dependent critical fields shown in Fig. 2c , which is notably larger than its bulk counterpart 30 . For the out-of-plane magnetic fields, \\(B_{{\\mathrm{C2}}}^ \\bot\\) , defined as the magnetic field corresponding to 0.5 R N , can be described by the linearized Ginzburg\u2013Landau (GL) expression 35 , 36 , \\(B_{{\\mathrm{C2}}}^ \\bot = {{{{{\\varPhi}}}}}_0\/2\\uppi \\xi \\left( 0 \\right)^2\\left( {1 - T\/T_{\\mathrm{C}}} \\right)\\) , where \u03a6 0 is the magnetic flux quantum and \u03be (0) is the zero-temperature GL in-plane coherence length. From the fitting, we can obtain \u03be (0) = 8.6 nm. Under the in-plane magnetic field, \\(B_{{\\mathrm{C2}}}^{||}\\) follows the 2D GL model 36 , \\(B_{{\\mathrm{C2}}}^{||} = {{{{{\\varPhi}}}}}_0\\sqrt {12} \/2\\uppi \\xi \\left( 0 \\right)d_{{\\mathrm{SC}}}\\sqrt {1 - T\/T_{\\mathrm{C}}}\\) , where d SC is the superconducting thickness. The fitting in Fig. 2c gives d SC = 3.8 nm. Also, the extrapolated zero-temperature \\(B_{{\\mathrm{C2}}}^{||}\\) reaches approximately 35 T, which is beyond the Pauli paramagnetic limit for Bardeen\u2013Cooper\u2013Schrieffer (BCS) superconductors of this device ( B P = 1.86 T C = 14.17 T). Figure 2d shows the normalized magnetoresistance R\/R N under different magnetic field direction as \u03b8 changes from 0\u00b0 to 90\u00b0 at 6.8 K; here the magnetic field rotates in the x \u2013 z plane in Fig. 2b inset and \u03b8 is the angle between the out-of-plane magnetic field and the positive direction of x axis. The extracted angular dependence of B C2 is shown in Fig. 2e , where a sharp cusp is observed when \u03b8 \u2248 0\u00b0. The 2D Tinkham formula 35 (red solid line) and 3D anisotropic GL model 8 (green solid line) can both fit the data at | \u03b8 | > 1\u00b0, but only the former can describe the sharp cusp at | \u03b8 | \u2264 1\u00b0, indicating the 2D superconductivity here. Figure 2f shows the current\u2013voltage ( I \u2013 V ) relation of device 02 (thickness approximately 4 nm) at various temperatures. The I \u2013 V characteristics follow a power law dependence V \u221d I \u03b1 (here \u03b1 is the power-law exponent), which agrees well with the Berezinskii\u2013Kosterlitz\u2013Thouless (BKT) transition model for a 2D superconductor 37 , 38 . At \u03b1 = 3, the BKT transition temperature T BKT = 5 K is obtained (Fig. 2f , inset). All these results above unambiguously conclude that the superconductivity here is of the 2D nature. Fig. 2: Two-dimensional superconductivity in few-layer 2M-WS 2 . a , b , Temperature dependence of the normalized resistance of a 2M-WS 2 device (device 01, thickness approximately 4 nm) measured under various out-of-plane and in-plane (along the a axis of the 2M-WS 2 crystal) magnetic fields, ( a ), under out-of-plane magnetic fields. ( b ), under in-plane magnetic fields. Under zero magnetic field, the device goes to superconducting at T C = 7.6 K, where T C is defined as the temperature corresponding to 50% R N . Inset in ( b ): schematic configuration of the angular-dependent magnetoresistance measurement. \u03b8 denotes the angle between the out-of-plane magnetic field and the positive direction of x axis (the magnetic field rotates in the x\u2013z plane, and the x axis is also the a axis of the 2M-WS 2 crystal). \u03b3 is defined as the angle between the in-plane magnetic field and the positive direction of the x axis. c , Temperature-dependent critical magnetic field B C2 of the device for the magnetic field along the out-of-plane ( \\(B_{{\\mathrm{C2}}}^ \\bot\\) , \u03b8 = 90\u00b0) and in-plane directions ( \\(B_{{\\mathrm{C2}}}^{||}\\) , \u03b8 = 0\u00b0, \u03b3 = 0\u00b0). The violet dashed line is the linear fit to \\(B_{{\\mathrm{C2}}}^ \\bot = {{{{{\\varPhi}}}}}_0\/2\\uppi \\xi \\left( 0 \\right)^2\\left( {1 - T\/T_{\\mathrm{C}}} \\right)\\) . The pink dashed line is the theoretical fit to \\(B_{{\\mathrm{C2}}}^\\parallel = {{{{{\\varPhi}}}}}_0\\sqrt {12} \/2\\uppi \\xi \\left( 0 \\right)d_{{\\mathrm{SC}}}\\sqrt {1 - T\/T_{\\mathrm{C}}}\\) . d , Normalized magnetoresistance of the device with the magnetic field direction rotating from in-plane to out-of-plane direction ( \u03b8 varies from 0\u00b0 to 90\u00b0, T = 7.2 K). e , The extracted angular dependence of B C2 fitted by both 2D Tinkham model \\(\\left( {\\left( {B_{{\\mathrm{C2}}}\\left( \\theta \\right)\\sin \\theta } \\right)\/B_{{\\mathrm{C2}}}^\\parallel } \\right)^2 + \\left| {\\left( {B_{{\\mathrm{C2}}}\\left( \\theta \\right)\\sin \\theta } \\right)\/B_{{\\mathrm{C2}}}^ \\bot } \\right| = 1\\) (red) and the 3D anisotropic GL model \\(\\left( {\\left( {B_{{\\mathrm{C2}}}\\left( \\theta \\right)\\sin \\theta } \\right)\/B_{{\\mathrm{C2}}}^\\parallel } \\right)^2 + \\left( {\\left( {B_{{\\mathrm{C2}}}\\left( \\theta \\right)\\sin \\theta } \\right)\/B_{{\\mathrm{C2}}}^ \\bot } \\right)^2 = 1\\) (green). Inset: a magnified view of the region around \u03b8 = 0\u00b0. f , Current\u2013voltage relation of a 2D 2M-WS 2 device (device 02, thickness approximately 4 nm) at various temperatures plotted on a logarithmic scale. The solid black line corresponds to V \u221d I 3 . Inset: power-law exponent \u03b1 (extracted by fitting the data in e to the power law V \u221d I \u03b1 ) as a function of temperature, where a BKT transition temperature T BKT = 5 K is obtained. Source data Full size image We then study the superconductivity characteristic of 2M-WS 2 under various B || directions. Figure 3a shows the normalized magnetoresistance R\/R N of device 03 (thickness approximately 4 nm) in the vicinity of superconducting transition with the magnetic field rotating in-plane at 7.6 K ( \u03b3 varies from 0\u00b0 to 360\u00b0). A two-fold oscillation behaviour is observed in which the minima of R\/R N appear at \u03b3 \u2248 0\u00b0 and 180\u00b0 (the magnetic field parallel to a axis) and the maxima at \u03b3 \u2248 90\u00b0 and 270\u00b0 (the magnetic field parallel to b axis). The oscillation amplitude becomes more prominent at a larger magnetic field, while it has been greatly suppressed when the temperature is above T C (Extended Data Fig. 1 ). This indicates that the two-fold oscillation of R\/R N comes from the quenching of superconductivity by the magnetic field rather than from the resistance anisotropy. To confirm this, we measure the \\(B_{{\\mathrm{C2}}}^{||}\\) of the device with \u03b3 varying from 0\u00b0 to 360\u00b0. As shown in Fig. 3b , \\(B_{{\\mathrm{C2}}}^{||}\\) also exhibits a two-fold symmetry where its maxima appear at \u03b3 \u2248 0\u00b0 and 180\u00b0 and the minima at \u03b3 \u2248 90\u00b0 and 270\u00b0 ( T = 7.4 K and 7.6 K; different definitions of \\(B_{{\\mathrm{C2}}}^{||}\\) including 0.5 R N and 0.75 R N are used here), consistent with the two-fold oscillation behaviour of R\/R N . The two-fold oscillation behaviour was also observed in the angular-dependent critical current measured under various B || directions (Fig. 3c ), suggesting the anisotropic response of the superconducting gap \u0394 to the B || , which will be further discussed. Fig. 3: Large in-plane upper critical fields and strong anisotropy in 2D 2M-WS 2 . a , Polar plot of angular-dependent normalized sheet resistance for a 2M-WS 2 device (device 03, thickness approximately 4 nm) under various in-plane magnetic fields measured at T = 7.6 K. b , Polar plot of angular-dependent in-plane critical magnetic fields \\(B_{{\\mathrm{C2}}}^{||}\\) at different temperatures. Different definitions of \\(B_{{\\mathrm{C2}}}^{||}\\) that correspond to 0.5 R N and 0.75 R N are used here. c , Polar plot of the critical current ( I C ) for a 2M-WS 2 device (device 04, thickness approximately 7 nm) under various in-plane magnetic fields at T = 2 K. d , Normalized magnetoresistance of a 2M-WS 2 device (device 05, thickness approximately 3 nm) with the magnetic field direction rotating from the crystal a axis to the b axis ( \u03b3 has various values from 0\u00b0 to 90\u00b0, T = 1.6 K). e , f , Normalized magnetoresistance of the device measured at various temperatures with the in-plane magnetic field direction applied along the a axis ( \u03b3 = 0\u00b0; e ) and the b axis ( \u03b3 = 90\u00b0; f ). g , Temperature-dependent critical magnetic field \\(B_{{\\mathrm{C2}}}^{||}\\) of the device with the in-plane magnetic field direction applied along the a axis ( \u03b3 = 0\u00b0) and the b axis ( \u03b3 = 90\u00b0). h , Theoretical calculated \\(B_{{\\mathrm{C2}}}^{||}\/B_{\\mathrm{p}} - T\/T_{\\mathrm{C}}\\) curves for bilayer 2M-WS 2 in the presence of an in-plane magnetic field along \u03b3 = 0\u00b0 (red solid line) and \u03b3 = 90\u00b0 (black solid line). i , Calculated angular dependence of \\(B_{{\\mathrm{C2}}}^{||}\/B_{\\mathrm{p}}\\) at T = 0.5 T C ( T C is set as 7.6 K during the calculation) for the case with SOPC and without SOPC. Source data Full size image Next we explore the \\(B_{{\\mathrm{C2}}}^{||}\\) of 2D 2M-WS 2 through high-magnetic-field measurements. Figure 3d depicts the normalized magnetoresistance of device 05 (thickness approximately 3 nm) at different B || with a tilt angle \u03b3 at 1.6 K. As the B || direction changes from the b axis ( \u03b3 = 90\u00b0) to the a axis ( \u03b3 = 0\u00b0), the superconducting transition gradually shifts to higher magnetic fields, and at \u03b3 = 0\u00b0, even the highest magnetic field (31.1 T) cannot fully quench the superconductivity. We further measure the magnetoresistance isotherms of the device under various temperatures at \u03b3 = 0\u00b0 (Fig. 3e ) and \u03b3 = 90\u00b0 (Fig. 3f ). Compared with \u03b3 = 0\u00b0, a much narrower superconducting transition is observed in the magnetoresistance isotherms at \u03b3 = 90\u00b0. As a result, the \\(B_{{\\mathrm{C2}}}^{||}\\) at \u03b3 = 0\u00b0 is much larger than that in \u03b3 = 90\u00b0 (Fig. 3d ). Also, for both in-plane magnetic field directions, \\(B_{{\\mathrm{C2}}}^{||}\\) goes beyond the Pauli limit ( B P = 14.14 T for this device, green dashed line in Fig. 3g ) at low temperatures: \\(B_{{\\mathrm{C2}}}^{||}\\) at \u03b3 = 0\u00b0 (90\u00b0) is 30.51 T (20.63 T), which is approximately 2.16 (1.46) times the B P . As the inversion symmetry is preserved in atomically thin 2M-WS 2 , the mechanisms that rely on inversion symmetry breaking, such as Ising superconductivity in non-centrosymmetric MoS 2 (refs. 8 , 10 ), NbSe 2 (refs. 11 , 12 ) and TaS 2 (ref. 13 ), the electron scattering process involved in Rashba-type SOC 39 and asymmetric spin\u2013orbit coupling 40 cannot explain the observed enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) here. Besides, \\(B_{{\\mathrm{C2}}}^{||}\\) is strongly anisotropic, and the spin\u2013orbit locking is expected to be absent in 2M-WS 2 , readily ruling out the possibility of type II Ising superconductivity 9 . Note that the superconductivity here is in the clean limit. In Supplementary Text 4 , we show that the effect of spin\u2013orbit scattering also cannot interpret the enhanced \\(B_{{\\mathrm{C2}}}^{||}\\) here. Nevertheless, we find that the observed enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) and its anisotropy are fully consistent with the theory of the SOPC superconductivity 28 . To validate this quantitatively, we constructed a low-energy effective Hamiltonian H N ( k ) ( k labels the momentum) for bilayer 2M-WS 2 (Supplementary Text 7 ), where the model parameters are obtained from the fit to the realistic band structures. Importantly, the SOPC term \\(\\hat {\\mathbf {g}}\\) \u00b7 \u03c3 is taken into account in the model, with \\({{{{{\\hat{{\\mathbf{ g}}}}}}}} = \\left( {A_yk_y,A_xk_x,A_zk_y} \\right)s_x\\) dictated by the C 2 h point group symmetry of 2M-WS 2 . Here, A is the SOPC coefficient, and Pauli matrices, s and \u03c3 , operate on orbital and spin space, respectively. To obtain the \\(B_{{\\mathrm{C2}}}^{||}\\) , we solved the linearized gap equation (see Supplementary Text 8 for detail) $$\\frac{1}{{U_0}} = \\frac{1}{2}{\\int} {\\frac{{d{{{\\mathbf{k}}}}}}{{(2\\uppi )^2}}\\mathop {\\sum}\\limits_{i,j} {\\left| {O_{ij}\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right)} \\right|} ^2\\frac{{1 - f\\left( {E_i\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right)} \\right) - f\\left( {E_j\\left( {{{{\\mathbf{k}}}}, - {{{\\mathbf{B}}}}} \\right)} \\right)}}{{E_i\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right) + E_j\\left( {{{{\\mathbf{k}}}}, - {{{\\mathbf{B}}}}} \\right)}},}$$ where U 0 denotes the attractive interaction strength, f is the Fermi\u2013Dirac distribution function and the overlap function \\(O_{ij}({{{\\mathbf{k}}}},{{{\\mathbf{B}}}}) = \\left\\langle {\\left. {u_i\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right)} \\right|u_j\\left( {{{{\\mathbf{k}}}}, - {{{\\mathbf{B}}}}} \\right)} \\right\\rangle\\) with \\(H_{\\mathrm{N}}\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right)\\left| {u_i\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right) = E_i} \\right|u_i\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right),\\) \\(H_{\\mathrm{N}}\\left( {{{{\\mathbf{k}}}},{{{\\mathbf{B}}}}} \\right) = H_{\\mathrm{N}}\\left( {{{\\mathbf{k}}}} \\right) + H_z,\\) and the Zeeman term \\(H_{\\mathrm{z}} = u_{\\mathrm{B}}{{{\\mathbf{B}}}} \\cdot {{{{\\sigma }}}}\\) capturing the paramagnetic effect. E and u denote the eigenenergy and eigenstate of H N ( k , B ), u B is the Bohr magneton, i , j are band indices. The calculated \\(B_{{\\mathrm{C2}}}^{||}\\) \u2013 T relation at \u03b3 = 0\u00b0 and \u03b3 = 90\u00b0 is shown in Fig. 3h . The corresponding angular dependence of \\(B_{{\\mathrm{C2}}}^{||}\\) is shown in Fig. 3i (blue solid line). Indeed, we found a two-fold anisotropic enhancement in \\(B_{{\\mathrm{C2}}}^{||}\\) , consistent with our experiments. It is worth noting that the observed two-fold symmetric superconducting states under in-plane magnetic fields here, respecting the crystal symmetry C 2 h , do not indicate nematic superconductivity 41 where the superconducting state breaks the three-fold rotational symmetry, as studied in iron-based superconductors 42 , 43 , 44 , doped Bi 2 Se 3 (refs. 45 , 46 , 47 ), superconducting magic-angle graphene 48 and few-layer NbSe 2 (refs. 49 , 50 ). Instead, we point out that this anisotropic enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) in atomically thin 2M-WS 2 directly results from the strong SOPC, which renormalizes the effect of external Zeeman fields near the topological band crossing, leading to this anisotropic enhancement in \\(B_{{\\mathrm{C2}}}^{||}\\) . We have also calculated the normalized spin susceptibility \u03c7 S \/ \u03c7 0 of bilayer 2M-WS 2 in Extended Data Fig. 2a (also see Supplementary Text 10 for details), where distinct differences between \u03b3 = 0\u00b0 and \u03b3 = 90\u00b0 are obtained. Here \u03c7 S is the spin susceptibility of superconducting states, and \u03c7 0 is the Pauli spin susceptibility of free electron gas. It can also be seen that due to the presence of SOPC, the normal state susceptibility \u03c7 N is anisotropically reduced to values that are smaller than \u03c7 0 ( T > T C regime in Extended Data Fig. 2a ). Then the anisotropic enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) becomes apprehensible since, for a SOPC superconductor, \\(B_{{\\mathrm{C2}}}^{||}\\) at low temperature can be estimated using 28 \\(B_{{\\mathrm{C2}}}^{||} \\cong B_{\\mathrm{p}}\\sqrt {\\chi _0\/\\chi _{\\mathrm{N}}}\\) . The angular dependence of \u03c7 N is further displayed in Extended Data Fig. 2b , being consistent with the two-fold enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) in Fig. 3i . To demonstrate that the SOPC is crucial for the anisotropic enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) , we artificially turned off the SOPC and replotted the angular dependence of \u03c7 N and \\(B_{{\\mathrm{C2}}}^{||}\\) . As shown in Extended Data Figs. 2b and 3i , the two-fold anisotropic reduction of \u03c7 N and enhancement of \\(B_{{\\mathrm{C2}}}^{||}\\) indeed disappear. To further understand the superconductivity in 2D 2M-WS 2 , we carried out tunnelling spectroscopy measurements on 2M-WS 2 tunnelling devices under various in-plane magnetic field conditions. A schematic device structure is displayed in the left inset of Fig. 4a (right inset, the optical image of the device), where the gold electrode, aluminium oxide (AlO x ) and 2D superconducting 2M-WS 2 form a typical normal metal\u2013insulator\u2013superconductor junction. Figure 4a demonstrates the normalized tunnelling conductance ( G S \/ G N ) of a few-layer 2M-WS 2 tunnelling device (device 06, thickness approximately 5 nm) under different in-plane magnetic field directions at 2 K ( B || = 9 T; \u03b3 varies from 0\u00b0 to 90\u00b0). There are two symmetric peaks in the G S \/ G N , which are due to the tunnelling of normal electrons through the electron and hole branches of the quasiparticles 35 in the superconducting 2M-WS 2 . Under the same B || magnitude with different in-plane directions ( \u03b3 varies from 0\u00b0 to 90\u00b0), the separation between these two symmetric peaks of the G S \/ G N shrinks, which indicates the size of superconducting gap \u0394 is suppressed anisotropically at different in-plane magnetic field directions. The corresponding 2D plot of the angular-dependent G S \/ G N is shown in Fig. 4b ( \u03b3 varies from 0\u00b0 to 360\u00b0), where a prominent two-fold modulation behaviour is observed. To provide quantitative analysis on the tunnelling conductance spectra, we fit the angular dependence tunnelling conductance spectra to the Blonder\u2013Tinkham\u2013Klapwijk (BTK) model for the normal metal\u2013insulator\u2013superconductor junction 35 , 51 . The deduced angular-dependent superconducting gap \u0394 under B || is illustrated in Fig. 4c (pink dots, B || = 8 T; green dots, B || = 9 T). A clear two-fold symmetry for the angular-dependent \u0394 is observed. The maximum values ( \u0394 8T-MAX \u2248 1.2 meV; \u0394 9T-MAX \u2248 1.15 meV) of the superconducting gap are around \u03b3 = 0\u00b0 and 180\u00b0, while the minimum values ( \u0394 8T-MIN \u2248 0.94 meV; \u0394 9T-MIN \u2248 0.85 meV) are around \u03b3 = 90\u00b0 and 270\u00b0, which agrees well with the observed two-fold symmetry of the angular-dependent \\(B_{{\\mathrm{C2}}}^{||}\\) above because the maximum (minimum) points of \u0394 correspond to the smallest (largest) suppression of the superconductivity under the in-plane magnetic fields in 2D 2M-WS 2 . We note that the anisotropic response of \u0394 to in-plane magnetic field here does not indicate an anisotropic gap under zero magnetic field in 2D 2M-WS 2 . Additionally, in Extended Data Fig. 3 , we show that the temperature-dependent tunnelling spectra of the device follow the BCS theory and give an extrapolated \u0394 (0) \u2248 1.33 meV at zero magnetic field. Fig. 4: Tunnelling spectroscopy of 2D 2M-WS 2 under in-plane magnetic fields. a , Normalized tunnelling conductance as a function of bias voltage ( V B ) of a 2M-WS 2 tunnelling device (device 06, thickness approximately 6 nm) with the in-plane magnetic field direction changing from 0\u00b0 to 90\u00b0. Left inset: schematic structure of the 2M-WS 2 tunnelling device, where the gold electrode, aluminium oxide (AlO x ) and 2D superconducting 2M-WS 2 form a typical normal metal\u2013insulator\u2013superconductor junction. Right inset: an optical image of a typical 2M-WS 2 tunnelling device. Scale bar, 6 \u03bcm. b , Colour plot of the normalized tunnelling conductance of the device 06 measured under different in-plane magnetic field directions ( B || = 9 T; \u03b3 changes from 0\u00b0 to 360\u00b0, T = 2 K). c , The extracted superconducting gap from the tunnelling spectra as a function of \u03b3 under different in-plane magnetic fields (purple and green dots are B || = 8 T and 9 T, respectively). d , Normalized tunnelling conductance of a 2M-WS 2 tunnelling device (device 07, thickness approximately 3 nm) under various in-plane magnetic fields ( \u03b3 = 0\u00b0, T = 1.6 K). e , Colour plot of the normalized tunnelling conductance of the device as a function of bias voltage and in-plane magnetic field (the magnetic field is along a axis of the crystal, \u03b3 = 0\u00b0, T = 1.6 K). f , Extracted magnetic-field - dependent superconducting gap with the magnetic field direction along the a axis and b axis of the crystal, respectively. The error bars representing the height of the \u0394 values are obtained during the BTK fitting. Source data Full size image Figure 4d shows the G S \/ G N as a function of bias voltage V for a 2D 2M-WS 2 tunnelling device (device 07, thickness approximately 3 nm) with the B || up to 32 T ( \u03b3 = 0\u00b0, T = 1.6 K; for \u03b3 = 90\u00b0, see Supplementary Fig. 14 ). The corresponding contour plot of the magnetic-field-dependent G S \/ G N is shown in Fig. 4e . As the B || increases, the tunnelling behaviour becomes less obvious. The extracted \u0394\u2013B || relations along the a axis ( \u03b3 = 0\u00b0, blue circles) and b axis ( \u03b3 = 90\u00b0, pink circles) are illustrated in Fig. 4f , in which a large anisotropy is observed. At \u03b3 = 0\u00b0 and \u03b3 = 90\u00b0, the superconducting gap can persist in the B || up to 32 T and 22.5 T, respectively. These values are far beyond the Pauli limit approximately 13.95 T of this device, which agrees well with the observed largely enhanced \\(B_{{\\mathrm{C2}}}^{||}\\) in atomically thin 2M-WS 2 above. Also, as the B || increases, \u0394 decreases continuously to zero at the upper critical field in both low- and high-temperature regimes (Supplementary Fig. 14 ), indicating a possible second-order superconductor-to-metal phase transition (Supplementary Text 6 ). In Extended Data Fig. 4 , our theoretical calculations show that for atomically thin 2M-WS 2 with spin\u2013orbit\u2013parity coupled superconductivity, the calculated \u0394 \/ \u0394 0 ( \u0394 0 = 1.764 k B T C is the zero-field BCS gap) can indeed persist much beyond the Pauli limit, agreeing with our experimental observations (also see Supplementary Text 11 for the role of weak orbital effect in the SOPC superconductivity). It is noted that the quasiparticle peaks are broadened and the peak separations become less obvious at relatively high magnetic field regimes in Fig. 4d . As a result, the error bar of \u0394 becomes larger in the relatively high magnetic field regimes, reflecting the reduced precision near the critical point. Nevertheless, our tunnelling experiments here still clearly show that the superconducting gap in atomically thin 2M-WS 2 possesses an anisotropic magnetic response along different in-plane magnetic field directions, and it persists much above the Pauli limit. In conclusion, our study demonstrated that atomically thin 2M-WS 2 is an unusual centrosymmetric superconductor that exhibits strong SOPC arising from the topological band inversion, leading to the large and anisotropically enhanced \\(B_{{\\mathrm{C2}}}^{||}\\) . Our findings thus uncover a mechanism for generating an anisotropically enhanced \\(B_{{\\mathrm{C2}}}^{||}\\) in centrosymmetric superconductors with topological band inversions. The application of this mechanism to other centrosymmetric superconducting transition metal dichalcogenides remains to be explored. Methods Sample growth High-quality 2M-WS 2 crystals were synthesized by the topochemical method of K + deintercalation from K 0.7 WS 2 (potassium ion intercalated tungsten disulfide) crystals. The parent compound K 0.7 WS 2 was prepared by stoichiometric mixing of K, S and K 2 S 2 (dipotassium disulfide, prepared via liquid ammonia) in an argon glove box. The mixed reagent was pressed and sealed in an evacuated silica tube with the condition of 10 \u22125 torr. The tube was heated up to 850 \u00b0C for 5 \u00b0C min \u22121 , maintained at this temperature for 3,000 min and cooled to 550 \u00b0C at a rate of 0.1 \u00b0C min \u22121 . The as-synthesized K 0.7 WS 2 (0.1 g) crystals were dispersed and stirred in acidic K 2 Cr 2 O 7 (potassium dichromate, 0.01 mol l \u22121 ) aqueous solution for 1 h at room temperature. Finally, the 2M-WS 2 crystals were obtained after washing in distilled water several times and drying in the vacuum oven. Sample characterization Scanning transmission electron microscopy characterization was conducted using a probe-side corrected FEI Titan G2 80-200 ChemiSTEM microscope, operated at 200 kV with convergence semi-angles of 21 mrad and collection inner (outer) semi-angle ranges of approximately 48 (196) mrad. Selective-area electron diffraction and TEM bright-field images were obtained using the same microscope but operated in TEM mode. For the (scanning)TEM samples, 2D 2M-WS 2 was exfoliated on SiO 2 (285 nm)\/Si substrates and then spin-coated with a poly(methyl methacrylate) polymer layer. After etching away the SiO 2 layer in the KOH solution, the polymer layer with 2D 2M-WS 2 was fished up and transferred onto a copper TEM grid with a holey carbon support film. Device fabrication The device fabrication process was carried out in the cleanroom. Different thicknesses of 2M-WS 2 were obtained through mechanical exfoliation of bulk single crystals onto pre-patterned SiO 2 (285 nm)\/Si substrates using polydimethylsiloxane stamps. Multi-terminal electrical contacts were fabricated by standard electron beam lithography (EBL) process using poly(methyl methacrylate)\/methyl methacrylate bilayer polymer and subsequent metal deposition of Ti\/Au (5 nm\/80 nm). For the tunnelling device, an EBL and metal deposition process was performed to fabricate the first pair of electrodes on the exfoliated atomically thin 2M-WS 2 , and a thin layer of Al (0.1\u20130.5 nm) was then deposited and oxidized in the air to act as an insulating layer. Another pair of electrodes were subsequently fabricated to form the normal metal\u2013insulator\u2013superconductor junction structure. Transport measurements Four-terminal temperature-dependent magnetotransport, I \u2013 V and differential conductance measurements were carried out in a Physical Property Measurement System (Quantum Design). Typically, the external filter is not involved in the measurement circuit, while twisted pair of wiring is used to reduce the environmental noise. High-magnetic-field transport experiments were performed in water-cooled resistive magnets at the High Magnetic Field Laboratory in Hefei and the National High Magnetic Field Laboratory in Tallahassee. During the measurements, a rotating in-plane magnetic field with a misalignment of <0.1\u00b0 was applied to the devices using rotating probes and in-plane sample holders (Supplementary Fig. 2 ). A multi-channel lock-in amplifier system (SR830 and SR865) was used for the measurement of alternating current (a.c.) resistance. During the a.c. resistance measurements, the applied current frequency is between 10 and 100 Hz. Current-driven I \u2013 V measurements were performed using Agilent 2912 and Keithley 2182A. In differential conductance measurements, a direct current bias current (generated by Agilent 2912) superimposed with a small a.c. bias current was applied to the normal metal\u2013insulator\u2013superconductor junction device through one of the split electrodes. The applied small a.c. bias current is chosen to be as small as possible but also large enough to obtain a decent signal-to-noise ratio, typically \u2264500 nA. The a.c. voltage drop was collected via another electrode of the split pair using SR830 and SR865 (see Supplementary Text 1 for detail). Density functional theory calculations The Vienna Ab initio Simulation Package 52 was used to perform the density functional theory calculations 53 . The projector augmented wave method 54 and the Perdew\u2013Burke\u2013Ernzerhof exchange\u2013correlation functional in the generalized gradient approximation 55 , 56 were adopted, and the SOC could be self-consistently included when necessary. For the calculation of bulk 2M-WS 2 , a 10 \u00d7 10 \u00d7 4 k -mesh was used, while in the case of monolayer and bilayer 2M-WS 2 , the adopted k -mesh was 10 \u00d7 10 \u00d7 1, and a 15-\u00c5-thick vacuum layer was added along the out-of-plane direction. The crystal structure was obtained from the experimental results 30 . Data availability The data that support the plots within this paper and other findings of this study are available from the corresponding authors upon reasonable request. Source data are provided with this paper. ","News_Body":"In recent years, many physicists and material scientists have been studying superconductors, materials that can conduct direct current electricity without energy loss when cooled under a particular temperature. These materials could have numerous valuable applications, for instance generating energy for imaging machines (e.g., MRI scanners), trains, and other technological systems. Researchers at Fudan University, Shanghai Qi Zhi Institute, Hong Kong University of Science and Technology, and other institutes in China have recently uncovered a new mechanism to generate anisotropically-enhanced in-plane upper critical field in atomically thin centrosymmetric superconductors with topological band inversions. Their paper, published in Nature Physics, specifically demonstrated this mechanism on a thin layer of 2M-WS2, a material that has recently attracted much research attention. \"In 2020, a paper by our theoretical collaborator Prof. K.T. Law proposed that 2D centrosymmetric superconductors with a topological band inversion, such as 1T\u2032-WTe2 exhibit a distinct type of superconductivity, called spin-orbit-parity coupled (SOPC) superconductivity,\" Enze Zhang, one of the researchers who carried out the study, told Phys.org. \"SOPC is predicted to produce novel superconductivity near the topological band crossing with both largely enhanced and anisotropic spin susceptibility with respect to in-plane magnetic field directions. At that time, we were conducting research on the superconducting properties of atomically thin 2M-WS2, so after talking with Prof. K.T. Law, we felt that the emergent van der Waals superconductor 2M-WS2 would most likely be a promising candidate for spin-orbit-parity coupled superconductivity.\" The structure of monolayer 2M-WS2 is identical to that of 1T\u2032-WTe2, the material previously investigated by Prof. Law and his team. 2M-WS2, however, has a unique stacking mode, which distinguishes it from other transition metal dichalcogenides. The researchers previously found that in its bulk form, this material exhibit a high superconducting transition temperature TC of 8.8 K. In addition, theoretical calculations suggested that atomically thin layers of 2M-WS2 hold topological edge states with band inversion. In their experiments, Zhang and his colleagues measured the in-plane upper critical field at a high magnetic field and confirmed the violation of the Pauli limit law. They also observed a strongly anisotropic two-fold symmetry in the material, in response to the in-plane magnetic field direction. \"Tunneling experiments conducted under high in-plane magnetic fields also showed that the superconducting gap in atomically thin 2M-WS2 possesses an anisotropic magnetic response along different in-plane magnetic field directions, and it persists much above the Pauli limit,\" Zhang explained. \"Using self-consistent mean-field calculations, our theoretical collaborators conclude that these unusual behaviors originate from the strong spin-orbit-parity coupling arising from the topological band inversion in 2M-WS2.\" The researchers' experiments spanned across several steps. Firstly, the team performed magneto-transport measurements on atomically thin 2M-WS2 and found that its in-plane upper critical field is not only far beyond the Pauli paramagnetic limit, but also exhibits a strongly anisotropic two-fold symmetry in response to the in-plane magnetic field direction. Subsequently, they used tunneling spectroscopy to collect measurements under high in-plane magnetic fields. These measurements revealed that the superconducting gap in atomically thin 2M-WS2 possesses an anisotropic magnetic response along different in-plane magnetic field directions, which persists much above the Pauli limit. Finally, the researchers performed a series of self-consistent mean-field calculations to better understand the origin of the unusual behaviors they observed in their sample. Based on their results, they concluded that these behaviors originate from the strong spin-orbit-parity coupling arising from the topological band inversion in 2M-WS2, which effectively pins the spin of states near the topological band crossing and renormalizes the effect of external Zeeman fields anisotropically. \"We uncovered a new mechanism for generating an anisotropically-enhanced in-plane upper critical field in atomically thin centrosymmetric superconductors with topological band inversions, highlighting 2D 2M-WS2 as a wonderful platform for the study of exotic superconducting phenomena such as higher-order topological superconductivity and further device applications,\" Zhang said. \"The novel properties found here are highly nontrivial as they directly reflect a strong SOPC inheriting from the topological band inversion in the normal state of 2M-WS2, which had been ignored for many years in previous studies of centrosymmetric superconductors.\" In recent years, more research teams worldwide have been exploring the properties and mechanisms of centrosymmetric superconducting transition metal dichalcogenides (TMDs), such as monolayer superconducting 1T\u2032-MoS2, and 1T\u2032-WTe2, due to the characteristic co-existence of topological band structure and superconductivity within them. The recent paper by Zhang and his colleagues could pave the way towards the exploration of large enhanced and strongly anisotropic in-plane upper critical fields, which could further improve the current understanding of these materials' exotic physics. \"We now plan to explore the usual superconducting properties (such as the in-plane upper critical field and tunneling spectroscopy behavior at high magnetic field) of more atomically thin centrosymmetric superconductors with topological band inversions,\" Zhang added. ","News_Title":"Study observes spin-orbit-parity coupled superconductivity in thin 2M-WS2","Topic":"Physics"}
{"Paper_Body":"Abstract Fibre and bulk optical isolators are widely used to stabilize laser cavities by preventing unwanted feedback. However, their integrated counterparts have been slow to be adopted. Although several strategies for on-chip optical isolation have been realized, these rely on either integration of magneto-optic materials or high-frequency modulation with acousto-optic or electro-optic modulators. Here we demonstrate an integrated approach for passively isolating a continuous-wave laser using the intrinsically non-reciprocal Kerr nonlinearity in ring resonators. Using silicon nitride as a model platform, we achieve single ring isolation of 17\u201323 dB with 1.8\u20135.5-dB insertion loss, and a cascaded ring isolation of 35 dB with 5-dB insertion loss. Employing these devices, we demonstrate hybrid integration and isolation with a semiconductor laser chip.     Main The effort to integrate high-performance optical systems on-chip has made tremendous progress in recent years. Advances in ultra-low-loss photonic platforms 1 , nonlinear photonics 2 and heterogeneous material integration 1 , 3 have enabled fully integrated turnkey frequency-comb sources 1 , 4 , on-chip lasers with hertz linewidth 5 , terabits-per-second (Tbps) communications on-chip 6 , 7 , on-chip optical amplifiers 8 and much more. Although these systems will continue to improve, a lack of integrated optical isolation limits their performance. Optical isolators allow for the transmission of light in one direction while preventing transmission in the other. This non-reciprocal behaviour is critical in optical systems in order to stabilize lasers and reduce noise by preventing unwanted back-reflection 9 . In traditional fibre and bulk optical systems, non-reciprocal transmission is achieved by the use of Faraday-effect-induced non-reciprocal polarization rotation under an external magnetic field 9 , 10 , 11 . This approach can be replicated on-chip by integrating magneto-optic materials into waveguides 10 . However, the scalability of the approach remains a substantial challenge due to the required custom material fabrication and lack of complementary metal\u2013oxide\u2013semiconductor (CMOS) compatibility. Furthermore, magneto-optic materials require a very strong magnet for their operation due to their weak effects in the visible to near-infrared (NIR) wavelength range 12 , 13 and are therefore difficult to operate in an integrated platform. More recently, there has been remarkable progress in integrating magnet-free isolators using an active drive to break reciprocity. This drive has taken the form of a synthetic magnet 14 , 15 , stimulated Brillouin scattering 16 , 17 and spatio-temporal modulation 18 , 19 , 20 . However, the requirement for an external drive increases the system complexity, often requires additional fabrication, and consumes power. Additionally, high-power radiofrequency drives contribute large amounts of electromagnetic background that can interfere with the sensitive electronics and photodetection in photonic integrated circuits. This poses inevitable challenges to the scalability and adoption of such devices. Therefore, to maximize the scalability and integration into current photonic integrated circuits, an ideal isolator would be fully passive and magnet-free. Optical nonlinearity is a promising path towards breaking reciprocity 21 , 22 , 23 , 24 , 25 , and is inherently present in most widely utilized photonic platforms, such as silicon nitride 2 , 26 , silicon 22 , gallium phosphide 27 , tantala 28 , silicon carbide 29 , 30 and lithium niobate 31 , 32 . Unfortunately, due to dynamic reciprocity, many proposals for non-reciprocal transmission using optical nonlinearities cannot function as isolators 33 . However, by carefully choosing the mode of operation, isolation using optical nonlinearity is possible and has been demonstrated with discrete components 24 . In this Article we demonstrate integrated continuous-wave isolators using the Kerr effect present in thin-film silicon-nitride ring resonators. The Kerr effect breaks the degeneracy between the clockwise and counterclockwise modes of the ring and allows for nonreciprocal transmission. These devices are fully passive and require no input besides the laser that is being isolated. As such, the only power overhead is the small insertion loss from coupling of the ring resonator. Additionally, many integrated optical systems that would benefit from isolators already have high-quality silicon-nitride or commensurate components and could easily integrate this type of isolator with CMOS-compatible fabrication 1 . By varying the coupling of the ring resonators we can trade off insertion loss and isolation. As two examples, we demonstrate devices with a peak isolation of 23 dB with 4.6-dB insertion loss and isolation of 17 dB with a 1.3-dB insertion loss with 90 mW of optical power. As we are using an integrated photonics platform, we can reproducibly fabricate and cascade multiple isolators on the same chip, allowing us to demonstrate two cascaded isolators with an overall isolation ratio of 35 dB. Finally, we butt-couple a semiconductor laser-diode chip to the silicon-nitride isolators and demonstrate optical isolation in a system on a chip. Theory of operation The Kerr effect is the change in refractive index of a material due to its third-order nonlinearity in susceptibility, \u03c7 (3) . In the presence of two electric fields, the nonlinear polarization corresponding to this term is given by \\({P}^{(3)}{(t)}={\\epsilon }_{0}{\\chi }^{(3)}{({E}_{1}{\\rm{e}}^{-i{\\omega }_{1}t}+{E}_{2}{\\rm{e}}^{-i{\\omega }_{2}t}+{\\rm{c.c.}})}^{3}\\) . Expanding this polynomial and keeping only the terms with the same frequencies, we find that \\({P}^{(3)}({\\omega }_{1})={3}{\\epsilon }_{0}{\\chi }^{(3)}(| {E}_{1}{| }^{2}+{2}| {E}_{2}{| }^{2}){E}_{1}{\\rm{e}}^{-i{\\omega }_{1}t}\\) and \\({P}^{(3)}({\\omega }_{2})={3}{\\epsilon }_{0}{\\chi }^{(3)}({2}| {E}_{1}{| }^{2}+| {E}_{2}{| }^{2}){E}_{2}{\\rm{e}}^{-i{\\omega }_{1}t}\\) . Thus, there is an effective increase in the refractive index proportional to the optical intensity. Critically, the index change differs by a factor of two depending on the source of the optical power. The field that is degenerate with the mode under consideration contributes a refractive index increase of 3 \u03f5 0 \u03c7 (3) | E | 2 , self-phase modulation (SPM). The field that is non-degenerate contributes a refractive index increase of 6 \u03f5 0 \u03c7 (3) | E | 2 , cross-phase modulation (XPM). This difference provides an intrinsic non-reciprocity. If a strong pump beam is sent through a waveguide, and a weak probe is sent through in the other direction, the probe will accrue an additional phase shift due to the Kerr effect that is twice that of the pump. We can apply the same principle to construct an isolator. Consider the set-up shown in Fig. 1a . A strong pump (red) is sent through a ring resonator with degenerate clockwise and counterclockwise resonances. This pump heats the ring, leading to a reciprocal thermo-optic increase in refractive index and corresponding decrease in resonance frequency. Additionally, the high power in the ring leads to an SPM of the clockwise mode and an XPM of the counterclockwise mode. This shifts the resonance of the counterclockwise mode twice as far as the clockwise pump mode. The now split resonances allow for a near-unity transmission in the pump direction but substantially reduce the transmission at the same frequency in the reverse direction (blue). This reduction is represented by the Lorentzian lineshape of the cavity. Following ref. 24 , we can calculate the expected isolation by combining this transmission reduction with the SPM resonance shift: $${I}={\\frac{1}{1+{(2Q\\frac{{{\\Delta }}\\omega }{{\\omega }_{0}})}^{2}},}$$ (1) where the shift \u0394 \u03c9 is given by $${{\\Delta }}{\\omega }={\\omega }_{0}{\\frac{{n}_{2}}{n}\\frac{Q\\lambda }{2\\uppi {V}_{{{{\\rm{mode}}}}}}}{\\eta }{P}_{{{{\\rm{in}}}}},$$ (2) where Q is the loaded quality factor of the ring, n 2 is the nonlinear refractive index, n is the linear refractive index, V mode is the mode volume of the ring, and \u03b7 is the coupling efficiency of the pump to the ring. We can characterize the power required for isolation by considering the input power required to isolate by 3 dB. We will refer to this power level as the isolation threshold, P thresh , given by $${P}_{{{{\\rm{thresh}}}}}={\\frac{n}{{n}_{2}}}{\\frac{\\uppi {V}_{{{{\\rm{mode}}}}}}{{Q}^{2}\\lambda \\eta }}.$$ (3) Fig. 1: Theory of operation. a , Schematic showing the operation principle of the integrated nonlinear optical isolators. Plot shows transmission (T) vs. frequency (\u03c9). b , Illustration of the isolator coupled directly to the laser that drives it, in the presence of the laser only (red), unwanted backward transmission only (blue) and the laser with backward transmission. When the laser is on, the backward transmission is no longer resonant and the laser is isolated. c , Image of a silicon-nitride device. Scale bar, 100 \u03bcm. d , Theoretical (dashed line) and experimental (blue data points) backwards transmission with varied input pump power and at maximum pump detuning, illustrating the Lorentzian transmission shape. Full size image This isolation is achieved solely by the intrinsic non-reciprocity of the ring, so no additional power is required for operation. Critically, the operation is unaffected by dynamic reciprocity. When a backwards-propagating signal is at the same frequency as the pump, dynamic reciprocity does not apply, and when a signal is at a different frequency from the pump, there is reciprocal but near-zero transmission (Supplementary Section 1 ). Additionally, it is important to note that this isolation ratio holds true not only for backwards-propagating signals with powers that are small compared to the pump, but even for backwards signals commensurate to and stronger than the pump. When there is already pump power circulating in the ring, the backwards wave is not resonant with the cavity. Thus, the required input power to negate the mode splitting is in fact many times higher than the power of the pump 34 , 35 . Although the bandwidth of the isolation is limited by the resonance splitting, it is possible to add an additional linear filter that indefinitely extends the isolation bandwidth (Supplementary Section 2 ). Without this additional filter, the 3-dB bandwidth of the isolation can be given by $${\\omega }_{3{{{\\rm{dB}}}}}={2}{{\\Delta }}{\\omega }-{\\sqrt{2}}{\\sqrt{{{\\Delta }}{\\omega }^{2}-\\frac{{\\omega }_{0}^{2}}{4{Q}^{2}}}},$$ (4) which is on the order of the linewidth of the cavity and grows as the isolation increases. As this type of isolator requires continuous pump power (either with a continuous-wave pump or a pump that is pulsed at the ring free-spectral range), but no additional driving or modulation, it is ideal for directly isolating the output of a laser (Fig. 1b ). The laser itself acts as the sole driver of isolation, and the device incurs no power consumption, losing power only to the small insertion loss from traversing the ring. There is no need for strong magnetic fields, active optical modulation or high-power radiofrequency drives, and device operation is not limited to a single photonic platform or wavelength range. Device integration and measurement As the isolation depends on Q 2 , the mode volume, the nonlinear refractive index and the input power, it is critical to implement devices with a material that can support high-quality microresonators, has an appreciable \u03c7 (3) and can handle very high optical intensities without incurring loss. Here we demonstrate integrated isolators using silicon nitride as a model system, as it has become one of the most prominent platforms for integrated nonlinear photonics 1 . We use thin-film silicon nitride (<400 nm), as it has the potential for CMOS integration compatibility given the lower film stress present 36 , 37 . In addition, the thin-silicon-nitride process allows for geometric dispersion properties that easily lead to a strong normal dispersion 37 , allowing us to suppress spurious optical parametric oscillation (Supplementary Section 3 ). To maximize Q 2 \/ V mode while keeping the isolator compact, we use a ring diameter of 200 \u03bcm, as shown in Fig. 1c . To measure the isolation of these devices, we use the pump\u2013probe set-up shown in Fig. 2a . As the pump and probe are sourced from the same laser, they have the same optical frequency. For the first set of measurements, shown in Fig. 2b,c , the pump and probe wavelengths are scanned across the ring resonance. In Fig. 2d the pump is kept fixed. We send a high-power pump through the ring and simultaneously modulate and send a low-power probe through the ring in the opposite direction. We then scan the pump and probe across the resonance and read the reverse transmission using a lock-in amplifier. During the scan, the pump thermally pulls the ring until the ring unlocks at the peak of its resonance 38 . As the laser approaches the frequency of the ring, more optical power couples to the resonance. As a consequence of a small linear material absorption, this heats the ring and detunes the resonance further away from the laser. This continues until the laser frequency matches that of the resonance and is coupled maximally to the ring. Once the laser detunes past this point, the power in the ring begins to decrease, allowing the ring to cool and collapse back to the original resonance position. By monitoring the probe transmission at the resonance peak, we can obtain a direct measurement of the isolation (Supplementary Section 4 ). Additionally, by varying the pump power, we can measure the power-dependent isolation (Fig. 2b,c ). As the pump power is increased, the peak isolation is redshifted and scales as a Lorentzian. We find excellent agreement between our measurements (Fig. 2b ) and the expected transmission from a simple model of a thermally pulled ring with a Lorentzian power-dependent isolation (Fig. 2b , inset). Fig. 2: Isolation measurement. a , Schematic of the measurement set-up for characterizing the nonlinear optical isolators. EDFA, erbium-doped fibre amplifier; EOM, electro-optic modulator. PC, polarization controller; LO, 90-kHz electronic oscillator. b , Pump-power-dependent measurement of backwards transmission. Inset: theoretical pump power dependence. The line colours in the inset correspond to the colours in the main panel. c , Corresponding theoretical (dashed line) and experimental (blue data points) device isolation. Data-point colours correspond to the colours used in b . d , Pulsed backward transmission measurement with increasing pump power (0 mW, 40 mW, 80 mW). The inset shows a magnification of the section of the plot in the dashed box. e , Theoretical (dashed line) and experimental (blue data points) frequency dependence of the backwards transmission. Here, the probe is split into two sidebands with an EOM, and this sideband separation is swept with a frequency synthesizer. As expected, the backwards frequency response is shifted in proportion to the pump power. Full size image We also validate the operation of the isolator with a static pump frequency. The ring remains locked to the laser, and we can directly measure the backwards transmission of the device by sending optical pulses at the same frequency as the pump (Fig. 2d ). Here, the resonator locking is initiated by tuning the laser frequency, but this can also be achieved by thermally tuning the ring (Supplementary Section 5 ). As the maximum transmission and isolation occur at the peak of the resonance, where the resonance can no longer follow the laser, locking can be disturbed by changes in ambient temperature. This can be alleviated through thermal stabilization of the ring 39 . However, the large thermal pulling allows ample overhead in laser detuning: for this device under 90-mW input power, a 1-GHz detuning from the unlocking point corresponds to only a 0.3-dB reduction in isolation and a 0.15-dB increase in insertion loss. Because of this, we are able to operate close to the maximum transmission without any temperature control of the photonic isolator chip and remain stably locked over the duration of the experiment. Finally, we can measure the frequency response of the isolation by modulating the probe using an electro-optic modulator (EOM). This generates sidebands that we can sweep across the resonance. As only the redshifted sideband will be resonant with the redshifted backwards resonance, we can sweep the sideband frequency to map out the frequency response (Fig. 2e ). We find, as expected from the XPM modulation, that the backward transmission has a Lorentzian profile detuned from the pump by the SPM resonance shift, \u0394 \u03c9 . To maximize the performance of these isolators, it is important to consider both insertion loss and isolation. In this system, these are determined by the coupling rates to the two waveguides, \u03ba 1 and \u03ba 2 , and the scattering rate of the ring into the environment, \u03b3 . Ideally, all power is transmitted into the ring, and all of the power in the ring is transmitted to the output port. This is made possible by increasing the ring coupling rates, but this has the effect of reducing the Q of the resonance and thus lowering the isolation. To maximize the isolation, the power must be transferred to the ring efficiently, but the coupling rates should be minimized to preserve the Q . This, of course, increases insertion loss. More precisely, the ring sees a power of \\({\\frac{4{\\kappa }_{1}({\\kappa }_{2}+\\gamma )}{{({\\kappa }_{1}+{\\kappa }_{2}+\\gamma )}^{2}}}\\) , the Q is impacted by a factor of \\({\\frac{1}{{\\kappa }_{1}+{\\kappa }_{2}+\\gamma }}\\) , and the insertion loss is given by \\({\\frac{4{\\kappa }_{1}{\\kappa }_{2}}{{({\\kappa }_{1}+{\\kappa }_{2}+\\gamma )}^{2}}}\\) . To interrogate this trade-off experimentally, we fabricated an array of 16 air-clad silicon-nitride isolators with varying coupling strengths and coupling asymmetries (Fig. 3b,c ). We find these devices have an intrinsic quality factor of ~5 million (Supplementary Section 7 ). As expected, devices with weaker and more asymmetric coupling show higher isolation, but also higher insertion loss. We highlight the performance of two of the devices\u2014a device with 1.8-dB insertion loss and an isolation threshold of 12.9 mW, and a device with 5.5-dB insertion loss and an isolation threshold of 6.5 mW (Fig. 3d ). These devices show peak isolations at 90 mW of 16.6 dB and 23.4 dB, respectively. Fig. 3: Performance optimization. a , Schematic of the isolator ring illustrating the key parameters: \u03ba 1 , \u03ba 2 and \u03b3 \u2014the input coupling rate, output coupling rate and intrinsic loss rate, respectively. b , Heatmaps showing the measured insertion loss and peak isolation for varied coupling rates \u03ba 1 and \u03ba 2 . The colour bar limits are set by the min and max of each plot (white: 1.0-dB insertion loss, 3.3-dB peak isolation; dark blue: 10.1-dB insertion loss, 23.4-dB peak isolation). Well-performing parameters are highlighted with blue, green and orange circles. c , Correlations of the isolation and insertion losses from b . d , Pump-power-dependent isolation for the three highlighted rings. Full size image As these isolators are integrated and can have low insertion loss, it is possible to fabricate and cascade multiple devices on the same chip, enabling an exponential enhancement in isolation (Fig. 4a ). To test this, we fabricated two rings, the second slightly red-detuned from the first. This allows for the thermal shift to bring both rings onto resonance and lock them there. The isolation is maximized and overall insertion loss minimized at a given pump power when the second ring is red-detuned by a factor of the single ring insertion loss times the thermal pulling of the first ring (Supplementary Section 8 ). To characterize the isolation of cascaded rings, we first measure the power-dependent isolation of a single ring (Fig. 4c ), using the same pump\u2013probe measurement as described in Fig. 2a . We then repeat this measurement for two cascaded rings, one slightly red-detuned from the second. These results are shown in Fig. 4d,e . The multiplicative effect of the cascaded rings enables us to achieve an isolation of 35 dB with an insertion loss of ~5 dB. Fig. 4: Isolator cascade. a , Schematic of cascaded isolator rings. b , Optical micrograph of fabricated cascaded isolator rings. Scale bar, 200 \u03bcm. c , Theoretical (dashed line) and experimental (blue data points) power-dependent single-ring isolation. d , Transmission in the forwards and backwards direction from the cascaded isolator rings with a 110-mW pump. e , Theoretical (dashed line) and experimental (blue data points) power-dependent isolation of cascaded rings. The theoretical fit is calculated by multiplying the isolation ratio from a single ring to a second ring redshifted from the first. Measurements start from 40 mW, as this much pump power is needed to overlap the two ring resonances. Full size image Finally, we demonstrate isolation using a distributed-feedback (DFB) laser chip (Fig. 5a ). To maximize the on-chip pump power, we couple the DFB laser to the chip using an oxide-clad inverted taper designed to match the output mode of the laser 4 . We first characterize the isolation by coupling the DFB laser to a lensed fibre and performing a pump\u2013probe measurement, similar to Fig. 2a . To tune the DFB laser across the ring resonance we modulate its temperature using a Peltier device and a thermistor for feedback. We observe isolation up to 13.6 dB with 65-mW input power (Fig. 5b ), slightly lower than before due to the small reduction in the Q factor. We then directly butt-couple the DFB laser and isolator, and thermally lock the ring to the laser. To verify its isolation, we send pulses backwards through the device using a secondary laser, and measure their transmission (Fig. 5c,d ). To ensure that the secondary laser is at the same frequency as the DFB, we mix the laser outputs on a photodiode and minimize their beat-tone. Fig. 5: DFB hybrid integration. a , Optical image of hybrid integration of a DFB laser with the isolator. b , Power-dependent isolation measured with the amplified DFB laser. Blue data points show measurement and dashed line shows theoretical fit. c , Schematic of experimental measurement set-up for direct measurement of the hybrid integrated DFB\u2013isolator operation. d , Transmission of backwards pulses with the directly coupled DFB laser on and off. Full size image Conclusion We have demonstrated on-chip optical isolators utilizing the Kerr effect that are fully passive. By tuning the coupling parameters we trade off between insertion loss and isolation, demonstrating devices with an insertion loss of only 1.8 dB with 17-dB isolation, and single-ring isolation of up to 23 dB. Due to the integrated nature of these isolators, they can be easily cascaded to improve performance. By cascading two rings, we achieve 35-dB isolation with 5-dB insertion loss. Finally, we demonstrate the application of such a device to isolate the output of an edge-coupled DFB laser chip. As these devices are fully passive and magnet-free, they require no external drive and can operate without generating any electromagnetic interference or magnetic field background. In spite of this, their performance is still competitive with state-of-the-art active and magnetic integrated isolators (Supplementary Table 1 ) 11 , 13 , 15 , 18 , 19 , 20 , 40 , 41 , 42 . Furthermore, better-controlled fabrication from commercial foundries will allow for higher quality factors 43 and enable cascading of more than two rings, pushing the power threshold for 20-dB isolation down to below 2 mW and the achievable isolation to over 70 dB (Supplementary Section 10 ). As many hybrid and heterogeneously integrated optical systems already contain high-quality photonics in Kerr materials, this type of isolator can be immediately incorporated into state-of-the-art integrated photonics. Methods Device fabrication Thin-film silicon nitride (310 nm) was deposited on a silicon dioxide\/silicon carrier wafer using low-pressure chemical vapour deposition. The isolator device patterns were defined using electron-beam lithography (JEOL JBX-6300FS), using ZEP520A as the electron resist. Post development, the patterns were transferred onto silicon nitride by inductively coupled plasma etching with CHF 3 \/CF 4 chemistry. After the etch, the resist was removed using Piranha solution, and the silicon-nitride chips were subsequently annealed in a N 2 environment at 1,100 \u00b0C. Isolator measurements A scanning laser (Toptica) was split into two paths using a directional coupler. One path served as the pump and one as the probe. The pump path was passed through a polarization controller and was amplified by an erbium-doped fibre amplifier (EDFA; IPG) before being sent to the chip. The probe path was modulated using an EOM (Optilab), passed through a polarization controller, and amplified by an EDFA (Thorlabs) before being sent to the chip. The backwards transmission was measured using a photodiode (Thorlabs), a lock-in amplifier (Stanford Instruments) and an oscilloscope (Rigol). Inverse-designed grating couplers optimized for transmission at 1,550 nm were used to couple to and from the chip. To minimize leakage from the probe input fibre to the detection fibre, the grating inputs were oriented perpendicular to each other. For fixed and scanning measurements of power-dependent isolation, the EOM was modulated with a 90-kHz signal from an arbitrary waveform generator (Rigol) and the same signal was used for lock-in detection. For frequency-dependent measurements, the EOM was driven by an amplified (Minicircuits) 90-kHz lock-in signal mixed (Minicircuits) with a high-frequency modulation from a frequency synthesizer (Rohde and Schwarz). DFB operation The DFB laser was driven by a precision source (Keithley) with 380-mW electrical power. To thermally stabilize and tune the frequency of the laser, the laser mount was cooled by a Peltier device using a 10-k\u03a9 thermistor to provide feedback control with a temperature controller (Thorlabs). Data availability All data are available from the corresponding authors upon reasonable request. ","News_Body":"Lasers are transformational devices, but one technical challenge prevents them from being even more so. The light they emit can reflect back into the laser itself and destabilize or even disable it. At real-world scales, this challenge is solved by bulky devices that use magnetism to block the harmful reflections. At chip scale, however, where engineers hope lasers will one day transform computer circuitry, effective isolators have proved elusive. Against that backdrop, researchers at Stanford University say they have created a simple and effective chip-scale isolator that can be laid down in a layer of semiconductor-based material hundreds of times thinner than a sheet of paper. \"Chip-scale isolation is one of the great open challenges in photonics,\" said Jelena Vu\u010dkovi\u0107, a professor of electrical engineering at Stanford and senior author of the study appearing Dec. 1 in the journal Nature Photonics. \"Every laser needs an isolator to stop back reflections from coming into and destabilizing the laser,\" said Alexander White, a doctoral candidate in Vu\u010dkovi\u0107's lab and co-first author of the paper, adding that the device has implications for everyday computing, but could also influence next-generation technologies, like quantum computing. Small and passive The nanoscale isolator is promising for several reasons. First, this isolator is \"passive.\" It requires no external inputs, complicated electronics, or magnetics\u2014technical challenges that have stymied progress in chip-scale lasers to date. These additional mechanisms lead to devices that are too bulky for integrated photonics applications and can cause electrical interference that compromises other components on the chips. Another advantage is that the new isolator is also made from common and well-known semiconductor-based material and can be manufactured using existing semiconductor processing technologies, potentially easing its path to mass production. The new isolator is shaped like a ring. It is made of silicon nitride, a material based on the most commonly used semiconductor\u2014silicon. The strong primary laser beam enters the ring and the photons begin to spin around the ring in a clockwise direction. At the same time, a back-reflected beam would be sent back into the ring in the opposite direction, spinning in a counterclockwise fashion. \"The laser power that we put in circulates many times and this allows us to build up inside the ring. This increasing power alters the weaker beam, while the stronger one continues unaffected,\" explains co-first author Geun Ho Ahn, a doctoral candidate in electrical engineering of the phenomenon that causes the weaker beam to stop resonating. \"The reflected light, and only the reflected light, is effectively canceled.\" The primary laser then exits the ring and is \"isolated\" in the desired direction. Vu\u010dkovi\u0107 and team have built a prototype as a proof of concept and were able to couple two ring isolators in a cascade to achieve better performance. \"Next steps include working on isolators for different frequencies of light,\" said co-author Kasper Van Gasse, a post-doctoral scholar in Vu\u010dkovi\u0107's lab. \"As well as tighter integration of components at chip scale to explore other uses of the isolator and improve performance.\" ","News_Title":"New chip-scale laser isolator opens new research avenues in photonics","Topic":"Physics"}
{"Paper_Body":"Abstract Recent simulations and experiments have shown that shear-thickening of dense particle suspensions corresponds to a frictional transition. Based on this understanding, non-monotonic rheological laws have been proposed and successfully tested in rheometers. These recent advances offer a unique opportunity for moving beyond rheometry and tackling quantitatively hydrodynamic flows of shear-thickening suspensions. Here, we investigate the flow of a shear-thickening suspension down an inclined plane and show that, at large volume fractions, surface kinematic waves can spontaneously emerge. Curiously, the instability develops at low Reynolds numbers, and therefore does not fit into the classical framework of Kapitza or \u2018roll-waves\u2019 instabilities based on inertia. We show that this instability, that we call \u2018Oobleck waves\u2019, arises from the sole coupling between the non-monotonic (S-shape) rheological laws of shear-thickening suspensions and the flow free surface. Introduction How microscopic interactions affect the macroscopic flow behavior of complex fluids is at the core of soft matter physics. Recently, it has been shown that shear-thickening in dense particulate suspensions corresponds to a frictional transition at the microscopic scale; when the imposed shear stress exceeds the inter-particle short-range repulsive force, the grain contact interaction transits from frictionless to frictional 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 . During this transition, the proliferation of frictional contacts can be so massive that it triggers a remarkable macroscopic rheological response: the rate of shear of the suspension decreases when the imposed shear stress is increased. As a result, highly concentrated shear-thickening suspensions have peculiar S-shape rheological laws 9 , 10 , which have been rationalized by a frictional transition model 3 , 11 , 12 . So far, the consequences of the frictional transition and its associated S-shape rheology have been essentially investigated in rheometers, where instabilities, shear bands and spatiotemporal patterns have been documented 11 , 13 , 14 , 15 . By contrast, very little is known about the behavior of shear-thickening suspensions in real hydrodynamic flow configurations beyond rheometry, in spite of the numerous applications 16 , 17 , 18 . An archetypical case, which is widely encountered in industrial and geophysical applications, is the incline plane flow configuration. As previously reported 19 and illustrated in Fig. 1 a (see also Supplementary movie 2 ), when a thin layer of shear-thickening suspension flows down an inclined plane, surface waves of wavelengths much larger than the thickness can develop spontaneously and grow as they propagate downstream. This longwave free-surface instability may seem reminiscent of the Kapitza instability observed when a thin liquid film flows down a slope 20 , 21 , or more generally of the so called \u201croll waves\u201d instability observed from turbulent flows in open channels 22 , 23 , 24 , to avalanches of complex fluids like mud 25 , 26 or granular media 27 . These latter two instabilities rely on the same primary mechanism: the amplification of kinematic surface waves at high velocity owing to inertial effects 28 . For a Newtonian liquid in the laminar regime, the destabilization occurs only when the Reynolds number of the flow, R e = \u03c1 u 0 h 0 \/ \u03b7 , where \u03c1 is the fluid density, u 0 its mean velocity, h 0 the flow thickness and \u03b7 the fluid viscosity, exceeds the Kapitza threshold, \\(R{e}_{K}=5\/(6\\tan \\theta )\\) , which is typically much larger than 1 for a small tilting angle \u03b8 of the incline 29 , 30 , 31 . By contrast, the growth of surface waves observed in Fig. 1 a for a dense shear-thickening suspension occurs at a Reynolds number of only \u22481, i.e., far below the Kapitza threshold R e K \u2248 5 predicted for \u03b8 = 10 \u2218 (see also ref. 19 ). This suggests that a different instability mechanism is at play for dense shear-thickening suspensions, yet its origin remains an open question. Fig. 1: Experimental characterization of the instability onset. a Non-inertial surface waves emerging spontaneously when a concentrated suspension of cornstarch particles flows down an incline (volume fraction \u03d5 = 0.45, inclination angle \u03b8 = 10 \u2218 and normalized flow Reynold number R e \/ R e K \u2248 0.2). b Sketch of the experimental setup. We use the progressive drainage of the reservoir to quasi-steadily vary the flow rate. The instability onset is determined by measuring the wave amplitudes both at the top and at the bottom of the incline with two laser sheets and cameras. For \u03d5 \u2264 0.4, an oscillation of the gate is added to impose a controlled perturbation. c Spatiotemporal plots of the laser sheet transverse-position versus time, indicating the vertical oscillations (blue and red arrows) of the free surface, at the top and at the bottom of the incline ( \u03d5 = 0.33, \u03b8 = 2 \u2218 , R e \u2248 37). d Reynolds number of the flow, R e , and e amplitude of the perturbation at the top, \u0394 h 1 , and at the bottom, \u0394 h 2 , during the drainage of the suspension reservoir ( \u03d5 = 0.36, \u03b8 = 3 \u2218 ). The instability onset ( \u0394 h 1 = \u0394 h 2 ) is given by R e c \u2248 28 (black-dashed-line). Full size image Here, we investigate the origin of this instability by studying the flow of a shear-thickening suspension down an inclined plane over a wide range of volume fractions and flow rates. We confirm that this instability is not inertial and fundamentally different than the classical Kapitza or Roll waves instabilities. We provide experimental evidence together with a theoretical explanation, which show that this destabilization arises from the coupling between the flow free surface and the non-monotonic (S-shape) rheological laws of shear-thickening suspensions. Results and discussion Evidence of an instability distinct from the classical Kapitza or roll waves instabilities We perform experiments with shear-thickening aqueous suspensions of commercial native cornstarch (Maisita\u00ae,  ). We vary the particle volume fraction over a wide range (0.30 < \u03d5 < 0.48) and characterize the onset of stability (the value of \u03d5 refers to the dry volume of cornstarch computed from its dry weight and density, 1550 kg m \u22123 ). We use a 1 m long and 10 cm wide inclined plane covered with a diamond lapping film (663-3M with roughnesses of ~45 \u03bcm) to insure rough boundary conditions. The suspension is released from a reservoir at the top of the plane through a gate with an adjustable aperture (Fig. 1 b). A scale, placed at the bottom end of the incline (not shown in the schematic), provides the instantaneous flow rate q of the suspension. To probe the stability of this free-surface flow for moderate volume fractions \u03d5 \u2264 0.4, the gate is mounted on a translating stage imposing a small sinusoidal modulation of its aperture (3 Hz, \u00b1100 \u03bcm). At large volume fractions however ( \u03d5 \u2273 0.4), no forcing is required because the flow is so unstable that it is dominated by noise amplification of its most unstable mode. Two low-incident laser sheets and two cameras are used to measure the mean film thickness h 0 ~2\u221210 mm, and the crest-to-crest amplitude of the waves upstream and downstream of the incline (Fig. 1 c). The calibration of the laser sheet projections on the film surface yields a local measurement of h 0 with a precision of 10 \u03bcm. We use a protocol designed to characterize the instability onset with a single experiment for each volume fraction \u03d5 and inclination \u03b8 . The flow rate is varied quasi-steadily, either using the progressive drainage of the reservoir (decreasing flow rate) or by slowly increasing the gate aperture (increasing flow rate). At each instant, an effective Reynolds number of the flow is computed from the instantaneous flow rate q and from the mean film thickness h 0 using the relation \\(Re=3{q}^{2}\/(g{h}_{0}^{3}\\sin \\theta )\\) , where g is the gravitational acceleration. Note that the flow rate is varied sufficiently slowly that at each instant, the flow rate is constant along the incline. With such a definition based on mean quantities only, the Reynolds number can be applied to any rheology and is directly related to the Froude number \\(F={u}_{0}^{2}\/(g{h}_{0}\\cos \\theta )\\) commonly used to described roll waves 24 by \\(Re={3F}\/{\\tan} \\theta\\) , where u 0 = q \/ h 0 is the mean flow velocity. R e also reduces to the standard expression R e = \u03c1 u 0 h 0 \/ \u03b7 for a Newtonian fluid of viscosity \u03b7 and density \u03c1 in the laminar regime (Nusselt velocity profile). Figure 1 d shows the concomitant evolution of R e and of the wave amplitude upstream ( \u0394 h 1 ) and downstream ( \u0394 h 2 ) the incline during the drainage of the reservoir, starting from an unstable situation where \u0394 h 2 > \u0394 h 1 . The stability onset is precisely reached when \u0394 h 1 = \u0394 h 2 , which provides us with the critical Reynolds number R e c (dashed line in Fig. 1 c), the critical flow thickness h c , and the critical basal shear stress, \\({\\tau }_{c}=\\rho g{h}_{c}\\,\\sin \\theta\\) . We have verified that the same instability onset is obtained by carrying successive steady-state measurements at various constant flow rates. A new freshly prepared suspension of cornstarch is used for each measurement. Experiments are repeated four times for each volume fraction. Figure 2 a shows the critical Reynolds number R e c , normalized by the Kapitza threshold for a Newtonian fluid, as a function of \u03d5 . For the lowest volume fraction investigated, \u03d5 = 0.30, the stability threshold is close to R e K (it is typically 50% above, owing to the finite forcing frequency and finite width of the plane 32 , 33 ). As the volume fraction is increased, R e c becomes increasingly larger relative to R e K , reaching \u22485 R e K at \u03d5 = 0.41. This behavior is actually expected from Kapitza\u2019s inertial mechanism for a medium that is continuously shear-thickening 34 , like our cornstarch suspension over that range of volume fractions (0.35 \u2272 \u03d5 \u2272 0.41). More strikingly, for \u03d5 \u2273 0.41 the relative critical Reynolds number drops drastically, down to two orders of magnitude below the Kapitza threshold at the largest volume fraction investigated ( \u03d5 = 0.48, for which R e \u2248 0.15 and F \u2248 10 \u22122 ). Clearly, in this domain, the flow destabilization can no longer be explained within the Kapitza framework since inertial effects are negligible. Note that inertia is also negligible at the particle scale, since the Stokes number, \\(St \\sim {(d\/{h}_{0})}^{2}Re\\) , where d ~ 10 \u03bcm is the particle size, is ~10 5 times smaller than R e . As shown in Fig. 2 b, this qualitative change in the onset of instability around \u03d5 = 0.41 is also observed in the evolution of the critical shear stress \u03c4 c with \u03d5 . Similarly, the surface wave speed at the instability threshold changes significantly and abruptly. Its value c c , normalized by the mean flow velocity u 0 drops from 3, which is Kapitza\u2019s prediction for a Newtonian layer in the long wavelength limit, to ~2 for volume fractions exceeding \u22480.41 (see Fig. 2 c). These results confirm that above \u03d5 \u2248 0.41, a longwave free-surface instability, which is fundamentally distinct from the Kapitza instability, emerges. In the following, we call this instability, which to our knowledge as no equivalent in classical fluids, \u201cOobleck waves\u201d. Fig. 2: Onset of instability. a Critical Reynolds number of the instability normalized by the Kapitza threshold R e c \/ R e k versus volume fraction \u03d5 . Inset: R e c \/ R e k versus inclination angle \u03b8 for \u03d5 = 0.45. b Critical shear stress \u03c4 c versus \u03d5 . c Normalized critical wave speed c c \/ u 0 versus versus \u03d5 . Inset: c c \/ u 0 versus speed of the kinematic waves c kin \/ u 0 , the black line shows that the waves propagate at the speed of the surface kinematic waves. Dashed-blue-line: Kapitza prediction (inertia+Newtonian fluid). Solid-red-line: prediction of the linear stability analysis ( \\(A={\\rm{d}}\\tilde{\\dot{\\gamma }}\/{\\rm{d}}{\\tilde{\\tau }}_{b}{| }_{{\\tilde{\\tau }}_{b} = 1}=0\\) ) describing the coupling between the flow free-surface and the suspension shear-thickening rheology. Different symbols indicate different inclination angles \u03b8 : \u25c7 2 \u2218 , \u25bf 3 \u2218 , \u22b3 6 \u2218 , \u22b2 9 \u2218 , \u25cb 10 \u2218 , \u25b3 22 \u2218 . The error bars indicate the standard deviation between experimental measurements. Different background colors highlight which instability emerges: Kapitza (gray) or Oobleck waves (red). Full size image Oobleck waves arise from the S-shape rheology and kinematic wave propagation To understand the origin of Oobleck waves, we characterize the rheology of the cornstarch suspension in a cylindrical-Couette rheometer (Fig. 3 ). We find that the volume fraction at which Oobleck waves appear ( \u03d5 \u2248 0.41) corresponds precisely to \u03d5 DST , the volume fraction at which the shear-thickening transition becomes discontinuous. Indeed, we analyze our rheological data along Wyart & Cates\u2019s model, which assumes that the effective viscosity of the suspension, \\(\\eta (\\phi ,\\tau )={\\eta }_{s}{({\\phi }_{J}(\\tau )-\\phi )}^{-2}\\) , diverges at a critical volume fraction, \u03d5 J , that depends on the applied shear stress, \u03c4 , according to \\({\\phi }_{J}(\\tau )={\\phi }_{0}(1-{e}^{-{\\tau }^{* }\/\\tau })+{\\phi }_{1}{e}^{-{\\tau }^{* }\/\\tau }\\) , where ( \u03b7 s , \u03c4 * , \u03d5 0 , \u03d5 1 ) are material constant. Here, \u03b7 s is a prefactor proportional to the solvent viscosity, \u03c4 * is the short-range repulsive stress scale above which the frictional transition occurs, which may be tuned by changing the particle roughness or surface chemistry, \u03d5 0 (resp. \u03d5 1 ) is the jamming volume fraction at which the suspension viscosity diverge at low (resp. large) stress, with \u03d5 1 being dependent on the inter-particle friction coefficient 3 . By fitting our measurements with this model, we find that the rheological curve (shear stress \u03c4 versus shear rate \\(\\dot{\\gamma }\\) ) becomes S-shaped when \u03d5 \u2265 0.41 \u00b1 0.005 = \u03d5 DST (see Fig. 3 and Methods for the fitting procedure). This suggests that negatively sloped portion in the rheological curve ( \\({\\rm{d}}\\dot{\\gamma }\/{\\rm{d}}\\tau \\, < \\, 0\\) ) is a key ingredient of the instability. S-shaped flow curves, and more generally rheograms with a negatively sloped region, are known to produce unstable flow conditions 35 , 36 . Previous studies on shear-thickening suspensions have based their analysis on this feature to explain for instance the emergence of random fluctuations 37 reported initially by Boersma et al. 38 , and the oscillations observed when an object moves in a shear-thickening fluid 39 , 40 or in rheometric configurations 15 , 41 . However, all these models require inertia to predict an instability. By contrast, here, the instability seems to be of a fundamentally different nature. First, it can occur at very low Reynolds and Froude numbers, for which inertial effects are negligible. Second, at the instability onset, the unstable mode propagates at the speed of the surface kinematic waves defined by c kin \u2261 d q \/d h 0 28 (Fig. 2 c inset). This indicates that the coupling between the flow and the free-surface deformation, which was not considered in previous studies, is essential to explain the emergence of Oobleck waves. Fig. 3: Rheograms of the aqueous cornstarch suspension. Shear-stress \u03c4 versus shear rate \\(\\dot{\\gamma }\\) for various volume fractions \u03d5 . Solid lines: fit by Wyart & Cates rheological laws setting the jamming volume fraction for frictionless and frictional particles to \u03d5 0 = 0.52 \u00b1 0.005 and \u03d5 1 = 0.43 \u00b1 0.005, respectively, the short-range repulsive stress scale above which the frictional transition occurs to \u03c4 * = 12 \u00b1 2 Pa and the prefactor to \u03b7 s = 0.91 \u00b1 0.01 mPa s. The rheograms are negatively sloped ( \\({\\rm{d}}\\dot{\\gamma }\/{\\rm{d}}\\tau <0\\) ) in the region highlighted in blue. Full size image Oobleck waves instability mechanism We now show how the negative slope in the rheology coupled with a gravity-driven free-surface flow can yield to an instability without invoking inertial effects. In the zero-Reynolds number limit, the force balance on a slice of suspension, as depicted in Fig. 4 a, imposes that the basal stress, \u03c4 b , is equal to the sum of the projected weight of the slice, \\(\\rho gh\\sin \\theta\\) , where h ( x , t ) is the local flow thickness, and of the longitudinal pressure gradient induced by the free-surface deflection. For wavelengths much larger than the flow thickness and the capillary length, the pressure profile perpendicular to the plane can be assumed to be hydrostatic \\(P(x,z,t)=\\rho g\\cos \\theta (h(x,t)-z)\\) , where x is the flow direction and z the height within the flowing layer in the perpendicular direction 21 . The depth-averaged force balance is then given by $${\\tau }_{b}=\\rho gh\\sin \\theta -\\rho gh\\cos \\theta \\frac{\\partial h}{\\partial x}.$$ (1) Let us now consider a perturbation of a base flow of constant thickness, as illustrated in Fig. 4 b. A local increase of the flow thickness implies that \u2202 h \/\u2202 x becomes positive upstream of the perturbation and negative downstream. To satisfy the force balance ( 1 ), the basal shear stress \u03c4 b upstream must therefore decrease, whereas it must increase downstream. However, owing to the S-shape rheology of the suspension, when \\({\\rm{d}}\\dot{\\gamma }\/{\\rm{d}}\\tau \\, < \\, 0\\) , a decrease (resp. increase) in \u03c4 b implies a local increase (resp. decrease) of the flow rate \\(\\dot{\\gamma }\\) . Therefore, the shear rate increases upstream and decreases downstream, inducing a net inward mass flux underneath the bump and the amplification of the initial perturbation (red arrows in Fig. 4 b). Fig. 4: Instability mechanism. a Depth-averaged forces acting along the flow x direction on a slice of suspension of width d x (shaded in blue): basal force \u03c4 b d x , projected weight of the slice \\(\\rho ghdx\\sin \\theta\\) and hydrostatic pressure \\(\\rho g\\cos \\theta {h}^{2}(x)\/2\\) , where \u03c4 b is the basal shear stress, g is gravity, h ( x ) is the flow thickness, \u03b8 is the plane inclination angle. b Positive feedback for a shear-thickening suspension with a S-shaped rheological curve: a local increase of the flow thickness h implies that \u2202 h \/\u2202 x is positive (resp. negative) upstream (resp. downstream) of the perturbation. Force balance (see Eq. ( 1 )) then implies that the basal shear stress \u03c4 b upstream (resp. downstream) must decrease (resp. increase). When the suspension rheogram is negatively sloped ( \\({\\rm{d}}\\dot{\\gamma }\/{\\rm{d}}\\tau <0\\) ), this yields a local increase (resp. decrease) of the flow rate \\(\\dot{\\gamma }\\) . The combination of these two feedback cycles (gray arrows) induces a net inward mass flux towards the bump (red arrows) that amplifies the initial perturbation (red vertical arrow). Full size image Quantitative depth-averaged model without inertia To go beyond this qualitative picture, we perform a linear stability analysis of a steady uniform flow of thickness h 0 and depth-averaged velocity u 0 using the Saint-Venant approximations (long wavelength limit 27 , 42 ) and neglecting inertia (lubrication approximation 21 ). The equations are written using the dimensionless variables \\(\\tilde{h}=h\/{h}_{0}\\) , \\(\\tilde{x}=x\/{h}_{0}\\) , \\(\\tilde{u}=u\/{u}_{0}\\) , \\(\\tilde{t}=t{u}_{0}\/{h}_{0},\\) \\({\\tilde{\\tau }}_{b}={\\tau }_{b}\/\\rho g{h}_{0}\\sin \\theta\\) and linearized by writing \\(\\tilde{h}=1+{h}_{1}\\) , \\(\\tilde{u}=1+{u}_{1}\\) and \\({\\tilde{\\tau }}_{b}=1+{\\tau }_{1}\\) with ( h 1 , u 1 , \u03c4 1 \u226a 1). Under these conditions, the mass conservation, \\({\\partial}_{\\tilde{t}} h+{\\partial}_{\\tilde{x}}(hu)=0\\) , and the force balance ( 1 ) become \\({\\partial}_{\\tilde{t}} {h_{1}}+{\\partial}_{\\tilde{x}}{h_{1}}+{\\partial}_{\\tilde{x}}{u_{1}}=0\\) and \\({\\tau }_{1}={h}_{1}-\\tan {\\theta }^{-1}{\\partial }_{\\tilde{x}}{h}_{1}\\) , respectively. The linearization of the normalized shear-rate \\(\\tilde{\\dot{\\gamma }}(\\tilde{{\\tau }_{b}})\\equiv \\tilde{u}\/\\tilde{h}\\) , gives A \u03c4 1 = u 1 \u2212 h 1 , where \\(A={\\rm{d}}\\tilde{\\dot{\\gamma }}\/{\\rm{d}}{\\tilde{\\tau }}_{b}{| }_{{\\tilde{\\tau }}_{b} = 1}\\) is the slope of the rheological curve for the base state basal stress \\({\\tau }_{b}=\\rho g{h}_{0}\\sin \\theta\\) , obtained from the integration of the flow velocity profile (see Methods). Taking the spatial derivative of the force balance and substituting \u03c4 1 and \\({\\partial}_{\\tilde{x}}{u_{1}}\\) using the rheology and mass balance, respectively, lead to a single partial differential equation for the free-surface perturbation h 1 : $$\\frac{\\partial {h}_{1}}{\\partial \\tilde{t}}+\\tilde{c}\\frac{\\partial {h}_{1}}{\\partial \\tilde{x}}=\\frac{A}{\\tan \\theta }\\frac{{\\partial }^{2}{h}_{1}}{\\partial {\\tilde{x}}^{2}},$$ (2) where \\(\\tilde{c}={c}_{{\\rm{kin}}}\/{u}_{0}=2+A\\) is the dimensionless speed of the kinematic waves 28 . Interestingly, the perturbation amplitude is found to follow a diffusion equation in the reference frame of the kinematic waves, with an effective \u201cdiffusion coefficient\u201d \\(A\/\\tan \\theta\\) . When A < 0, i.e., when the slope of the rheological law \\({u}_{0}\/{h}_{0}=\\dot{\\gamma }({\\tau }_{b})\\) is negative, anti-diffusion occurs, which leads to an amplification of all perturbations, whereas for A > 0 the flow is stable. The onset of instability is thus given by A = 0. This criteria can be expressed in terms of a critical Reynolds number R e c and a critical basal shear stress \u03c4 c using the Wyart & Cates rheological laws (see Methods). Figures 2 a, b show that, for \u03d5 \u2265 \u03d5 DST , these predictions (red-solid lines) capture very well the value of the critical Reynolds R e c and its dramatic drop over two decades when increasing \u03d5 , as well as the order of magnitude and the drop of the critical shear stress \u03c4 c with \u03d5 . The decrease of the instability onset with increasing \u03d5 is a direct consequence of Wyart and Cates\u2019 rheological laws 3 , where the DST onset stress also decreases with \u03d5 . Physically, it comes from the fact that, when approaching the maximal possible packing fraction \u03d5 0 , less and less frictional contacts are required to reach the DST region. The model also predicts a weak dependence of R e c on the plane inclination angle, as observed experimentally (Fig. 2 a inset). This overall agreement is all the more conclusive that it is rooted on physically based constitutive laws, of which the rheological parameters are measured independently, without further fitting. Another strong prediction of the model is that, at the instability onset ( A = 0), the speed of the unstable mode is equal to the speed of the kinematic waves c c \/ u 0 = 2. This prediction is fully consistent with the drop and value of the normalized wave speed observed experimentally for \u03d5 > \u03d5 DST (see Fig. 2 c). These results conclusively show that surface waves can emerge from the coupling between a negatively sloped rheology and the flow free surface, without the need for inertial effects. The Oobleck waves instability mechanism highlighted in this study is not limited to shear-thickening suspensions and could be extended to any other complex fluids having a rheology with a negatively sloped region (e.g., granular materials and geomaterials exhibiting velocity-weakening rheology 43 , 44 , concentrated polymers or surfactant solutions 35 , 36 , liquid crystals 45 , active self-propelled suspensions 46 ). More generally, our analysis shows that gravity forces, which are usually stabilizing for gravity-driven free-surface flows, can become destabilizing in the presence of a non-monotonic rheology. Our result could thus be extended to other stabilizing forces such as capillary forces arising from the free-surface deformation. We thus anticipate that other interesting instabilities may be explained directly, or in the light of our study. Finally, in a broader context, our study reveals that kinematic waves can be unstable in an overdamped medium, where inertia is negligible. These waves, which are observed in a wide range of situations (e.g., traffic and pedestrian flows 47 , sediment transport 48 , fluidized bed 49 , surge, and floods 23 ), result from mass conservation and a general relationship between a local flow rate and a local concentration 50 , 51 (e.g., number of vehicles or pedestrians on a road, solid packing fraction in a suspension, depth of the flow). However, in all these systems, their emergence from a uniform state requires an inertial lag between the flow rate and the concentration 28 . In our system, the situation is very different as these waves are unstable not from inertia, but from the intrinsic constitutive flow rule of the material. Whether this description of kinematic waves could be extended to more complex systems, such as crowds 47 or active self-propelled particles 46 , are interesting topics to address in future studies. Methods Rheological data fitting procedure Rheograms of the aqueous suspension of cornstarch are measured for various volume fractions in a narrow-gap cylindrical-Couette cell (Fig. 5 a) using a rheometer (Anton Paar MCR 501). The height of the shear-cell (40 mm) is sufficiently large to neglect sedimentation effects of the particles during the measurement. Similarly to the procedure followed by Guy et al. 4 , the viscosity below ( \u03c4 \u226a \u03c4 * ) and above ( \u03c4 \u226b \u03c4 * ) the shear-thickening transition are extracted and plotted versus \u03d5 (Fig. 5 b). The low viscosity branch (frictionless branch) is first fitted with \\(\\eta (\\phi )={\\eta }_{s}{({\\phi }_{0}-\\phi )}^{-2}\\) , with \u03b7 s and \u03d5 0 as fitting parameters. This yields \u03b7 s = 0.91 \u00b1 0.01 mPa.s and \u03d5 0 = 0.52 \u00b1 0.005. The large viscosity branch (frictional branch) is then fitted with \\(\\eta (\\phi )={\\eta }_{s}{({\\phi }_{1}-\\phi )}^{-2}\\) , using the previous estimation of \u03b7 s , and letting \u03d5 1 as the only fitting parameter. Note that the rheograms are obtained using both very rough (square symbols) and rough walls (circle symbols), by covering the cell-walls with sand papers of different grades (roughnesses of \u224880 \u03bcm and \u224815 \u03bcm, respectively). The two measurements overlap, except in the frictional branch at high volume fraction (shaded symbols in Fig. 5 b). For instance, the data from \u03d5 = 0.4 and 0.41 are included in the fitting procedure, while 0.42 and 0.43 are not, because the first two points overlap, independently of the roughness of the boundaries, whereas for 0.42 and 0.43 systematic deviations are observed indicating slippage or other artefacts. All data points which are interpreted as biased measurements (transparent symbols) are discarded from the fitting procedure; this yields \u03d5 1 = 0.43 \u00b1 0.005. Once the values of \u03b7 s , \u03d5 0 and \u03d5 1 are set, we determine the value of \u03c4 * by fitting the full rheograms \\(\\tau (\\dot{\\gamma })\\) with Wyart & Cates laws: \\(\\tau ={\\eta }_{s}{({\\phi }_{J}(\\tau )-\\phi )}^{-2}\\dot{\\gamma }\\) , with \\({\\phi }_{J}(\\tau )={\\phi }_{0}(1-{e}^{-{\\tau }^{* }\/\\tau })+{\\phi }_{1}{e}^{-{\\tau }^{* }\/\\tau }\\) . The best fit, shown in Fig. 5 c, is obtained for \u03c4 * = 12 \u00b1 2 Pa. The value of \u03c4 * represents the critical shear stress required to overcome the inter-particle repulsive force and activate frictional contacts between particles. For an inter-particulate force f and a particle size d , the critical shear stress is expected to be of order f \/ d 2 . The value we obtain (\u224812 Pa) is consistent with the values already reported in the literature for cornstarch in water. Fig. 5: Rheological data fitting procedure. a Cylindrical-Couette geometry used to characterize the rheology of the aqueous suspension of cornstarch (yellow). The gray area represents the cylinder rotating in the direction indicated by the black arrow. b Viscosity (Pa.s) below ( \u03c4 \u226a \u03c4 * ) and above ( \u03c4 \u226b \u03c4 * ) the shear-thickening transition versus \u03d5 . Blue-solid-line: frictionless branch, red-solid-line: frictional branch, black-dashed-lines: jamming volume fractions for the frictionless and frictional branches defining \u03d5 0 and \u03d5 1 , respectively. Square and circle symbols correspond to measurements performed with different wall roughness. The data points made transparent are not used in the fitting procedure as for these points, the measurement depend on the wall roughness. c Shear stress \u03c4 versus shear rate \\(\\dot{\\gamma }\\) measured at various volume fraction \u03d5 . Solid lines: Wyart & Cates rheological laws. The blue shaded area highlights the region where the rheograms are negatively sloped ( \\({\\rm{d}}\\dot{\\gamma }\/{\\rm{d}}\\tau <0\\) ). Full size image Computation of \u03c4 c and R e c To compute the critical shear stress \u03c4 c and the critical Reynold number R e c from the instability criteria resulting from the linear stability analysis \\(A\\equiv {\\rm{d}}\\tilde{\\dot{\\gamma }}\/{\\rm{d}}\\tilde{{\\tau }_{b}}{| }_{\\tilde{{\\tau }_{b}} = 1}=0\\) , we need to relate the shear rate \\(\\dot{\\gamma }\\equiv {u}_{0}\/{h}_{0}\\) , defined as the ratio of the depth-averaged flow velocity to the flow thickness, to the basal shear stress \u03c4 b and the basal suspension viscosity \u03b7 ( \u03c4 b ). For a steady uniform flow down an inclined plane of slope \u03b8 , the momentum equation applied to a surface layer of thickness h 0 \u2212 z gives $$\\tau (z)=\\rho g\\sin \\theta ({h}_{0}-z)=\\eta (z)\\frac{{\\rm{d}}\\hat{u}(z)}{{\\rm{d}}z},$$ (3) where the second equality uses the definition of viscosity, \\(\\eta =\\tau \/({\\rm{d}}\\hat{u}\/{\\rm{d}}z)\\) , and \\(\\hat{u}(z)\\) is the local velocity parallel to x . From the proportionality between \u03c4 and h 0 \u2212 z , the local velocity can be expressed as $$\\hat{u}(\\tau )=\\frac{1}{\\rho g\\sin \\theta }\\mathop{\\int}\\nolimits_{\\tau }^{{\\tau }_{b}}\\frac{\\tau ^{\\prime\\prime} }{\\eta (\\tau ^{\\prime\\prime} )}{\\rm{d}}\\tau ^{\\prime\\prime} .$$ (4) Using the definition of the depth-averaged flow velocity, \\({u}_{0}=\\mathop{\\int}\\nolimits_{0}^{{\\tau }_{b}}\\hat{u}(\\tau ^{\\prime} )\\ {\\rm{d}}\\tau ^{\\prime} \/{\\tau }_{b}\\) , we obtain the expression of the depth-averaged shear rate $$\\dot{\\gamma }=\\frac{{\\tau }_{b}}{3\\eta ({\\tau }_{b})}{\\mathcal{G}}({\\tau }_{b}),$$ (5) where $${\\mathcal{G}}(\\tau )=\\frac{3\\eta (\\tau )}{{\\tau }^{3}}\\mathop{\\int}\\nolimits_{0}^{\\tau }\\mathop{\\int}\\nolimits_{\\tau ^{\\prime} }^{\\tau }\\frac{\\tau ^{\\prime\\prime} }{\\eta (\\tau ^{\\prime\\prime} )}{\\rm{d}}\\tau ^{\\prime\\prime} {\\rm{d}}\\tau ^{\\prime} ,$$ (6) embeds the shear-thickening of the suspension. By definition, \\({\\mathcal{G}}=1\\) for a Newtonian fluid. From ( 5 ) and ( 6 ) we obtain $$A\\equiv \\frac{{\\rm{d}}\\tilde{\\dot{\\gamma }}}{{\\rm{d}}\\tilde{{\\tau }_{b}}}{| }_{\\tilde{{\\tau }_{b}} = 1}=\\frac{{\\tau }_{b}}{\\dot{\\gamma }}\\frac{{\\rm{d}}\\dot{\\gamma }}{{\\rm{d}}{\\tau }_{b}}{| }_{{\\tau }_{b} = \\rho g\\sin \\theta {h}_{0}}=\\frac{3}{{\\mathcal{G}}({\\tau }_{b})}-2,$$ (7) where we have used \\(\\frac{{\\rm{d}}}{{\\rm{d}}\\tau }(\\mathop{\\int}\\nolimits_{0}^{\\tau }\\mathop{\\int}\\nolimits_{{\\tau }^{\\prime}}^{\\tau }\\frac{{\\tau }^{^{\\prime\\prime} }}{\\eta ({\\tau }^{^{\\prime\\prime} })}{\\rm{d}}{\\tau }^{^{\\prime\\prime} }{\\rm{d}}{\\tau }^{\\prime})={\\tau }^{2}\/\\eta (\\tau )\\) . Note that in ( 7 ), the basal stress \u03c4 b , the shear rate \\(\\dot{\\gamma }\\) and the derivative are taken at the base state, i.e., for \\(\\dot{\\gamma }={u}_{0}\/{h}_{0}\\) and \\({\\tau }_{b}=\\rho g\\sin \\theta {h}_{0}\\) . Finally, the critical shear stress \u03c4 c , at which the flow destabilizes (black-solid-line plotted in Fig. 2 b of the main text), is obtained numerically by finding the value of \u03c4 b for which A ( \u03c4 c ) = 0, i.e., \\({\\mathcal{G}}({\\tau }_{b}={\\tau }_{c})=3\/2\\) . From the value of \u03c4 c , we obtain the critical Reynolds number (black-solid-line plotted in Fig. 2 (a) of the main text) $$R{e}_{c}\\equiv \\frac{3{u}_{0}^{2}}{g{h}_{0}\\sin \\theta }=\\frac{3{{\\tau }_{c}}^{3}{\\left({\\phi }_{J}({\\tau }_{c})-\\phi \\right)}^{4}}{9{\\eta }_{s}^{2}\\rho {g}^{2}{\\sin }^{2}\\theta }{[{\\mathcal{G}}({\\tau }_{c})]}^{2}=\\frac{3{{\\tau }_{c}}^{3}{\\left({\\phi }_{J}({\\tau }_{c})-\\phi \\right)}^{4}}{4{\\eta }_{s}^{2}\\rho {g}^{2}{\\sin }^{2}\\theta }.$$ (8) Data availability The data that support the findings of this study are available from  (DOI 10.5281\/zenodo.4247592). ","News_Body":"\"Oobleck\" is a strange fluid made of equal parts of cornstarch and water. It flows like milk when gently stirred, but turns rock-solid when impacted at high speed. This fascinating phenomenon, known as shear-thickening, results in spectacular demonstrations like running on a pool of Oobleck without submerging into it, as long as the runner doesn't stop. Researchers from Aix-Marseille University in France have now studied the regular and prominent surface waves that form when a Oobleck flows down an inclined slope (see Figure 1). Similar waves can be observed on gutters and windows on rainy days. However, the scientists noted qualitative differences with water waves; waves in Oobleck grow and saturate much faster. In order to unveil the origin of Oobleck waves, they conducted careful experiments with a mixture of cornstarch and water down an inclined plane. The researchers measured the onset of wave appearance and their speed using controlled perturbation of the flow and laser detection to estimate the fluid film thickness. These experiments revealed that for concentrated Oobleck, the onset of destabilization is different for destabilization in a Newtonian fluid such as water. This surprising observation led the team to look for a scenario to explain their formation. Their results are presented in a paper published on December 18 in Communication Physics. In this article, they conclude that for Oobleck, waves do not arise from the effect of inertia, as for water, but from Oobleck's specific flowing properties. Under impact, as shown by recent studies, Oobleck suddenly changes from liquid to solid because of the activation of frictional contacts between the starch particles. When flowing down a slope, this proliferation of frictional contacts leads to a very curious behavior: The flow velocity of the suspension decreased when the imposed stress increased\u2014like stepping on the gas pedal causing a car to decelerate. Researchers have shown that this effect couples to the flow free surface and can spontaneously generate a regular wave pattern. The proposed mechanism is generic. These findings could thus provide new grounds to understand other flow instabilities observed in various configurations, particularly in industrial processes facing problematic flow instabilities when conveying Oobleck-like materials such as concrete, chocolate or vinyl materials. ","News_Title":"Researchers unveil the origin of Oobleck waves","Topic":"Physics"}
{"Paper_Body":"Abstract Efficient generation of human induced pluripotent stem cell (hiPSC)-derived human intestinal organoids (HIOs) would facilitate the development of in vitro models for a variety of diseases that affect the gastrointestinal tract, such as inflammatory bowel disease or Cystic Fibrosis. Here, we report a directed differentiation protocol for the generation of mesenchyme-free HIOs that can be primed towards more colonic or proximal intestinal lineages in serum-free defined conditions. Using a CDX2 eGFP iPSC knock-in reporter line to track the emergence of hindgut progenitors, we follow the kinetics of CDX2 expression throughout directed differentiation, enabling the purification of intestinal progenitors and robust generation of mesenchyme-free organoids expressing characteristic markers of small intestinal or colonic epithelium. We employ HIOs generated in this way to measure CFTR function using cystic fibrosis patient-derived iPSC lines before and after correction of the CFTR mutation, demonstrating their future potential for disease modeling and therapeutic screening applications. Introduction Three-dimensional tissue-specific organoids represent a powerful tool to study both normal development and disease. Organoids have been generated from a variety of primary tissue samples, including small intestine 1 , 2 , stomach 3 , colon 4 , and pancreas 5 . Since the discovery of the Wnt-activated LGR5 + stem cell niche at the base of small intestinal and colonic crypts 1 , previous studies have reported the generation of 3D intestinal organoids containing crypt-like structures from murine and human LGR5 + intestinal stem cells in the presence of Wnt stimulation, epidermal growth factor (EGF) signaling, and Noggin 2 . However, the invasive procedures to obtain intestinal and colonic biopsy samples present a major challenge for larger scale applications of human intestinal organoids. The discovery of induced pluripotent stem cells (iPSCs) 6 has led to the development of multiple directed differentiation protocols, resulting in the in vitro generation of various endoderm-derived tissue types of interest, including liver 7 , stomach 8 , pancreas 9 , proximal 10 , 11 , 12 and distal 13 lung, kidney 14 , as well as intestine 15 . Moreover, the three-dimensional culture systems that generate organoids allow cells to self-organize, promoting further maturation and differentiation into target cell types that more closely resemble their in vivo counterparts 16 , 17 . The efficient generation of iPSC-derived human intestinal organoids (HIOs) serves not only as a relevant tool to study development, but has great potential for patient-specific in vitro disease modeling and high-throughput drug screening applications. HIOs positive for intestinal markers such as the intestinal homeobox transcription factor Cdx2 18 , 19 and intestinal epithelium marker Cdh17 have been generated from iPSCs using activin A to derive SOX17 + \/FOXA2 + endoderm, followed by Wnt3A and FGF4 (with serum) to specify CDX2 + hindgut (Hindgut Medium), and R-spondin, EGF, and the BMP inhibitor, noggin (Intestinal Medium or IM) to promote intestinal specification and crypt-like formation 15 . More recently, distal patterning of iPSC-derived HIOs to generate SATB2 + colonic organoids was achieved through BMP2 stimulation 20 . These factors have all been shown to play a role in intestinal specification and epithelial proliferation during embryonic development 21 . Interestingly, this protocol often generates HIOs containing both epithelial and mesenchymal stromal cells 15 , 20 , necessitating a FACS-based approach to isolate epithelial cell adhesion molecule positive (EpCAM + ) cells in order to interrogate epithelial-specific populations 22 , complicating their use in disease modeling or drug screening applications to isolate epithelial-specific factors. The derivation of HIOs from intestinal crypts using the LGR5 + adult stem cell population can generate organoids in the absence of mesenchyme 2 , raising questions as to whether intestinal progenitors derived from iPSCs are comparable to native crypts in generating HIOs. Moreover, a directed differentiation protocol using fully defined culture conditions is still lacking, as current protocols rely on the addition of exogenous serum. Here we describe a protocol using a well-defined, serum-free media for the robust de novo generation of epithelial iPSC-derived HIOs devoid of mesenchyme. In addition, we report the generation of a hiPSC CDX2-GFP reporter line that highlights the role of CDX2 as a specific marker for the emergence of iPSC-derived intestinal progenitors. This platform enables the study of both normal development as well as disease states of the gut (exemplified by cystic fibrosis), supporting the generation of patient-specific iPSC-derived organoids for interrogation, genetic manipulation, and large-scale drug screening applications. Results Generation of intestinal progenitors from iPSCs We and others have previously shown that dual-smad inhibition of the BMP\/TGF\u03b2 signaling pathways (with dorsomorphin and SB431542) in definitive endoderm derived from iPSCs and ESCs promotes the development of endoderm competent to form anterior foregut derivatives, such as NKX2-1 positive lung or thyroid lineages 10 , 11 , 12 , 13 , 23 , 24 . Indeed, we performed fluorescence activated cell sorting (FACS) of cells expressing the anterior foregut endodermal transcription factor NKX2-1 or a combination of cell surface markers CD47 hi \/CD26 lo (NKX2-1 + ) to enrich for a population of progenitors which can then be differentiated into proximal and distal lung lineages from human iPSCs 11 , 12 , 13 . In this protocol, prior single-cell sequencing of day 15 progenitors revealed the presence of cells expressing non-lung endodermal markers, including CDX2, and these non-lung lineages were enriched in the NKX2-1 negative fraction of cells (refs. 25 , 26 and Supplementary Fig. 1 ). Thus, we sought to investigate the potential of this differentiation approach to obtain intestinal organoids in defined, mesenchyme-free (MF) and serum-free culture conditions, in comparison to the previously described mesenchyme-containing (MC) protocol 15 (Fig. 1a ). Fig. 1: Emergence of intestinal-competent progenitors from iPSCs. a Schematic of comparison between mesenchyme-containing (MC) HIO vs mesenchyme-free (MF) directed differentiation protocols. b Mean Average (MA) Plots of significantly differentially expressed genes that were either upregulated (red dots) or downregulated (blue dots) in digital gene expression analysis from day 42 (D42) organoids sorted for CD47 on day 15, comparing the CD47 hi (Alveolospheres, left) and CD47 lo (HIOs, right) cultured in CK-DCI, as compared with day 8 (D8) progenitors ( p -value = <0.05 calculated as described 82 ). c Gene set enrichment analysis using Enrichr analyzing the top tissue types when referenced to the human gene atlas. Length of red bars indicates combined enrichment score. All bars are adjusted p -value = <0.05 calculated as described 28 , 29 . d Representative micrographs of whole mounts of day 85 organoids derived from BU3NGST NKX2-1 GFPneg sorted outgrowth demonstrate colocalized expression of Cdx2 and Villin (VIL) (scale bar = 50 \u03bcm, representative of n = 3 differentiations). Full size image Two independent human iPSC lines, bBU1c2 27 and BU3- NKX2-1 GFP -SFTPC tdTomato 11 , 13 (BU3NGST) were differentiated into CXCR4\/c-Kit +\/+ definitive endoderm, then treated with dual-smad inhibition as described above. Endodermal cells were then further incubated in conditions to promote lineage specification through Wnt activation with the GSK3\u03b2 inhibitor CHIR99021 (CHIR), BMP4, and retinoic acid (RA) (ref. 13 and Fig. 1a ). At day 15, cells were sorted to isolate the NKX2-1 + and NKX2-1 \u2212 fractions using a published cell sorting algorithm developed by our group 11 , based on either CD47 hi\/lo (for BU1) or NKX2-1 GFP +\/\u2212 (for BU3NGST) and plated into 3D Matrigel droplets. Both sorted populations were cultured in a defined, serum-free media containing CHIR and KGF together with Dexamethasone, cAMP and IBMX (CK + DCI), previously shown by our group to generate type II alveolar epithelial cells from NKX2-1 + lung progenitors 13 . Both iPSC lineages differentiated into endoderm and day 15 progenitors with similar efficiencies (Supplementary Fig. 2 ). Over the course of 3 weeks, these cultures grew from single cells into self-organizing 3D structures. In order to define the transcriptional identity of these organoids, we performed RNA sequencing of bBU1c2 at day 42 of differentiation, comparing the CD47 hi (enriched for NKX2-1-expressing cells) as well as the CD47 lo (enriched for non NKX2-1-expressing cells) outgrowth to day 8 of differentiation. As shown in Fig. 1b , the CD47 hi cells sorted on day 15, re-plated in 3D Matrigel in CK + DCI, and analyzed on day 42, were enriched for expression of transcripts encoding typical lung markers such as NKX2-1 , as well as markers of type 2 alveolar cells including SLC34A2 , NAPSA , LPCAT1 , SFTPC , and SFTPB . In contrast, organoids generated from the day 15 CD47 lo outgrowth expressed genes of mixed tissue identity, including small intestine ( CDX2 , LYZ , and CDH17 ), colon ( SATB2 and CEACAM5 ), and liver ( SERPINA1 and HNF4\u03b1 ). When analyzed using Enrichr 28 , 29 (referenced to the human gene atlas), the number one hit in \u2018Cell Type\u2019 for the top 350 significantly upregulated genes in the CD47 hi outgrowth was fetal lung, while the top hits for the CD47 lo outgrowth were colon and small intestine (Fig. 1c ). The data also showed that Vimentin ( VIM ), a mesenchymal marker, was significantly downregulated, while the canonical epithelial marker EpCAM, was significantly upregulated, in the CD47 lo sorted cells (see below). Whole mounts of BU3NGST GFP-derived organoids at day 85 stained for Cdx2 and the intestinal brush border component Villin showed robust 3D epithelial organoid formation, containing a significant number of Cdx2\/Villin co-expressing cells (Fig. 1d ). In addition, the NKX2-1 + cells grew into spheres comprised of type II alveolar epithelial cells, as previously described 13 . scRNAseq captures the emergence of intestinal progenitors The use of dual-SMAD inhibition was originally reported to strongly induce anterior foregut specification in pluripotent stem cell-derived endoderm 23 . To further understand progenitor cell lineage commitment at single-cell resolution early on in differentiation, we performed single-cell mRNA sequencing (scRNAseq) as depicted in Fig. 2a . We differentiated the C17 NKX2-1-GFP (C17) 30 iPSC line as described above, and performed scRNAseq using the 10x Chromium platform at day 6 (after 3 days of dual-smad inhibition) and day 13 (after 7 days of CHIR and BMP4 stimulation) of directed differentiation. At day 6, we analyzed 2215 cells, at a depth of 53,297 reads per cell, while at day 13 of differentiation, 2763 cells were analyzed with 53,471 reads per cell. We then performed dimensionality reduction as visualized using uniform manifold approximation and projection (UMAP), depicting the day 6 and 13 cells in the same plot (Fig. 2b ). Unsurprisingly, these two populations clustered independently from one another in an unsupervised manner, indicating the major transcriptional changes in cell identity that are known to occur during the early stages of directed differentiation. Fig. 2: scRNAseq of day 6 and day 13 progenitors. a Experimental schematic of scRNAseq, performed at day 6 (following 3 days of dual-smad inhibition) and day 13 (following 7 days of specification in CBRa). b UMAP visualization of cells at days 6 and 13 of differentiation. c UMAP visualization of expression of specific endodermal ( SOX17 and FOXA2 ), lung ( NKX2-1 , SOX2 , and SOX9 ) and intestinal (CDX2) markers at days 6 and 13 of differentiation. Color scale indicates normalized log fold change of gene expression. d UMAP visualization of expression of intestinal stem cell markers LGR5 , OLFM4 , and TACTSTD2 (TROP2) in cells at days 6 and 13 of differentiation. Full size image We sought to look at a subset of genes that mark endoderm ( SOX17 and FOXA2 ), anterior foregut ( SOX2 ), as well as intestinal ( CDX2 ) and Lung\/Thyroid ( NKX2-1 ) progenitors 31 , 32 . The dorsal and ventral foregut endoderm are marked by SOX2 and NKX2-1, respectively 33 , 34 , while the boundary of SOX2 and CDX2 expression in developing endoderm marks the foregut from the posterior endodermal tissues 32 . After 3 days of endodermal specification, followed by 3 days of dual-smad inhibition, the anterior marker SOX2 was expressed in only a subset of cells at day 6 (Fig. 2c ). In addition, by day 13 a significant number of cells expressed CDX2 and SOX17 , which notably do not overlap with the NKX2-1 -expressing cells. Furthermore, the pancreatic master regulator SOX9 is not widely expressed in day 13 cells (Fig. 2c ), suggesting that our progenitor population at day 13 does not contain a significant proportion of pancreas-competent cells 35 . We also sought to examine the expression of intestinal stem cell markers early on in directed differentiation. Figure 2d demonstrates that there are a significant number of cells at day 6 that express TROP2 and some expressing OLFM4 , while a subset of cells at day 13 express LGR5 . Overall, these data support the presence of a large CDX2 + progenitor population with the potential to give rise to HIOs. Proximal specification of intestinal progenitors Having demonstrated that the NKX2-1 \u2212 population contains gut-competent progenitors, we then sought to identify culture conditions that favor the emergence of regionally-patterned populations of intestinal-specific organoids. Day 15 BU3NGST cells were first sorted to isolate GFP negative cells, and cultured in a range of media conditions, including the previously published Noggin\/R-Spondin-based intestinal media (IM) 15 , as well as CK + DCI, and a variety of combinations of FGF4, KGF and Wnt activation (Fig. 3a ). The intestinal media with CHIR and KGF (IM + CK) was shown to generate more organoids than any other media condition (Fig. 3b, c ). In an attempt to ascertain regional identity and to further characterize the transcriptional profile of these organoids, quantitative real-time PCR (qRT-PCR) was performed for a variety of genes (Fig. 3d ). CDX2 , intestinal-specific cadherin CDH17 36 , and VIL1 (Villin) 37 were all highly expressed as compared with undifferentiated iPSCs in CK + DCI and IM + CK, as well as in the previously published intestinal medium 15 supplemented with CHIR (enabling us to discern the effect of CHIR from KGF). Notably, these markers were expressed at similar levels compared with a primary control (Fig. 3d ). However, PDX1 , a homeobox transcription factor essential for duodenal and pancreatic development 38 , as well as GATA4 , another proximal small intestinal marker, were significantly upregulated in the IM + CK condition as compared with both the CK + DCI organoids as well as adult colon (Fig. 3d ). Notably, the IM + CK organoids had significantly lower levels of SATB2 20 , Albumin ( ALB ), and Pepsinogen C ( PGC ) 39 expression compared with the CK + DCI HIOs (and IM + CHIR), suggesting that the IM + CK condition results in HIOs that are more homogenous and express markers specific to intestinal lineages, while preventing the emergence of hepatic and gastric lineages (Supplementary Fig. 3 ). The CK + DCI HIOs expressed significantly more SATB2 , a colonic marker, than either the IM + CK or IM + CHIR HIOs (Fig. 3d ). Organoids grown in all conditions express high levels of lysozyme, an antimicrobial protein expressed by Paneth cells throughout the GI tract 40 . Expression of the intestinal markers Cdx2 and Villin was also confirmed by immunohistochemistry (Fig. 3e ), further validating that the IM + CK conditions generated the most robust intestinal-specific organoids. Fig. 3: Proximal small intestinal specification following dual-smad inhibition. a Experimental Schematic of directed differentiation. b Representative micrographs of cells grown in different media conditions at day 34 of differentiation after sorting for NKX2-1 GFP negative cells at day 15 (scale bar = 300 \u03bcm, representative of n = 2 independent wells per condition). c Quantification of number of organoids per well at day 50 in different culture conditions. Error bars represent s.e.m. from n = 2 independent wells per condition. d qRT-PCR from day 54 HIOs cultured in either IM + CK, CK + DCI, or IM + CHIR normalized to day 0 hiPSCs and compared with a primary control (human adult colon) (2 \u2212\u0394\u0394CT , technical triplicates normalized to GAPDH or ACTB (\u03b2-ACTIN), n = 3 independent differentiations except IM + CHIR (IM + C, n = 2 independent differentiations). Error bars represent the s.d., statistical significance, where indicated, determined by one way-ANOVA followed by Tukey test, * p < 0.05, ** p < 0.005, **** p < 0.0001). e Whole mount immunofluorescence of day 60 organoids stained for Villin (VIL), Cdx2 and DNA (blue) (scale bar = 100 \u03bcm, representative of n = 3 differentiations). Full size image Generation of a CDX2-GFP reporter iPSC line BU1CG We demonstrated that both NKX2-1 \u2212 and CD47 lo sorted cells are enriched for intestinal progenitors that have the potential to grow into CDX2 + organoids that express a variety of markers specific for intestinal epithelium. However, in order to identify, profile, and purify putative CDX2 + intestinal progenitors throughout each stage of directed differentiation, we targeted the CDX2 locus with an eGFP fluorescent reporter, generating a CDX2-GFP knock-in reporter cell line (Fig. 4a and Supplementary Fig. 4 ). Using CRISPR\/Cas9, we gene-edited a normal iPSC line, bBU1c2 27 , using a synthesized self-linearizing DNA oligonucleotide containing a 2A-eGFP-polyA flanked by two 400 base pair homology arms as a donor and a Cas9-GFP plasmid, eliminating the need for subsequent selection marker excision (see Methods). Due to the presence of a self-cleaving 2A peptide and a targeted insertion site just upstream of the endogenous CDX2 stop codon, the CDX2 gene was not inactivated as a result of gene editing (Fig. 4a ). PCR confirmed that the construct was inserted into the desired locus in 70% of the clones picked from one of the two sgRNA\u2019s used (Fig. 4b Clone 109, hereafter referred to as BU1CG). Off target screening for the top three most likely off target gene insertion sites ( NHLRC4 , RAI4 , and SPP3 ) based on the sgRNA sequence revealed no aberrant indels (Supplementary Fig. 4c ). Fig. 4: A CDX2-GFP iPSC reporter line for intestinal differentiation. a Detailed schematic of the reporter construct and PCR screening primer sites (arrows). b Positive PCR screening of mono-allelic (17) and bi-allelic (109) knock-in clones derived from iPSC line bBU1c2. c Experimental schematic of differentiation of BU1CG into HIOs. d qRT-PCR for CDX2 expression in cells at day 15 of differentiation comparing sorted GFP + to both GFP \u2212 sorted cells as well as pre-sort (2 \u2212\u0394\u0394CT technical triplicates normalized to GAPDH , n = 3 independent sorts, error bars represent the s.d., p = 0.007, ** p < 0.01) as determined by unpaired Student\u2019s t -test). e FACS analysis of GFP expression during the first two weeks of differentiation ( n = 4 independent differentiations, error bars represent the s.d.). f Quantification of number of organoids per independent well obtained at day 50 of differentiation (error bars indicate s.e.m. from n = 2 independent wells per condition). g Whole mount immunofluorescence of HIOs at D40 of differentiation stained for Cdx2, GFP, and DNA (blue) (scale bar = 50 \u03bcm, representative of n = 6 organoids from n = 2 differentiations). h Representative micrographs of HIOs at day 34 of differentiation using different media conditions (originally sorted at day 14 for CDX2 GFP , scale bar = 100 \u03bcm, representative fields of view of n = 2 wells per condition). i Representative micrographs showing the formation of HIOs from BU1CG after sorting GFP + cells at day 14 (scale bar = 200 \u03bcm). Inset shows limited outgrowth from GFP\u2212 sorted cells cultured in the same media conditions (IM + CK) (scale bar = 200 \u03bcm) (representative of n = 3 differentiations). j Representative micrographs showing HIOs at day 45 comparing CK + DCI vs IM + CK conditions (scale bar = 200 \u03bcm, representative of n = 6 differentiations). Full size image The selected BU1CG line showed a stable iPSC morphology upon passage and normal karyotype (Supplementary Fig. 4d ), and was subsequently differentiated into definitive endoderm, yielding on average 76.7% CXCR4\/c-KIT double positive cells ( n = 7, representative flow cytometry in Supplementary Fig. 5a ). Cells were then differentiated into intestinal progenitors using the MF protocol, and sorted for CDX2 GFP at day 15 of differentiation, and then plated as single cells in 3D Matrigel droplets, and incubated in multiple culture conditions similar to the approach outlined for BU3NGST (Fig. 4c ). At day 15, we generated an average of 1.377 \u00d7 10 7 cells per input well of iPSCs containing 2 \u00d7 10 6 cells ( n = 3 differentiations, Supplementary Fig. 5b ). We confirmed the fidelity of the reporter by sorting cells at day 15 based on GFP and as expected, expression of CDX2 tracked with the GFP + sorted cells (Fig. 4d ). Taking advantage of the reporter, we followed the emergence of these putative intestinal progenitors based on CDX2 GFP expression. As shown in Fig. 4e , CDX2 GFP positive cells began to emerge at day 8 of differentiation, and by day 13 they represented 41.166 \u00b1 20.53% ( n = 6, mean \u00b1 s.d.) of all cells in the culture. Staining of mature HIOs at day 40 further confirmed the fidelity of the GFP reporter, depicting nuclear Cdx2 staining colocalizing with cytoplasmic GFP (Fig. 4f ). Confirming our previous findings, IM + CK and CK + DCI generated significantly more CDX2 GFP organoids per input cell as compared with the other combinations of intestinal medium, Wnt, and FGF4\/KGF stimulation (Fig. 4g, h ). Immunofluorescence and light microscopy analyses of the resulting organoids revealed luminal, organized multicellular structures with high CDX2 GFP expression (Fig. 4i, j ). In contrast, sorting GFP negative cells for re-plating and further outgrowth in the same IM + CK conditions resulted in almost complete depletion of gut-competent cells with poor outgrowths containing significantly fewer GFP+ cells (0.49% \u00b1 0.052, n = 3 mean \u00b1 s.d.). Not surprisingly, when the GFP negative cells were cultured in CK + DCI conditions the outgrowth showed high expression of the lung marker, NKX2-1 (Supplementary Fig. 5c ), most of them negative for CDX2 GFP (Supplementary Fig. 5d ). These data provide strong evidence for the early emergence of putative intestinal progenitors during the MF protocol and suggests that most, if not all intestinal capacity resided in the CDX2 GFPpos population, as the CDX2 GFPneg cells failed to form robust 3D structures when cultured in the IM-CK condition (Fig. 4i , left inset). iPSC-derived HIOs grow in the absence of mesenchymal support Intestinal organoids grown from intestinal crypts can self-sustain their in vitro expansion in the absence of mesenchymal support 2 , 41 , something that has yet to be recapitulated with iPSC-derived organoids. As mentioned above, previously reported iPSC-derived intestinal directed differentiation protocols also lead to the generation of Cdx2 negative mesenchyme, which was proposed to secrete a variety of factors that induce and support the growth of the intestinal epithelium 42 . We sought to determine if the HIOs obtained using our protocol, in fact, differentiated in the absence of mesenchymal support (Fig. 5a ) compared side by side with the previously described protocol 15 . Light\/Fluorescence microscopy demonstrated the presence of CDX2 GFPpos HIOs differentiated using both the MC and MF protocols (Fig. 5b ). However, there were clear morphologic differences, highlighted by the presence of GFP negative cells surrounding the CDX2 GFP positive epithelium in the MC organoids (Fig. 5b , top). Staining for Vimentin revealed that these GFP negative cells were mostly positive for Vimentin which were not present in the MF protocol (Fig. 5c ), findings that were confirmed by RNA-Seq (Supplementary Fig. 3 ). Representative flow cytometry for the epithelial-specific marker EpCAM (Fig. 5d ) demonstrated that in the MF protocol virtually all cells were epithelial, in contrast to the MC protocol where up to 50% of the cells were EpCAM negative. Fig. 5: iPSC-derived HIOs grow in the absence of mesenchymal support. a Experimental schematic of MF and MC directed differentiations. b Light microscopy representative micrographs of merge images from BU1CG-derived HIOs cultured under MC vs MF conditions (scale bar = 100 \u03bcm, representative of n = 3 differentiations). c Representative fluorescent micrographs of HIOs derived using the MC vs MF Protocol stained for the mesenchymal marker Vimentin along with Cdx2 (scale bar = 50 \u03bcm, representative of n = 5 organoids from n = 3 differentiations). d Flow Cytometry of single-cell suspensions from HIOs differentiated using the MC vs MF protocol stained with the epithelial marker EpCAM. e Comparison of the % of EPCAM + cells as measured by flow cytometry in HIOs cultured in different media conditions at day 54 of differentiation (paired Student\u2019s t -test, * p < 0.05). f Flow cytometry for EpCAM expression of cells at days 6, 8, and 10 of both MC and MF differentiations. g UMAP representation of EpCAM expression at days 6 and 13 of differentiation by sc-RNAseq. Full size image Furthermore, we decided to investigate whether the ability of the HIOs to grow in the absence of mesenchyme was established during emergence of CDX2 GFP positive cells regardless of which protocol we used. Serial flow cytometry for EpCAM over the early stages of differentiation demonstrated the maintenance of EpCAM expression in the MF protocol and gradual loss of EpCAM expression in the MC protocol (Fig. 5f ). In addition, scRNAseq of cells at days 6 and 13 (as described in Fig. 2 ) demonstrated that there is widespread expression of EpCAM in cells at those time points of differentiation in the MF protocol, supporting our flow cytometry findings (Fig. 5g ). Using this same dataset, we also tracked expression of mesenchymal and mesodermal markers at these time points, including VIM , COL1A1 , COL3A1 , FN1 , THY1 , and ACTA2 (Supplementary Fig. 6a ). With the exception of VIM and FN1 , the vast majority of cells at day 13 did not express the other markers listed above. It has been reported that epithelial cells express mesenchymal genes including VIM and FN1 during early organogenesis (E9-11.5) 43 , 44 , which may explain why our day 13 cells express these markers. Finally, we compared the outgrowths of hindgut obtained via the first 8 days of the MC protocol 15 to CDX2 positive cells obtained at day 15 of the MF protocol cultured in 3D Matrigel\u00ae in several media conditions. As shown in Fig. 5e , HIOs derived from CDX2 GFPpos sorted cells contained significantly more EpCAM + cells (99.25 \u00b1 .49%, n = 6 mean \u00b1 s.d.) compared with MC differentiated cells (49.92 \u00b1 29.9%, n = 6 mean \u00b1 s.d.), regardless of the media. In order to interrogate whether the day 15 FACS step of the MF differentiation is responsible for mesenchymal depletion, we also performed an MF differentiation that omitted the day 15 sort, and plated progenitors into 3D culture conditions at day 15. At day 30, 88.6% of cells were EpCAM + by flow cytometry (Supplementary Fig. 6b ), supporting the notion that this differentiation protocol enables the emergence of intestinal organoids without the need for mesenchymal support. MF HIOs contain a variety of intestinal epithelial cell types The intestinal epithelium is made up of a diverse group of cell types each occupying a specific functional niche. These include absorptive enterocytes, secretory Paneth cells that secrete Lysozyme ( LYZ ), enteroendocrine cells that express Chromogranin A ( CHGA ), and mucin-secreting goblet cells, among others 45 . In order to assess whether our HIOs contained cell types present in mature intestinal epithelium, we performed immunohistochemistry comparing the organoids grown in CK-DCI vs. IM-CK conditions (Fig. 6a\u2013c ). We observed the presence of enteroendocrine cells (positive for chromogranin A, Fig. 6a ) and Paneth cells (positive for Lysozyme, Fig. 6b ), in the context of organoids comprised mostly of Villin-expressing putative enterocytes, at the expected densities for these particular cell types in both conditions. We also noticed that the IM + CK organoids were largely negative for colonic mucin Muc2, while the CK + DCI HIOs showed robust staining for luminal Muc2 (Fig. 6c ), suggesting that the IM-CK conditions promote a more proximal identity in contrast to the CK-DCI conditions. In order to further investigate this, we performed qRT-PCR for a panel of genes (Fig. 6d ) specifically expressed by proximal or distal intestinal epithelium. Indeed, the gene expression profile of the HIOs obtained through these two different conditions strongly supported our previous observations that the CK-DCI protocol gives rise to heterogeneous organoids containing many cells expressing colonic markers, as well as markers of several GI tract tissues, particularly when compared with RNA from a primary control from an adult human colonic tissue sample. In contrast, the IM-CK conditions promote the emergence of HIOs with a more defined identity corresponding to proximal small intestine\/duodenum. These HIOs express high levels of CDX2 , LYZ , CDH17 , GATA4 , and PDX1 , while expressing significantly lower levels of the distal colonic markers SATB2 and MUC2 . Fig. 6: CDX2 GFP+ sorted organoids show conserved regional specificity. a \u2013 c Sections of paraffin embedded organoids cultured in either IM + CK or CK + DCI stained for Chromogranin A (CHGA), Lysozyme (LYZ), Villin (VIL), Cdx2, and colonic mucin Muc2 (scale bar = 50 \u03bcm, representative of n = 5 organoids from n = 2 differentiations). d qRT-PCR for various genes of interest, from D54 HIOs cultured in either CK + DCI or IM + CK as compared with a primary control normalized to day 0 hiPSCs (2 \u2212\u0394\u0394CT , technical triplicates normalized to GAPDH or ACTB (\u03b2-ACTIN), n = 3 differentiations, error bars represent the s.d., statistical significance where indicated determined by unpaired Student\u2019s t -test, * p < 0.05). Full size image Patient HIOs are suitable for disease modeling While the translational potential of organoids to the ultimate goal of cell or tissue replacement therapy is still years away, they have already demonstrated significant utility in disease modeling, particularly in the context of monogenic disorders such as Familial Adenomatous Polyposis (FAP) 46 and cystic fibrosis (CF) 12 , 47 , 48 . In CF, cell-based models have proved invaluable in developing CFTR modulators. Primary airway epithelial cells are the gold-standard for predicting CFTR drug-responsiveness however significant progress has been made using rectal organoids to theratype CF patients 49 , 50 , 51 . Given that CFTR is highly expressed in the intestinal epithelium, we tested, in proof-of-concept experiments, the feasibility of measuring CFTR function in our iPSC-derived HIOs (see schematic, Fig. 7a ). Fig. 7: Patient specific mesenchyme-free-derived HIOs are suitable for disease modeling. a Schematic overview of experiment to measure CFTR function in \u0394F508, \u0394F508-corrected and WT HIOs ( n = 3 independent differentiations per iPSC line). b Representative micrograph of whole mount of distally patterned HIOs generated from the C17 iPSC line at day 54 of differentiation, sorted for NKX2-1 GFP\u2212 at day 15, and cultured in CK + DCI (scale bar = 50 \u03bcm, representative of n = 2 differentiations). c qRT-PCR for CFTR expression in HIOs cultured in both IM + CK and CK + DCI at day 54 of differentiation (2 \u2212\u0394\u0394CT , technical triplicates normalized to GAPDH ; n = 3). d Average baseline organoid size of WT, \u0394F508 and \u0394F508 - corrected HIOs ( n = 192 mean spheres analyzed per iPSC line). e Quantification of the steady-state lumen area (SLA) in \u0394F508 and \u0394F508-corrected HIOs (see also Supplementary Fig. 7b ). f Representative micrographs of HIOs pre- and post-24-h forskolin treatment (representative scale bar = 200 \u03bcm in upper left image, images represent n = 3 biological replicates per cell line and an average of n = 8 wells per replicate). g Quantification of change in whole well CSA in response to 24-h forskolin treatment (normalized to T = 0 h whole well CSA). Statistical significance where indicated determined by unpaired Student\u2019s t -test, * p < 0.05, ** p < 0.005 ( n = 3 independent differentiations per cell line, error bars represent the s.d.). Full size image In order to do so, it was first necessary to test whether iPSCs carrying CFTR mutations were capable of differentiating into HIOs using our MF protocol. For this purpose, we used our published cystic fibrosis (CF)-specific iPSC line, C17 11 and differentiated cells toward distal lineages using the CK + DCI condition. Similar to what was observed in previous differentiations with wild type cell lines, C17 was successfully differentiated into 3D HIOs expressing Cdx2 and Villin (Fig. 7b ). qRT-PCR confirmed that differentiated HIOs expressed CFTR relative to undifferentiated cells, but at lower levels when compared with adult colon (Fig. 7c ). To control for the potential effects of genetic background on CFTR measurement we used an iPSC line from an individual homozygous for the \u0394F508 mutation ( \u0394F508 ). This \u0394F508 iPSC line was previously gene-edited to correct the \u0394F508 mutation in one allele ( \u0394F508-corrected ) 30 . \u0394F508 , \u0394F508-corrected and WT HIOs were differentiated until day 30, as above. We applied methodologies including steady-state lumen area (SLA) and forskolin-induced swelling (FIS), previously developed for the analysis of CFTR function in rectal organoids 48 , 51 . At baseline the average organoid size was significantly smaller in \u0394F508 compared with WT and \u0394F508-corrected HIOs (Fig. 7d ). SLA was also significantly lower in \u0394F508 compared with \u0394F508-corrected HIOs (Fig. 7e and Supplementary Fig. 7b ). In response to FIS, WT HIOs started to swell within 30 min (Supplementary Fig. 7a ). After 24 h of forskolin, no significant change in whole well cross-sectional area (CSA) was detected in \u0394F508 HIOs (mean CSA 1.073 \u00b1 0.04702); whereas \u0394F508-corrected and WT HIOs significantly increased in CSA (2.190 \u00b1 0.3051 in the \u0394F508-corrected and 2.190 \u00b1 0.1950 in the WT, n = 3 independent differentiations per cell line, data represents mean \u00b1 s.d.) (Fig. 7f, g , Supplementary Movies 1 \u2013 6 , representative of n = 3 differentiations and n = 3 wells per condition). Discussion The original description of the use of dual-smad inhibition by Green et al. 23 to induce differentiation of anterior foregut from endoderm-patterned pluripotent stem cells suggested that these conditions particularly suppressed CDX2 expression in endodermal cells. Our findings show that, although lung competence is induced in a subset of cells, many are, in fact, not patterned toward anterior foregut endoderm following 72 h of dual-smad inhibition. Indeed, our scRNA-seq data showed that there is significant heterogeneity throughout directed differentiation (Fig. 2b, c ), and by day 13, a significant number of cells were CDX2 + (Fig. 2c ). This result was confirmed using our BU1CG line that showed robust upregulation of CDX2 GFP starting at day 8 of differentiation (Fig. 4 ). A potential explanation for this discrepancy might lie in the fact that most of the data in the Green et al. studies were obtained at earlier time points during differentiation using a single human embryonic stem cell (hESC) line. Their original protocol relied on the use of embryoid bodies as a starting point, which could generate mesodermal derivatives that may affect the outcome of the differentiation. Our data suggests that although dual-smad inhibition can facilitate the emergence of progenitors with anterior foregut capacity, it does not prevent the differentiation of a robust progenitor population capable of specifying into many endodermal lineages including more posterior gut tube derivatives, such as small and large intestine, liver, pancreas, and stomach, primarily as a result of strong putative activation of the Wnt\/\u03b2-Catenin pathway due to treatment with GSK3\u03b2 inhibitor CHIR99021. It is important to acknowledge that at this point we cannot rule out additional Wnt-independent effects of CHIR99021. While the original protocol for the generation of HIOs from human iPSCs employed Wnt3A as the primary Wnt agonist to promote intestinal progenitor specification 15 , many subsequent manuscripts have described directed differentiation of iPSCs to HIOs that are reliant on CHIR99021 for Wnt\/\u03b2-Catenin activation 8 , 20 , 52 , 53 , 54 . In addition, CHIR99021 has been well characterized as a strong inducer of the Wnt\/\u03b2-Catenin signaling pathway 55 . It was intriguing to find that cells sorted based on CDX2 GFP (or CD47\/ NKX2-1 ) at day 15 of differentiation and cultured in the same media conditions gave rise to very different cell lineages. Indeed, sorted CD47 hi (or NKX2-1 + ) cells vs CD47 lo (or NKX2-1 \u2212 ) cells cultured in CK + DCI gave rise to very different organoids, i.e., alveolar-like vs posterior lineages (gut, liver, stomach), respectively. Whether or not this results from a true cell fate decision and commitment to an anterior vs posterior fate established early during differentiation (accomplished via establishment of lineage specific epigenetic marks as shown in other systems 56 , 57 , 58 ) is currently unknown and merits further investigation. The fact that addition of KGF to the intestinal media generated the most robust conditions for proximal intestinal specification (IM + CK), with higher numbers of intestinal organoids is unsurprising, particularly given the role KGF has been shown to play in vivo in the intestinal epithelium. At the time of its original discovery in 1989, human KGF, which was ultimately re-classified as fibroblast growth factor 7 (FGF7), was shown to exert powerful paracrine effector functions on epithelial cell growth 59 . Since then, others have reported that KGF plays a direct role in epithelial proliferation across a range of cell types in the GI tract, including multiple epithelial cell lines 60 , 61 , as well as goblet cells 62 . Furthermore, increased KGF activity has been associated with epithelial regeneration in inflammatory bowel disease, both in animal models 63 and human biopsy samples 64 . From a mechanistic point of view, KGF is known to be expressed by the mesenchyme acting via FGFR2b present in the intestinal epithelia 65 , 66 . This is the opposite to FGF4 which is expressed by the epithelium and acts on receptors present in the mesenchyme 67 . This might explain the key difference between the MC protocol and ours, and the fact that we can achieve robust differentiation in the absence of mesenchymal support, whereas prior protocols stimulate concomitant mesenchymal outgrowth. The functional interaction of intestinal epithelium with intestinal mesenchymal lineages is certainly of interest. However, for certain questions, notably the investigation of epithelial intrinsic defects, the ability to study an epithelial-only culture system is a major advantage, and would diminish potential experimental noise generated from a mesenchyme-containing system. An epithelial-only model facilitates the simple measurement of epithelial intrinsic function or dysfunction in a reductionist framework. Finally, we selected to study cystic fibrosis in our HIO model based on the suitability of these organoids to study the epithelial expression of CFTR in the intestine, in addition to the established assays to measure CFTR function using primary rectal organoids 48 , 68 . We determined, in proof-of-concept experiments, that MF HIOs provide an organoid-based read-out of CFTR function. It is worth noting that for the purposes of these experiments, organoids remained in the differentiation media containing cAMP. Thus, organoids with functional CFTR protein are activated at baseline and this likely dampens the magnitude of acute CFTR activation with forskolin. There are several preclinical, cell-based models to assess CFTR dysfunction and rescue 49 . Important future questions of the CFTR assay in HIOs include: (1) Do \u0394F508 HIOs detect CFTR rescue in response to CFTR modulators?, (2) Does in vitro CFTR rescue in HIOs predict clinical efficacy?, and (3) How does the HIO platform compare to established platforms in terms of sensitivity and positive predictive value? Compared with routine rectal biopsies, it takes several months to first reprogram and subsequently differentiate iPSCs into HIOs. However, there are key areas where the unique properties of HIOs could be helpful. First, given that iPSCs are able to be expanded indefinitely, there is the potential to scale-up HIO production for screening purposes. Second, the ability to gene-edit iPSCs and differentiate these cells into multiple tissue types could overcome the genetic variability and overwhelming effect of the infected, inflammatory milieu that limits experiments using primary human tissue. An iPSC-based approach offers the potential to study key questions, including the role of genetic modifiers in the heterogeneity of CF phenotypes. At a molecular level, the iPSC system could be applied to determine cell-type and tissue-specific differences in the regulation of CFTR expression 69 . Nevertheless, despite significant progress in the development of increasingly potent CFTR modulators for residual function mutations 70 , 71 , there remains a subset of individuals whose mutations result in little to no CFTR protein and who represent a major unmet therapeutic challenge. The optimal preclinical platform for these individuals remains to be determined and human, scalable, patient-specific cell-based platforms that express CFTR at higher levels than primary bronchial epithelial cells may prove helpful in drug development. Methods hiPSC generation and culture and expansion All parental hiPSC lines previously published by our group (bBU1) 27 and others (C17, dF508) 12 , 30 were derived from normal donors (bBU1) or an individual with a published compound heterozygote CFTR mutation (C17), and have been shown to have a normal karyotype (46XY). All lines were maintained in feeder-free conditions using mTESR\u00ae1 (StemCell Technologies), and passaged onto hESC Matrigel\u00ae (Corning cat. no. 354277) coated 10 cm, 6-well, 12-well, and 24-well tissue culture dishes (Corning) as per the manufacturer\u2019s instructions. All human subjects studies were performed under signed consent and approved by the Boston University Institutional Review Board (IRB), protocol H-32506. Cloning of CDX2-eGFP into a blunt ended cloning vector Using an approach outlined by Zhang and colleagues 72 , we used a synthetic self-linearizing oligonucleotide construct (sequence provided upon request) as a donor without the need for subsequent selection marker excision. Oligonucleotide constructs for the donor, guide RNAs, and sequencing\/screening primers were ordered from Integrated DNA Technologies (IDT). All sequencing reagents are listed in Supplementary Table 3 . CRISPR Guide RNA sequence and target sites were selected using the CHOPCHOP 73 , 74 and MIT CRISPR Design Tools. The synthetic donor construct was cloned into the blunt ended cloning vector, pJET.2, using the CLONEJet PCR Cloning Kit (ThermoFisher cat. no. K1231). The SpCas9-2A-GFP (PX458) plasmid with cloning backbone for sgRNA was obtained from the Zhang Lab through Addgene (Addgene #48138) 75 . Gene editing of bBU1c2 Parental bBU1c2 iPSCs from one confluent six-well plate and 2 10 cm dishes were dissociated from their tissue culture vessels using ReLeSR (StemCell Technologies), and dissociated into single-cell suspensions. In total, 6 \u00d7 10 6 cells were nucleofected with 5 \u03bcg of guide plasmid DNA and 5 \u03bcg of donor plasmid DNA, and re-plated on fresh 10 cm Matrigel-coated dishes. The cells were nucleofected on an Amaxa\u2122 4D-Nucleofector\u2122 using the Lonza Nucleofector P3 Primary Cell 4D-Nucleofector\u2122 X Kit (V4XP-3024), as per the manufacturer\u2019s instructions, on the \u201cHESCell H9\u201d program. Screening and banking of BU1CG Two days after nucleofection, the cells were dissociated once more into single cells, and sorted for Cas9-GFP in order to select for cells that had been successfully nucleofected with the Cas9-sgRNA plasmid. Of the 4 \u00d7 10 6 cells sorted, 0.5% of them were GFP + . The cells were then plated at varying dilutions on 10 cm dishes, and allowed to grow into colonies, which were then mechanically picked using a p20 pipette and re-plated into one well of a 24-well plate containing warm mTeSR1\u00ae with 5 \u03bcM Y27632 . Genomic DNA was extracted from 96 clones using the QIAamp DNA Mini Kit (QIAGEN cat. no. 51304), and screened by PCR using the Herculase II Fusion DNA Polymerase (Aligent cat. no. 600675, as per manufacturer\u2019s instructions) for successful donor construct insertion. Amplified DNA was visualized by gel electrophoresis using GelRed\u00ae Nucleic Acid Gel Stain (Biotium cat. no. 41002) and imaged using a BioRad GelDoc\u2122XR System, along side the 1 Kb Plus DNA Ladder (ThermoFisher cat. no. 10787018). The gel presented in the main Fig. 4b is an uncut, unedited, native gel. Sequencing was performed by GENEWIZ\u00ae. hiPSC differentiation into day 15 HIO progenitors After reaching >95% confluency, cells were differentiated into HIOs using a protocol adapted from refs. 11 , 13 , 25 . hiPSC colonies were dissociated into single cells using Gentle Cell Dissociation Reagent (StemCell Technologies cat. no. 07174), and re-plated at a density of 2 \u00d7 10 6 cells per well of a Matrigel-coated six-well tissue culture plate in mTeSR1 supplemented with Y27632 (Tocris, 5 \u03bcM). After 24 h, cells were then differentiated into definitive endoderm using the StemCell Technologies StemDiff Definitive Endoderm Kit (Cat#05110), as per manufacturer\u2019s instructions. Cells were then assessed by flow cytometry for anti-CXCR4-PE (ThermoFisher MHCXCR404) and anti-c-kit-APC (Biolegend 323205). At day 3, cells were split 1:3 as described above into new hESC Matrigel\u00ae coated 6-well plates, and incubated with DS\/SB (see ref. 25 ), containing Dorsomorphin (2 \u03bcM Stemgent, cat. no. 04-0024) and SB431542 (10 \u03bcM Tocris, cat. no. 1614) supplemented with Y27632 for 24 h, followed by DS\/SB without Y27632 for 48 h. At day 6, cells were split again 1:3 as described above, and incubated in CB\/RA containing CHIR99021 (CHIR) (3 \u03bcM, Tocris, cat. no. 4423), rhBMP4 (10 ng\/mL, R&D Systems, cat no. 314-BP), and retinoic acid (RA) (100 nM, Sigma, cat. no. R2625-50MG). Basal media for both DS\/SB and CBRA consisted of complete serum-free differentiation medium (cSFDM), containing IMDM (ThermoFisher) and Ham\u2019s F12 (ThermoFisher) with B27 Supplement with retinoic acid (Invitrogen), N2 Supplement (Invitrogen), 0.1% bovine serum albumin Fraction V (Invitrogen), monothioglycerol (Sigma), Glutamax (ThermoFisher), ascorbic acid (Sigma), and primocin. For a comprehensive list of reagents and catalog numbers, please see Supplementary Table 1 , for media recipes, see Supplementary Table 2 , and for antibodies, please see Supplementary Table 4 . Sorting and re-plating of day 15 progenitors into 3D HIOs At days 14\u201315, cells were sorted using the protocol outlined in ref. 25 and the surface marker algorithm described by our group in ref. 11 . Cells were dissociated using 0.05% Trypsin-EDTA (ThermoFisher), and washed in DMEM with 20% FBS. Cells were then strained using a 40 \u03bcm filter, spun at 300 \u00d7 g for 5 min, and resuspended in FACS buffer containing 5 \u03bcM Y27632 , and stained for CD47-PerCP\/Cy5.5 (BioLegend, cat. no. 323110) and CD26-PE (BioLegend, cat. no. 302705) for 30 min on ice, protected from light. Cells were then washed with additional DMEM\/20% FBS, spun down again at 300 \u00d7 g for 5 min, and resuspended in new FACS buffer containing 10 nM Calcein Blue in DMSO (ThermoFisher, cat. no. C1429). Cells were then sorted for either the CD47 lo \/CD26 hi or the CD47 lo \/GFP + populations using an operator-assisted MoFlo Astrios EQ (Beckman Coulter) at the Boston University Flow Cytometry Core Facility (FCCF). After sorting, cells were spun down and resuspended in 3D intestinal Matrigel (Corning 354234), in droplets of 50\u2013100 \u03bcL (supplemented with media conditions outlined below), at a density of 0.5\u20131 \u00d7 10 3 cells\/\u03bcL, and plated on a pre-warmed 24-well tissue culture plate. After allowing the droplets to solidify for 20 min in a 37 \u00b0C incubator, cells were treated with a variety of different media conditions described in Supplementary Tables 1 , 2 , supplemented with Y27632 . After 3\u20134 days, fresh media was added without Y27632 , and with further media replacement performed every 3\u20134 days, depending on confluency. Passaging of three-dimensional HIOs Any media was aspirated, and each well of HIOs was treated with 1 mL of Cell Recovery Solution (Corning cat. no. 354253), and placed at 4 \u00b0C for 30 min. Wells were then washed with PBS, and all contents were spun down at 300 \u00d7 g for 5 min. Organoids were pipetted gently, and resuspended in fresh Matrigel droplets supplemented with media, taking care not to break up organoids. Split densities varied based on original confluency and experimental needs. Organoids were plated and were cultured for up to 100 days with splits every 1\u20132 weeks as needed. Organoid immunofluorescence and microscopy Images of whole, live organoids were captured in their tissue culture vessel embedded in 3D Matrigel droplets and submerged in culture media, using a Keyence BZ-X710 All-in-one Fluorescence Microscope. For whole mounts, organoids were dissociated from their Matrigel droplet as described above, washed with PBS, and then fixed in 4% paraformaldehyde (Electron Microscopy Sciences, cat. no. 19208) at room temperature for 30 min. Whole mount HIOs were then washed with PBS, and blocked in 4% normal donkey serum (NDS) with 0.5% Triton X-100 (Sigma) for 30 min. They were then incubated overnight in primary antibody (see Supplementary Table 4 ) in 0.5% Triton X-100 and 4% NDS. Samples were then washed in 4% NDS and incubated with secondary antibody from Jackson Immunoresearch (1:300 anti-rabbit IgG (H + L), 1:500 anti-chicken IgY, or anti mouse IgG (H + L)) for 45 min at room temperature. Nuclei were stained with Hoechst dye (Thermo Fisher, 1:500). Whole organoids were then mounted with flouromont-G (Southern Biotech) on cavity slides and cover-slipped. For paraffin sectioning, samples were fixed as described above, and washed with PBS. Organoids were then embedded into HistoGel\u2122 Specimen Processing Gel (Richard Allen Scientific), and submitted to the Boston University Experimental Pathology Core Facility for paraffin embedding. Sections were then deparaffinized, followed by an antigen retrieval in a laboratory microwave for 3 min at full power, and 8 min at 30% power, and were set aside to cool for 30 min. Sections were then washed and stained as described above (See Supplementary Table 4 for full list of primary and secondary antibodies). Both stained whole mount and paraffin embedded sections were visualized with either a Zeiss LSM 700 laser scanning confocal microscope or a Nikon Eclipse Ti2 Series Microscope, and processed and analyzed in Fiji. Flow cytometry Cells for flow cytometry were dissociated using Gentle Cell Dissociation Reagent, followed by resuspension in FACS Buffer comprising of PBS \u2212\/\u2212 with 0.5% FBS. Antibodies for assessment of definitive endoderm, and the day 15 sort for lung\/intestinal progenitors are listed above, with appropriate isotype (IgG1) and unstained controls. To assess EpCam expression, organoids were dissociated as described above, and then further incubated with 0.05% Trypsin for 20 min at 37 \u00b0C. After incubation, the reaction was inactivated with DMEM\/20% FBS, and the cells were mechanically dissociated by pipetting. Cells were the spun down, resuspended in FACS Buffer, and stained with anti-EpCAM-APC (BioLegend, cat. no. 324208) for 20 min at room temperature, protected from light. Cells were then washed, resuspended in fresh FACS buffer, and strained into BD FACS tubes (Corning cat. no. 352235). All experiments were performed on a BD FACSCalibur\u2122 or STRATEDIGM S1000EON and analyzed using FlowJo. Forskolin swelling assay Forskolin-induced swelling was performed in organoids at days 29\u201331 of differentiation, using a similar protocol to previously published work 12 , 48 . Three independent differentiations were performed for each cell line; organoids were plated in three-dimensional Matrigel (at least six wells per differentiation) and incubated in fresh media for 1\u20132 days prior to forskolin treatment. Images were taken using a Keyence BZ-X700 fluorescence microscope immediately prior to (time 0 h) and 24 h after (time 24 h) the addition of 5 \u00b5M forskolin (Sigma). Imaging analysis was performed using ImageJ; sphere cross-sectional surface area was calculated using a binary analysis of circular (circularity > 0.3) and well sized (area > 900 \u00b5m 2 ) organoids. Whole well sphere cross-sectional area at time 0 was set to 1 and the ratio of time 24 h to time 0 cross-sectional area is indicated as normalized cross-sectional area. Time lapse images were captured using a Keyence BZ-X700 microscope with serial imaging of a mapped well (one per condition) every 2.5 min. Steady-state lumen area calculation As previously developed 51 , steady-state lumen area (SLA) was calculated by determining the ratio of lumen to whole organoid cross-sectional area. Using images captured as above, ImageJ was used for quantification (average of 30 organoids per cell line). Epithelial and luminal perimeter was measured manually for each image. RNA isolation and qRT-PCR analysis RNA was isolated from all samples using the RNeasy Kit (QIAGEN cat. no. 74014), either immediately after dissociation from tissue culture vessels, or after storage at \u221220 \u00b0C in RNAlater (ThermoFisher cat. no. 7020) as per the manufacturer\u2019s instructions. RNA was then reverse transcribed to cDNA using the SuperScript\u2122 III First-Strand Synthesis System (Invitrogen cat. no. 18080093) as per the manufacturer\u2019s recommended parameters. RNA was quantified using a NanoDrop\u2122 Lite Spectrophotometer (ThermoFisher) and input was standardized across all samples, to ensure normalized cDNA yields for downstream PCR applications. qRT-PCR was performed using both the TaqMan\u00ae or SYBR\u00ae Green (Applied Biosystems) master-mixes as per manufacturer\u2019s instructions, and the QuantStudio 7 Flex Real-Time 384 Well PCR System with barcoded 384-well plates. Relative fold change above undifferentiated iPSC was determined by calculating the \u0394\u0394Ct, using either GAPDH (TaqMan) or ACTB (SYBR). For primer sequences, see Supplementary Table 3 . scRNAseq of days 6 and 13 progenitors Surrogate wells of differentiated iPSCs (C17) toward endodermal lineages (as described above) from a single MF differentiation were isolated at days 6 and 13. Briefly, cells were disassociated and brought to a single-cell suspension with AccuMax, counted and resuspended in appropriate volume. Cell isolation, capture, and library prep followed the 10x Genomics scRNA-Seq (V2) protocol. Libraries produced were quantified by a Kapp kit and sequenced on an Illumina NextSeq 500. The Cell Ranger software pipeline produced the FASTQ and Counts matrix files. Day 6 library generated 2215 cells at a depth of 53,297 reads\/cell with mean genes per cell detected at 4090\/cell. Day 13 library generated 2763 cells at a depth of 53,471 with mean genes per cell detected at 3103\/cell. Seurat ver. 3.0 was used to further process data. Data was merged and normalized using the regularized negative binomial regression method 76 with cell degradation (i.e., mitochondrial percentage) regressed out during data scaling. Dimensionality reduction methods like PCA and UMAP were used to represent gene expression. Louvain method was used for clustering. Differential expression tests were done with MAST 77 . The dataset supporting the conclusions of this experiment is available in the GEO repository, accession GSE140405 . Bulk RNA sequencing by digital gene expression In order to test differential gene expression, we performed 3\u2032 tag digital gene expression profiling (DGE). Cells (bBU1) were differentiated to day 15, sorted for CD47 lo as described above, and plated in 3D Matrigel in CK + DCI and IM + CHIR. At day 42, RNA was isolated from HIOs as described above. In contrast to traditional bulk RNA-Seq, which generates sequencing libraries from the whole transcripts, 3\u2032 tag DGE only covers the terminal fragment of a transcript, complementary to 3\u2032-end sequences 57 . Restricting the sequencing coverage to a small part of the transcripts reduces the number of reads required to profile the full transcriptome. RNA was extracted and amplified from all described samples as described above. Subsequently, library preparation and sequencing were performed at the Broad Institute. Reads were aligned to the ENSEMBL human reference genome GRCh38.9 78 using STAR 79 . We used edgeR package 80 to import, filter and normalize the count matrix, followed by the g limma package 81 and voom 82 , for linear modeling and differential expression testing using empirical Bayes moderation to estimate gene-wise variability before significance testing based on the moderated t -statistic. We used a corrected p -value 83 of 0.05 as threshold to call differentially expressed genes. Functional characterization was done using Enrichr 28 , 29 . The dataset supporting the conclusions of this experiment is available in the GEO repository, accession GSE128922 . Statistical analysis Experimental data for Flow Cytometry and RT-PCR are reported as mean \u00b1 s.d. All statistical analysis was performed using GraphPad Prism Software, with statistical significance determined by one way-ANOVA followed by Tukey\u2019s test (>2 groups, n = 3 per group except IM + CHIR n = 2) or student\u2019s two-tailed unpaired T -test (2 groups, n = 3 per group) or paired two-tailed t -test, where * = p < 0.05, ** = p < 0.005, **** = p < 0.0001. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability The authors declare that all data supporting the findings of this study are available within the article and its supplementary material files, or from the corresponding author on reasonable request. The dataset supporting the conclusions of the bulk RNA-sequencing experiment (Fig. 1 ) has been deposited in the GEO repository under accession code: GSE128922 . The dataset supporting the conclusions of the scRNAseq experiment (Fig. 2 ) has been deposited in the GEO repository under accession code: GSE140405 . Further details of iPSC derivation, characterization, and culture are available for free download at  . Code availability All data analysis was performed using publicly available methodologies. The bulk RNA sequencing was analyzed using the tools described in the methods section above. The scRNA-seq data was analyzed on a platform using the Cell Ranger and Seurat ver 3.0 pipelines. ","News_Body":"Boston researchers have developed a new way to generate groups of intestinal cells that can be used, among others, to make disease models in the lab to test treatments for diseases affecting the gastrointestinal system. Using human induced pluripotent stem cells, this novel approach combined a variety of techniques that enabled the development of three-dimensional groups of intestinal cells called organoids in vitro, which can expand disease treatment testing in the lab using human cells. Published online in Nature Communications, this process provides a novel platform to improve drug screenings and uncover novel therapies to treat a variety of diseases impacting the intestine, such as inflammatory bowel disease, colon cancer and Cystic Fibrosis. Researchers at the Center for Regenerative Medicine (CReM) of Boston University and Boston Medical Center used donated human induced pluripotent stem cells (hiPSCs), which are created by reprogramming adult cells into a primitive state. For this study, these cells were pushed to differentiate into intestinal cells using specific growth factors in order to create organoids in a gel. This new protocol allowed the cells to develop without mesenchyme, which typically in other protocols, provides support for the intestinal epithelial cells to grow. By taking out the mesenchyme, the researchers could study exclusively epithelial cells, which make up the intestinal tract. In addition, using CRISPR technology, the researchers were able to modify and create a novel iPSC stem cell line that glowed green when differentiated into intestinal cells. This allowed the researchers to follow the process of how intestinal cells differentiate in vitro. \"Generating organoids in our lab allows us to create more accurate disease models, which are used to test treatments and therapies targeted to a specific genetic defect or tissue\u2014and it's all possible without harming the patient,\" said Gustavo Mostoslavsky, MD, Ph.D., co-director of CReM and faculty in the gastroenterology section at Boston Medical Center. \"This approach allows us to determine what treatments could be most effective, and which are ineffective, against a disease.\" Using this new protocol, the researchers generated intestinal organoids from iPSCs containing a mutation that causes Cystic Fibrosis, which typically affects several organs, including the gastrointestinal tract. Using CRISPR technology, the researchers corrected the mutation in the intestinal organoids. The intestinal organoids with the mutation did not respond to a drug while the genetically corrected cells did respond, demonstrating their future potential for disease modeling and therapeutic screening applications. The protocol developed in this study provides strong evidence to continue using human iPSCs to study development at the cellular level, tissue engineering and disease modeling in order to advance the understanding\u2014and possibilities\u2014of regenerative medicine. \"I hope that this study helps move forward our collective understanding about how diseases impact the gastrointestinal tract at the cellular level,\" said Mostoslavsky, who also is associate professor of medicine and microbiology at Boston University School of Medicine. \"The continual development of novel techniques in creating highly differentiated cells that can be used to develop disease models in a lab setting will pave the way for the development of more targeted approaches to treat many different diseases.\" ","News_Title":"Researchers develop new protocol to generate intestinal organoids in vitro","Topic":"Medicine"}
{"Paper_Body":"Abstract Ecosystems face both local hazards, such as over-exploitation, and global hazards, such as climate change. Since the impact of local hazards attenuates with distance from humans, local extinction risk should decrease with remoteness, making faraway areas safe havens for biodiversity. However, isolation and reduced anthropogenic disturbance may increase ecological specialization in remote communities, and hence their vulnerability to secondary effects of diversity loss propagating through networks of interacting species. We show this to be true for reef fish communities across the globe. An increase in fish-coral dependency with the distance of coral reefs from human settlements, paired with the far-reaching impacts of global hazards, increases the risk of fish species loss, counteracting the benefits of remoteness. Hotspots of fish risk from fish-coral dependency are distinct from those caused by direct human impacts, increasing the number of risk hotspots by ~30% globally. These findings might apply to other ecosystems on Earth and depict a world where no place, no matter how remote, is safe for biodiversity, calling for a reconsideration of global conservation priorities. Introduction The effects of human activities on our planet are so pervasive 1 that many denote the current epoch as the Anthropocene 2 . In these challenging times for biodiversity, species face extinction 3 , 4 , and ecosystems deteriorate under the synergic influence of global hazards (such as climate change) and local human stressors (such as overexploitation) 5 , 6 . Since global hazards act indeed globally, while local ones are associated with proximity to human activities, their combined effect is expected to decrease with the remoteness of the local ecosystem (Fig. 1a ). Therefore, pristine and isolated ecosystems\u2014sometimes referred to as \u201cwilderness areas\u201d\u2014are considered sanctuaries that have the potential to preserve nature during the current and future biodiversity crises 7 . Fig. 1: Theoretical and empirical relationships between remoteness vs local\/global hazards and ecosystem vulnerability from ecological dependencies. a Theoretical expectation of a decrease in local and local + global hazards with remoteness, and a counteracting increase in ecosystem vulnerability due to ecological dependencies. b Comparison between reef remoteness, measured as travel time (in log e transformed hours) from a reef locality to the closest major city 21 , and local hazards (cumulative local impacts on reef localities for 2013, consisting of six impacts related to fishing activities, light pollution, shipping, nutrient pollution, organic chemical pollution, and direct human interactions on coastal and near-coastal habitats 19 ). c Comparison between reef remoteness and global hazards (cumulative global impacts on reef localities for 2013, consisting of warming, acidification, and sea level rise 19 ). d Comparison between reef remoteness and cumulative local + global impacts. e Comparison between reef remoteness and bleaching susceptibility quantified, for each reef locality, as the average bleaching alert level from 1985 to 2019. f Comparison between reef remoteness and fish-coral dependency (quantified as the fraction of fish diversity directly or indirectly connected to corals through a coral \u2192 fish \u2192 fish network path at 1761 reef localities at a resolution of 1\u00b0 \u00d7 1\u00b0). For each relationship, we report the Spearman\u2019s rank correlation coefficient (r s ). Full size image However, local anthropogenic disturbances can favour generalist species over specialized ones 8 , 9 , 10 , as corroborated by previous work showing a positive relationship between the degree of ecological specialization and time with no disturbances in in-silico ecological networks 11 . In addition, due to the reduced in-flow of individuals into communities, we might also expect a higher specialization of ecological interactions in isolated habitats 12 . Specialized consumers can be more efficient in using their (few) resources when these are available but have, in principle, a higher co-extinction risk than generalist species 13 , 14 . Thus, while specialization increases ecological networks\u2019 robustness to species loss under stable environmental conditions, it also makes them more fragile towards potential cascading effects of primary extinctions (triggered, for example, by warming) 11 . Therefore, undisturbed and isolated communities should have many specialized interactions increasing their vulnerability to global change (Fig. 1a ). Such an ecological mechanism depicts a component of risk which is distinct and adds up to that stemming from the increased chances of local extinction that species are experiencing in isolated habitats 15 . Here we test whether a positive relationship between ecological specialization\/vulnerability and remoteness exists in natural systems, and whether the resulting increased risk of species loss in remote areas can question the common reliance on remote areas as biodiversity strongholds. For these goals, we focused on one of the most biologically diverse and socio-economically significant ecosystems on the planet, coral reefs, which, despite international attention and global protection programmes, continue to deteriorate under the influence of local human impacts (such as physical destruction and pollution) and the increasing effects of climate change (such as coral bleaching) 16 , 17 , 18 , 19 . By assessing the local dependency of fish assemblages on corals across the world\u2019s oceans, we show that the increase in the frequency and strength of fish-coral associations with distance from human settlements, combined with the global reach of coral bleaching, obliterate the benefits of remoteness on reef fish local extinction risk. Results and discussion Exploring the risk-remoteness relationship in reef fish We quantified remoteness as travel time to major cities 20 , 21 (Fig. 2a ). This measure captures both the local impact of direct anthropogenic disturbances (Fig. 1b ) and geographical isolation (Supplementary Fig. 1 ), being therefore well suited to test our hypotheses. Using a global dataset providing standardized measures of anthropogenic impacts on oceans 19 , we quantified the cumulative risk of species loss for reef fish assemblages from local and global hazards. Local hazards stem from direct human activities (six impacts related to fishing activities plus light pollution, shipping, nutrient pollution, organic chemical pollution, and direct human impacts on coastal and near-coastal habitats). They decline with increasing remoteness from human settlements (Figs. 1 b, 2b ). Global hazards are related to global processes such as ocean warming, ocean acidification and sea-level rise. They also decline with increasing remoteness but in a much weaker way (Figs. 1 c, 2c ). These patterns indicate that the necessary conditions for the risk-remoteness relationship to occur are met (Fig. 1a ). Fig. 2: Global maps of reef remoteness, local and global hazards, bleaching susceptibility and fish-coral dependency. a Global remoteness of coral reefs, quantified as travel time (in log e transformed hours) from the target reef locality to the closest major city 21 . b log e transformed local hazards (cumulative local impacts on reef localities for 2013, consisting of: six impacts related to fishing activities, light pollution, shipping, nutrient pollution, organic chemical pollution and direct human interactions on coastal and near-coastal habitats 19 ). c log e transformed global hazards (cumulative global impacts on reef localities for 2013, consisting of: warming, acidification and sea level rise 19 ). d log e transformed local + global hazards; e global bleaching susceptibility, quantified as the average bleaching alert level from 1985 to 2019. f Fish-coral dependency, quantified as the proportion of fish species that are directly or indirectly connected to corals through an identified coral \u2192 fish \u2192 fish network path at 1761 reef localities at a resolution of 1\u00b0 \u00d7 1\u00b0. Full size image Given that we were able to demonstrate the necessary conditions empirically, we then addressed our primary questions. Specifically, we explored (i) the relationship between reef remoteness and strength of fish-coral ecological interactions; and (ii) the potential effect of such a relationship on the shape of the risk-remoteness relationship for reef fish. Such explorations required first assessing the degree of fish-coral dependency globally. The fish species known from literature to rely exclusively on corals for food or shelter represent only a fraction (~20%) of local coral reef fish diversity 22 , 23 , 24 , 25 . However, experimental evidence suggests that the loss of corals may affect more than a half of fish diversity 26 , as also supported by recent statistical estimates 27 . This mismatch highlights that assessing fish assemblages\u2019 vulnerability to coral loss requires considering the dense network of elusive, direct, and indirect links 28 that create interaction pathways from coral to fish species. To assess the influence of both direct and indirect coral-fish links on fish species persistence, we collected information on the global distribution and ecological traits of 9,143 fish species associated with coral reefs. We used these data and analytical approaches of previous studies 29 , 30 , 31 , 32 to identify potential trophic and habitat-related associations between corals and fish, and between prey and predatory fish species. We constructed local-scale networks of potential coral \u2192 fish \u2192 fish interactions (on a spatial grid of 1\u00b0 \u00d7 1\u00b0 covering 1761 reef localities worldwide) by combining previously published information on fish dependency on corals, spatial co-occurrences of species (accounting for species niche and biogeographical history), and the ecological traits of fish species. Finally, we quantified the dependency of fish assemblages on corals as the proportion of fish species in each locality (i.e., 1\u00b0 \u00d7 1\u00b0 cell in our grid) with direct or indirect links to corals within the local ecological network (Fig. 2f ). This crucial step enables identifying indirect dependencies that would not have been apparent by just tallying coral dependent fish species from the literature. We found that the dependency of fish assemblages on corals increases with coral reefs\u2019 remoteness. These results support the remoteness-specialization hypothesis (Fig. 1f ) and provide an important confirmation that the co-evolutionary mechanisms affecting the emergence of specialization in ecological networks identified by theoretical work 11 , 12 also apply to real-world systems. Furthermore, the average percentage of fish species identified as dependent on corals by our network approach (38% \u00b1 10% s.d.) matches a recent global scale estimate obtained with a completely independent statistical model (41% \u00b1 18% s.d.) 27 , corroborating the idea that a world without corals might have half as many fish species. We then decomposed the fish-coral dependency by distinguishing between fish directly associated with corals (i.e., having a minimum distance to corals in the network of one link) compared to fish indirectly linked to corals (i.e., having a minimum distance to corals of more than one link). We found that the relative importance of directly associated fish increases with remoteness (Fig. 3 ), which further strengthens the support for the hypothesis. Not only does the overall fish coral dependency increase with remoteness from a quantitative perspective, but the relative contribution of direct dependencies becomes stronger. Since we expect the effects of coral loss to be stronger on directly coral-associated fish than on indirectly associated fish, this result reinforces the idea that remote communities will be substantially more affected than accessible ones as the impacts of global change propagate across ecological networks. An extensive set of sensitivity analyses confirm that these results are not affected by potential biases in the availability of information on fish ecology and distribution, nor are they driven by geographical variation in functional redundancy or species abundances (see \u201cMethods\u201d and Supplementary Fig. 2 ). Fig. 3: The relative contribution of direct fish-coral dependency increases with reef remoteness. We decomposed the total fish-coral dependency (i.e. the total fraction of fish species having at least one path to corals in the local coral \u2192 fish \u2192 fish networks) by distinguishing between fish species having a minimum distance of 1 step (i.e. network link) to corals, and fish species having a minimum distance to corals >1 step. While the fraction of fish with direct associations with corals increases with remoteness, that of indirectly associated fish decreases ( a ). Thus, as we move away from human influence, the relative contribution of direct fish-coral dependency increases from 26 to 68% on average ( b ). The plots summarize the results obtained in 1761 reef localities at a resolution of 1\u00b0 \u00d7 1\u00b0. Solid lines represent average values, while shaded areas represent standard deviations. The Spearman\u2019s rank correlation coefficients (r s ) were computed on the full set of results ( n = 1761), and not on the averaged values. Remoteness of coral reefs was quantified as travel time (in log e transformed hours) from the target reef locality to the closest major city 21 . Full size image Mapping fish risk hotspots The effect of global and local hazards and that of ecological dependencies show a striking spatial complementarity in determining global reef-fish risk. We mapped areas of high local + global hazards (falling in or above the 70th percentile) as well as areas of high combined fish-coral dependency and bleaching susceptibility. The latter are reef localities in or above the 70th percentiles of both fish-coral dependency and bleaching susceptibility, and comprise 9.4% of reef localities (165 1\u00b0 \u00d7 1\u00b0 cells of our global reef map). Comparing the two maps reveals how only nine reef localities, or 0.5% of areas highly threatened by local and global hazards also have a high fish-coral dependency and bleaching susceptibility. Thus, when we consider as hotspots of risk all localities from either of the two maps, the total number of reef fish assemblages at risk increases by 29%, from 535 to 691 reef localities (39.2% of reefs) (Fig. 4 ). Further, our study reveals that the fish communities on some of the most remote coral reefs are at relatively high risk of local species extinction (Figs. 2 and 4 ). Fig. 4: Spatial comparison between hot-spots of risk from local and global hazards vs. hotspots of risk from fish-coral dependency combined with bleaching risk. a Magenta pixels are reef localities (at a resolution of 1\u00b0 \u00d7 1\u00b0) falling above the 70 th percentile of local+global hazards (based on 2013 cumulative human impacts on reef localities 19 as in Fig. 2d ); cyan pixels are reef localities falling simultaneously above the 70 th percentile of fish-coral dependency (fraction of fish diversity per reef locality directly or indirectly connected to corals through the coral \u2192 fish \u2192 fish network, as in Fig. 2f ) and above the 70 th percentile of bleaching susceptibility (quantified, for each reef locality, as the average bleaching alert level from 1985 to 2019 as in Fig. 2e ); dark blue pixels are reef localities falling in both of the previous categories. b Percentage of reef localities worldwide where the fish community is put at risk by either local+global hazards (magenta line) or by fish-coral dependency combined with bleaching susceptibility (cyan line) for increasing values of remoteness, quantified as travel time (in log e transformed hours) from the target reef locality to the closest major city 21 . c Frequency of reef risk hotspots from either local+global hazards (magenta line), fish-coral dependency combined with bleaching susceptibility (cyan line), or both (dark blue line), for increasing values of remoteness (frequency relative to the respective total number of risk hotspots; data were pooled to the first decimal digit of remoteness). d Percentage of reef localities worldwide where the fish community is put at risk by either local + global hazards (magenta line), fish-coral dependency combined with bleaching susceptibility (cyan line), at least one of these two sources of risk (dashed dark blue line), or both (continuous dark blue line), for different percentile thresholds used to identify hotspots. The thresholds were identified (and applied) independently for local + global hazards, fish-coral dependency and bleaching susceptibility. Full size image Thus, the effects of local and global hazards in reef fish assemblages and those of ecological dependencies combined with bleaching vulnerability show a remarkable complementarity. Many areas that are not hotspots of risk from global or local hazards are potential hotspots of risk due to ecological network fragility and vice versa. This pattern is a strong warning that the ongoing biodiversity crisis is truly global and that distance from human influence does not guarantee safety. In turn, it highlights a profound need to account for ecological dependencies when assessing the risk global change poses to particular species. Accounting for ecological dependencies in risk assessment The very different nature of the risk sources makes exploring the potential effect of the remoteness-specialization relationship on global risk projections challenging. Here, the risk assessment framework provided by IPCC\u2019s fifth assessment report\u2014which quantifies risk as the combination of vulnerability, exposure, and hazard 5 \u2014might provide a formal layout to tackle the challenge. As a proof of concept, we devised an equation which quantifies risk by combining additively global and local hazards with the effect of ecological dependencies as applied to our fish-coral case study. To include the effect of ecological dependencies, we had to identify a potential \u201ctrigger\u201d capable of transforming the vulnerability stemming from fish-coral dependency into an additional component of local risk. An obvious trigger is local susceptibility to bleaching events 16 , 17 , 18 , which we identified based on bleaching alert level data from 1985 to 2019 (Fig. 2e ; see \u201cMethods\u201d for details). Bleaching is a global hazard (in that its cause does not originate from a single point source) that can have local effects. Bleaching susceptibility can indicate the probability of local coral mortality and loss. Combining bleaching susceptibility with the local estimate of fish-coral dependency (from the network analysis) quantifies, therefore, a local risk for fish communities stemming from the bottom-up effects of coral loss across coral-fish networks. Depending on the different weights assigned to either the risk component stemming from global and local hazards or to the one stemming from ecological dependencies (i.e., the \u03b1 and \u03b2 terms in Eq. 4 ) we can identify different patterns for the risk-remoteness relationship. The two extremes correspond to the risk emerging from, alternatively, only local and global hazards, or only ecological dependencies triggered by local bleaching susceptibility (Fig. 5 ). However, under the parsimonious assumption that both sources of risk are equally important for fish species (i.e., for example, that a coral dependent fish species would be equally threatened by mass coral mortality as by overfishing) the risk-remoteness relationship becomes flat, providing a strong argument that distance from humans does not make a fish community any safer. Fig. 5: Fish-coral dependency modifies the risk-remoteness relationship. Coral reef remoteness was quantified as travel time (in log e transformed hours) from the target reef locality to the closest major city 21 . The blue dots represent risk quantified as the sum of threats from local + global hazards on reefs (as in Fig. 2d ), while magenta dots represent risk quantified as bleaching susceptibility \u00d7 fish-coral dependency. Both components of risk (i.e., local + global hazards and bleaching susceptibility \u00d7 fish-coral dependency) were rescaled between 0 and 1. The two rescaled risks components are then combined into a single risk assessment equation where risk = [ \u03b1 (local + global hazards) + \u03b2 (bleaching susceptibility \u00d7 fish-coral dependency)]\/2. The lines in the plot represent the slopes of the trend lines from different parametrizations of the risk equation. When equal weight is given to the two risk components, risk remains almost constant across remoteness values (trend line slope = \u22120.002, black dashed line). Full size image The risk-remoteness relationship in global conservation With reef fish providing protein to half a billion people worldwide 33 and the critical importance of fish for addressing micronutrient deficiencies 34 , our results have profound societal implications; remote coral reefs won\u2019t be able to compensate for the losses of coral and fish species directly impacted by human activities, threatening the livelihoods of millions. Also, our study reveals an essential macroecological and eco-evolutionary mechanism that might dramatically amplify risks from global change in natural systems. The risk patterns observed for reef fish communities suggest that our already disconcerting projections about biosphere fragility might be overly optimistic. Moreover, the results of our study temper any hopes that, by protecting wilderness areas, we safeguard biodiversity vaults that can withstand the past and ongoing environmental destruction and changes brought by the Anthropocene. Therefore, aggressively addressing global hazards while supporting local management and conservation at both intensely used and remote locations emerges as the only hope to reverse the current biodiversity crisis. Methods Fish distribution We rasterized a detailed reef distribution vector map 35 at 5 \u00d7 5 latitude\/longitude degrees (by considering as reef area each cell in the raster intersecting a polygon in the original shapefile). We collected all the occurrences of fish species intersecting the rasterized reef area from both the Ocean Biogeographic Information System 36 and the Global Biodiversity Information Facility 37 . We used taxonomic and biogeographical (i.e., latitudinal\/longitudinal extremes for a given species) information from FishBase 38 to exclude potential incorrect occurrences (i.e., all the records falling outside the known species ranges). We also restricted the list to all the species for which FishBase provided relevant ecological information (as these were needed to evaluate prey-predator species interactions and identify indirect links between fish species and coral, see below). The filtered list comprises 9143 fish species. For these species, we used occurrence data to generate species ranges. For this, we used the \u03b1-hull procedure 39 , but instead of pre-selecting an \u03b1 parameter and using it for all species, we developed a procedure to obtain conservative species ranges while including most of the known occurrences. First, we selected a very small \u03b1 (0.001), to obtain a hull including most of the occurrences. Then, we progressively incremented \u03b1 in small amounts (0.005) by computing, for each increment, the ratio between the relative reduction in the resulting hull area (in respect to the previous hull), and the relative reduction of occurrences included in the hull (in respect to the total number of available occurrences for the target species). We stopped increasing \u03b1 when the ratio became <10. This procedure ensured that only isolated sites far from the core distribution of a species were excluded, while the range was stretched as much as possible around known occurrences. After delineating ranges for each species, we rasterized the reef vector map at a higher resolution (1 \u00d7 1 latitude\/longitude degree) and used it as a reference layer to extract fish occurrences at each reef location. This resolution is finer than that used by other global studies on reef fish diversity and distribution 40 , 41 . We took the 1\u00b0 \u00d7 1\u00b0 reef raster as the reference grid in all subsequent analyses and spatial interpolations, considering all the reef cells hosting at least five fish species ( n = 1761). Fish distribution validation To validate the fish distribution data, we compared them with a smaller independent dataset (GASPAR) providing fish occurrences for 196 globally distributed reef localities 42 , which we rasterized against the same reference grid used for our fish and coral distribution data. Because this dataset is based on comprehensive check-lists, its information can be considered as ascertained presence-absence data. Thus, we compared our list of fish occurrences (at one degree) in each cell where data from the GASPAR dataset were also available, computing true skill statistics score as TSS = [( a \u00d7 d ) \u2212 ( b \u00d7 c )]\/[( a + c ) \u00d7 ( b + d )], with a being predicted & observed occurrences; b being predicted, but not observed occurrences; c being observed but not predicted occurrences; and d being not observed and not predicted occurrences. We obtained a median TSS of 0.53, with a median sensitivity (the proportion of correctly predicted presences) of 0.60, and a median specificity (the proportion of correctly predicted absences) of 0.96, indicating that our mapped ranges were sufficiently conservative and rarely generated false presences. Finally, given that we were analysing coral reef fishes, we excluded a few grid cells where our methods returned no fish species. Environmental data We obtained environmental data (surface temperature, salinity, pH, and total chlorophyll as a proxy for productivity) at a spatial resolution of 5 arcmin from Bio-ORACLE v2.0 43 , and we upscaled these data on the reference reef grid (averaging the variable values in each 1 \u00d7 1 latitude\/longitude degree grid). Human impact As a measure of human impact on reef localities, we used the 14 cumulative human impact layers (for 2013) 19 available at  . For the purposes of our analysis, we categorized them into \u201clocal hazards\u201d stemming from direct human impacts (specifically, six impact layers related to fishing activities plus light pollution, shipping, nutrient pollution, organic chemical pollution, and direct human interactions on coastal and near-coastal habitats, such as trampling); and \u201cglobal hazards\u201d related to planetary wide processes (warming, acidification and sea level rise). The original dataset has a resolution of 1 km 2 and was therefore upscaled on the reference reef grid (averaging the variable values in each 1 \u00d7 1 latitude\/longitude degree grid). Time travel to cities We quantified the \u201cremoteness\u201d of each reef locality in terms of travel time (based on the fastest possible local means of terrestrial and aquatic transportation, hence excluding air travel) to the closest human settlement. For this, we used the procedure described in Weiss et al. 21 which consists of first combining information on land types and use, topography, distribution of roads and railways, position of national borders to derive a friction surface raster map indicating the average speed at which humans can travel through each pixel; and then applying an algorithm to identify the least costly paths (i.e. those requiring the shortest travel time) from each pixel to a target locality (e.g. a city) 21 . The original publication 21 provides a global map of accessibility that does not include water localities, which is clearly problematic for reefs. We therefore produced a new map of travel time (in hours) including also water pixels (at the same resolution of Weiss et al. 21 , i.e. 1 km 2 ) by using their friction map, the same layer of human urban centre (the \u2018high-density centres\u2019 variant of the Global Human Settlements 44 ) and the same cost distance algorithm (cumulative cost distance, which we computed using SAGA gis 45 ). Then, we upscaled the high-resolution map on our grid of 1 \u00d7 1 degree reef localities (computing the mean accessibility per each 1 \u00d7 1 degree cell). Bleaching susceptibility We downloaded annual layers reporting maximum bleaching alert level at the global scale and at a resolution of 50 km from 1985 to 2019 46 . Alert levels range from 0 (no stress) to 4 (mortality likely). We upscaled each layer on the reef reference grid (averaging alert level data) and computed an index of bleaching susceptibility as the average of recorded alert level in each coral reef pixel of the reference raster. Building ecological networks of fish \u2192 fish interactions We built networks of fish \u2192 fish interactions by using a multi-step procedure. (1) We generated a model capable of predicting the probability of occurrence of a prey-predator interaction between two given fish species based on some of their functional and ecological traits. For this, we obtained information on fish body size, trophic level, minimum and maximum depth, and habitat preference for 17,722 fish species from FishBase 38 , OBIS 36 and GBIF 37 (from the latter two sources, we specifically derived complementary data on species depth occurrences, which we used to fill in gaps in FishBase). We combined this information with a large dataset of known prey-predator interactions assembled from the Global Biotic Interactions dataset, GLOBI 47 . After filtering GLOBI according to the set of species with available ecological information and removing replicated records, we obtained 11,188 individual prey-predator pairs (for a total of 2643 species). We then identified an identical number of absences (pairs of species not interacting, and hence not having a link in the network). GLOBI includes only observed interactions, while it does not provide explicit information on non-interacting species. Although one can ideally generate a list of absences by sampling from all pairwise combinations of species not listed by GLOBI, this procedure might lead to the mislabelling of an actual prey-predator pair as a non-interacting pair simply because the species combination is missing from the database. To reduce this risk and generate \u201creliable\u201d pseudo absences (that is, truly representative of associations not possible in the real world), we used a stochastic approach where we sampled species pairs at random from all possible species combinations not present in GLOBI with the important addition of two constraints; namely, the prey needed to be at least 30% larger than the predator and\/or the predator needed to have a trophic level \u22643.0 (according to FishBase trophic classification). (2) We then used a random forest classifier (a machine learning technique; we used the Python package Scikit-learn 48 ) where the dependent variable was the presence or (pseudo) absence of interactions, and the independent variables were prey and predator traits (prey body size, prey trophic level, prey min and max depth and eight dummy variables for habitat; and the same variables for predator, for a total of 24 independent variables). We first explored the ability of the model by training it on a random subsample (50%) of the dataset (including true presences and pseudo absences), and then testing it on the remaining fraction. The model performed very well, being capable of predicting observed (true positives) and unobserved interactions (true negatives) in the testing set with an exceptional precision and accuracy (TSS = 0.93; type I error rate = 0.05; type II error rate = 0.02). After this first exploration, we used the full dataset to train the model to be used on the actual data. Out-of-bag validation score in the final model based on the complete dataset was >0.97. The random forest predictor was used to assess the probability of trophic interaction between a large list of potential interactions generated by combining all fish species from our reef fish occurrence dataset known to rely mainly or exclusively on fish for their survival (i.e. \u201ctrue piscivores\u201d, FishBase trophic level > 3.5), with all the fish in the dataset. The full list included 31,768,450 potential interactions, that we reduced to 6,721,450 interactions by keeping only the interacting pairs identified by the random forest classifier with a probability \u22650.9. (3) If the ecological dependency between two species is actually manifested then the two species must obviously co-occur at some locations, and vice-versa, co-occurrence is a necessary pre-requisite for an ecological dependency. Following this logic, we took a final, additional step to further filter and improve the fish \u2192 fish interaction list. In particular, we quantified the tendency for species to co-occur in the same locality as one potential proxy layer for species interactions, complementary to our other approaches. There are various factors that can affect the co-occurrence of two species. In a simplification, this can emerge from stochasticity, shared environmental requirements, shared evolutionary history, and ecological dependencies. We attempted to disentangle the effect of the last factor from the first three. For each target species pair, we computed overlap in distribution as the raw number of reef localities where both target species were found. Then, we compared this number with the null expectation obtained by randomizing the distribution of species occurrences across reef localities. We designed a null model accounting for randomness, species niche and biogeographical history, and hence randomizing the occurrence of species only within areas where they could have possibly occurred according to environmental conditions and biogeographical factors (e.g., in the absence of hard or soft barriers). To implement the null model, we first excluded from the list of potential localities all the areas outside the biogeographical regions where the target species had been recorded, with regions identified according to Spalding et al. 49 . Then, within the remaining areas, we identified all the reef localities with climate envelopes favourable to target species survival. For this, we identified the min and max of major environmental drivers (mean annual surface temperature, salinity, pH) where the target species occurred, and then we identified all the localities with conditions not exceeding these limits. We generated, for each pairwise species comparison, one thousand randomized sets of species occurrences by rearranging randomly species occurrence within all suitable localities. We quantified co-occurrence between the species pair in each random scenario. Finally, we compared the observed co-occurrence with the random co-occurrences, computing a p -value as the fraction of null models with co-occurrence identical or higher than the observed one. We kept only the pairs with a p -value < 0.05. This further reduced the fish \u2192 fish list to 1,365,863 interactions. We used the networks to build site-specific networks interactions in all 1\u00b0 \u00d7 1\u00b0 reef localities of our reference grid, by filtering it according to local fish species diversity. Measuring fish-coral dependency We compiled from literature 22 , 23 , 24 , 25 a list of fish species known to be associated with corals, in terms of habitat and\/or trophic specialization. This list includes 44% of the fish species we used in our analysis (4040\/9,143). As above, we used the known associations (or lack thereof) in the dataset to identify coral dependency in the unassessed fish. For this, we trained two independent random forest classifiers (again using the Python package Scikit-learn 48 ), one to model generic habitat associations, and the other one to model corallivory. In both models, the dependent variable was the presence\/absence of coral-association, and the independent variables were the same ecological features used to predict fish \u2192 fish trophic interactions (i.e. prey body size, prey trophic level, prey min and max depth and eight dummy variables for habitat), plus an additional variable quantifying the fraction of documented coral-associated species in the family of the target fish. Both models showed high precision and accuracy (with a TSS of 0.57 for the habitat association model, and of 0.81 for the corallivory model). Combining the list of coral dependent species from literature ( n = 897) with our model predictions ( n = 356) yielded a total of 1253 fish species. We linked all the coral-dependent species in the local fish \u2192 fish networks to a symbolic \u201ccoral\u201d node. Then, we quantified the overall dependency of fish assemblages on corals in each reef locality as the fraction of fish having at least one (unidirectional) path to corals across network links. We opted for this simple and intuitive measure after finding it produced virtually identical results to several, more complex, measures of fish-coral dependency that we explored (such as weighted and unweighted network distance between individual fish species and coral genera, and dependency values estimated using co-extinction simulations 50 ). For each network, we also quantified, separately, the fraction of fish species directly associated with corals (i.e., having a minimum distance to corals in the network of one link) and indirectly associated with corals (i.e. having a minimum distance to corals of more than one link). Risk assessment framework Following the definitions from the IPCC\u2019s fifth assessment report, we separate vulnerability (combination of sensitivity and adaptive capacity) from exposure to an extrinsic forcing agent (\u2018hazard\u2019). Then we quantify risk as the combination of vulnerability, exposure, and hazard 5 . Assuming, for illustrative purposes, a combined linear effect of local and global hazards on the risk experienced by a target system, we can model the latter ( R ) as: $$R=E\\times ({{{H}}}_{{{{{{\\rm{local}}}}}}}\\times {{{V}}}_{{{{{{\\rm{local}}}}}}}+{{{H}}}_{{{{{{\\rm{global}}}}}}}\\times {{{V}}}_{{{{{{\\rm{global}}}}}}}),$$ (1) with E being exposure, and H local , H global , V local and V global being local and global hazards and their respective vulnerabilities. If we then focus on average per-species risk, and assume no relationship between a system\u2019s remoteness and its intrinsic vulnerability to local and global hazards, we can further simplify the equation by setting E , V local and V global to 1: $$R={{{H}}}_{{{{{{\\rm{local}}}}}}}+{{{H}}}_{{{{{{\\rm{global}}}}}}}$$ (2) To account for the effect of the expected increase in ecological dependencies with remoteness 8 in the illustrative risk assessment model described by Eq. ( 2 ), we can add one term to quantify the combined effect of the vulnerabilities emerging from ecological dependencies combined with the exposure to relevant hazards capable of exploiting such vulnerabilities and triggering cascading effects through interaction links (\u201ctriggers\u201d): $$R=[\\alpha ({{{H}}}_{{{{{{\\rm{local}}}}}}}+{{{H}}}_{{{{{{\\rm{global}}}}}}})+\\beta ({{{{{\\rm{ecological}}}}}}\\,{{{{{\\rm{dependency}}}}}}\\times {{{{{\\rm{triggers}}}}}})]\/2$$ (3) Here, \u03b1 and \u03b2 are weights that can be used to modulate the relative importance of the two risk components (impacts from humans and global change vs ecological dependencies). Assuming that both risk components are rescaled in [0,1], to keep R in [0,1], we need to set 0 \u2264 \u03b1 \u2264 2 and \u03b2 = 2 \u2212 \u03b1 . Applying the risk assessment framework to reef fish communities We modelled the local risk of a reef fish community (in each 1\u00b0 \u00d7 1\u00b0 grid cells in the reef reference raster) using two different approaches. First, we quantified the risk as originating from the sum of local and global hazards (Eq. ( 2 )), where local and global hazards refer to the human impact layers 19 , as described in the \u201cHuman impact\u201d section above. Then, we re-assessed risk for each reef fish assemblage when accounting also for the risk component possibly deriving from ecological (fish-coral) dependencies combined with a relevant hazard (e.g., death of coral species due to bleaching) capable of triggering cascading effects across species interaction links by adapting Eq. ( 3 ): $$R=[\\alpha ({{{H}}}_{{{{{{\\rm{local}}}}}}}+{{{H}}}_{{{{{{\\rm{global}}}}}}})+\\beta ({{{{{\\rm{coral}}}}}}\\,{{{{{\\rm{dependency}}}}}}\\times {{{{{\\rm{coral}}}}}}\\,{{{{{\\rm{bleaching}}}}}}\\,{{{{{\\rm{susceptibility}}}}}})]\/2$$ (4) Fish-coral dependency and coral bleaching susceptibility were assessed as described in the sections above. To make the different components of risk comparable, prior to computing risk, we rescaled both local + global hazards and fish-coral dependency \u00d7 coral bleaching susceptibility between 0 and 1 across all reef localities. We did the same for the two sets of risk assessment values obtained using either Eqs. ( 1 ) or ( 2 ) (to permit direct comparison between the shapes of the risk-remoteness relationships). Both equations ideally provide the average risk of a species in a given locality, that is they assume exposure = 1. Also, they assume that the average local degree of vulnerability towards either local or global hazard is constant among localities; therefore, the respective vulnerability terms can be removed from the risk equations given that they are constants which would affect each locality the same. See the \u201cPotential caveats in the risk assessment equations\u201d section below for additional discussion on these issues. Assumptions of the risk assessment equations In this study we demonstrated how the framework of environmental risk assessment could incorporate species dependencies to more thoroughly examine the relationship between risk and remoteness. The proposed risk assessment equations are not intended to provide a definitive global risk assessment of reef fish assemblages. Instead, they are functional to assessing if, and to what degree, the risk component stemming from ecological dependencies can affect the expected relationship between risk and remoteness. The exact form of the equations is not overly important. In our equations we assumed constant vulnerability of fish assemblages to local and global hazards. That is, we ignored hazard-specific vulnerabilities. Although fish on coral reefs are likely vulnerable to the various hazards to different extents, modelling this amount of complexity would be extremely difficult. Considering the multiplicity of hazards per locality, and their potential complex interactions, it would be extremely challenging to obtain precise and realistic values for each of them to test our assumptions. However, we were able to compile several proxies of potential vulnerability to some of the main hazards, and in particular we computed the average vulnerability to fishing for all fish species in each reef locality, using the vulnerability measure provided by FishBase and based on the method by Cheung et al. 51 . Based on geographic distributions of the species, we determined the temperature, pH, and organic matter limits for each species, and then we used these data as indicators of each species potential tolerance to changes in temperature, acidification and organic pollution. Based on species habitat preference as defined by FishBase, we determined the fraction of demersal, benthopelagic, and coral associated species, as likely more affected by direct human disturbances (such as trampling); and the proportion of pelagic fish as potentially affected by shipping. We then compared those vulnerability proxies with remoteness, finding no strong relationships which would need to be incorporated into the risk equations (Supplementary Fig. 3 ). Then, we explored if our results held when exposure was taken into account (i.e., projecting the average per-species risk to the full fish assemblages). Exposure is a typical parameter involved in environmental risk assessment. For this, we multiplied the risk for the (log e -transformed) corresponding fish diversity. The observed patterns (Supplementary Fig. 4 ) were consistent with those relative to average species risk, which means that our conclusions scale up to fish assemblages. Again, the results of our study do not provide absolute estimates of risk for any of the fish species or coral reefs. However, with further research, we believe such estimates could be realistically obtained given sufficient species-specific data and more information about how the detrimental effects of each hazard are manifested. Sensitivity analyses We performed various analyses to check the robustness of our results and conclusions against potential biases stemming from data availability. In particular, we focused on potential relationships between the quality and quantity of information on species ecology and distribution, and remoteness. First, we checked for unequal distribution of sampling effort, under the hypothesis that remote localities could be less investigated than those close to human settlements. A comparison between the number of fish records available from OBIS 36 and GBIF 37 vs remoteness across all 1\u00b0 \u00d7 1\u00b0 reef localities revealed that this is not the case, with sampling effort remaining relatively high across all localities regardless of remoteness (Supplementary Fig. 2a , R 2 = 0.0008). We then explored whether the availability and quality of the ecological information we used in our analyses decreased with remoteness. For this we evaluated how the TSS values obtained from the comparison between the species ranges devised with our procedure and independent species distribution data from the GASPAR dataset 42 varied across reef localities with remoteness. We found no relationship (Supplementary Fig. 2b , R 2 = 0.0292). We also looked at the individual species TSS values obtained by comparing the distribution of a target species devised by our procedure with that according to the GASPAR dataset. Consistently with the previous result, we found no pattern linking the average of local species\u2019 TSS values to remoteness (Supplementary Fig. 2c , R 2 = 0.0001). We also explored whether remoteness affected negatively the fraction of species (for which we had distributional data) to be discarded in each locality due to the lack of the ecological information needed in our analyses. Again, the analysis revealed no effect of remoteness on data availability (Supplementary Fig. 2d , R 2 = 0.0992). Another potential question arising from our conclusions is whether they would still be valid when species abundances are considered alongside species diversity. To explore this issue, we tested whether the relative abundance of coral-dependent fish changes with remoteness using all the data available from the Reef Life Survey (RLS) dataset 52 . Finding that coral dependent fish become less abundant as remoteness increases would weaken our results, as the increasing species-level vulnerability stemming from coral dependency would be counterbalanced by the reduction in the overall number of individuals threatened by coral loss. This is not the case. On average, coral associated fish are more abundant than the other species (with an average number of individuals per survey of 782 for coral associated species vs 658 for non associated species). More importantly, the local proportion of associated individuals is unaffected by remoteness (Supplementary Fig. 2e , R 2 = 0.0002). Finally, we tested whether our results could be driven or confounded by a potential relationship between functional redundancy and remoteness. We quantified functional redundancy in each locality as one minus the ratio between the number of unique functional entities and total species richness. We identified functional entities using the method and functional diversity datasets as in Mouillot et al. 53 . We found no relationship between functional redundancy and remoteness (Supplementary Fig. 2f , R 2 = 0.0042). Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this article. Data availability All the data used in the analysis are freely available online from the sources listed in the Method section, and particularly: (1) reef distribution map:  ; (2) fish occurrence data:  ; and GBIF (Actinopterygii,  ; Elasmobranchii,  ; Holocephali,  ; Sarcopterygii,  ); (3) fish ecology data:  ; (4) ocean impact layers:  ; (5) the friction surface map needed to compute accessibility:  ; (6) human settlement data:  ; (7) bleaching alert data:  ; (8) environmental layers:  ; (9) marine eco-regions:  ; (10) fish trophic interactions:  ; (11) reef fish abundance data:  . (12) GASPAR dataset:  . The list of coral-associated fish compiled from literature as well as the data used for the fish range validation and the dataset of functional traits are provided together with all scripts used in the analyses at  54 . Code availability All the scripts and data permitting to replicate the analyses and reproduce the figures are available from  54 . ","News_Body":"An international research team led by Associate Professor Giovanni Strona from the University of Helsinki has identified a general macroecological mechanism that calls for a reconsideration of global conservation strategies. \"To truly understand how global change is affecting natural communities and to identify effective strategies to mitigate the ongoing dramatic biodiversity loss, it is fundamental to account for the overarching complexity emerging from biotic interactions. As we show in our new research, doing this might reveal important counterintuitive mechanisms,\" Giovanni Strona says. The researchers combined a massive dataset of fish distribution and ecological traits for more than 9,000 fish species. Using artificial intelligence techniques, they generated thousands of networks mapping the interactions between corals and fish and those between fish prey and fish predators in all reef localities worldwide. They quantified, for each locality, the degree of fish dependency on corals. This analysis confirmed what Strona and colleagues showed in another paper published earlier this year: coral loss might detrimentally affect, on average, around 40 per cent of fish species in each coral reef area. The researchers also found that the dependency between fish and corals becomes stronger the further away they are from humans. This means that fish communities in remote reefs might be the most vulnerable to the cascading effects of coral mortality. Areas of critical vulnerability Next, the researchers asked whether the increased risk that stems from the potential cascading effects of coral mortality might counteract the benefits that remote fish communities experience because they are far away from direct impacts of human activities. \"For this, we devised a novel risk assessment framework that is applicable to any ecosystem. It combines local anthropogenic impacts such as overfishing and pollution and global impacts like climate and environmental change with the risk deriving from ecological interactions,\" explains Mar Cabeza, head of the Global Change and Conservation Lab at the University of Helsinki. The framework revealed that taking into account ecological dependencies flattens the expected negative relationship between extinction risk for fish communities and remoteness. \"For example, the hotspots of risks for fish communities from local human-derived impacts and global change are almost perfectly the same as the hotspots of risk from fish coral dependencies. This produces a global map of risk for fish communities where no place is safe, regardless of distance from humans,\" Giovanni Strona says. \"The validity and relevance of these findings might extend far beyond reef fish, depicting a world where remote localities, rather than safe havens for biodiversity, might be, instead, areas of critical vulnerability,\" Mar Cabeza concludes. The research was published in Nature Communications. ","News_Title":"Remote areas are not safe havens for biodiversity","Topic":"Biology"}
{"Paper_Body":"Abstract Natural D-serine (D-Ser) has been detected in animals more than two decades ago, but little is known about the physiological functions of D-Ser. Here we reveal sleep regulation by endogenous D-Ser. Sleep was decreased in mutants defective in D-Ser synthesis or its receptor the N-methyl-D-aspartic receptor 1 (NMDAR1), but increased in mutants defective in D-Ser degradation. D-Ser but not L-Ser rescued the phenotype of mutants lacking serine racemase (SR), the key enzyme for D-Ser synthesis. Pharmacological and triple gene knockout experiments indicate that D-Ser functions upstream of NMDAR1. Expression of SR was detected in both the nervous system and the intestines. Strikingly, reintroduction of SR into specific intestinal epithelial cells rescued the sleep phenotype of sr mutants. Our results have established a novel physiological function for endogenous D-Ser and a surprising role for intestinal cells. Introduction Amino acids exist in stereoisomers, with all common amino acids except glycine having L- and D-enantiomers depending on the relative spatial arrangement surrounding the \u03b1-carbon. Though L-amino acids were traditionally thought to be the only natural form, D-amino acids have been found in biological organisms. Free D-serine (D-Ser) has been found in species ranging from bacteria to mammals 1 , 2 , 3 , 4 . D-Ser is an effective co-agonist of the N-methyl-D-aspartate subtype of glutamate receptor (NMDAR) 5 , 6 . D-Ser is synthesized from L-Ser by serine racemase (SR) 7 and degraded by D-amino acid oxidase (DAAO) 4 and SR 8 . Distribution of D-Ser and NMDAR as determined by chemical measurement 9 and immunohistochemistry 10 supports D-Ser as an endogenous coagonist acting on the glycine modulatory site of the NR1 subunits of the NMDAR 11 , 12 . A role for endogenous D-Ser in synaptic transmission was confirmed by selective degradation of D-Ser with DAAO which attenuated NMDAR function and its rescue by D-Ser 13 . It was proposed that the synaptic NMDAR is activated by D-Ser, whereas the extrasynaptic NMDAR is gated by glycine 14 . Sleep is important for animals and is regulated by both circadian and homeostatic processes 15 . While significant progress has been made in the molecular understanding of circadian rhythm, much less is known about homeostatic regulation of sleep. For more than a decade, Drosophila has been used as a model for genetic studies of sleep 16 , 17 . Genes and brain regions regulating sleep have been identified 18 , 19 , 20 , 21 . Recently, NMDAR and D-Ser have been indicated to participate in sleep regulation in both flies and mammals 22 , 23 , 24 . However, whether D-Ser regulates sleep remains unclear. Here, through a genetic screen followed by a thorough investigation of the synthases, the oxidases, and the receptor of D-Ser, combined with pharmacological genetic epistasis experiments, we report evidence that sleep is regulated by D-Ser through NMDAR1. Furthermore, the synthases, the oxidases, and the receptor of D-Ser have all been found to be expressed in the central nervous system and in the intestine. Strikingly, the intestinal but not neuronal expression has been proved to be important for sleep regulation, indicating a novel role of the intestine in sleep regulation. Taken together, these results suggest that D-Ser made by intestinal SR promotes sleep through NMDAR1 in Drosophila . Results Decreased sleep in shmt mutants and rescue by L-Ser or D-Ser In a screen of homozygous P-element insertion lines for mutations affecting sleep, we found that sleep duration was decreased when a P element was inserted into the CG3011 gene. Analysis of its sequence (Fig. 1a and Supplementary Fig. 1 ) indicates that CG3011 encodes the serine hydroxymethyltransferase (SHMT), which participates in the synthesis of L-Ser 25 , 26 (Fig. 1b ). There are three isoforms of shmt in fly, the original mutant uncovered by our screen contained a P element insertion in the 5\u2032 non-coding region of isoform A (Fig. 1a ). To investigate the function of Drosophila SHMT, we generated mutations in the shmt gene by using CRISPR-Cas9. Deletion of all three isoforms caused lethality, whereas frameshift mutations introducing a STOP codon in the first coding exon of shmt affecting only isoform A resulted in viable shmt mutants ( shmt-es in Fig. 1a ). The mRNA level of isoform A shmt in shmt-es was significantly decreased compared with wild type ( wt ) flies detected by quantitative polymerase chain reaction (qPCR) analyses (Fig. 1c ). The shmt-es mutants were backcrossed into an isogenic Canton-S (CS) line in our lab 27 , and used in further analysis. Fig. 1 Sleep phenotypes of shmt mutants. a A schematic representation of a point mutation leading to a premature stop codon in shmt (thus shmt-early stop or shmt-es ). Also shown is the amino acid sequences of the shmt-es mutant line used here. Single gRNA generated insertion and\/or deletion (indel) in the shmt gene, introducing a frameshift and a stop codon (asterisk). b A diagram of D-Ser synthesis pathway. c mRNA level of isoform A shmt in shmt-es was significantly reduced. d Sleep profiles of shmt-es (red) ( n = 57) and wt (black) ( n = 236) flies, plotted in 30 min bins. White background indicates the light phase (ZT 0\u201312); shaded background indicates the dark phase (ZT 12\u201324). e Statistical analyses. Daytime and nighttime sleep durations were significantly reduced in shmt-es flies. In this and other figures, open bars denote daytime sleep and filled bars nighttime sleep. f Drug treatment of both L- and D-Ser rescued the nighttime sleep duration of shmt-es flies to the wt level. The number of flies used in the experiment was denoted under each bar. *** P < 0.001, n.s. P > 0.05, Mann\u2013Whitney test was used in ( c , e ), two-way ANOVA test with Bonferroni posttests was used in ( f ) to compare the sleep durations between wt and shmt-es , Kruskal\u2013Wallis test with Dunn\u2019s posttest was used in ( f ) to compare the sleep durations of shmt-es under different drug treatments. Error bars represent s.e.m. Male flies were used Full size image Sleep was measured in wt and shmt-es flies by video recording and analysis. When tested under the 12 h light\/12 h dark (LD) condition, durations of both nighttime sleep and daytime sleep were significantly decreased in shmt-es flies (Fig. 1d, e ). Because L-Ser is the substrate for D-Ser synthesis (Fig. 1b ) 7 , we tested whether the sleep phenotype of shmt mutants was attributed to L- or D-Ser by rescuing shmt mutants with either L-Ser or D-Ser. After eclosion, flies were raised with either sucrose or sucrose supplemented with L-Ser or D-Ser for 3 days before being transferred into recording tubes with the same media. Feeding either L-Ser or D-Ser could rescue the sleep defect of shmt-es flies (Fig. 1f ). Thus, the sleep defect of shmt-es flies could be due to the lack of either L- or D-Ser. Decreased sleep and increased arousal in sr mutants SR is responsible for D-Ser production in vivo 28 , 29 , 30 . Drosophila SR is encoded by CG8129 (Supplementary Fig. 2 ) 31 . To investigate the function of D-Ser, we generated sr knock-out (srko) flies in which most of the coding region of sr was deleted (Fig. 2a ). Under LD condition, the nighttime sleep duration was significantly reduced in srko flies (Fig. 2b, c ). We also generated four other sr mutants, including two deletion mutants sr-middle and sr-long (Supplementary Fig. 3a ), and two insertion mutants SRKO-Gal4 and SRKO-Flp with the coding region replaced by the yeast Gal4 or Flp gene (Supplementary Fig. 3b ). The duration of nighttime sleep but not that of daytime sleep was also reduced in these four mutants (Supplementary Fig. 3c-f ). Because the nighttime sleep duration was decreased in all five sr mutants as well as the shmt-es mutants, we thereafter focused on the role of D-Ser in nighttime sleep, but not the daytime sleep which was observed in only the shmt-es mutants but none of the five sr deletion mutants. Fig. 2 Sleep phenotype of sr mutants. a A schematic representation of the CG8129 gene with the red bar indicating the region deleted in srko mutants. Two transcription variants (NM_141629, NM_169273) generate two proteins of 469aa (NP_649886) and 316aa (NP_731340), respectively. They have an identical C-terminal part while the longer variant has additional 153aa at the N-terminal region. In srko , aa 114\u2013469 in the longer form and aa 1\u2013316 in the shorter form were deleted. b Sleep profiles of srko (red) ( n = 42) and wt (black) ( n = 69) flies, plotted in 30 min bins. c Statistical analyses. Nighttime sleep durations were significantly reduced in shmt-es flies. d Drug treatment of D-Ser, but not L-Ser, rescued the nighttime sleep duration of srko flies to wt flies fed with mock. The number of flies used in the experiment was denoted under each bar. e Arousal rates of srko and wt flies under light stimuli. The arousal rate of srko flies was significantly increased. Numbers of flies that were aroused by the stimuli (open bars) and that kept sleep (filled bars) were plotted. Light stimuli were applied to wt and mutant flies as indicated. Arousal rate was denoted under each bar. f Drug treatment of D-Ser, but not L-Ser, rescued the arousal rate of srko flies to the wt level. *** P < 0.001, ** P < 0.01, * P < 0.05, n.s. P > 0.05. Mann\u2013Whitney test was used in ( c ), two-way ANOVA test with Bonferroni posttests was used in ( d ) to compare the sleep durations between wt and srko under the same treatment, Kruskal\u2013Wallis test with Dunn\u2019s posttest was used in ( d ) for other statistical analyses. Fisher\u2019s exact test was used in ( e , f ). Error bars represent s.e.m. Male flies were used Full size image While sleep duration can directly reflect sleep defect, stimuli-induced arousal rate can reflect sleep intensity. Previous studies have shown that sleep duration and arousal can be regulated separately 32 , 33 . We tested arousal response to light in wt and srko flies using similar method as previous studies 34 . On the 4th night after being transferred to the recording tubes, sleeping flies were shined with light pulses for 1 s at zeitgeber time (ZT) 16, and then the numbers of flies that were awaken and that kept sleep were counted. The arousal rate was significantly elevated in srko flies under stimulus (Fig. 2e ). Latency to sleep was increased in srko flies whereas circadian rhythm and sleep recovery after sleep deprivation were not significantly different between srko and wt flies (Supplementary Fig. 4 ). Taken together, these results indicate that sr is required for regulation of nighttime sleep and stimulus-induced arousal response. Role of D-Ser in sleep and arousal SR is the only known enzyme responsible for D-Ser synthesis in vivo (Fig. 1b ) 35 , L-Ser and D-Ser could be converted reciprocally to each other by SR. Sleep defect in shmt-es mutant flies is consistent with a role for either D- or L-Ser, whereas the phenotypes in srko mutants suggest that D-Ser is important for sleep. To further distinguish between D- and L-Ser, they were separately applied to srko flies. As discussed earlier, both L-Ser and D-Ser could rescue the sleep defect of shmt-es flies (Fig. 1f ). However, only D-Ser, but not L-Ser, could rescue the sleep defect of srko flies (Fig. 2d ). No significant sleep change was observed in srko flies fed with L-Ser compared to mock. By contrast, the nighttime sleep duration of srko was rescued by D-Ser to the level of wt flies fed with mock. When we examined the arousal response, we found that the arousal rate of srko flies was also rescued to the wt level by D-Ser, but not by L-Ser (Fig. 2f ). These results suggest that D-Ser, but not L-Ser, is important for sleep and arousal. Increased sleep and decreased arousal in daao-dko mutants D-Ser is degraded by DAAOs (Fig. 3a ). There are two genes encoding DAAO in Drosophila : CG12338 and CG11236 36 . To investigate their functional significance, we generated deletion mutants for each of the gene (Fig. 3b ). Fig. 3 Sleep phenotype of daao mutants. a A diagram of D-Ser degradation pathway. b Schematic representations of CG12338 and CG11236 genes. The red bars indicate regions deleted in mutant flies. For CG12338 , two transcription variants (NM_001299363, NM_136759) generate the same protein of 335aa (NP_001286292, NP_610603) and most of the coding region except the first 97 base pairs (bp) was deleted in the CG12338 knock-out ( cg12338ko ) flies. For CG11236 , two transcription variants (NM_135231, NM_001258985) generate two proteins of 341aa (NP_609075) and 338aa (NP_001245914), respectively. CG11236 knock-out ( cg11236ko ) contained the first 94aa because of deletion of 284 bp to 889 bp (NM_135231) or 284 bp to 880 bp (NM_001258985) from each transcript which resulted in frameshifts. c Sleep profiles of cg12338ko (blue) ( n = 42), cg11236ko (magenta) ( n = 46), double knock-out ( daao-dko , red) ( n = 40), and wt (black) ( n = 200) flies, plotted in 30 min bins. d Statistical analyses. Both daytime and nighttime sleep durations were significantly increased in daao-dko flies, nighttime sleep duration was significantly decreased in cg11236ko flies. e Arousal rate was significantly reduced in daao-dko flies. Numbers of flies were plotted for wt (black) and daao-dko (red) flies. *** P < 0.001. Mann\u2013Whitney test was used in ( d ), Fisher\u2019s exact test was used in ( e ). Error bars represent s.e.m. Male flies were used Full size image When a single daao was interrupted, there was not much change in sleep durations: nighttime sleep duration was significantly reduced in CG11236ko flies, while daytime sleep duration of CG11236ko and daytime and nighttime sleep durations of CG12338ko were not different from that in the wt (Fig. 3c, d ). In order to tell that if this is due to the redundant function of these two daao genes, we generated double knock-out ( daao - dko ) flies with both genes interrupted. We found that both daytime and nighttime sleep durations were significantly increased in daao - dko flies (Fig. 3c, d ), and the arousal rate of daao - dko flies was significantly decreased (Fig. 3e ). The opposite sleep and arousal phenotype of daao - dko flies to that in the srko flies further supports that D-Ser promotes sleep and inhibits the arousal response in Drosophila . D-Ser regulation of sleep through NMDAR1 D-Ser is a co-agonist of the NMDA receptor (NMDAR) 13 , 14 . There are two NMDAR subunits in Drosophila but only NMDAR1 contains the D-Ser binding site 37 . Pan-neuronal NMDAR1 knockdown by elav-Gal4 driven RNA interference (RNAi) reduces sleep in Drosophila 22 , but it did not distinguish whether the sleep effect was caused by D-Ser or by other NMDAR1 agonists, such as glycine. Recently, a study has found that NMDAR-mediated field excitatory post-synaptic potentials (NMDA-fEPSPs) and D-Ser levels fluctuate with sleep need in mice 24 , further raising the possibility of D-Ser regulating sleep through NMDAR1, in both flies and mammals. To investigate whether NMDAR1 regulates sleep, we generated nmdar1ko by replacing the first three coding exons of nmdar1 with 2A-Gal4-STOP right after the start codon (Fig. 4a ). Similar with srko flies, sleep duration was significantly decreased and arousal rate significantly increased in nmdar1ko flies (Fig. 4b\u2013d ). Fig. 4 Regulation of sleep by D-Ser upstream of the NMDAR1. a A schematic representation of nmdar1 gene. The single transcription variant (NM_169059) generates a protein of 997aa (NP_730940). The sequence from aa2 to 107 was deleted and replaced with T2A-Gal4-STOP-3P3-RFP in nmdar1ko flies. b Sleep profiles of nmdar1ko (red) ( n = 36) and wt (black) ( n = 69) flies, plotted in 30 min bins. c Statistical analyses. Daytime and nighttime sleep durations were significantly reduced in nmdar1ko flies. d Arousal rate of nmdar1ko flies (red) was significantly higher than that of wt (black). Numbers of flies that were aroused (filled bars) and that kept sleep (open bars) were plotted for nmdar1ko and wt flies. e , f Neither L- nor D-Ser affected the sleep duration ( e ) or the arousal rate ( f ) of nmdar1ko flies. Numbers below each bar represent the number of flies tested in ( e ). g , h The sleep phenotype ( g ) and arousal phenotype ( h ) of daao-dko flies were masked by nmdar1ko in triple knockout flies. Nighttime sleep durations were significantly increased in daao-dko (green) ( n = 46) flies, and significantly decreased in nmdar1ko (blue) ( n = 32) and triple knockout (red) ( n = 34) flies, compared to wt (black) ( n = 47) flies ( g ). Arousal rate was significantly decreased in daao-dko (green) flies, and significantly increased in nmdar1ko (blue) and triple knockout (red) flies, compared to wt (black) flies ( h ). *** P < 0.001, ** P < 0.01, * P < 0.05, n.s. P > 0.05. Mann\u2013Whitney test was used in ( c ), two-way ANOVA test with Bonferroni posttests was used in ( e ) to compare the sleep durations between wt and nmdar1ko under the same treatment, Kruskal\u2013Wallis test with Dunn\u2019s posttest was used in ( e ) for other statistical analyses and in ( g ). Fisher\u2019s exact test was used in ( d ), ( f ), and ( h ). Error bars represent s.e.m. Male flies were used Full size image We carried out two experiments to investigate the relation between D-Ser and NMDAR1 in sleep and arousal: pharmacological application of L-Ser and D-Ser on nmdar1ko flies, and generation of triple knock-out flies lacking the nmdar1 and the two daao genes. Although D-Ser rescued the sleep defect and arousal defect in shmt-es and srko flies (Figs. 1f and 2d, f ), neither L-Ser nor D-Ser could affect the sleep duration and arousal rate of nmdar1ko mutants (Fig. 4e, f ). These results support that D-Ser lies downstream of SHMT and SR but upstream of NMDAR1 in sleep regulation. To test the epistasis relationship of daao genes and the nmdar1 gene, triple knockout flies carrying all three mutations of CG12338ko , CG11236ko , and nmdar1ko were generated by combining daao-dko and nmdar1ko together. Triple knockout flies phenocopy nmdar1ko flies in sleep duration and arousal rate: sleep duration was increased and arousal rate decreased in daao-dko flies, while sleep duration was decreased and arousal rate increased in nmdar1ko and triple knockout flies (Fig. 4g, h ). No significant difference was detected between the sleep duration and arousal rate of nmdar1ko and triple knockout flies (Fig. 4g, h ). These results indicate that nmdar1 acts downstream of daao . Thus, both the pharmacological experiment and the genetic epistasis experiment support that D-Ser functions through NMDAR1 to regulate sleep in Drosophila . Expression patterns of shmt , sr , daao , and nmdar1 To examine the expression of genes participated in the synthesis, degradation, and function of D-Ser, we fused Gal4 in-framely to shmt , sr , CG12338 , CG11236 , and nmdar1 , making the shmt-KIGal4 , SR-KIGal4 , CG12338-KIGal4 , CG11236-KIGal4 , and nmdar1-KIGal4 lines (Supplementary Table 1 ). Then UAS-mCD8::GFP was driven by these lines to label the membrane of cells expressing them respectively. We found that all five lines were expressed in the brain, the ventral nerve cord (VNC), and the gut (Fig. 5 , Supplementary Fig. 5 ). shmt-KIGal4 was expressed in the glia cell and neurons in the brain and the VNC (Fig. 5a, b ), and in the midgut (Fig. 5c , Supplementary Fig. 5a ). SR-KIGal4 was expressed in four neurons in the subesophageal ganglion (SOG) of the brain (Fig. 5d ), in four pairs of neuronal tracts projecting to the prothoracic, mesothoracic, metathoracic neuromere (PN, MN, MtN), and the abdominal center (AC) of the VNC (Fig. 5e ), and in the midgut enterocytes (ECs) (Fig. 5f , Supplementary Fig. 5b ). Fig. 5 Expression patterns of shmt , sr , daao , and nmdar1 . Expression patterns of shmt-KIGal4 ( a \u2013 c ), SR-KIGal4 ( d \u2013 f ), CG12338-KIGal4 ( g \u2013 i ), CG11236-KIGal4 ( j \u2013 l ), and nmdar1-KIGal4 ( m \u2013 o ) labeled by mCD8::GFP in the brain ( a , d , g , j , m ), the ventral nerve cord (VNC) ( b , e , h , k , n ), and the gut ( c , f , i , l , o ). The tissues were immunostained with anti-GFP and anti-DLG in ( d , e ), immunostained with anti-GFP in ( c , f , i , l , o ), immunostained with anti-GFP and nc82 in other panels. Scale bars are 30 \u03bcm Full size image CG12338-KIGal4 was expressed in the MB, the PI, and the SOG of the brain (Fig. 5g ), in the MN, the MtN, and the AC of the VNC (Fig. 5h ), and in the midgut, the Malpighian tubules, and the neurons projecting to the hindgut (Fig. 5i , Supplementary Fig. 5c ). CG11236-KIGal4 was expressed similarly to SR-KIGal4 : in quite a few neuron tracts projecting to the SOG of the brain (Fig. 5j ), in four pairs of neuronal tracts projecting to the PN, the MN, the MtN, and the AC of the VNC (Fig. 5k ), and in the midgut ECs (Fig. 5l , Supplementary Fig. 5d ). nmdar1-KIGal4 was expressed broadly in the brain, including the PI, the SOG, the FSB, and the superior neuropils (SNP) (Fig. 5m ), in the AC and the afferent neurons of the PN of the VNC (Fig. 5n ), and in neurons projecting to the proventriculus, the midgut regions R1 and R5, and the hindgut (Fig. 5o , Supplementary Fig. 5e ). Intestinal SR in sleep regulation Given that the synthases, the oxidases, and the receptor of D-Ser were all found to be expressed in the central nervous system and the gut, we next seek to identify which part is required for D-Ser to promote sleep by reintroducing UAS-SR into different parts in the srko background. The nighttime sleep duration of sr mutants was rescued by the reintroduction of UAS-SR back into sr -expressing cells (Fig. 6a ) labeled by SRKO-Gal4 in which 2A-Gal4-STOP was fused to the start codon of sr (Supplementary Table 1 ). However, pan-neuronal expression of sr driven by Elav-Gal4 failed to rescue the nighttime sleep duration (Fig. 6b ), suggesting that sr does not function in neurons to promote sleep. Furthermore, we labeled non-neuronal sr -expressing cells by SRKO-Gal4, Elav-Gal80 , in which neuronal expression but not intestinal expression of SRKO-Gal4 was blocked by Elav-Gal80 (Fig. 6g\u2013i ). Reintroduction of sr back into non-neuronal sr -expressing cells also rescued the sleep defect of sr mutants (Fig. 6j ), suggesting that neuronal sr is not necessary for sleep promoting. Taken together, these results suggest that neuronal sr is neither sufficient nor necessary for sleep promoting, thus sr functions elsewhere to regulate sleep. Fig. 6 Requirement of intestinal but not neural SR in sleep regulation. a Reintroduction of sr in sr -expressing cells rescued the nighttime sleep defect of sr mutants. Nighttime sleep durations of srko\/SRKO-Gal4 (green), srko\/srko,UAS-SR (blue), SRKO-Gal4\/srko,UAS-SR (red), and wt (black) flies were plotted. b Reintroduction of sr pan-neuronally failed to rescue the sleep defect of sr mutants. Nighttime sleep durations of Elav-Gal4\/Y; srko\/srko (green), srko\/srko,UAS-SR (blue), Elav-Gal4\/Y; srko\/srko,UAS-SR (red), and wt (black) flies were plotted. c \u2013 e sr -expressing cells and MyoIA -expressing cells overlap in the fly gut. The nuclei of MyoIA -expressing cells were labeled by StingerRed driven by MyoIA-Gal4 ( d ), and the sr -expressing cells were labeled by GFP driven by SRKI-LexA ( c ). f Expression patterns of LexAop-Flp, UAS-FRT-STOP-FRT-GFP, MyoIA-Gal4, SRKI-LexA flies in the gut. Cells co-expressing MyoIA and sr were labeled with GFP. g \u2013 i sr -expressing non-neuronal cells were labeled by SRKO-Gal4,Elav-Gal80 ( i ), whereas the neural cells expression was blocked ( g , h ). j Reintroduction of sr driven by SRKO-Gal4,Elav-Gal80 rescued the sleep defect of sr mutants. Nighttime sleep durations of srko\/SRKO-Gal4,Elav-Gal80 (green), srko\/srko,UAS-SR (blue), SRKO-Gal4,Elav-Gal80\/srko,UAS-SR (red), and wt (black) flies were plotted. k \u2013 m Expression patterns of MyoIA-Gal4 in the brain ( k ), the VNC ( l ), and the gut ( m ) labeled by mCD8::GFP. n Expression of sr in MyoIA -expressing cells rescued the nighttime sleep defect of sr mutants. Nighttime sleep durations of srko\/srko,MyoIA-Gal4 (green), srko\/srko,UAS-SR (blue), srko,MyoIA-Gal4\/srko,UAS-SR (red), and wt (black) flies were plotted. o \u2013 q MyoIA -expressing non-neuronal cells were labeled by MyoIA-Gal4,Elav-Gal80 ( q ), whereas the neural cells expression was blocked ( o , p ). r Expression of sr driven by MyoIA-Gal4,Elav-Gal80 rescued the sleep defect of sr mutants. Nighttime sleep durations of srko\/srko,MyoIA-Gal4,Elav-Gal80 (green), srko\/srko,UAS-SR (blue), srko,MyoIA-Gal4,Elav-Gal80\/srko,UAS-SR (red), and wt (black) flies were plotted. The tissues were immunostained with anti-GFP in ( c \u2013 f ), ( i , m ), and ( q ), immunostained with anti-GFP and nc82 in ( g , h , k , l , o , p ). Scale bars are 500 \u03bcm in ( i , m , q ), 30 \u03bcm in other panels. Numbers below each bar represent the number of flies tested. Kruskal\u2013Wallis test with Dunn\u2019s posttest, *** P < 0.001, n.s. P > 0.05. Error bars represent s.e.m. Male flies were used Full size image Because sr was expressed in the midgut ECs (Fig. 5f , Supplementary Fig. 5b ), we used MyoIA-Gal4 which was known to drive GFP expression in midgut ECs 38 to test the role of intestinal SR in sleep regulation. mCD8::GFP was driven by MyoIA-Gal4 to label MyoIA -expressing cells, MyoIA-Gal4 was expressed in the midgut ECs (Fig. 6m ) as well as in the neurons in the brain and the VNC (Fig. 6k, l ), while no expression of MyoIA-Gal4 was found in the genital and the internal surface of the abdominal cuticle which is covered by the fat body and the oenocytes (Supplementary Fig. 6 ). Colocalization of MyoIA and sr was detected in the gut by simultaneously labeling sr -expressing cells with GFP and the nuclei of MyoIA -expressing cells with StingerRed (Fig. 6c\u2013e ). We also intersected MyoIA and sr by expressing UAS-FRT-STOP-FRT-GFP in MyoIA -expressing cells, and expressing LexAop-Flp in sr -expressing cells. Thus, in cells that co-expressing MyoIA and sr , the stop cassette between the UAS and GFP was removed by the Flp recombinase, labeling the cells with GFP (Fig. 6f ). Expression of sr in MyoIA -expressing cells rescued the sleep defect of sr mutants (Fig. 6n ). We also used Elav-Gal80 to block the neuronal expression of MyoIA-Gal4 (Fig. 6o\u2013q ), and the nighttime sleep duration of sr mutants was rescued by expression of sr in non-neuronal MyoIA -expressing cells (Fig. 6r ). Moreover, the sleep duration was significantly decreased and sleep latency significantly increased when sr was knocked-down with RNAi specifically in the gut (Supplementary Fig. 7a, d ) or daao was overexpressed specifically in the gut (Supplementary Fig. 7b, c, e, f ) with daao cDNA driven by MyoIA-Gal4, Elav-Gal80 . Taken together, these results support that SR expressed in intestinal cells is important for regulating sleep in Drosophila . Discussion Our findings have revealed both a novel function for D-Ser and a novel role for intestinal cells. Results from mutations of five genes (two genes required for D-Ser synthesis, two for D-Ser degradation, and one for the D-Ser receptor), taken together with those from the pharmacological application of L- and D-Ser, support the conclusion that D-Ser plays an important role through NMDAR1 in regulating sleep in Drosophila . Furthermore, results from genetic rescue experiments with neuronal and intestinal drivers indicate that intestinal SR regulates sleep. The evidence for D-Ser function in sleep is strong. Phenotypic analysis indicates that D-Ser is important for nighttime sleep and arousal. Nighttime sleep and arousal phenotypes of shmt , and sr mutants are opposite to those of daao-dko mutants, consistent with roles of D-Ser in increasing sleep and decreasing arousal. Further support was provided by the finding that D-Ser could rescue the sleep and arousal phenotypes in shmt and sr mutants, whereas L-Ser was unable to rescue the sleep and arousal phenotypes in sr mutants. In Drosophila , while the functional significance of D-Ser in sleep is clear, a role for L-Ser appears unlikely but cannot be completely ruled out at this point. While NMDAR1 could affect circadian rhythm in mice 39 , 40 , it is surprising for a well-known excitatory receptor to promote sleep. Our results from nmdar1 knockout flies and sr knockout flies provide the strongest in vivo evidence for an essential role of NMDAR1 in promoting sleep. These results are consistent with, but cleaner than, the previous RNAi results in flies 22 . NMDAR1 has recently been implicated in regulating sleep in flies 22 , 23 : pan-neuronal knocked down of NMDAR1 or NMDAR2 through RNAi or feeding of the NMDAR antagonist MK801 to flies reduced sleep duration 22 . So far, regional RNAi had failed to reveal specific regions where NMDAR1 regulates sleep 22 . NMDAR1 expression has been detected in the R2 ring of the EB which is important for sleep homeostasis 23 , though it remains unknown whether NMDAR1 in the R2 ring regulates sleep. Roles for D-Ser in regulating mammalian sleep and arousal remain to be investigated. The saturation level of the glycine binding site in the NMDAR1 correlates with sleep need in mice 24 . Total serine level increased to ~487% during slow wave sleep (SWS) in the ventrolateral posterior nuclei (VLPN) of the cat thalamus 41 . D-Ser reduces sedative response induced by alcohol in flies and rodents 31 , 42 . A report of the effect of daao ablation on promoting sedative response in mice under novel environment 43 was refuted by further analysis 44 . Because D-Ser was only increased in some but not all regions in the brain after the elimination of a single daao 45 , the function of D-Ser in mammalian sleep remains unclear. Both glial and neuronal distribution of D-Ser and SR have been reported in mammals 35 , 46 . Early studies have detected D-Ser to be present in glia 10 , 47 , 48 . D-Ser has been considered as a major gliotransmitter 49 , 50 , 51 and its release is triggered by non-NMDAR glutamate receptor 48 , 49 . However, SR and D-Ser were also found in neurons 47 , 52 , 53 . Both the neuronal and glial release of D-Ser have been detected 54 . Using conditional SR knocked out mice, the majority of SR (65%) and extracellular D-Ser have been suggested to be of neuronal origin 55 . Our present study with Drosophila using SR-KIGal4 identified only neuronal but no glial expression of SR. A striking finding here is that SR is not only expressed in the nervous system, but that it is expressed and functions in intestinal cells to regulate sleep in Drosophila . Through region-specific rescue, knock-down, and overexpression studies (Fig. 6 , Supplementary Fig. 6 ), we found that the expression of SR in cells labeled by MyoIA-Gal4, Elav-Gal80 is essential for sleep regulation. We have examined the expression patterns only in the CNS, the gut, the genital, the fat body, and the oenocytes, therefore, although it is most likely that SR in the intestinal cells functions to regulate sleep, functions in other organs or cells could not be completely ruled out. This is the first time that a gene has been found to function in the intestines to regulate sleep in any animal species. Why and how intestinal SR regulates sleep remains elusive. Sleep disorders have been found to be associated with gastrointestinal and metabolism pathology in human 56 and animals 57 . The intestine is a tissue made up of a large variety of cells 58 that could both sense the environment and communicate with the central nervous system. In mammals and flies, crosstalk between enteroendocrine cells and neurons through neuropeptide signaling have been identified to regulate processes such as energy homeostasis and development 59 , 60 , and the gut microbiome has been implicated in the regulation of behaviors such as locomotion and anxiety 61 , 62 , 63 . We have now demonstrated an essential role for an endogenous gene in the intestine in sleep regulation. How D-Ser produced in the intestine functions through the NMDAR to regulate sleep, and whether other cells, such as glia, participate in the circuit requires further studies. Our work should stimulate further investigations of whether sr or other genes function in the gastrointestinal system to regulate sleep or other neuronal functions. Methods Fly lines and rearing conditions All fly stocks were reared on standard corn meal at 25 \u00b0C and 50% humidity on a 12:12 LD schedule unless otherwise noted. Elav-Gal4, Elav-Gal80, UAS-mCD8::GFP , UAS-StingerRed , LexAop-GFP were from the Bloomington Stock Center. MyoIA-Gal4 were generously provided by R. Xi (National Institute of Biological Science, Beijing). UAS-srRNAi (v110407) and UAS-Dicer (v60009) were from the Vienna Drosophila RNAi Center. Generation of transgenic, knockout, and knockin flies We generated UAS-SR , UAS-CG12338 , and UAS-CG11236 flies by inserting the coding sequences of CG8129-RB , CG12338-RA , and CG11236-RA respectively into the PACU2 vector from the Jan lab at UCSF 64 , and then inserted the construct into attp2 site. The coding sequences were amplified from the 1st strand cDNA made by the PrimeScript\u2122 II 1st strand cDNA synthesis kit (Takara, 6210A) from total RNA of wt flies isolated with TRIzol reagent (Invitrogen). We generated fly mutants with CRISPR\/Cas9. A pSP6-2sNLS-spcas9 plasmid and a pMD19-T gRNA scaffold vector were obtained from Dr. R. Jiao 65 . After the sequence of single strand guide RNA (sgRNA) was designed, the corresponding DNA template was amplified from the pMD19-T gRNA scaffold vector, and then transcribed in vitro (Promega, P1320) to obtain the sgRNA. pSP6-2sNLS-spcas9 vector was linearized by restriction enzyme XbaI (New England BioLabs, USA, R0145), transcribed in vitro (Ambion, USA, AM1340), and added with Poly(A) (New England BioLabs, USA, M0276) to obtain spCas9 mRNA. To generate shmt-es mutant, we injected one single strand guide RNA (sgRNA) and a spCas9 mRNA into Canton-S (CS) embryos to generate indel induced by site-directed cleavage in CG3011 . F2 indel flies were identified by PCR and confirmed by sequencing. Mutants with a stop codon in the coding region of CG3011 were selected for further studies. To generate srko , CG12338ko , and CG11236ko flies, we injected two sgRNAs and a spCas9 mRNA into CS embryos. The target region between the two sgRNAs was deleted. F2 knock-out flies were identified by PCR and sequencing. To obtain Gal4\/LexA lines, we injected two sgRNA, spCas9 mRNA, and a donor plasmid into CS embryos. Gal4\/LexA in the donor plasmid was integrated into specific sites of the genome through homologous recombination. A sgRNA and a spCas9 mRNA were used to improve the efficiency of homologous recombination. To construct the donor plasmid, two homologous arms were amplified by PCR from the fly genome using restriction enzyme-tailed PCR primers and the products were inserted into appropriate restriction sites in the pBlueScriptII vector. T2A-Gal4\/LexA-loxP-3P3-RFP-loxP sequence (constructed by Bowen Deng in the Lab) was inserted between the two homologous arms. T2A-Gal4\/LexA was kept in-frame with the 5\u2032 homologous arm. 3P3-RFP was used as a selection marker after injection. F2 flies with RFP observed in the eyes was selected and confirmed by PCR and sequencing (Supplementary Table 1 ). Sequences of the primers used for identification of the fly lines are presented in Supplementary Table 2 . Behavioral assays Sleep analysis was performed in a video-based recording system. 5\u20138 days old flies were placed in 65 mm \u00d7 5 mm tubes containing fly food. Infrared LED lights were used to provide constant illumination, and videos were recorded by a camera with 704 \u00d7 576 resolution. Videos were taken at 5 frames\/s. And 1 frame\/s was extracted for fly tracing. The position of a fly was tracked by a program based on OpenCV. Briefly, flies were extracted by subtracting background which was updated for each frame in order to prevent environmental light shift. The center of a fly was calculated, the speed was defined as changes of the center from the previous frame to the current frame. Sleep was defined with more than 5 min bout of inactivity, sleep latency was defined as the time in minutes from the moment light was turned off to the onset of sleep 16 , 17 . Arousal response was measured at ZT16 (4 h after lights off) under 1 s light stimulation (100\u2013200 lux) 34 . The percentage of flies that were aroused by light stimuli from sleep was calculated as arousal rate. Sleep deprivation was performed by placing a silica gel holder with recording tubes horizontally into a holding box. The box was rotated clock-wise or counter clock-wise and bumped to plastic stoppers under the control of a servo motor. The box was rotated continuously for 9 times during each episode, and the setup was activated every 3 min for 12 h during the night. Sleep was completely deprived as confirmed by DAM recording during deprivation 66 . Because sleep homeostasis is more significant in females than that in males 67 , we used females to examine sleep homeostasis. For analysis of circadian rhythm, flies were treated and recorded in the same condition as the sleep assay, except that the experiments were performed in DD. Activity was measured for 5\u20138 days and calculated in ActogramJ 68 . The period length was calculated by Chi-square method. Drug treatment L-Ser (S0035) and D-Ser (S0033) were from Tokyo Chemical Industry and mixed in the 5% sucrose and 2% agar medium. Flies were maintained on food containing 2.9 g\/L L-Ser, D-Der (treatment group) or no Ser (control group) after eclosion for 3 days before being transferred to the recording tube with the same medium. Immunohistochemistry and confocal imaging Flies were anesthetized and dissected in cold phosphate-buffered saline (PBS). Brains were fixed in 4% paraformaldehyde (weight\/volume) for 1 h at room temperature (RT), washed in PBST (PBS containing 0.2% Triton X-100, vol\/vol) for 10 min three times, blocked in PBSTS (PBS containing 2% Triton X-100, 10% normal goat serum, vol\/vol) for 12 h at 4 \u00b0C, incubated with primary antibodies in the dilution buffer (PBS containing 0.25% Triton X-100, 1% normal goat serum, vol\/vol) for 12 h at 4 \u00b0C, and washed with the washing buffer (PBS containing 1% Triton X-100, vol\/vol, 3% NaCl, g\/ml) for 10 min three times. Brains were then incubated with secondary antibodies in the dilution buffer for 12 h at 4 \u00b0C in darkness, and washed three times in the washing buffer for 10 min each. In the case of the third antibody, brains were further incubated with third antibodies at 4 \u00b0C overnight and washed. Finally, brains were mounted in Focusclear (Cell Explorer Labs, FC-101) and imaged on Zeiss LSM710 confocal microscope. The intestines, after dissection, were fixed with 4% paraformaldehyde for 0.5 h, followed by 30 s in heptanes and methanol (1:1, vol\/vol), washed with 100% methanol for 5 min twice, and washed with PBST for 10 min three times, and immunostained for GFP as described in the previous paragraph. Images were processed by Zeiss software and Imaris (bitplane) for 3D reconstruction. Chicken anti-GFP (1:1000) (Abcam Cat# 13970; RRID:AB_300798), mouse anti-Bruchpilot (1:40) (DSHB Cat# 2314866, nc82; RRID: AB_2314866) were used as primary antibodies with AlexaFluor488 anti-chicken (1:500) (Life Technologies Cat# A11039; RRID:AB_2534096) and AlexaFluor633 anti-mouse (1:500) (Life Technologies Cat# A21052; RRID: AB_141459) being used as respective secondary antibodies. Mouse 4F3 anti-DLG (1:50) (DSHB Cat# 4F3 anti-discs large; RRID: AB_528203) was used as primary antibody with biotin-conjugated goat anti-mouse (1:200) (Invitrogen Cat# B2763; RRID: AB_2536430) being the secondary antibody and AlexaFluor635 streptavidin (1:500) (Invitrogen Cat# S32364) being the third antibody. Quantitative PCR Total RNA was extracted from ~60 flies aged 5\u20138 days using TRIzol reagent (Invitrogen) and then reverse transcripted by the PrimeScript TM RT Master Mix kit (Takara, RR036A). Quantitative PCR analysis was then performed using TransStart Top Green qPCR SuperMix kit (TransGen, AQ131-03) in the Applied Biosystems 7900HT Fast-Time PCR system. The sequences of primers used to detect shmt and actin42a (endogenous control) RNA are as follows: shmt-F: 5\u2032-CAGCCGTTTACAAAGACATGCA-3\u2032 shmt-R: 5\u2032-GAATGGCGTTGGTGATGGTT-3\u2032 act42a-F: 5\u2032-CTCCTACATATTTCCATAAAAGATCCAA-3\u2032 act42a-R: 5\u2032-GCCGACAATAGAAGGAAAAACTG-3\u2032 Statistics All statistical analyses were carried out with Prism 5 (GraphPad Software). Fisher\u2019s exact test was used to compare arousal rates. Mann\u2013Whitney test was used to compare two columns of data. Kruskal\u2013Wallis test followed by Dunn\u2019s posttest was used to compare multiple columns of data. Two-way ANOVA followed by Bonferroni post-tests was used to compare drug rescue effects. Additional Mann\u2013Whitney test was used to compare mutants with wt flies under different treatments. Statistical significance is denoted by asterisks: * P < 0.05, ** P < 0.01, *** P < 0.001. Data availability The data that support the findings of this study are available upon reasonable request. Code availability Code used for tracing the fly were conducted in C++, and further analyses were conducted in Matlab. All the code are available upon reasonable request. ","News_Body":"A team of researchers affiliated with several institutions in China has found that an amino acid made in fruit fly intestines plays a key role in regulating their sleep. In their paper published in the journal Nature Communications, the group describes their study of D-serine in Drosophila melanogaster and what they found. Scientists have known about D-serine for many years, but thought that it only existed in bacteria. Recently, however, researchers found that humans also produce the amino acid, as do fruit flies. But until now, it was not known what function it served. In this new effort, the researchers found that, at least in fruit flies, it helps regulate sleep. To learn more about the amino acid, the researchers edited the genes of fly specimens to halt its production and found that doing so resulted in the flies sleeping only half as much as normal flies. But they also found something else. Fruit flies actually produce D-serine in two places\u2014in their intestines and their brains. Logic would suggest that the acid produced in the brain would be the one associated with sleep, but the researchers found that the opposite was true. When they turned off the genes that controlled production of the enzyme, serine racemase, which syntheses D-serine in the intestines, the flies slept less, but when they did the same for those made in the brain, they saw no change in sleep habits. The researchers report that they have no idea how an amino acid produced in the intestines can impact sleep patterns, noting that sleep regulation is probably carried out by the central nervous system. Prior research has shown that sleep is a very old evolutionary development, which suggests its control is likely similar across species. They suggest that more research is needed to find the answers to other questions surrounding D-serine\u2014for instance, is it produced in other parts of the body? Does it play a role in regulating sleep in humans, and if so, how? ","News_Title":"Amino acid in fruit fly intestines found to regulate sleep","Topic":"Biology"}
{"Paper_Body":"Abstract Tumour-specific CD8 T cell dysfunction is a differentiation state that is distinct from the functional effector or memory T cell states 1 , 2 , 3 , 4 , 5 , 6 . Here we identify the nuclear factor TOX as a crucial regulator of the differentiation of tumour-specific T (TST) cells. We show that TOX is highly expressed in dysfunctional TST cells from tumours and in exhausted T cells during chronic viral infection. Expression of TOX is driven by chronic T cell receptor stimulation and NFAT activation. Ectopic expression of TOX in effector T cells in vitro induced a transcriptional program associated with T cell exhaustion. Conversely, deletion of Tox in TST cells in tumours abrogated the exhaustion program: Tox -deleted TST cells did not upregulate genes for inhibitory receptors (such as Pdcd1 , Entpd1 , Havcr2 , Cd244 and Tigit ), the chromatin of which remained largely inaccessible, and retained high expression of transcription factors such as TCF-1. Despite their normal, \u2018non-exhausted\u2019 immunophenotype, Tox -deleted TST cells remained dysfunctional, which suggests that the regulation of expression of inhibitory receptors is uncoupled from the loss of effector function. Notably, although Tox -deleted CD8 T cells differentiated normally to effector and memory states in response to acute infection, Tox -deleted TST cells failed to persist in tumours. We hypothesize that the TOX-induced exhaustion program serves to prevent the overstimulation of T cells and activation-induced cell death in settings of chronic antigen stimulation such as cancer.     Main Using an inducible model of autochthonous liver cancer in which SV40 large T antigen (TAG) is the oncogenic driver and tumour-specific antigen 7 (Fig. 1a and Extended Data Fig. 1a ), we recently showed that CD8 + T cells expressing a restricted T cell receptor (TCR) specific for TAG (hereafter referred to as TCR TAG cells) differentiate to an epigenetically encoded dysfunctional state, exhibiting hallmarks of TST cell dysfunction including the expression of inhibitory receptors and loss of effector cytokines 3 , 5 . Numerous transcription factors were dysregulated in dysfunctional TCR TAG cells (such as NFAT, TCF-1, LEF1, IRF4 and BLIMP1) compared with functional effector or memory TCR TAG cells generated during acute infection with Listeria (using a recombinant Listeria monocytogenes strain that expressed TAG epitope I ( Lm TAG)) 5 . However, many of these transcription factors are also crucial for the development of normal effector and memory T cells 8 ; thus, we set out to identify transcription factors that were specifically expressed in dysfunctional TCR TAG cells. We analysed our RNA sequencing (RNA-seq) data 5 and found that the gene encoding the nuclear factor TOX was highly expressed in dysfunctional TCR TAG cells, but low in functional naive, effector and memory TCR TAG cells (Fig. 1b ). TOX is a nuclear DNA-binding factor and a member of the high-motility group box superfamily that is thought to bind DNA in a sequence-independent but structure-dependent manner 9 . Although TOX is required during thymic development of CD4 + T lineage cells, natural killer and innate lymphoid cells 10 , 11 , 12 , and in regulating CD8 T cell-mediated autoimmunity 13 , its role in tumour-induced T cell dysfunction is unknown. To assess TOX expression during CD8 T cell differentiation in acute infection and tumorigenesis, congenically marked naive TCR TAG cells were transferred into (i) wild-type C57BL\/6 (B6) mice immunized with Lm TAG, or (ii) tamoxifen-inducible liver cancer mice (AST\u00d7Cre-ER T2 ; AST denotes albumin-floxStop-SV40 large T antigen) treated with tamoxifen (Fig. 1a and Extended Data Fig. 1a, b ). TOX was expressed at low levels early after Listeria infection but declined to baseline levels (by day 5 after infection) and remained low in memory T cells (Fig. 1c and Extended Data Figs. 1c , 2 ). By contrast, during tumour progression, TOX expression increased in TCR TAG cells and remained high (Fig. 1c and Extended Data Figs. 1c , 2 ). High expression of TOX correlated with high expression of several inhibitory receptors and low expression of TCF-1 (Fig. 1d and Extended Data Figs. 1d , 2b, c ). Moreover, TOX-expressing TCR TAG cells failed to produce the effector cytokines IFN\u03b3 and TNF after stimulation ex vivo with cognate peptide or phorbol myristate acetate (PMA) and ionomycin (Fig. 1e and Extended Data Fig. 1e\u2013g ). Fig. 1: TOX is highly expressed in tumour-infiltrating CD8 T cells of mouse and human tumours. a , Experimental scheme for acute infection (green) and tumorigenesis (red). E 3 and E 7 , effector cells isolated 3 and 7 days after immunization, respectively; M, memory cells; T 7 and T 14\u201360 , T cells isolated from liver tumours at 7 and 14\u201360 days after transfer. b , Reads per kilobase of transcript per million mapped read (RPKM) values of Tox . n = 3 (naive (N), memory); n = 6 (E 5\u2013 7 ); n = 14 (T 14\u201360 ) TCR TAG cells isolated from liver tumour lesions of AST\u00d7Cre-ER T2 mice at 14, 21, 28, 35 and more than 60 days after transfer 5 . c , Expression levels of TOX protein in TCR TAG cells during Listeria infection (green) or tumorigenesis (red), assessed by flow cytometry at indicated time points with n = 2\u20133 mice. MFI, mean fluorescent intensity; Tam, tamoxifen. d , Expression of TOX, TCF-1 and PD-1 in TCR TAG cells isolated from liver tumour lesions 35 days after transfer (T 35 ; red, n = 5); memory TCR TAG cells are shown as control (M; green). e , IFN\u03b3 and TNF production of memory TCR TAG cells (M; green, n = 2) and liver tumour-infiltrating TCR TAG cells (T; red, n = 3). Data are representative of more than five independent experiments. f \u2013 h , TOX expression in human tumour-infiltrating CD8 + T cells isolated from patients with melanoma ( n = 4) ( f ), breast cancer ( n = 4) ( g ), and lung cancer ( n = 6) ( h ). Each symbol represents an individual mouse (for b \u2013 e ) or individual patient (for f \u2013 h ). Data are mean \u00b1 s.e.m. * P \u2264 0.05, ** P \u2264 0.01, *** P \u2264 0.001, two-sided Student\u2019s t -test. Source Data Full size image Persistent antigen encounter or TCR stimulation drives expression of inhibitory receptors and T cell exhaustion during chronic infections 14 and in tumours 3 , 15 . Therefore, we analysed the expression of TOX and inhibitory receptors in GP33 virus-specific CD8 T (TCR P14 ) cells during acute infection with lymphocytic choriomeningitis virus (LCMV) Armstrong and chronic infection with LCMV clone 13 (Extended Data Fig. 2 ). TOX was transiently expressed early during acute infection with LCMV Armstrong but declined to baseline by day 5 after infection. In chronic infection with LCMV clone 13, TOX expression progressively increased in TCR P14 cells, remained increased, and correlated with high expression of several inhibitory receptors (Extended Data Fig. 2 ). We confirmed TOX expression in the mouse B16F10 (B16) melanoma model. B16 tumours overexpress two melanoma-associated proteins, TRP2 and PMEL, which are recognized by TRP2-specific (TCR TRP2 ) and PMEL-specific (TCR PMEL ) CD8 T cells, respectively 16 , 17 . Naive transgenic TCR TRP2 or TCR PMEL cells were adoptively transferred into B16 tumour-bearing mice, and again we found that dysfunctional, tumour-infiltrating TCR TRP2 and TCR PMEL cells expressed high levels of TOX and inhibitory receptors, and low levels of TCF-1 (Extended Data Fig. 3a\u2013c ). Thus, persistent upregulation of TOX in T cells is induced in settings of chronic antigen stimulation such as chronic infection and cancer. Next, we examined the expression of TOX in human CD8 + tumour-infiltrating lymphocytes (TILs) and peripheral blood mononuclear cells (PBMCs) from patients with melanoma, breast, lung and ovarian cancer (Fig. 1f\u2013h and Extended Data Fig. 3d\u2013g ). CD45RO + PD-1 hi CD39 hi CD8 + TILs expressed high levels of TOX compared with CD45RO + PD-1 lo CD39 lo or CD45RA + TILs in the same tumour or CD45RO + PD-1 hi PBMCs from the same patient. PD-1 hi TILs expressed higher levels of TOX, CD39, TIM-3 and LAG-3 than PD-1 lo TILs from the same tumour (Extended Data Fig. 3g ). Thus, TOX is highly expressed in subsets of human TILs, and TOX expression in TILs correlates with other characterized markers of T cell exhaustion. To determine the role of tumour antigen stimulation versus the tumour immunosuppressive microenvironment in TOX induction, we co-transferred equal numbers of naive tumour-specific TCR TAG (Thy1.1) cells and non-tumour-specific TCR OT1 (Ly5.1) cells, which express a K b -restricted TCR specific for ovalbumin (OVA), into the liver of tumour-bearing AST\u00d7Alb-Cre (AST mice crossed with Alb-Cre mice) or wild-type B6 control mice (Fig. 2a ). One day later, recipient AST\u00d7Alb-Cre and B6 mice were immunized with Listeria co-expressing the TAG epitope I and OVA epitopes; TCR TAG and TCR OT1 cells expanded equally well and expressed similar levels of activation and proliferation markers CD44 and Ki67 (Extended Data Fig. 4a ). In B6 hosts, neither TCR TAG nor TCR OT1 cells upregulated TOX or inhibitory receptors, and both differentiated into functional memory T cells (Fig. 2b, c ). In tumour-bearing AST\u00d7Alb-Cre mice, TCR TAG cells upregulated TOX, PD-1, LAG-3, 2B4, CD38, CD39, TIM-3 and CD69, lost expression of TCF-1, and lost the ability to produce IFN\u03b3 and TNF or express CD107. By contrast, bystander TCR OT1 cells from the same liver tumours did not upregulate TOX or inhibitory receptors and remained functional (Fig. 2b, c and Extended Data Fig. 4a ). This finding is consistent with recent single-cell RNA-seq studies that describe distinct CD8 T cell populations in human tumours, including dysfunctional, tumour-reactive TOX hi T cells, and bystander cytotoxic T cells that are TOX low and lack hallmarks of chronic antigen stimulation 18 , 19 . Fig. 2: Chronic TCR stimulation drives TOX expression in tumour-specific CD8 T cells. a , Experimental scheme of TCR TAG (TAG) and TCR OT1 (OT1) T cell co-transfer. b , Top, expression profiles of TAG (red) and OT1 (black) isolated from the spleens of B6 mice (top; n = 6 (OT1), n = 4 (TAG)) or the livers of AST\u00d7Alb-Cre mice (bottom; n = 8 (OT1), n = 8 (TAG)), 3\u20134 weeks after transfer and immunization. Bottom, MFI values of TOX expression relative to naive T cells. Each symbol represents an individual mouse. Data are representative of three independent experiments. c , Intracellular IFN\u03b3 and TNF production of TAG and OT1 isolated 3\u20134 weeks after transfer and immunization from spleens of B6 mice (left) or liver tumour lesions of AST\u00d7Cre mice (right). Data are representative of three independent experiments. d , MA plot of the RNA-seq dataset. Significantly DEGs are shown in red. e , ATAC-seq signal profiles across the Tox and Tcf7 loci. Peaks uniquely lost or gained in TAG compared with OT1 are highlighted in red. Data are mean \u00b1 s.e.m. *** P \u2264 0.001, two-sided Student\u2019s t -test. NS, not significant. Source Data Full size image RNA-seq and assay for transposase-accessible chromatin using sequencing (ATAC-seq) analyses of liver tumour-infiltrating TCR TAG and TCR OT1 cells revealed 2,347 differentially expressed genes (DEGs) and 19,071 differentially accessible peaks, including in Tox , Tcf7 and numerous inhibitory receptor-encoding genes (Fig. 2d , Extended Data Fig. 4b and Supplementary Table 1 ). Gene set enrichment analyses (GSEA) of the DEGs between TCR TAG and TCR OT1 cells revealed enrichment for gene sets of (i) T cell exhaustion during chronic viral infection 20 , and (ii) gene programs induced by a mutant, constitutively active form of NFAT1 in T cells resulting in anergy or exhaustion 21 (Extended Data Fig. 4c ). ATAC-seq revealed that DEGs had accompanying changes in chromatin accessibility: Tox , Pdcd1 (encoding PD-1), Entpd1 , Cd38 and Cd244 loci were more accessible in TCR TAG cells than in TCR OT1 cells, whereas the Tcf7 locus was less accessible (Fig. 2e , Extended Data Fig. 4d\u2013f and Supplementary Table 2 ). Chromatin accessibility analysis of TILs from patients with melanoma and lung cancer 5 showed that PD-1 hi TILs uniquely gained several peaks of open chromatin in TOX and lost multiple peaks in TCF7 when compared with human naive CD45RA + CD8 + PBMCs, or central memory CD45RA \u2212 CD45RO + CD62L hi CD8 + PBMCs from healthy donors 5 (Extended Data Fig. 5a ). NFAT is a crucial regulator of T cell exhaustion and dysfunction 22 , and NFAT1-binding sites in genes encoding negative regulators and inhibitory receptors have increased chromatin accessibility in dysfunctional and exhausted T cells 4 , 5 , 21 , 23 , 24 . Thus, we compared published NFAT1 chromatin immunoprecipitation with high-throughput sequencing (ChIP\u2013seq) data 21 with our published 5 and newly generated ATAC-seq datasets (Fig. 2 ) and found evidence that NFAT1 bound to regions within the Tox locus with significantly increased chromatin accessibility in dysfunctional TCR TAG cells (Extended Data Fig. 5b ). To inhibit NFAT, we treated AST\u00d7Cre-ER T2 mice adoptively transferred with TCR TAG cells with the calcineurin inhibitor FK506 as previously described 5 , 25 , 26 . We found that TCR TAG cells from FK506-treated mice had decreased expression of TOX and PD-1, and increased levels of TCF-1 (Extended Data Fig. 5c ), suggesting that NFAT regulates TOX expression. To determine whether ectopic expression of TOX in effector CD8 T cells in vitro was sufficient to induce exhaustion in the absence of chronic antigen and TCR stimulation, we transduced effector TCR TAG cells generated in vitro with retroviral vectors encoding full-length TOX fused to green fluorescent protein (GFP) or GFP alone (Fig. 3a ). After transduction, effector TCR TAG cells were cultured for 6 days with IL-2 (without any additional TCR stimulation) and sorted for GFP expression (Extended Data Fig. 6a ). RNA-seq analysis revealed 849 DEGs between TOX\u2013GFP + and GFP + T cells (Fig. 3b , Extended Data Fig. 6b and Supplementary Table 3 ). GSEA revealed that the transcriptional program of TOX\u2013GFP + TCR TAG cells was significantly enriched for genes associated with chronic infections and tumours, with reduced expression of several genes encoding transcription factors ( Tcf7 , Lef1 and Id3 ), and increased expression of genes encoding inhibitory receptors ( Pdcd1 , Cd244 , Havcr2 and Entpd1 ) and transcription factors such as Ahr , Nfil3 , Prdm1 and Id2 (Fig. 3b, c and Extended Data Fig. 6c\u2013g ). Despite expressing numerous exhaustion-associated genes, TOX\u2013GFP + TCR TAG cells remained highly functional and proliferative (Extended Data Fig. 6d\u2013f ). Fig. 3: Ectopic expression of TOX is sufficient to induce a global molecular program characteristic of T cell exhaustion. a , Experimental scheme (see also Methods ). b , MA plot of RNA-seq dataset. Significantly DEGs are coloured in red. c , Heat map of RNA-seq expression (row-normalized log 2 (counts per million) for DEGs; false discovery rate (FDR) < 0.10) in TOX\u2013GFP + and GFP + TCR TAG cells. Full size image Next, we examined how genetic deletion of Tox affected CD8 T cell differentiation during acute infection or in tumours. TCR TAG mice were crossed to Tox flox\/flox mice 10 and mice expressing Cre-recombinase under the distal Lck promoter to generate TOX-knockout TCR TAG mice (Fig. 4a and Extended Data Fig. 7a ). TCR TAG cells from TOX-knockout TCR TAG mice developed normally and similarly to littermate control mice (Extended Data Fig. 7b, c ). Naive TOX-knockout and wild-type (Thy1.1 + ) TCR TAG cells were adoptively transferred into B6 (Thy1.2 + ) mice and immunized 1 day later with Lm TAG. TOX-knockout and wild-type TCR TAG cells expanded equally well in response to Lm TAG immunization (Fig. 4b ), became CD44 hi and CD62L lo , formed similar numbers of KLRG1 lo CD127 hi memory precursors and KLRG1 hi CD127 lo short-lived effector cells 8 (Extended Data Fig. 7d ), differentiated into memory T cells (3\u20134 weeks after immunization), and produced similar amounts of IFN\u03b3 and TNF after ex vivo stimulation with peptide (Fig. 4c and Extended Data Fig. 7e ). Thus, TOX is not required for the differentiation of naive T cells into effector and memory T cells during acute infection. Fig. 4: Phenotypic, functional, transcriptional and epigenetic analysis of TOX-deficient T cells. a , Experimental scheme. b , c , Percentage of wild-type (WT; black) and knockout (KO; red) Thy1.1 + effector ( b ) or memory ( c ) TCR TAG cells isolated from spleens 7 days ( b ) or 3 weeks ( c ) after Lm TAG infection, respectively. For b , n = 8 (WT); n = 7 (KO); for c , n = 5 (WT); n = 5 (KO); two independent experiments. d , Left, wild-type and knockout TCR TAG cells isolated from malignant liver lesions 5\u20138 days after transfer into AST\u00d7Cre-ER T2 (Thy1.1 + Thy1.2 + ) mice. Middle, ratio of the percentage of wild-type and knockout T cells. Right, TOX expression of liver-infiltrating wild-type and knockout TCR TAG cells; naive TCR TAG cells are shown in grey as a control. e , Expression profiles of liver-infiltrating wild-type and knockout TCR TAG cells 8\u201310 days after adoptive transfer. Naive TCR TAG cells are shown in grey. Data are representative of more than five independent experiments ( n = 4 (PD-1\/LAG-3); n = 2 (2B4); n = 6 (CD39\/CD38)). f , Left, intracellular IFN\u03b3 and TNF production of wild-type ( n = 4) and TOX-knockout ( n = 4) TCR TAG cells isolated 10 days after transfer from liver lesions of AST\u00d7Cre mice. Right, specific lysis of TAG-peptide-pulsed EL4 cells in chromium release assays by wild-type ( n = 6) and knockout ( n = 6) TCR TAG cells isolated and flow-sorted from liver tumour lesions. Results from two independent experiments. Memory (Mem) TCR TAG cells are shown as a control. g , Percentage of Ki67-positive wild-type and knockout TCR TAG cells from malignant liver lesions 6\u20138 days after transfer into AST\u00d7Cre mice. Data are from three independent experiments. h , Wild-type and knockout donor TCR TAG cells 19 days after transfer in liver tumours (WT, n = 5; KO, n = 5). Data are representative of two independent experiments. In b \u2013 h , each symbol represents an individual mouse. i , MA plot of RNA-seq data. Significantly DEGs are in red. j , Chromatin accessibility of wild-type and knockout TCR TAG cells. Each row represents one peak (differentially accessible between wild-type and knockout; FDR < 0.05) displayed over a 2-kb window centred on the peak summit; regions were clustered with k -means clustering. Genes associated with peaks within individual clusters are highlighted. k , ATAC-seq signal profiles across the Tox and Tcf7 loci. Peaks uniquely lost or gained in knockout TCR TAG cells are highlighted in red or blue, respectively. Data are mean \u00b1 s.e.m. * P \u2264 0.05, ** P \u2264 0.01, *** P \u2264 0.001, two-sided Student\u2019s t -test. Source Data Full size image Next, we adoptively transferred naive TOX-knockout and wild-type TCR TAG cells into AST\u00d7Cre mice. TOX-knockout and wild-type TCR TAG cells equivalently infiltrated the liver (Fig. 4d ), proliferated and upregulated CD44, CD69 and CD25 (Fig. 4e and Extended Data Fig. 7f ). Notably, by 8\u201310 days after transfer, TOX-knockout TCR TAG cells did not upregulate inhibitory receptors including PD-1, LAG-3, CD38, CD39 and 2B4, in contrast to wild-type TCR TAG cells (Fig. 4e and Extended Data Fig. 7f ). Nevertheless, TOX-knockout and wild-type TCR TAG cells showed comparable reductions in the production of IFN\u03b3 and TNF, the expression of CD107, granzyme B (GZMB), and the specific lysis of TAG-peptide-pulsed EL4 target cells (Fig. 4f and Extended Data Fig. 7g\u2013i ). Thus, despite their normal, \u2018non-exhausted\u2019 phenotype (Fig. 4e ) and proliferative capacity (Fig. 4g ), TOX-knockout TCR TAG cells remained dysfunctional, revealing that the regulation of inhibitory receptors is uncoupled from T cell effector function. Notably, by 2\u20133 weeks after transfer, very few TOX-knockout TCR TAG cells could be found in liver tumour lesions, whereas wild-type TOX TCR TAG cells persisted throughout the course of tumour progression (Fig. 4h and Extended Data Fig. 8a ). Indeed, TOX-knockout TCR TAG cells had increased levels of active caspases 3 and 7, increased annexin V staining, and an enrichment of apoptosis genes, although the expression of pro- and anti-apoptotic proteins such as BIM, BCL-2 and BCL-xL was similar between knockout and wild-type TCR TAG cells (Extended Data Fig. 8b\u2013e ). We performed RNA-seq and ATAC-seq analyses from TOX-knockout and wild-type TCR TAG cells isolated from liver tumours of AST\u00d7Cre mice 8\u20139 days after adoptive transfer and identified 679 DEGs and 12,166 differentially accessible chromatin regions, respectively (Fig. 4i, j , Extended Data Fig. 9 and Supplementary Tables 1, 2 ). TOX-knockout TCR TAG cells had low expression of genes encoding transcription factors and inhibitory receptors including Nfil3 , Prdm1 , Cish , Pdcd1 , Entpd1 , Tigit , Havcr2 and Cd38 , and high expression of the transcription factors Tcf7 , Lef1 and Id3 . GSEA of DEGs between TOX-knockout and wild-type TCR TAG cells revealed strong enrichment for genes and pathways associated with T cell exhaustion during chronic infection and tumorigenesis (Extended Data Fig. 9b ). Transcriptional differences were associated with corresponding changes in chromatin accessibility patterns of the respective genes (Fig. 4j and Extended Data Fig. 9c\u2013g ). For example, the loci of Tox , Pdcd1 , Cd38 and Entpd1 were less accessible in TOX-knockout TCR TAG cells than in TOX wild-type TCR TAG cells, whereas the loci of Tcf7 , Cd28 , Fyn and Il7r were more accessible (Fig. 4k and Extended Data Fig. 9e ). More accessible regions in TOX-knockout TCR TAG cells showed significant enrichment for Gene Ontology (GO) terms associated with (i) cytokine and chemokine receptor activity; (ii) chromatin binding and bending, regulatory region DNA binding; and (iii) \u03b2-catenin binding (Extended Data Fig. 9f ). We also found enrichment of apoptosis pathways in TOX-knockout TCR TAG cells and increased expression of genes associated with apoptosis such as Fas , Tnf , Gas2 and Tnfrs25 (which encodes DR3) (Extended Data Figs. 8e , 9e ). In summary, TOX is specifically required for T cell differentiation in settings of chronic antigen stimulation (such as tumours and chronic infection). A key finding of our study is that the regulation of inhibitory receptor expression is uncoupled from the loss of effector function in dysfunctional TST cells. Supporting this point is the notable phenotypic and transcriptional similarities between dysfunctional TOX-knockout TCR TAG TILs (Fig. 4 ) and functional TOX-negative, bystander TCR OT1 TILs (Fig. 2 and Extended Data Fig. 10a, b ). TOX-deficient TST cells failed to persist in tumours, and we hypothesize that the TOX-induced gene regulation of inhibitory receptors and other exhaustion-associated molecules serve as a physiological negative feedback mechanism to prevent overstimulation of antigen-specific T cells and activation-induced cell death in settings of chronic antigen stimulation such as chronic infection and cancer (Extended Data Fig. 10c ). Methods Mice AST (Albumin-floxStop-SV40 large T antigen (TAG)) mice were previously described 3 , 5 , 7 . TCR TAG transgenic mice (B6.Cg-Tg(TcraY1,TcrbY1)416Tev\/J) 27 , Cre-ER T2 (B6.129-Gt(ROSA)26Sor tm1(cre\/ERT2)Tyj \/J), Alb-Cre (B6.Cg-Tg(Alb-cre)21Mgn\/J), TCR OT1 (C57BL\/6-Tg(TcraTcrb)1100Mjb\/J), Ly5.1 (B6.SJL-Ptprc a Pepc b \/BoyJ), B6.Cg-Tg(Lck-icre)3779Nik\/J (dLck-Cre) and C57BL\/6J Thy1.1 mice were purchased from The Jackson Laboratory. Tox flox\/flox mice 10 were previously described, and obtained from M. Glickman, with permission from J. Kaye. Tox flox \/ flox mice were crossed to TCR TAG and dLck-Cre 28 mice to obtain TCR TAG Tox \u2212\/\u2212 (knockout) mice. TCR TRP2 mice were obtained from N. Restifo, with permission from A. Hurwitz. TCR TRP2 and TCR TAG mice were crossed to Thy.1.1 mice to generate TCR TRP2 and TCR TAG Thy.1.1 mice, respectively. TCR OT1 mice were crossed to Ly5.1 mice to generate TCR OT1 Ly5.1 mice. AST mice were crossed to Cre-ER T2 (Cre recombinase fused to tamoxifen-inducible oestrogen receptor) or Alb-Cre mice to obtain AST\u00d7Cre-ER T2 and AST\u00d7Alb-Cre mice, respectively. TCR PMEL and TCR P14 mice were purchased from The Jackson Laboratory. AST mice were also crossed to Thy1.1 mice to generate AST\u00d7Cre-ER T2 Thy1.1\/Thy1.2 mice. All mice were bred and maintained in the animal facility at MSKCC. Experiments were performed in compliance with the MSKCC Institutional Animal Care and Use Committee regulations. B16 tumour model Approximately 5 \u00d7 10 5 \u20131 \u00d7 10 6 B16 tumour cells were injected into C57BL\/6J wild-type mice. Once tumours were established (1\u20132 weeks later), around 2 million naive TCR TRP2 or TCR PMEL (Thy1.1 + ) T cells were adoptively transferred and isolated from tumours at indicated time points. Tumour volumes did not exceed the permitted volumes specified by the MSKCC IACUC protocol. Adoptive transfer studies during acute Listeria infection and in AST\u00d7Cre-ER T2 tumour models Naive CD8 + splenocytes from TCR TAG Thy1.1 transgenic mice were adoptively transferred into AST\u00d7Alb-Cre mice, or AST\u00d7Cre-ER T2 mice and treated with 1 mg tamoxifen 1\u20132 days later. For TCR TAG and TCR OT1 co-transfer experiments, 3\u20134 \u00d7 10 4 TCR TAG Thy1.1 and TCR OT1 Ly5.1 CD8 + splenocytes were adoptively transferred into AST\u00d7Alb-Cre mice or B6 control mice; 1 day later, mice were infected with 5 \u00d7 10 6 colony-forming units (CFU) L. monocytogenes ( Lm ) TAG-I OVA (co-expressing TAG-I epitope and OVA epitope SIINFEKL). For the generation of effector and memory TCR TAG CD8 + T cells, 100,000 CD8 + splenocytes from TCR TAG Thy1.1 wild-type or knockout mice were adoptively transferred into congenic B6 mice; 1 day later, mice were infected with 5 \u00d7 10 6 CFU Lm TAG. Effector TCR TAG CD8 + T cells were isolated from the spleens of B6 host mice and analysed 5\u20137 days after Listeria infection; memory TCR TAG CD8 + T cells were isolated from spleens of B6 host mice and analysed at least 3 weeks after Listeria infection. For wild-type and knockout studies, CD8 + splenocytes from TCR TAG (wild-type) or TCR TAG TOX-knockout mice were adoptively transferred into AST\u00d7Cre-ER T2 (and 1\u20132 days later, mice were treated with 1 mg tamoxifen) or into AST\u00d7Alb-Cre mice. For these studies, we define knockout TCR TAG as TOX-deficient T cells. LCMV clone 13 and LCMV Armstrong infection model LCMV infection was done as previously described 29 . In brief, 10,000 TCR P14 cells were adoptively transferred intravenously into congenic 6\u20138-week-old C57BL\/6 mice, and mice were infected 1 day later with LCMV Armstrong (2 \u00d7 10 5 plaque-forming units (PFU), intraperitoneally) or LCMV clone 13 (2 \u00d7 10 6 PFU, intravenously). In mice receiving LCMV clone 13, CD4 T cells were depleted with 200 \u03bcg anti-CD4 antibody (clone GK1.5) 2 days before T cell transfer 29 . Antibodies for flow cytometric analysis For mouse studies, the following antibodies were purchased: from BioLegend: 2B4 (m2B4), BCL-2 (BCL\/10C4), CD101 (Moushi101), CD11c (N418), CD127 (A7R34), CD19 (6D5), CD25 (PC61.5), CD3 (145-2C11), CD38 (90), CD39 (Duha59), CD40 (3\/23), CD44 (IM7), CD62L (MEL-14), CD69 (H1.2F3), CD70 (FR70), CD80 (16-10A1), CD86 (GL-1), CD90.1 (OX-7 and HIS51), CD90.2 (30-H12 and 53-2.1), CXCR5 (L138D7), Eomes (Dan11mag), GZMB (GB11), IFN\u03b3 (XMG1.2), IL-2 (JES6-5H4), KLRG1 (2F1), LAG-3 (C9B7W), MHC I-A\/I-E (M5\/114.15.2), PD-1 (RMP1-30), T-bet (4B10), TIM-3 (RMT3-23), TNF (MP6-XT22), and 7-amino-actinomycin (7-AAD); from BD Biosciences: annexin V, CD95 (Jo2), Ki67 (B56), Vb7 (TR310); BCL-xL (H-5; Santa Cruz Biotechnology); BIM (C34C5; Cell Signaling Technology), CD8 (53-6.7; eBioscience), CTLA-4 (UC10-410-11; Tonbo Biosciences), TCF-1 (C63D9; Cell Signaling Technology), TIGIT (GIGD7; eBioscience). For human studies, the following antibodies were purchased: CD39 (A1; BioLegend), CD45RA (HI100; BioLegend), CD45RO (UCHL1; BioLegend), CD8 (RPA-T8; BioLegend), LAG-3 (17B4; Enzo Life Sciences), PD-1 (EH12.1; BD Biosciences) and TIM-3 (F38-2E2; BioLegend). For flow cytometric detection and analysis of mouse and human TOX, anti-human\/mouse TOX antibody clone REA473 was used (Miltenyi Biotec); antibody clone REA293 was used as TOX isotype (Miltenyi Biotec). Tamoxifen treatment Tamoxifen was purchased from Sigma-Aldrich. A tamoxifen stock solution (5 mg ml \u22121 in corn oil) was prepared by warming tamoxifen in 1-ml sterile corn oil at 50 \u00b0C for approximately 15 min, then further diluted in corn oil to obtain the stock concentration of 5 mg ml \u22121 . Tamoxifen (1 mg; 200 \u03bcl) was administered once intraperitoneally into AST\u00d7Cre-ER T2 mice. Flow cytometric analysis Flow cytometric analysis was performed using BD Fortessa FACS Cell Analyzers; cells were sorted using BD FACS Aria (BD Biosciences) at the MSKCC Flow Core Facility. Flow data were analysed with FlowJo (Tree Star). Listeria infection The L. monocytogenes ( Lm ) \u0394 actA \u0394 inlB strain 30 expressing the TAG epitope I (206-SAINNYAQKL-215, SV40 large T antigen) together with the OVA SIINFEKL epitope was generated by Aduro Biotech as previously described 3 , 5 . The Lm strain was constructed using the previously described strategy 31 . Experimental vaccination stocks were prepared by growing bacteria to early stationary phase, washing in PBS, formulated at approximately 1 \u00d7 10 10 CFU ml \u22121 , and stored at \u221280 \u00b0C. Mice were infected intraperitoneally with 5 \u00d7 10 6 CFU of Lm TAG. Cell isolation for subsequent analyses Spleens were mechanically disrupted with the back of a 3-ml syringe, filtered through a 70-\u03bcm strainer, and red blood cells were lysed with ammonium chloride potassium buffer. Cells were washed twice with cold RPMI 1640 media supplemented with 2 \u03bcM glutamine, 100 U ml \u22121 penicillin\/streptomycin, and 5\u201310% FCS. Liver tumour and B16 tumour tissues were mechanically disrupted and dissociated with scissors (in 1\u20132 ml of cold complete RPMI). Dissociated tissue pieces were transferred into a 70-\u03bcm strainer (placed into a 60-mm dish with 1\u20132 ml of cold complete RPMI) and further dissociated with the back of a 3-ml syringe. Cell suspension was filtered through 70-\u03bcm strainers. Tumour homogenate was spun down at 400 g for 5 min at 4 \u00b0C. Pellet was resuspended in 15 ml of 3% FCS in HBSS, 500 \u03bcl (500 U) heparin, and 8.5 ml Percoll, mixed by several inversions, and spun at 500 g for 10 min at 4 \u00b0C. Pellet was lysed with ammonium chloride potassium buffer and cells were further processed for downstream applications. Human samples PBMC and tumour samples were obtained from patients with cancer enrolled on a biospecimen procurement protocol approved by the MSKCC Institutional Review Board (IRB). Each patient signed an informed consent form and received a patient information form before participation. Human samples were analysed using an IRB-approved biospecimen utilization protocol. Breast cancer samples were selected from patients who had evidence of a dense mononuclear cell infiltrate on conventional haematoxylin and eosin (H&E) staining. For human ovarian tumour samples (Extended Data Fig. 3 ): tumour samples were obtained as per protocols approved by the IRB. All patients provided informed consent to an IRB-approved correlative research protocol before the collection of tissue (Memorial Sloan Kettering Cancer Center IRB 00144 and 06-107). Human peripheral blood lymphocytes were obtained from the New York Blood Center or from patients where indicated. Human tumours were mechanically disrupted as described for solid mouse tumours, centrifuged on Percoll gradients and further assessed by flow cytometric analysis. FK506 studies Naive TCR TAG (Thy1.1 + ) cells were transferred into AST\u00d7Cre-ER T2 (Thy1.2 + ) mice, which were treated with tamoxifen 1 day later. On days 2\u20138, mice were treated with the calcineurin inhibitor FK506 (Prograf, 5 mg ml \u22121 ) (2.5 mg per kg per mouse intraperitoneally, once daily). Control mice were treated with PBS. All mice were analysed on day 10. TOX overexpression experiments Mouse Tox cDNA (accession number NM_145711.4) without the stop codon fused in-frame with the coding sequence of a monomeric form of green fluorescent protein (mGFP) was obtained from OriGene Technologies (MR208435L2). PCR cloning was used to amplify TOX\u2013mGFP, which was then cloned into the pMIGR1 retroviral vector to generate pMIGR1 TOX\u2013mGFP using the restriction enzymes EcoRI and PacI. pMIGR1 TOX\u2013mGFP and control pMIGR1-GFP containing only mGFP were used for retroviral transduction of TCR TAG CD8 + T cells as follows: on day 1, the retroviral packaging cell line Plat-Eco (Cell Biolabs) was transfected using Effectene (Qiagen) following the manufacturer\u2019s instructions. On day 2, splenocytes from TCR TAG mice were isolated and stimulated with soluble anti-CD3 and anti-CD28 antibodies. On day 3, activated splenocytes were resuspended in the viral supernatant containing 50 U ml \u22121 IL-2 and 5 \u00b5g ml \u22121 Polybrene (Santa-Cruz Biotechnology), transferred to 12-well plates, and spun at 1,000 g for 90 min. This process was repeated the next day. Transduced T cells were cultured for six additional days, replacing media and adding fresh IL-2 (100 U ml \u22121 ) every other day. T cells were collected and flow-sorted for high GFP expression for downstream transcriptome analysis. Intracellular cytokine and transcription factor staining Intracellular cytokine staining was performed using the Foxp3\/Transcription Factor Staining Buffer Set (eBioscience) per manufacturer\u2019s instructions. In brief, T cells were mixed with 2 \u00d7 10 6 congenically marked splenocytes and incubated with TAG epitope I peptide (0.5 \u03bcg ml \u22121 ) or OVA peptide (0.1 \u03bcg ml \u22121 ) for 4\u20135 h at 37 \u00b0C in the presence of GolgiPlug (brefeldin A). Where indicated, naive splenocytes or APCs were activated either in vivo (single intraperitoneal injection of 50 \u00b5g lipopolysaccharide (LPS; Sigma; L2630), 24 h before euthanization) 32 or in vitro (1-h pulse at 37 \u00b0C with 1 \u00b5g ml \u22121 LPS followed by extensive washing) 33 . Where indicated, cells were also stimulated with PMA (20 ng ml \u22121 ) and ionomycin (1 \u03bcg ml \u22121 ) for 4 h. After staining for cell-surface molecules, the cells were fixed, permeabilized and stained with antibodies to IFN\u03b3, TNF and GZMB. Intracellular transcription factor staining was performed using the Foxp3\/Transcription Factor Staining Buffer Set (eBioscience) as per the manufacturer\u2019s instructions. Annexin V staining Apoptosis was assessed by flow cytometry using V450 Annexin V (BD Biosciences; 560506) and 7-AAD following the manufacturer\u2019s instructions. Active caspase-3\/7 analysis For the flow cytometric analysis of active caspase-3\/7, cells were incubated with 500 nM CellEvent Caspase 3\/7 Green Detection Reagent (Invitrogen; C10423) for 30 min at 37 \u00b0C. Chromium release assay Mouse EL4 lymphoma cells were loaded with 150 \u03bcCi of [ 51 Cr]sodium chromate for 2 h. TAG epitope I peptide (SAINNYAQKL) at a concentration of 1 \u03bcg ml \u22121 was added during last 30 min of incubation. 51 Cr-labelled, TAG-I-pulsed EL4 cells were co-cultured with flow-sorted memory TCR TAG T cells or wild-type or knockout TOX TCR TAG T cells isolated and flow-sorted from liver tumours of AST\u00d7Cre mice (6\u20138 days after transfer) at a 5:1 (effector:target) ratio for 16 h. Medium alone or 2% Triton-X was added to set spontaneous or total lysis, respectively. Specific killing was calculated using following formula: percentage lysis = ((test counts per min \u2212 spontaneous counts per min)\/(total counts per min \u2212 spontaneous counts per min))\u00d7100. Sample preparation for ATAC-seq and RNA-seq Replicate samples were isolated from spleens or livers and sorted as follows: (i) naive TCR TAG Thy1.1 + T cells were sorted by flow cytometry (CD8 + \/CD44 lo ) from spleens of TCR TAG Thy1.1 transgenic mice. (ii) Wild-type and knockout TOX TCR TAG T cells were sorted from livers of established AST\u00d7Cre mice 8\u20139 days after transfer. Cells were gated on CD8 + Thy1.1 + PD-1 hi\/lo LAG hi\/lo CD39 hi\/lo . A small aliquot of sorted cell populations was used to confirm TOX expression (for wild-type) and TOX deficiency (for knockout). (iii) TCR OT1 and TCR TAG T cells were sorted from livers of established AST\u00d7Cre mice 20\u201321 days after transfer\/ Listeria infection. After flow-sorting, all samples for downstream ATAC-seq analysis were frozen in 10% FCS in DMSO and stored at \u221280 \u00b0C; samples for RNA-seq were directly sorted into Trizol and frozen and stored at \u221280 \u00b0C. Transcriptome sequencing Samples for RNA-seq were sorted directly into TRIzol LS (Invitrogen). The volume was adjusted to 1 ml with PBS and samples frozen and stored at \u221280 \u00b0C. RNA was extracted using RNeasy mini kit (Qiagen) per instructions provided by the manufacturer. After ribogreen quantification and quality control of Agilent BioAnalyzer, total RNA underwent amplification using the SMART-seq V4 (Clonetech) ultralow input RNA kit for sequencing (12 cycles of amplification for 2\u201310 ng of total RNA). Subsequently, 10 ng of amplified cDNA was used to prepare Illumina Hiseq libraries with the Kapa DNA library preparation chemistry (Kapa Biosystems) using 8 cycles of PCR. Samples were barcoded and run on a Hiseq 4000, in a 50-bp\/50-bp paired-end run, using the TruSeq SBS Kit v3 (Illumina). ATAC-seq Frozen 25,000\u201350,000 cells were thawed and washed in cold PBS and lysed. Transposition was performed at 42 \u00b0C for 45 min. After purification of the DNA with the MinElute PCR purification kit (Qiagen), material was amplified for five cycles. Additional PCR cycles were evaluated by quantitative PCR. Final product was cleaned by Ampure Beads at a 1.5\u00d7 ratio. Libraries were sequenced on a Hiseq 2500 1T in a 50-bp\/50-bp paired-end run, using the TruSeq SBS Kit v.3 (Illumina). Bioinformatics methods The quality of the sequenced reads was assessed with FastQC and QoRTs (for RNA-seq samples (ref. 34 and Babraham Bioinformatics v.0.11.7  (2010)). Unless stated otherwise, all plots involving high-throughput sequencing data were obtained with custom R scripts (see github.com\/friedue\/Scott2019 for the code; R: A Language and Environment for Statistical Computing  (2014); and ref. 35 ). RNA-seq DNA sequencing reads were aligned with default parameters to the mouse reference genome (GRCm38) using STAR 36 . Gene expression estimates were obtained with featureCounts using composite gene models (union of the exons of all transcript isoforms per gene) from Gencode (version M17) 37 , 38 . DEGs DEGs were determined with DESeq2. The q -value cut-offs for the final lists of DEG were as follows: (i) TOX\u2013GFP versus GFP: 849 DEGs with q < 0.10; (ii) TAG versus OT1: 2,347 DEGs with q < 0.05; and (iii) wild-type versus knockout: 679 DEGs with q < 0.05. Pathway and GO term enrichment analyses Gene set enrichment analyses were done using GSEA 39 on RPKM values against a gene set permutation (the seed was set to 149). Heat maps Heat maps were created using log 2 (counts per million) of genes identified as differentially expressed by DESeq2 (adjusted P < 0.05 unless otherwise noted). Rows were centred and scaled using z -scores. ATAC-seq ATAC-seq data 5 were downloaded from GEO (accession GSE89308). These datasets were processed in the same manner as the newly generated datasets described in this study. Alignment and identification of open chromatin regions The data was processed following the recommendations of the ENCODE consortium (The ENCODE Consortium ATAC-seq Data Standards and Prototype Processing Pipeline  ). Reads were aligned to the mouse reference genome (version GRCm38) with BWA-backtrack 40 . Post-alignment filtering was done with samtools and Picard tools to remove unmapped reads, improperly paired reads, non-unique reads, and duplicates (ref. 41 and Broad Institute Picard  (2015)). To identify regions of open chromatin represented by enrichments of reads, peak calling was performed with MACS2 42 . For every replicate, the narrowpeak results of MACS2 were used after filtering for adjusted P < 0.01. Differentially accessible regions Regions where the chromatin accessibility changed between different conditions were identified with diffBind ( DiffBind: Differential Binding Analysis of Chip-Seq Peak Data  (2011)) with the following options: minOverlap=4, bUseSummarizeOverlaps=T, minMembers=2, bFullLibrarySize=TRUE. A total of 12,166 differentially accessible peaks were identified between wild-type and knockout TCR TAG cells (see Fig. 4 ); 19,071 differentially accessible peaks were identified between TCR TAG and TCR OT1 cells (see Fig. 2 ). Coverage files Individual coverage files per replicate normalized for differences in sequencing depths between the different samples were generated with bamCoverage of the deepTools suite 42 using the following parameters: -bs 10 --normalizeUsing RPGC --effectiveGenomeSize 2150570000 --blackListFileName mm10.blacklist --ignoreForNormalization chrX chrY --ignoreDuplicates --minFragmentLength 40 -p 1. To create merged coverage files of replicates of the same condition, we used multiBigwigSummary to obtain the sequencing-depth-normalized coverage values for 10 bp bins along the entire genome, that is, for every condition, we obtained a table with the coverage values in every replicate within the same bin. Subsequently, we chose the mean value for every bin to represent the coverage in the resulting \u2018merged; file (see github.com\/friedue\/Scott2019 for the code that was used). Merged coverage files were used for display in IGV and for heatmaps. Heat maps Heat maps displaying the sequencing-depth-normalized coverage from different ATAC-seq samples were generated with computeMatrix and plotHeatmap of the deepTools suite 43 . Every row corresponds to a single region that was determined to be differentially accessible when comparing either TCR TAG (TAG) to TCR OT1 (OT1) T cells or wild-type to TOX-knockout TCR TAG T cells. The plots display the centre of each differentially accessible peak region \u00b1 1 kb; the colour corresponds to the average normalized coverage across all replicates of the respective condition. Gene labels indicate genes that overlapped with a given differentially accessible region (anywhere along the gene). Combining RNA-seq and ATAC-seq data The relationship between RNA-seq and ATAC-seq was explored via \u2018diamond\u2019 plots for select genes detected as differentially expressed via DESeq2. Each gene was represented by a stack of diamond-shaped points coloured by the associated chromatin state of the gene (blue indicating closing and red indicating opening). The bottom-most point in each stack corresponds to the log 2 -transformed fold change in expression for that gene. NFAT1 ChIP\u2013seq (publicly available) NFAT1 ChIP\u2013seq samples were generated as previously described 21 from cells expressing endogenous NFAT1 (wild type) or lacking NFAT1 (knockout). Cells lacking endogenous NFAT1 were transduced with an empty GFP vector (mock) or with a vector containing a mutated form of NFAT (CA-RIT-RV). Either cell type was either left resting (none) or stimulated with PMA and ionomycin (P + I) for 1 h. We downloaded the sequencing results (fastq files generated by SOLiD sequencing technology) from the Sequence Read Archive (GEO series GSE64407); see Supplementary Table 4 for further details. SOLiD adapters had to be trimmed off, which we did with cutadapt 44 specifying --format=sra-fastq --minimum-length 15 --colorspace and the sample specific adapter sequences via -g and -a (see  for the sample-specific adapters). The trimmed reads were subsequently aligned to the mouse genome version GRCm38 with bowtie1 using the colorspace option 45 . Coverage tracks normalized for differences in sequencing depths were be generated with bamCoverage of the deepTools suite (v.3.1.0) 42 using the following parameters: -bs 10 --normalizeUsing RPGC --effectiveGenomeSize 2150570000 --blackListFileName mm10.blacklist --ignoreForNormalization chrX chrY --ignoreDuplicates --minFragmentLength 40 -p 1. Blacklisted regions were downloaded from  . Regions of statistically significant read enrichments in the ChIP samples compared with the corresponding input samples (peaks) were identified with MACS2 (2.1.1.20160309) 42 using ChIP and corresponding input files and the following parameters: -g 1.87e9 -p 0.01 --keep-dup all. For final peak files, the narrowpeak outputs of MACS2 were used, keeping only peaks with adjusted P values below 0.01. Digital droplet PCR TOX\u2013GFP-overexpressing and GFP-overexpressing TCR TAG T cells were sorted directly into TRIzol (Invitrogen). RNA was extracted with chloroform. Isopropanol and linear acrylamide were added, and the RNA was precipitated with 75% ethanol. Samples were resuspended in RNase-free water. Quantity was assessed by PicoGreen (ThermoFisher) and quality by BioAnalyzer (Agilent). Droplet generation was performed on a QX200 ddPCR system (Bio-Rad; 864001) using cDNA generated from 100 pg total RNA with the One-Step RT-ddPCR Advanced Kit for Probes (Bio-Rad; 1864021) according to the manufacturer\u2019s protocol with reverse transcription at 42 \u00b0C and annealing\/extension at 55 \u00b0C. Each sample was evaluated in technical duplicates. Reactions were partitioned into a median of approximately 30,000 droplets per well. Plates were read and analysed with the QuantaSoft sotware to assess the number of droplets positive for the gene of interest, reference gene ( Gapdh ; dMmuCPE5195283), both, or neither. PrimePCR ddPCR Expression Probe Assays were ordered through Bio-Rad for the following genes of interest: Lag3 (dMmuCPE5122546), Id2 (dMmuCPE5094018), Prdm1 (dMmuCPE5113738), Prf1 (dMmuCPE5112024), and Gzmb (dMmuCPE5093986). Data reporting No statistical methods were used to predetermine sample size. The investigators were not blinded to allocation during experiments and outcome assessment, and experiments were not randomized. Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this paper. Data availability All data generated and supporting the findings of this study are available within the paper. The RNA-seq and ATAC-seq data have been deposited in the Gene Expression Omnibus (GEO) under accession number GSE126974 . Source Data are provided with the online version of the paper. Additional information and materials will be made available upon request. ","News_Body":"Immune checkpoint therapy has revolutionized cancer therapy, leading to long-term remission for patients with advanced cancer. However, most cancer patients either do not respond or have only short-term responses to checkpoint therapy, which targets inhibitory receptors on T cells. A study published June 17 in Nature offers clues as to why blocking inhibitory receptors on tumor-infiltrating T cells may not always work. Mary Philip, MD, Ph.D., assistant professor of Medicine in the Division of Hematology and Oncology and a senior author on the story, together with Andrea Schietinger, Ph.D., of the Sloan Kettering Institute, found that the thymocyte selection-associated high-mobility group box protein, TOX, is expressed at high levels in dysfunctional tumor-infiltrating T cells in mice and humans. The investigators found that TOX controls the high expression of inhibitory receptors such as PD1 on dysfunctional tumor-infiltrating T cells. These inhibitory receptors act like brakes on T cells. The team deleted TOX from tumor-infiltrating T cells to see if that would restore their function. To their surprise, though the tumor-infiltrating T cells no longer expressed PD1 and other inhibitory receptors, the T cells were still dysfunctional and unable to eliminate cancers. Even more surprising, the T cells without TOX were unable to survive long term. The study demonstrates that control of the killing machinery in T cells is uncoupled from regulation of inhibitory receptors. \"Taking off the brakes is not enough to restore the killing capacity of anti-tumor cells. In fact, T cells need the brakes to avoid getting over-activated and dying,\" Philip said. The study follows a previous investigation published in Nature on May 25, 2018, by Philip and colleagues on T cell dysfunction in liver cancer using mouse models. Philip was the lead author of that study. The overarching goal of Philip's research group is to decipher the mechanisms regulating T cell dysfunction in cancers and to design new strategies to override these mechanisms to improve cancer immunotherapy. ","News_Title":"Study identifies critical regulator of tumor-specific T cell differentiation","Topic":"Medicine"}
